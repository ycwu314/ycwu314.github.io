<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[反应式编程系列 4：调度器]]></title>
    <url>%2Fp%2Freactive-p5-schedulers%2F</url>
    <content type="text"><![CDATA[reactive 是生产者 - 消费者模式，支持但是不强制多线程环境运行。Scheduler 抽象 Scheduler 是 reactive 的调度器，屏蔽了底层多线程交互的复杂性。Disposable 代表一个任务或者资源可以取消或者丢弃。Scheduler本质是一个线程池，其实现类通常使用 ExecutorService 或者 ScheduledExecutorService。 Worker 代表一个异步执行的任务。通常使用 Schedulers 工具类，类似 Executors，提供了创建 Scheduler 的快捷方法：a) 没有执行上下文(Schedulers.immediate()): 在处理时，将直接执行提交的 Runnable，从而在当前线程上有效地运行它们（可以视为“空对象”或无操作调度程序）。b) 单个可重用线程(Schedulers.single())。请注意，此方法对所有调用方都使用相同的线程，直到调度程序被释放为止。如果您需要每次调用一个专用线程，请对每个调用使用 Schedulers.newSingle()。c) 无限制的弹性线程池(Schedulers.elastic())。随着 Schedulers.boundedElastic() 的引入，Schedulers.boundedElastic()不再是首选方法，因为它倾向于隐藏背压问题并导致线程过多（请参见下文）。d) 有界弹性线程池 (Schedulers.boundedElastic())。像其前身 elastic() 一样，它根据需要创建新的工作池并重用空闲的工作池。闲置时间过长（默认值为 60s）的工作池也将被丢弃。与其前身的 elastic()有所不同，它对可以创建的线程数进行限制（默认为 CPU 核心数 x 10）。达到上限后，最多可再提交 10 万个任务，并在有线程可用时重新调度（当任务被设置延迟执行时，延迟计时是在线程可用时开始）。这是 I/O 阻塞任务的更好选择。Schedulers.boundedElastic()是一种为阻塞处理提供自己的线程的简便方法，这样它就不会占用其他资源。e) 为并行工作而调整的固定工作线程池 (Schedulers.parallel())。它创建的工作线程数量与 CPU 内核数量一样多。publishOn 和 subscribeOn 方法reactive stream 的publishOn 和subscribeOn方法可以切换执行 operator 的线程。不指定 Scheduler默认情况下，publish 和 subscribe 都使用主线程。12345678910@Testpublic void testPubSubByDefault() &#123; Flux.defer(() -&gt; &#123; log.info("defer thread &#123;&#125;", Thread.currentThread().getName()); return Flux.range(1, 3); &#125;).map(i -&gt; &#123; log.info("data(&#123;&#125;) map thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;).subscribe(i -&gt; log.info("data(&#123;&#125;) subscribe thread &#123;&#125;", i, Thread.currentThread().getName()));&#125;123456716:45:30.706 [main] INFO com.example.reactive.FluxMonoTest - defer thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(1) map thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(1) subscribe thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(2) map thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(2) subscribe thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(3) map thread main16:45:30.709 [main] INFO com.example.reactive.FluxMonoTest - data(3) subscribe thread main只有 publishOnRun onNext, onComplete and onError on a supplied Scheduler Worker.This operator influences the threading context where the rest of the operators in the chain below it will execute, up to a new occurrence of publishOn.Typically used for fast publisher, slow consumer(s) scenarios.publishOn影响在其之后的 operator 执行的线程池。1234567891011121314151617181920@Testpublic void testPublishOnOnly() &#123; Scheduler s1 = Schedulers.newParallel("s1", 2); Scheduler s2 = Schedulers.newParallel("s2", 2); Flux.defer(() -&gt; &#123; log.info("defer thread &#123;&#125;", Thread.currentThread().getName()); return Flux.range(1, 3); &#125;).publishOn(s1) .map(i -&gt; &#123; log.info("data(&#123;&#125;) map v1 thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;) .map(i -&gt; &#123; log.info("data(&#123;&#125;) map v2 thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;).publishOn(s2).filter(i -&gt; &#123; log.info("data(&#123;&#125;) map v3 thread &#123;&#125;", i, Thread.currentThread().getName()); return i &gt; 2; &#125;).subscribe(i -&gt; log.info("data(&#123;&#125;) subscribe thread &#123;&#125;", i, Thread.currentThread().getName()));&#125;123456789101117:24:02.439 [main] INFO com.example.reactive.FluxMonoTest - defer thread main17:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(1) map v1 thread s1-217:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(1) map v2 thread s1-217:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(2) map v1 thread s1-217:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(2) map v2 thread s1-217:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(3) map v1 thread s1-217:24:02.449 [s1-2] INFO com.example.reactive.FluxMonoTest - data(3) map v2 thread s1-217:24:02.449 [s2-1] INFO com.example.reactive.FluxMonoTest - data(1) map v3 thread s2-117:24:02.449 [s2-1] INFO com.example.reactive.FluxMonoTest - data(2) map v3 thread s2-117:24:02.449 [s2-1] INFO com.example.reactive.FluxMonoTest - data(3) map v3 thread s2-117:24:02.449 [s2-1] INFO com.example.reactive.FluxMonoTest - data(3) subscribe thread s2-1最初是在 main 线程执行 defer 函数 第一次 publishOn，切换到自定义的 s1 线程池，并且影响后续第一、第二个 map 操作 第二次 publishOn，切换到自定义的 s2 线程池，并且影响后续第三个 map 操作 subscribe 使用了 publishOn 的线程池subscribeOnsubscribeOn 会从源头影响整个执行过程。Run subscribe, onSubscribe and request on a specified Scheduler’s Scheduler.Worker. As such, placing this operator anywhere in the chain will also impact the execution context of onNext/onError/onComplete signals from the beginning of the chain up to the next occurrence of a publishOn.Typically used for slow publisher e.g., blocking IO, fast consumer(s) scenarios.123456789Scheduler s1 = Schedulers.newParallel("s1", 2);Flux.defer(() -&gt; &#123; log.info("defer thread &#123;&#125;", Thread.currentThread().getName()); return Flux.range(1, 5);&#125;).map(i -&gt; &#123; log.info("data(&#123;&#125;) map v1 thread &#123;&#125;", i, Thread.currentThread().getName()); return i;&#125;).subscribeOn(s1) .subscribe(i -&gt; log.info("data(&#123;&#125;) subscribe thread &#123;&#125;", i, Thread.currentThread().getName()));123456789101117:59:55.644 [s1-1] INFO com.example.reactive.FluxMonoTest - defer thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(1) map v1 thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(1) subscribe thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(2) map v1 thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(2) subscribe thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(3) map v1 thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(3) subscribe thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(4) map v1 thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(4) subscribe thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(5) map v1 thread s1-117:59:55.649 [s1-1] INFO com.example.reactive.FluxMonoTest - data(5) subscribe thread s1-1subscribeOn虽然在 defer、map 等操作后面，但是影响了使用的线程池。混合使用 publishOn 和 subscribeOn12345678910111213141516171819202122@Testpublic void testPubSub() &#123; Scheduler s1 = Schedulers.newParallel("s1", 2); Scheduler s2 = Schedulers.newParallel("s2", 2); Flux.defer(() -&gt; &#123; log.info("defer thread &#123;&#125;", Thread.currentThread().getName()); return Flux.range(1, 3); &#125;).map(i -&gt; &#123; log.info("data(&#123;&#125;) map v1 thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;).subscribeOn(s1) .map(i -&gt; &#123; log.info("data(&#123;&#125;) map v2 thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;) .publishOn(s2) .map(i -&gt; &#123; log.info("data(&#123;&#125;) map v3 thread &#123;&#125;", i, Thread.currentThread().getName()); return i; &#125;) .subscribe(i -&gt; log.info("data(&#123;&#125;) subscribe thread &#123;&#125;", i, Thread.currentThread().getName()));&#125;1234567891011121318:11:11.058 [s1-2] INFO com.example.reactive.FluxMonoTest - defer thread s1-218:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(1) map v1 thread s1-218:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(1) map v2 thread s1-218:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(2) map v1 thread s1-218:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(2) map v2 thread s1-218:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(3) map v1 thread s1-218:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(1) map v3 thread s2-118:11:11.062 [s1-2] INFO com.example.reactive.FluxMonoTest - data(3) map v2 thread s1-218:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(1) subscribe thread s2-118:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(2) map v3 thread s2-118:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(2) subscribe thread s2-118:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(3) map v3 thread s2-118:11:11.062 [s2-1] INFO com.example.reactive.FluxMonoTest - data(3) subscribe thread s2-1subscribeOn 使得 defer、第 1 个、第 2 个 map 使用了线程池 s1publishOn 之后，第三个 map 以及 subscribe 使用了线程池 s2内置线程池的函数 和时间相关的函数通常使用额外的线程池处理（Schedulers.parallel()），不受 publishOn 和subscribeOn约束。123public final Flux&lt;T&gt; delayElements(Duration delay) &#123; return delayElements(delay, Schedulers.parallel());&#125;小结 reactive 可以在单线程或者多线程模式运行。Sheduler 是 reactive 中的调度器，本质是线程池，屏闭了多线程交互的复杂性。publishOn 影响在其之后的 operator 执行的线程池（直到下一个 publishOn 调用）。而 subscribeOn 则会从源头影响整个执行过程。publishOn 适用于快生产者、慢消费者。subscribeOn 适用于慢生产者、快消费者（例如阻塞 IO）。 在大多数情况下，下一个 operator 直接使用上一个 operator 线程执行任务。部分和时间相关的函数使用独立线程池。参考Threading and Schedulers]]></content>
      <categories>
        <category>反应式编程</category>
      </categories>
      <tags>
        <tag>reactive</tag>
        <tag>反应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反应式编程系列 4：backpressure]]></title>
    <url>%2Fp%2Freactive-p4-backpressure%2F</url>
    <content type="text"><![CDATA[backpressure 是一种现象：生产速率高于消费速率，消费者通知生产者降低产生信号速率。backpressure 这种协调机制对于维持系统稳定具有重要作用。生产速率和消费速率不匹配是一个普遍存在的现象。严重者会产生大量信号积压、请求 SLA 得不到满足、甚至压垮消费者。通常有几种策略处理：缓冲。消费者缓冲多余请求。然而缓冲队列会消耗资源，因此不应该是无界的。另外，在缓冲队列中的信号，得不到及时处理，SLA 得不到保障。流控。消费者通知生产者，降低生产速率。丢弃。缓冲队列大小是有限的，一旦填满，就面临流控或者丢弃。backpressure 是对应流控的一个方式。在了解 backpressure 之前，先要了解“冷信号与热信号”。冷信号与热信号 .NET 框架 Reactive Extensions(RX) 提出了 Hot Observable 和 Cold Observable 的概念：Hot Observable 是主动的，尽管你并没有订阅事件，但是它会时刻推送，就像鼠标移动；而 Cold Observable 是被动的，只有当你订阅的时候，它才会发布消息。Hot Observable 可以有多个订阅者，是一对多，集合可以与订阅者共享信息；而 Cold Observable 只能一对一，当有不同的订阅者，消息是重新完整发送。 任何的信号转换即是对原有的信号进行订阅从而产生新的信号。冷信号类似于拉取模型，通常都是接收到请求后才生成信号，所以一般不存在背压的问题（如网络请求等）。而热信号则是会主动产生数据（推送模型，不管消费者是否请求，如鼠标移动事件等），当热信号产生的速度远大于订阅者消费的速度，就会产生不平衡，过多的热信号会挤压，这时就需要一种背压策略来解决这个问题。流控方式 常见的流控方式：Callstack blocking 调用链阻塞 backpressure 背压 / 反压 调用链阻塞的一个例子是，java 线程池的拒绝策略。当缓冲队列满了、切拒绝策略为 caller_run，那么生产者将不能向线程池提交任务。另一种流控方法是 backpressure。消费者主动通知生产者减少发送数据。例如 tcp 协议的接收窗口。backpressurebackpressure，也称为 Reactive Pull，下游根据自己接收窗口的情况来控制接收速率，并通过反向的 request 请求控制上游的发送速率。拉模式实现的反压比较简单。生产者和消费者通过 Subscription(订阅)关联。消费者通过 Subscription#request(n) 向生产者请求 n 个消息。生产者通过 onNext() 向消费者发送消息。1234567891011121314151617181920212223242526272829303132333435363738394041Flux.range(1, 21).log().subscribe(new Subscriber&lt;Integer&gt;() &#123; private int threshold = 5; private int fetchSize = 3; private Subscription subscription; private int processedCount = 0; @Override public void onSubscribe(Subscription subscription) &#123; this.subscription = subscription; this.subscription.request(fetchSize); &#125; @Override public void onNext(Integer obj) &#123; processedCount++; if (processedCount &gt; threshold) &#123; System.out.println("*** slow down for a while..."); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; processedCount = 0; subscription.request(1); &#125; else &#123; subscription.request(fetchSize); &#125; &#125; @Override public void onError(Throwable throwable) &#123; &#125; @Override public void onComplete() &#123; &#125;&#125;);输出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474817:16:54.954 [main] INFO reactor.Flux.Range.1 - | onSubscribe([Synchronous Fuseable] FluxRange.RangeSubscription)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | onNext(1)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | onNext(2)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | onNext(3)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.959 [main] INFO reactor.Flux.Range.1 - | onNext(4)17:16:54.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.960 [main] INFO reactor.Flux.Range.1 - | onNext(5)17:16:54.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:54.960 [main] INFO reactor.Flux.Range.1 - | onNext(6)*** slow down for a while...17:16:57.960 [main] INFO reactor.Flux.Range.1 - | request(1)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | onNext(7)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | onNext(8)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | onNext(9)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | onNext(10)17:16:57.960 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:57.961 [main] INFO reactor.Flux.Range.1 - | onNext(11)17:16:57.961 [main] INFO reactor.Flux.Range.1 - | request(3)17:16:57.961 [main] INFO reactor.Flux.Range.1 - | onNext(12)*** slow down for a while...17:17:00.961 [main] INFO reactor.Flux.Range.1 - | request(1)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | onNext(13)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | onNext(14)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | onNext(15)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | onNext(16)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:00.961 [main] INFO reactor.Flux.Range.1 - | onNext(17)17:17:00.962 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:00.962 [main] INFO reactor.Flux.Range.1 - | onNext(18)*** slow down for a while...17:17:03.963 [main] INFO reactor.Flux.Range.1 - | request(1)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | onNext(19)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | onNext(20)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | onNext(21)17:17:03.963 [main] INFO reactor.Flux.Range.1 - | request(3)17:17:03.964 [main] INFO reactor.Flux.Range.1 - | onComplete() 生产者端支持的背压策略：onBackpressureBufferonBackpressureDroponBackpressureErroronBackpressureLatest小结 因为现实场景是不可预估的，生产速度总是有一定的可能会大于下游消费的速度，所以 Buffer 是永远需要的。Buffer 要使用有界队列。无界 buffer 会消耗大量内存，导致生产者不稳定。参考 如何形象的描述反应式编程中的背压 (Backpressure) 机制？背压 (Back Pressure) 与流量控制 细说 ReactiveCocoa 的冷信号与热信号（二）：为什么要区分冷热信号]]></content>
      <categories>
        <category>反应式编程</category>
      </categories>
      <tags>
        <tag>reactive</tag>
        <tag>反应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反应式编程系列 3：使用 StepVerifier 进行单元测试]]></title>
    <url>%2Fp%2Freactive-p3-StepVerifier%2F</url>
    <content type="text"><![CDATA[流是动态数据，如何做单元测试？StepVerifier 通过订阅流，然后消费其产生的信号，逐个比较元素、错误信号、完成信号等，从而解决单元测试问题。基本用法 先发射 1，然后发射 2，最后产生 error。123456789101112@Testpublic void testStepVerifier() &#123; Flux a = Flux.just(1, 2, 3, 4).concatWith(Mono.error(new RuntimeException("err"))); // 使用 create 方法创建 StepVerifier.create(a) .expectNext(1) .expectNext(2) // 支持一次检查多个 value .expectNext(3, 4) .expectError(RuntimeException.class) .verify();&#125; 期望流正常结束 12345Flux a = Flux.just(1, 2, 3, 4);StepVerifier.create(a) .expectNext(1, 2, 3, 4) .expectComplete() .verify(); 判断元素是否符合一些特性 1234567Flux a = Flux.just("a", "aa", "a2323");StepVerifier.create(a) .expectNextMatches(i -&gt; ((String) i).startsWith("a")) .expectNextMatches(i -&gt; ((String) i).startsWith("a")) .expectNextMatches(i -&gt; ((String) i).startsWith("a")) .expectComplete() .verify(); 如果流的元素很多，逐个 expectNext() 显然不合适；使用 thenConsumeWhile() 替代：123456int max = 1000;Flux a = Flux.range(1, max);StepVerifier.create(a) .thenConsumeWhile(o -&gt; ((Integer) o) &lt;= max) .expectComplete() .verify();处理时间问题 流的数据是动态的，可能间隔一段时间才到来，怎么加快单元测试的执行速度？StepVerifier 使用 VirtualTimeScheduler 来解决上面问题：Prepare a new StepVerifier in a controlled environment using VirtualTimeScheduler to manipulate a virtual clock via StepVerifier.Step.thenAwait.The scheduler is injected into all Schedulers factories,which means that any operator created within the lambda without a specific scheduler will use virtual time.12345678910111213StepVerifier.withVirtualTime(() -&gt; Flux.range(1, 10).delayElements(Duration.ofMinutes(1))) // 触发订阅 .expectSubscription() // 触发时间飞逝 .thenAwait(Duration.ofSeconds(30)) .expectNextCount(0) .thenAwait(Duration.ofSeconds(10)) // 期望一段时间内没有事件 .expectNoEvent(Duration.ofSeconds(10)) .thenAwait(Duration.ofMinutes(100)) .expectNextCount(10) .expectComplete() .verify(); 参考Reactor 的测试——响应式 Spring 的道法术器]]></content>
      <categories>
        <category>反应式编程</category>
      </categories>
      <tags>
        <tag>reactive</tag>
        <tag>反应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载在 jar 文件中的资源]]></title>
    <url>%2Fp%2Fjava-load-file-inside-jar%2F</url>
    <content type="text"><![CDATA[访问 jar 内部文件，使用 getResourceAsStream() 而非 getResource()。 重构了每日统计邮件代码，把原来硬编码在 java 代码的 html 模板，改写为 mustache 模板，并且放置在 resources 目录。src\main\resources\stat\star_daily_stat_mail.html本地 idea 启动验证功能正常，但是打包 jar 文件后执行异常：1java.io.FileNotFoundException: file:/home/ubuntu/devops/deploy/prod/medical/medical.jar!/BOOT-INF/classes!/stat/star_daily_stat_mail.html (No such file or directory)读取文件，使用了 Class.java 的getResource123456789public java.net.URL getResource(String name) &#123; name = resolveName(name); ClassLoader cl = getClassLoader0(); if (cl==null) &#123; // A system class. return ClassLoader.getSystemResource(name); &#125; return cl.getResource(name);&#125;还有一个功能相近的 getResourceAsStream123456789 public InputStream getResourceAsStream(String name) &#123; name = resolveName(name); ClassLoader cl = getClassLoader0(); if (cl==null) &#123; // A system class. return ClassLoader.getSystemResourceAsStream(name); &#125; return cl.getResourceAsStream(name);&#125; 原来代码使用了 getResource() 获取 URL，再构造 File 对象读取文件内容。但是对于 jar 文件，URL 地址不能被 File 识别。因此要使用流的方式 (getResourceAsStream()) 访问 jar 内部的文件。小结：getResource 返回 URL 对象，由应用转换为 File。但是在 jar 环境不能成功。getResourceAsStream 在 jar 环境 / 非 jar 环境都能识别。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反应式编程系列 2： Flux 和 Mono]]></title>
    <url>%2Fp%2Freactive-p2-flux-mono%2F</url>
    <content type="text"><![CDATA[在反应式编程中，最核心的组件是发布者 Publisher 和订阅者 Subscriber。今天介绍的是 Publisher 中的 Flux 和 Mono。Flux 和 Mono 简介 Publisher&lt;T&gt; 是一个可以提供 0-N 个序列元素的提供者，并根据其订阅者 Subscriber&lt;? super T&gt; 的需求推送元素。发布者发布元素信号（onNext）、完成信号（onComplete）、错误信号（onError）。订阅者接受这 3 种信号，并进行消费。其中完成信号、错误信号都会终止流。在 reactor 里，Flux 和 Mono 都是 publisher。Flux 是一个发出 (emit) 0-N 个元素组成的异步序列的 Publisher，可以被 onComplete 信号或者 onError 信号所终止。Mono 是一个发出(emit)0-1 个元素的 Publisher，可以被 onComplete 信号或者 onError 信号所终止。 流的基本用法 使用 webflux 作为演示环境。12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt;&lt;/dependency&gt;首先要创建数据流。123456789101112131415161718// 接受 0..N 个输入 Flux.just(1, 2, 3).subscribe(System.out::println);// 范围，start、countFlux.range(1, 5).subscribe(System.out::println);// 从数组创建Flux.fromArray(new String[]&#123;"a", "b", "c"&#125;).subscribe(System.out::println);// 从 stream 创建Flux.fromStream(Arrays.asList(1, 2, 3).stream()).subscribe(System.out::println);// 从集合创建Flux.fromIterable(Arrays.asList(1, 2, 3)).subscribe(System.out::println);// 空元素Flux.empty().doOnComplete(() -&gt; System.out.println("complete on empty")).subscribe();Flux.just(); 可以从一个流创建另一个流：1234Flux a = Flux.just(1, 2, 3);Flux b = Flux.from(a);a.subscribe(i -&gt; System.out.println("a:" + i));b.subscribe(i -&gt; System.out.println("b:" + i));甚至可以从 CompletableFuture 创建 (仅限于 Mono)：123CompletableFuture&lt;Integer&gt; f = new CompletableFuture&lt;&gt;();Mono.fromFuture(f).subscribe(System.out::println);f.complete(999); 发布者也支持创建错误流：12// 错误流 Flux.error(new RuntimeException("err happened")).subscribe(); 上面提到发布者提供了信号支持，对应 doOnXXX 方法。12// 完成信号 Flux.empty().doOnComplete(() -&gt; System.out.println("complete on empty")).subscribe(); 流可以合并：12345678Flux.just(1, 2, 3) .concatWith(Mono.error(new RuntimeException())) .onErrorReturn(999) .concatWith(Flux.just(4)) .subscribe( System.out::println, System.err::println, () -&gt; System.out.println("Completed!"));创建流之后，可以使用中间转换函数，例如 filter、map、flatMap、then、zip、reduce 等（和 java 8 stream 相似）。最后使用 subscribe 方法触发订阅。流的高级操作 createcreate 可以自定义创建元素的方式。1234567// 输出 20 以内的奇数Flux.create(emitter -&gt; &#123; for (int i = 1; i &lt; 20; i += 2) &#123; emitter.next(i); &#125; emitter.complete();&#125;).subscribe(System.out::println);defer 函数defer 是懒初始化，每次 subscribe 的时候都会调用 supplier 获取 publisher 实例。 如果 Supplier 每次返回的实例不同，则可以构造出和 subscribe 次数相关的 Flux 源数据流。如果每次都返回相同的实例，则和 from(Publisher&lt;? extends T&gt; source)效果一样。1234567891011121314151617@Testpublic void testDefer() &#123; Flux a = Flux.just(new Date()); Flux b = Flux.defer(() -&gt; Flux.just(new Date())); a.subscribe(i -&gt; System.out.println("a\t" + i)); b.subscribe(i -&gt; System.out.println("b\t" + i)); try &#123; Thread.sleep(3000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; a.subscribe(i -&gt; System.out.println("a\t" + i)); b.subscribe(i -&gt; System.out.println("b\t" + i));&#125;1234a Wed Dec 23 17:09:32 CST 2020b Wed Dec 23 17:09:32 CST 2020a Wed Dec 23 17:09:32 CST 2020b Wed Dec 23 17:09:35 CST 2020两次订阅 a 流，数据一致。第二次订阅 b 流，又执行 new Date() 得到元素，因此两次订阅数值不一样。buffer缓冲 value，打包到一个 List，再发射。1234567Flux.range(1, 17).buffer(5).subscribe(System.out::println);// 输出 [1, 2, 3, 4, 5][6, 7, 8, 9, 10][11, 12, 13, 14, 15][16, 17]interval 以一定时间间隔发射元素。1234567// 从 0 开始递增的 Long 对象的序列 Flux.interval(Duration.ofMillis(100L)).subscribe(System.out::println);try &#123; Thread.sleep(1000);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;share 共享流，实现多播 123456789101112131415161718192021// share： // Returns: a Flux that upon first subscribe causes the source Flux to subscribe once, late subscribers might therefore miss items.Flux a = Flux.interval(Duration.ofMillis(200)).share();Flux b = Flux.from(a);b.subscribe(i -&gt; System.out.println("b:" + i));try &#123; Thread.sleep(1000L);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;// 注意：在开始新的共享流之前的信号都丢失了// 这里 c 的输出丢失了 0 ~ 4Flux c = Flux.from(a);c.subscribe(i -&gt; System.out.println("c:" + i));try &#123; Thread.sleep(1000L);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 输出 123456789101112131415b: 0b: 1b: 2b: 3b: 4b: 5c: 5b: 6c: 6b: 7c: 7b: 8c: 8b: 9c: 9cache 将此 Flux 转换为热源，并为进一步的用户缓存最后发射的信号。将保留一个无限量的 OnNeXT 信号。完成和错误也将被重放。改造上面 share 的例子 12345678910111213141516171819// 增加调用 cache()Flux a = Flux.interval(Duration.ofMillis(200)).share().cache();Flux b = Flux.from(a);b.subscribe(i -&gt; System.out.println("b:" + i));try &#123; Thread.sleep(1000L);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;// 注意：在开始新的共享流之前的信号都丢失了// 这里 c 的输出丢失了 0 ~ 4Flux c = Flux.from(a);c.subscribe(i -&gt; System.out.println("c:" + i));try &#123; Thread.sleep(1000L);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 输出 1234567891011121314151617181920b: 0b: 1b: 2b: 3b: 4c: 0c: 1c: 2c: 3c: 4b: 5c: 5b: 6c: 6b: 7c: 7b: 8c: 8b: 9c: 9 参考资料：FLUX CACHING IN PROJECT REACTOR: REPLAYING PAST DATA使用 cache 可以支持高并发场景。collect把 Flux 流转为 Mono。12Mono&lt;List&lt;Integer&gt;&gt; a = Flux.just(1, 2, 3).collectList();a.subscribe(System.out::println);distinct对于每一个 Subscriber，跟踪已经从这个 Flux 跟踪元素和过滤出重复。值本身被记录到一个用于检测的哈希集中。如果希望使用 distinct(Object::hashcode)更轻量级的方法，该方法不保留所有对象，但是更容易由于 hashcode 冲突而错误地认为两个元素是不同的。1Flux.just(1, 1, 2, 3, 4, 5, 2).distinct().subscribe(System.out::println);doOnNext在发射元素之前执行动作 1Flux.just(1, 2, 3).doOnNext(i -&gt; System.out.println(&quot;***&quot;)).subscribe(System.out::println); 输出 123456***1***2***3 流的高级操作 2map, flatMap, thenmap 和 flatMap 是常见的中间转换函数。12345678910Flux.just(1, 2, 3).map(i -&gt; i * i).subscribe(System.out::println);System.out.println("*****");Flux.just(1, 2, 3).flatMap(i -&gt; Flux.just(i * i).delayElements(Duration.ofSeconds(1))) .subscribe(System.out::println);try &#123; Thread.sleep(5000);&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;map 函数是同步转换元素 1map(Function&lt;? super T, ? extends V&gt; mapper)flatMap 函数是异步转换元素，因此 Function 必须是 Publisher 类型1flatMap(Function&lt;? super T, ? extends Publisher&lt;? extends R&gt;&gt; mapper) 另一个常见的操作是 then1Flux.just(1, 2, 3).then(Mono.just(999)).subscribe(System.out::println);输出 1999then 并不是等待上一个流的操作，而是直接丢弃，这和一般语义上的 then 操作不一样。zipWithzip 原意是“拉链”。zipWith 操作可以合并两个流，结果为 Tuple2。因为合并，只对位置相同的元素操作，多余元素被丢掉。123Flux a = Flux.just(1, 2, 3, 4, 5, 6, 7);Flux b = Flux.just("a", "b", "c");a.zipWith(b).subscribe(System.out::println); 输出 123[1,a][2,b][3,c]expand, expandDeep 基于一个递归的生成序列的规则扩展每一个元素，然后合并为一个序列发出：广度优先：expand(Function)深度优先：expandDeep(Function)用于遍历树形结构是很方便的。假设树的结构如下 123456A - AA - aa1B - BB - bb1 先定义节点 123456@Data@AllArgsConstructorstatic class Node &#123; private String id; private Node next;&#125; 然后初始化这棵树 123456Node aa1 = new Node("aa1", null);Node aa = new Node("AA", aa1);Node a = new Node("A", aa);Node bb1 = new Node("bb1", null);Node bb = new Node("BB", bb1);Node b = new Node("B", bb); 广度优先 1234567Flux.just(a, b).expand(i -&gt; &#123; if (i.next != null) &#123; return Mono.just(i.next); &#125; else &#123; return Mono.empty(); &#125;&#125;).subscribe(i -&gt; System.out.println(i.getId())); 输出 123456ABAABBaa1bb1 深度优先 1234567Flux.just(a, b).expandDeep(i -&gt; &#123; if (i.next != null) &#123; return Mono.just(i.next); &#125; else &#123; return Mono.empty(); &#125;&#125;).subscribe(i -&gt; System.out.println(i.getId())); 输出 123456AAAaa1BBBbb1 辅助功能 观察所有 stream 的信号，并且输出到日志 1Flux.just(1,2,3).log().subscribe(System.out::println); 输出 12345678916:52:29.605 [main] INFO reactor.Flux.Array.1 - | onSubscribe([Synchronous Fuseable] FluxArray.ArraySubscription)16:52:29.613 [main] INFO reactor.Flux.Array.1 - | request(unbounded)16:52:29.613 [main] INFO reactor.Flux.Array.1 - | onNext(1)116:52:29.613 [main] INFO reactor.Flux.Array.1 - | onNext(2)216:52:29.613 [main] INFO reactor.Flux.Array.1 - | onNext(3)316:52:29.614 [main] INFO reactor.Flux.Array.1 - | onComplete() 参考Spring Reactor 入门与实践]]></content>
      <categories>
        <category>反应式编程</category>
      </categories>
      <tags>
        <tag>reactive</tag>
        <tag>反应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反应式编程系列 1：简介]]></title>
    <url>%2Fp%2Freactive-p1-intro%2F</url>
    <content type="text"><![CDATA[project reactor 官网（Introduction to Reactive Programming）的笔记整理。whatwiki 百科对于反应式编程的描述：Reactive programming is an asynchronous programming paradigm concerned with data streams and the propagation of change. This means that it becomes possible to express static (e.g. arrays) or dynamic (e.g. event emitters) data streams with ease via the employed programming language(s).反应式编程 (reactive programming) 是一种基于数据流 (data stream) 和 变化传递 (propagation of change) 的声明式 (declarative) 的编程范式。划重点：异步编程 数据流 变化传递 反应式编程通常使用 Observer 设计模式实现。reactive streams 模式和 Iterator 设计模式类似，但是 Iterator 是 pull-based，reactive streams 是 push-based。和反应式编程相对应的是命令式（imperative）编程。以 Iterator 为例，在命令式编程，开发者明确指令何时从序列中读取数据，以及在精确的控制流中如何处理数据。在反应式编程中，与之对应的是 Publisher-Subscriber。Publisher 通知 Subscriber 新产生的值。“push”这个动作是“reactive”的关键。另外，计算逻辑以声明式（declarative）方式描述（TODO）。除了 push，反应式编程还提供异常处理、完成通知等 aspect。error-handling 和 completion 都会终止流。why阻塞式编程方式容易编写代码，但是停等（stop-wait）的工作方式很浪费性能。要提升程序性能：并发，使用更多线程、更多硬件资源 提高当前资源的使用效率 hmm，异步、非阻塞代码又是有挑战的。By writing asynchronous, non-blocking code, you let the execution switch to another active task that uses the same underlying resources and later comes back to the current process when the asynchronous processing has finished.JVM 提供 2 种异步模型：Callbacks： 不会返回 value，但是要求一个额外的 callback 参数（lambda 或者匿名类）。Futures： 立即返回Future&lt;T&gt;。但是 value 不是立即可用，要等计算结束才能访问。Callback 很难组合，很容易导致 callback 地狱。12345678910111213141516171819202122232425262728293031323334353637userService.getFavorites(userId, new Callback&lt;List&lt;String&gt;&gt;() &#123; public void onSuccess(List&lt;String&gt; list) &#123; if (list.isEmpty()) &#123; suggestionService.getSuggestions(new Callback&lt;List&lt;Favorite&gt;&gt;() &#123; public void onSuccess(List&lt;Favorite&gt; list) &#123; UiUtils.submitOnUiThread(() -&gt; &#123; list.stream() .limit(5) .forEach(uiList::show); &#125;); &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125; &#125;); &#125; else &#123; list.stream() .limit(5) .forEach(favId -&gt; favoriteService.getDetails(favId, new Callback&lt;Favorite&gt;() &#123; public void onSuccess(Favorite details) &#123; UiUtils.submitOnUiThread(() -&gt; uiList.show(details)); &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125; &#125; )); &#125; &#125; public void onError(Throwable error) &#123; UiUtils.errorPopup(error); &#125;&#125;); 上面例子，使用 reactor 改写的反应式代码：123456userService.getFavorites(userId) .flatMap(favoriteService::getDetails) .switchIfEmpty(suggestionService.getSuggestions()) .take(5) .publishOn(UiUtils.uiThreadScheduler()) .subscribe(uiList::show, UiUtils::errorPopup);Future 要比 callback 好一点，但是不能很好完成组合（虽然 java8 引入了 CompletableFuture）。1234567891011121314151617181920212223242526CompletableFuture&lt;List&lt;String&gt;&gt; ids = ifhIds(); CompletableFuture&lt;List&lt;String&gt;&gt; result = ids.thenComposeAsync(l -&gt; &#123; Stream&lt;CompletableFuture&lt;String&gt;&gt; zip = l.stream().map(i -&gt; &#123; CompletableFuture&lt;String&gt; nameTask = ifhName(i); CompletableFuture&lt;Integer&gt; statTask = ifhStat(i); return nameTask.thenCombineAsync(statTask, (name, stat) -&gt; "Name" + name + "has stats" + stat); &#125;); List&lt;CompletableFuture&lt;String&gt;&gt; combinationList = zip.collect(Collectors.toList()); CompletableFuture&lt;String&gt;[] combinationArray = combinationList.toArray(new CompletableFuture[combinationList.size()]); CompletableFuture&lt;Void&gt; allDone = CompletableFuture.allOf(combinationArray); return allDone.thenApply(v -&gt; combinationList.stream() .map(CompletableFuture::join) .collect(Collectors.toList()));&#125;);List&lt;String&gt; results = result.join(); assertThat(results).contains( "Name NameJoe has stats 103", "Name NameBart has stats 104", "Name NameHenry has stats 105", "Name NameNicole has stats 106", "Name NameABSLAJNFOAJNFOANFANSF has stats 121");使用 reactor 改写:123456789101112131415161718192021Flux&lt;String&gt; ids = ifhrIds(); Flux&lt;String&gt; combinations = ids.flatMap(id -&gt; &#123; Mono&lt;String&gt; nameTask = ifhrName(id); Mono&lt;Integer&gt; statTask = ifhrStat(id); return nameTask.zipWith(statTask, (name, stat) -&gt; "Name" + name + "has stats" + stat); &#125;);Mono&lt;List&lt;String&gt;&gt; result = combinations.collectList(); List&lt;String&gt; results = result.block(); assertThat(results).containsExactly( "Name NameJoe has stats 103", "Name NameBart has stats 104", "Name NameHenry has stats 105", "Name NameNicole has stats 106", "Name NameABSLAJNFOAJNFOANFANSF has stats 121");从命令式转变为反应式，有以下好处：组合性（composability）和可读性增强 把数组作为流（Data as a flow），并且使用一系列丰富的操作符 Nothing happens until you subscribe 背压（Backpressure）机制，消费者通知生产者发送速率过高 高层抽象，屏蔽了底层并发的复杂性 “composability”是指编排多个异步任务，并且使用前面异步任务的结果作为后面任务的输入。By “composability”, we mean the ability to orchestrate multiple asynchronous tasks, in which we use results from previous tasks to feed input to subsequent ones. Alternatively, we can run several tasks in a fork-join style 后续文章再深入讨论。]]></content>
      <categories>
        <category>反应式编程</category>
      </categories>
      <tags>
        <tag>reactive</tag>
        <tag>反应式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx gzip mime-type 配置]]></title>
    <url>%2Fp%2Fnginx-gzip-mimetypes%2F</url>
    <content type="text"><![CDATA[调试小程序接口，发现个别接口大小 20+KB，有点奇怪。一查发现是服务器没有正确配置压缩。http header 和压缩 accept-encoding 表示可以接收的编码类型（request header）。content-encoding表示当前发送内容的编码类型（response header）。请求 header：1accept-encoding: gzip, deflate, br响应 header：1content-encoding: gzipnginx 配置 nginx.conf 配置了 gzip on，但是没有gzip_types，默认只对text/html 压缩。12345Syntax: gzip_types mime-type ...;Default: gzip_types text/html;Context: http, server, locationEnables gzipping of responses for the specified MIME types in addition to “text/html”. The special value “*” matches any MIME type (0.8.29). Responses with the “text/html” type are always compressed.于是增加：1gzip_types application/javascript text/css application/json application/xml text/xml text/plainspringboot 配置 也可以在 springboot 的 application 文件配置：12server.compression.enabled=trueserver.compression.mime-types=application/javascript,text/css,application/json,application/xml,text/html,text/xml,text/plain]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saml 简介]]></title>
    <url>%2Fp%2Fsaml-intro%2F</url>
    <content type="text"><![CDATA[CAS SSO 使用了 SAML 协议，做下笔记。what is安全断言标记语言（英语：Security Assertion Markup Language，简称 SAML）。SAML 是 OASIS 安全服务技术委员会的一个产品。它在当事方之间交换身份验证和授权数据，尤其是在身份提供者和服务提供者之间交换。了解 SAML 可以看官方的技术规范：Security Assertion Markup Language (SAML) 2.0 Technical Overviewsaml 角色和流程 saml 三个核心角色：SP（service provider），服务提供者IDP（identity provider），认证用户并生成断言client，用户。 三者的典型交互流程如下：saml 组件和报文 saml 最核心组件是 assertion：XML-formatted tokens that are used to transfer user identity information, such as the authentication, attribute, and entitlement information, in the messages. 请求报文和响应报文 demo 如下。请求主要包含谁来请求验证。响应主要包含是否验证成功。因为 SAML 是基于 XML 的（通常比较长），完整认证请求消息 / 响应消息要经过压缩（为 Url 节省空间）和编码（防止特殊字符）才能传输。saml 使用场景 官方提供了 2 类使用场景：SSO联合身份认证。FederationSSOAs CarRentalInc.com trusts AirlineInc.com it knows that the user is validand creates a session for the user based on the user’s name and/or the user attributes.FederationFederation 是指一个用户在不同 service provider 以不同的用户名注册，然后以某种方式，把这个两个账号指向同一个人。This use case illustrates the “account linking” facet of federation.The same user is registered on both sites, however using different names.On CarRentalInc.com he is registered as jdoe and on HotelBookings.com as johnd.Account Linking enables a pseudonym to be established that links the two accountssaml 和云服务的实践 云服务可以提供基于 SAML 2.0 联合身份验证，实现企业内部账号和云服务提供商的互通。以腾讯云为例 留意 IDP 应该和具体的用户账号数据分离（比如 LDAP、database 等）。具体参照：基于 SAML 2.0 联合身份验证]]></content>
      <categories>
        <category>saml</category>
      </categories>
      <tags>
        <tag>saml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenID Connect 简介]]></title>
    <url>%2Fp%2Fopenid-connect-intro%2F</url>
    <content type="text"><![CDATA[what isOAuth2 提供了 Access Token 来解决授权第三方客户端访问受保护资源的问题。OpenID Connect(缩写为 OIDC)在这个基础上提供了 ID Token 来解决第三方客户端标识用户身份认证的问题。OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It allows Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.基本术语和流程 OpenID Connect 有很多组件，最核心的是 core。OpenID Connect 定义了几个术语：EU：End User：一个人类用户。RP：Relying Party , 用来代指 OAuth2 中的受信任的客户端，身份认证和授权信息的消费方；OP：OpenID Provider，有能力提供 EU 认证的服务（比如 OAuth2 中的授权服务），用来为 RP 提供 EU 的身份认证信息；ID Token：JWT 格式的数据，包含 EU 身份认证的信息。UserInfo Endpoint：用户信息接口（受 OAuth2 保护），当 RP 使用 Access Token 访问时，返回授权用户的信息OpenID Connect 流程：The RP (Client) sends a request to the OpenID Provider (OP).The OP authenticates the End-User and obtains authorization.The OP responds with an ID Token and usually an Access Token.The RP can send a request with the Access Token to the UserInfo Endpoint.The UserInfo Endpoint returns Claims about the End-User.123456789101112131415161718+--------+ +--------+| | | || |---------(1) AuthN Request--------&gt;| || | | || | +--------+ | || | | | | || | | End- |&lt;--(2) AuthN &amp; AuthZ--&gt;| || | | User | | || RP | | | | OP || | +--------+ | || | | || |&lt;--------(3) AuthN Response--------| || | | || |---------(4) UserInfo Request-----&gt;| || | | || |&lt;--------(5) UserInfo Response-----| || | | |+--------+ +--------+ 其中 AuthN=Authentication，表示认证；AuthZ=Authorization，代表授权。OpenID Connect 和 Oauth 2 的结合点在：AuthN 的时候，scope 必须包含 openid，这样就可以表明这是一个 OIDC 的请求。AuthZ 的返回，包含 ID token。结合一个例子来看就更清晰了：ps：OIDC 官方定义的一些 scope：ID tokenOpenID Connect 在 Oauth 2 上的核心扩展是 ID token，是一个授权服务器提供的包含用户信息（由一组 Cliams 构成以及其他辅助的 Cliams）的 JWT 格式的数据结构。ID Token 的主要构成部分：iss = Issuer Identifier：必须。提供认证信息者的唯一标识。一般是一个 https 的 url（不包含 querystring 和 fragment 部分）。sub = Subject Identifier：必须。iss 提供的 EU 的标识，在 iss 范围内唯一。它会被 RP 用来标识唯一的用户。最长为 255 个 ASCII 个字符。aud = Audience(s)：必须。标识 ID Token 的受众。必须包含 OAuth2 的 client_id。exp = Expiration time：必须。过期时间，超过此时间的 ID Token 会作废不再被验证通过。iat = Issued At Time：必须。JWT 的构建的时间。auth_time = AuthenticationTime：EU 完成认证的时间。如果 RP 发送 AuthN 请求的时候携带 max_age 的参数，则此 Claim 是必须的。nonce：RP 发送请求的时候提供的随机字符串，用来减缓重放攻击，也可以来关联 ID Token 和 RP 本身的 Session 信息。acr = Authentication Context Class Reference：可选。表示一个认证上下文引用值，可以用来标识认证上下文类。amr = Authentication Methods References：可选。表示一组认证方法。azp = Authorized party：可选。结合 aud 使用。只有在被认证的一方和受众（aud）不一致时才使用此值，一般情况下很少使用。OpenID Connect 还定义了一堆官方的 claims：StandardClaims。ID Token 必须使用 JWS 进行签名和 JWE 加密。ID token 是 OpenID Connect 的精髓所在：stateless session。因此后端服务可以更加灵活的扩容。向第三方传递身份。这个 id 可以传递给第三方应用，或者后端服务，用于识别身份。token 交换。ID token 也可以用来交换 access token。OpenID Connect 验证流程 OIDC 基于 Oauth 2，支持 3 种认证方式： 授权码模式 简化模式 混合模式 hybrid前两种和 Oauth 2 基本一致。这个就不展开了。混合模式允许直接向客户端返回 Access Token。因为直接返回，考虑到安全性，token 有效时间尽量短。在实现中，一个常见的改进是加入验证阶段 promt 字段。以微软的 OIDC 实现为例：Microsoft 标识平台和 OpenID Connect 协议 prompt 表示需要的用户交互类型。 此时唯一有效值为 login、none 和 consent。prompt=login 声明将强制用户在该请求上输入凭据，从而取消单一登录。 而 prompt=none 声明截然相反。 此声明确保用户不会看到任何交互式提示。 如果请求无法通过单一登录静默完成，Microsoft 标识平台终结点就会返回错误。prompt=consent 声明会在用户登录后触发 OAuth 同意对话框。 该对话框要求用户向应用授予权限。OpenID Connect 和 SSO之前研究了 CAS 的 SSO，OpenID Connect 也是可以做 SSO 的。目前正处于 draft 状态。单点登录 目前定义了针对移动设备的 SSO 流程：12345678910111213141516171819202122232425262728293031+----------+ +----------+ +-----------+ +------------+| Native | | Native | | System | | || App | | App | | Browser | | AS || #1 | | #2 | | | | |+----+-----+ +----+-----+ +-----+-----+ +-----+------+ | | | | | [1] Start OIDC AuthN | | +----------------+----------------&gt; | | | | | [2] /authorize | | | +----------------&gt; | | | | | | | | [3] authenticate | | | &lt;----------------| | | | | | | | [4] user creds | | | +----------------&gt; | | | | | | | | [5] callback | | | | &lt;----------------+ | [6] callback with code | | | &lt;--------------+------------------+ | | | | | | [7] exchange code for tokens | | +----------------+-----------------------------------&gt; | | | | | | [8] tokens (including device_secret) | | &lt;--------------+------------------+------------------+ | | | | | | | | | | | | + + + +这里新增了 device_secret 以及 device_sso（图中没有标记）。In the context of this extension the device secret may be shared between mobile apps that can obtain the value via the shared security mechanism (e.g. keychain on iOS). If a mobile app requests a device secret via the device_sso scope and a device_secret exists, then the client MUST provide the device_secret on the request to the /token endpoint to exchange code for tokens. 而当另一个 app 访问时，触发 token 交换和刷新 1234567891011121314151617+----------+ +----------+ +-----------+ +------------+| Native | | Native | | System | | || App | | App | | Browser | | AS || #1 | | #2 | | | | |+----+-----+ +----+-----+ +-----+-----+ +-----+------+ | | | | | | | | | | [9] token exchange | | +------------------+----------------&gt; | | | | | | | | | | | [10] refresh, access, [device_secret] | | &lt;----------------+------------------| | | | | | | | | | | | | + + + + 其中定义了新的 grant_type：urn:ietf:params:oauth:grant-type:token-exchange。 （Oauth 2 协议的好处是可以利用 grant_type 来自定义流程）参见：OpenID Connect Native SSO for Mobile Apps 1.0 - draft 03单点登出 和 CAS 类似，提供了 front channel 和 back channel 两种类型。front 方式依赖客户端向各个服务发送 logout 请求。back 方式由 OIDC server 负责向各个服务发送 logout 请求。back 方式相比 front 更加可靠。参见：OpenID Connect Front-Channel Logout 1.0 - draft 04OpenID Connect Back-Channel Logout 1.0 - draft 06OpenID Connect 小结 OAuth2 提供了 Access Token 来解决授权第三方客户端访问受保护资源的问题OIDC 在这个基础上提供了 ID Token 来解决第三方客户端标识用户身份认证的问题 基于 Oauth 2 的身份层 在 Oauth 2 请求中加入 scope=openidOauth 2 授权响应增加 ID tokenID token 是 JWT，并且使用 JWE 或者 JWS 保护ID token 包含了基本的用户信息，比如用户 idOIDC 可以扩展支持 SSO 资料Welcome to OpenID Connect]]></content>
      <categories>
        <category>OpenID Connect</category>
      </categories>
      <tags>
        <tag>OpenID Connect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理 zombie 进程]]></title>
    <url>%2Fp%2Flinux-kill-zombie-process%2F</url>
    <content type="text"><![CDATA[登录 vps 后提示有僵尸进程，顺手处理掉。123456System load: 0.03 Processes: 120Usage of /: 59.4% of 49.15GB Users logged in: 1Memory usage: 61% IP address for eth0: 172.16.0.2Swap usage: 40%=&gt; There is 1 zombie process.查找僵尸进程：123ubuntu@VM-0-2-ubuntu:~$ ps -ef | grep defunctubuntu 964 442 0 17:35 pts/1 00:00:00 grep --color=auto defunctnobody 6392 1478 0 03:11 ? 00:00:00 [mvn] &lt;defunct&gt;或者 12ubuntu@VM-0-2-ubuntu:~$ ps -A -o stat,ppid,pid,cmd | grep -e &apos;^[Zz]&apos; Z 1478 6392 [mvn] &lt;defunct&gt; 检查僵尸进程 1234ubuntu@VM-0-2-ubuntu:~$ top -p 6392 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6392 nobody 20 0 0 0 0 Z 0.0 0.0 0:00.00 mvn 注意 state 为 Z。顺便看下父进程状态 1234ubuntu@VM-0-2-ubuntu:~$ top -p 1478 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1478 nobody 20 0 4628 376 372 T 0.0 0.0 0:00.36 mvn 注意 state 为 T。Linux 进程状态：T (TASK_STOPPED or TASK_TRACED)，暂停状态或跟踪状态。清理僵尸进程方法是 kill -9 如果不行，则 kill 父进程]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes ingress 简介]]></title>
    <url>%2Fp%2Fkubernetes-ingress-intro%2F</url>
    <content type="text"><![CDATA[k8s 对外暴露服务方式 k8s 集群向外部暴露服务的方式有三种： NodePort，LoadBalancer 和 Ingress。NodePort 方式在服务变多的情况下会导致节点要开的端口越来越多，不好管理。LoadBalancer 更适合结合云提供商的 LB 来使用。但是云供应商的 LB 是要收费的，要考虑成本问题。Ingress 是 k8s 官方提供的用于对外暴露服务的方式。一般在云环境下是 LB + Ingress Controller 方式对外提供服务。Service 虽然解决了服务发现和负载均衡的问题，但对外访问的时候，NodePort 类型需要在外部搭建额外的负载均衡， 而 LoadBalancer 需要 Kubernetes 必须跑在支持的 cloud provider 上面。Ingress 就是为了解决这些限制的。ingress12345 internet |[Ingress]--|-----|--[Services]通常最外层流量先去到云厂商的 LoadBalancer，然后路由到 ingress controller。再贴一张 istio 服务网格下的对比。Ingress 一般会有三个组件:反向代理负载均衡器 IngressIngress Controller 反向代理负载均衡器 例如 nginx。IngressIngress 就是为进入集群的请求提供路由规则的集合。Ingress 可以给 service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 Ingress controller，它监听 Ingress 和 service 的变化，并根据规则配置负载均衡并提供访问入口。Ingress ControllerIngress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。一个 ingress 的例子 123456789101112131415161718apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: name-virtual-host-ingressspec: rules: - host: foo.bar.com http: paths: - backend: serviceName: service1 servicePort: 80 - host: bar.foo.com http: paths: - backend: serviceName: service2 servicePort: 80 常见的 ingress controllerKubernetes Ingress Controller实现：Go/Lua（nginx 是用 C 写的）这是社区开发的控制器，它基于 nginx Web 服务器，并补充了一组用于实现额外功能的 Lua 插件。NGINX Ingress Controller实现：GoNGINX 的控制器具有很高的稳定性、持续的向后兼容性，且没有任何第三方模块。Kong IngressKong Ingress 的一个重要特性是它只能在一个环境中运行（而不支持跨命名空间）。这是一个颇有争议的话题：有些人认为这是一个缺点，因为必须为每个环境生成实例；而另一些人认为这是一个特殊特性，因为它是更高级别的隔离，控制器故障的影响仅限于其所在的环境。Traefik最初，这个代理是为微服务请求及其动态环境的路由而创建的，因此具有许多有用的功能：连续更新配置（不重新启动）、支持多种负载均衡算法、Web UI、指标导出、对各种服务的支持协议、REST API、Canary 版本等。为了控制器的高可用性，你必须安装并连接其 Key-value store。Istio Ingress它是一个全面的服务网格解决方案——不仅可以管理所有传入的外部流量（作为 Ingress 控制器），还可以控制集群内部的所有流量。Istio 将 Envoy 用作每种服务的辅助代理。参考Ingressk8s 服务暴露之 ingress 与负载均衡K8s 工程师必懂的 10 种 Ingress 控制器]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth2 系列 3：安全讨论]]></title>
    <url>%2Fp%2Foauth2-p3-security%2F</url>
    <content type="text"><![CDATA[oauth2 协议的安全考虑。token 类型 oauth2 支持 2 种 token_type：Bearer 类型和 MAC 类型。bearerBearer Token (RFC 6750) 用于 OAuth 2.0 授权访问资源，任何 Bearer 持有者都可以无差别地用它来访问相关的资源，而无需证明持有加密 key。Bearer 实现资源请求有三种方式：Authorization Header、Form-Encoded Body Parameter、URI Query Parameter，这三种方式优先级依次递减。 因为 Bearer token 能够直接使用，为了提高安全性，应该使用 https 协议，防止中间人嗅探、修改、重放。使用 Bearer token 不需要对请求计算哈希。macmac token 除了携带授权服务器下发的 token，客户端还要携带时间戳，nonce，以及在客户端计算得到的 mac 值等信息，并通过这些额外的信息来保证传输的可靠性。因此 client 每次请求都要计算哈希，并且 server 用相同参数检验哈希是否正确。client 通过添加 id、ts、nonce、mac 字段到 Authorization 请求首部以发起对用户资源的请求。server 收到请求后：重新计算 mac 值，并与客户端传递的值进行比较 确保 (timestam, nonce, token) 三个维度之前没有被请求过，以防止重放攻击 验证 scope，以及 token参考资料：draft-hammer-oauth-v2-mac-token-05redirect_uri 和 codeoauth2 的授权码模式、简化模式都使用了 redirect_uri，回调到请求者的服务器。授权码模式请求：1GET http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth?response_type=code&amp;state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;client_id=product-app&amp;scope=openid&amp;redirect_uri=https%3A%2F%2Foauth.pstmn.io%2Fv1%2Fcallback HTTP/1.1简化模式请求：1GET http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth?nonce=4c9bdcf0-37dc-47bc-82a4-36a877d4d462&amp;response_type=token&amp;state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;client_id=product-app&amp;scope=openid&amp;redirect_uri=https%3A%2F%2Foauth.pstmn.io%2Fv1%2Fcallback攻击：如果访问 oauth2 服务器不是使用 https 协议，那么攻击者很容易使用中间人攻击，篡改 redirect_uri，指向恶意网站。应对方案：oauth2 访问优先使用 https 协议 oauth2 的实现就已经考虑到非 https 情况，在申请 client 的时候，指定合法的 redirect_uri 路径；并且在 oauth2 server 对 redirect_uri 进行校验。ps. 一个小技巧。在外部 oauth2 服务提供商配置了外网域名作为 redirect_uri，为了方便本地调试，修改本地 hosts 文件。 攻击：记录 code，然后重放获取 access token应对方案：oauth2 rfc 就考虑到这个情况。code 是一次性使用的。statestate 参数可以为空，但是官方推荐传入。在 oauth2 流程中，state 参数回由 oauth2 server 在 redirect_uri 中原样返回。开发者可以用这个参数验证请求有效性，也可以记录用户请求授权页前的位置。这个参数可用于防止跨站请求伪造（CSRF）攻击。对 oauth2 登录实施 CSRF 攻击条件比较苛刻，可以参见：小议 OAuth 2.0 的 state 参数]]></content>
      <categories>
        <category>oauth2</category>
      </categories>
      <tags>
        <tag>oauth2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth2 系列 2：keycloak 入门和 4 种模式抓包分析]]></title>
    <url>%2Fp%2Foauth2-p2-keycloak-and-analyze%2F</url>
    <content type="text"><![CDATA[上一篇文章简单介绍了 oauth2 的 4 种模式。深入理解的最好方式抓包分析。这里先准备 oauth 2 authorization server，选择开源的 keycloak。然后介绍 postman 工具，方便发送 oauth2 请求。最后对 4 种 oauth2 模式进行抓包分析。keycloak 准备 去官网下载 keycloak 最新版本，解压缩之后使用 standalone.bat 运行单机版本。默认的访问地址是 localhost:8080/auth。 登录之后创建管理员账号。 接下来要创建 realm、client、client scope、role、user。具体参照这篇文章即可：springboot 整合 keycloak。需要注意的是 client 配置。client protocol：有 openid connect 和 saml 两种，一般选择 openid connnect。access type: 如何触发登录流程。有 3 个选项：confidential：需要提供 secret 才能触发登录流程。public：不需要 secret。bearer-only：不会触发登录流程，例如两个 web service 之间的通信。当 access type 为 confidential，才会打开 Credential 选项页面。里面可以配置 secret。confidential 类型可以开启全部 4 种 oauth2 模式。public 模式不支持 client credential 模式。validate redirect urls：浏览器成功登录或者登出之后允许重定向的 url。支持通配符，支持多个。这里配置了 postman 的 url，用于后续测试。springboot 应用集成 keycloak 客户端 同样参照了这篇文章：springboot 整合 keycloak。application.properties配置大概如下，其中被 public-client 折腾一下：123456789101112# keycloak auth 服务器 keycloak.auth-server-url=http://localhost:8080/authkeycloak.realm=SpringBoot# clientIDkeycloak.resource=product-appkeycloak.public-client=true# if public client, then no need to set keycloak.credentials.secret# keycloak.credentials.secret=81feaf6b-4544-4cc9-b05e-ae501f918a78keycloak.securityConstraints[0].authRoles[0]=userkeycloak.securityConstraints[0].securityCollections[0].name= common userkeycloak.securityConstraints[0].securityCollections[0].patterns[0]=/products/*realm 里面 client 的 access type：public：则配置keycloak.public-client=true（默认为 false）confidential：则要配置keycloak.credentials.secret，内容为 keycloak -&gt; clients -&gt; Credential 页面的 secrect。 如果 keycloak client 配置为 confidential，但是没有在客户端应用配置 keycloak.credentials.secret，访问受保护资源就会提示报错：123452020-09-09 22:06:11.807 INFO 19256 --- [nio-8081-exec-1] o.keycloak.adapters.KeycloakDeployment : Loaded URLs from http://localhost:8080/auth/realms/SpringBoot/.well-known/openid-configuration2020-09-09 22:06:13.842 WARN 19256 --- [nio-8081-exec-2] a.a.ClientIdAndSecretCredentialsProvider : Client &apos;product-app&apos; doesn&apos;t have secret available2020-09-09 22:06:13.852 ERROR 19256 --- [nio-8081-exec-2] o.k.adapters.OAuthRequestAuthenticator : failed to turn code into token2020-09-09 22:06:13.852 ERROR 19256 --- [nio-8081-exec-2] o.k.adapters.OAuthRequestAuthenticator : status from server: 4002020-09-09 22:06:13.852 ERROR 19256 --- [nio-8081-exec-2] o.k.adapters.OAuthRequestAuthenticator : &#123;&quot;error&quot;:&quot;unauthorized_client&quot;,&quot;error_description&quot;:&quot;INVALID_CREDENTIALS: Invalid client credentials&quot;&#125;keycloak 的 rest 端点 为了得到 4 种模式的抓包，先要知道 keycloak 的 rest 端口。参照官方文档，一个 realm 提供的配置信息可以在这里查询 1http://&lt;keycloak server:port&gt;/auth/realms/&lt;realm&gt;/.well-known/openid-configuration123456789101112131415161718192021222324252627282930313233343536373839404142&#123; "issuer": "http://localhost:8080/auth/realms/SpringBoot", "authorization_endpoint": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth", "token_endpoint": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token", "introspection_endpoint": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token/introspect", "userinfo_endpoint": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/userinfo", "end_session_endpoint": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/logout", "jwks_uri": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/certs", "check_session_iframe": "http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/login-status-iframe.html", "grant_types_supported": [ "authorization_code", "implicit", "refresh_token", "password", "client_credentials" ], "response_types_supported": [ "code", "none", "id_token", "token", "id_token token", "code id_token", "code token", "code id_token token" ], // 省略一堆 "response_modes_supported": [ "query", "fragment", "form_post" ], "registration_endpoint": "http://localhost:8080/auth/realms/SpringBoot/clients-registrations/openid-connect", "token_endpoint_auth_methods_supported": [ "private_key_jwt", "client_secret_basic", "client_secret_post", "tls_client_auth", "client_secret_jwt" ], // 省略一堆&#125;authorization_endpoint 是授权入口。token_endpoint 是 token 入口，传入 authorization code，兑换成 access token。 使用 postman 操作 oauth2 流程 postman 可以很方便的操作 oauth2 流程。 grant type 可以选择不同模式。 其中 authorization code 和 implicit 模式需要提供 callback url。选择 authorize using browser，则 postman 可以自动提取 authorization code 或者 access token。因此要在 keycloak client 配置中把 postman 回调地址加进去。如果遇到问题，可以打开 postman console 查看请求和响应。路径是 View -&gt; Show postman console。授权码模式分析 （A）用户访问客户端，后者将前者导向认证服务器。（B）用户选择是否给予客户端授权。（C）假设用户给予授权，认证服务器将用户导向客户端事先指定的”重定向 URI”（redirection URI），同时附上一个授权码。（D）客户端收到授权码，附上早先的”重定向 URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。（E）认证服务器核对了授权码和重定向 URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。 向认证服务器发送请求 1GET http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth?response_type=code&amp;state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;client_id=product-app&amp;scope=openid&amp;redirect_uri=https%3A%2F%2Foauth.pstmn.io%2Fv1%2Fcallback HTTP/1.1query string 参数: 字段说明：response_type：表示授权类型，必选项，此处的值固定为”code”client_id：表示客户端的 ID，必选项 redirect_uri：表示重定向 URI，可选项scope：表示申请的权限范围，可选项state：表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值。 认证服务器响应，要求重定向到请求时指定的 redirect_uri12HTTP/1.1 302 FoundLocation: https://oauth.pstmn.io/v1/callback?state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;session_state=6ccd0eeb-3396-4af8-878e-98cc1de409c3&amp;code=8a4a4295-bc4e-4ece-8dae-6b687ede60e3.6ccd0eeb-3396-4af8-878e-98cc1de409c3.7ec3571e-47d2-410f-997f-f9cc3c207ebe字段说明：code：表示授权码，必选项。该码的有效期应该很短，通常设为 10 分钟，客户端只能使用该码一次，否则会被授权服务器拒绝。该码与客户端 ID 和重定向 URI，是一一对应关系。state：如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。用 authorization code 去交换 access token12345POST http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token HTTP/1.1Content-Type: application/x-www-form-urlencodedAuthorization: Basic cHJvZHVjdC1hcHA6ODFmZWFmNmItNDU0NC00Y2M5LWIwNWUtYWU1MDFmOTE4YTc4grant_type=authorization_code&amp;code=8a4a4295-bc4e-4ece-8dae-6b687ede60e3.6ccd0eeb-3396-4af8-878e-98cc1de409c3.7ec3571e-47d2-410f-997f-f9cc3c207ebe&amp;redirect_uri=https%3A%2F%2Foauth.pstmn.io%2Fv1%2Fcallback&amp;client_id=product-app字段说明：grant_type：表示使用的授权模式，必选项，此处的值固定为”authorization_code”。code：表示上一步获得的授权码，必选项。redirect_uri：表示重定向 URI，必选项，且必须与 A 步骤中的该参数值保持一致。client_id：表示客户端 ID，必选项。响应是一个 json：字段说明：access_token：表示访问令牌，必选项。token_type：表示令牌类型，该值大小写不敏感，必选项，可以是 bearer 类型或 mac 类型。expires_in：表示过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间。refresh_token：表示更新令牌，用来获取下一次的访问令牌，可选项。scope：表示权限范围，如果与客户端申请的范围一致，此项可省略。简化模式 简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了”授权码”这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。简化模式又叫隐式模式。keyloak 在简化模式需要提供 nonce 字段 ，避免重放攻击，否则请求报错：1210:53:39,937 ERROR [org.keycloak.services] (default task-1) KC-SERVICES0092: Missing parameter: nonce10:53:39,938 WARN [org.keycloak.events] (default task-1) type=LOGIN_ERROR, realmId=spring, clientId=product-app, userId=null, ipAddress=127.0.0.1, error=invalid_request, response_type=token, redirect_uri=https://oauth.pstmn.io/v1/callback, response_mode=fragment 因此修改 postman 的 auth url。guid 是 postman 提供的 guid 风格的随机函数。1http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth?nonce=&#123;&#123;$guid&#125;&#125;向认证服务器发送请求 1GET http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/auth?nonce=4c9bdcf0-37dc-47bc-82a4-36a877d4d462&amp;response_type=token&amp;state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;client_id=product-app&amp;scope=openid&amp;redirect_uri=https%3A%2F%2Foauth.pstmn.io%2Fv1%2Fcallback12HTTP/1.1 302 FoundLocation: https://oauth.pstmn.io/v1/callback#state=b85d54ce-8aba-4abf-8d35-79df50fdc3da&amp;session_state=d518ff7b-0020-4e69-aee9-c7ea4f22f461&amp;access_token=eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJKb2ZERWR2VWI4Uzk3VEV0bnM2LUQ3N1cxVzdGM0lnZHVhN1pLYmZxaWp3In0.eyJleHAiOjE1OTk3MDk0ODYsImlhdCI6MTU5OTcwODU4NiwiYXV0aF90aW1lIjoxNTk5NzA4MjQyLCJqdGkiOiI5YTNmMzBhNS00OWI1LTQ2ODQtOTc4My1iYjZhMmRjNGU3M2IiLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgwODAvYXV0aC9yZWFsbXMvU3ByaW5nQm9vdCIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiIwNzhkOTg4Yy0xMmI5LTRhZDgtYTA2YS1hZDExYWEzNTM3MzMiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJwcm9kdWN0LWFwcCIsIm5vbmNlIjoiNGM5YmRjZjAtMzdkYy00N2JjLTgyYTQtMzZhODc3ZDRkNDYyIiwic2Vzc2lvbl9zdGF0ZSI6ImQ1MThmZjdiLTAwMjAtNGU2OS1hZWU5LWM3ZWE0ZjIyZjQ2MSIsImFjciI6IjAiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsInVzZXIiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIiwiZW1haWxfdmVyaWZpZWQiOmZhbHNlLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0ZXN0dXNlciJ9.dTXMmVsdaHDuMH6oCyUpCIahZIYWdxYpv7txcU8ZMx90F8zJe17bTuamhZLyA_-X2iLfLaDtGrygHUBSitn5zyQq9luLQzxRKiyFrXDLg8YeH5jTBjgHdRiEp1aqChRKgsRs2la8sWyT1dnHeYuBXw5Exz0nA18EYFEI25j1QT40DSLPT9vc8OUfSbJQd9sxdLa_2J9GF5shWajQCeQJ45Ky-5RCCKbuwRidMCQHNWv1zhPFODOJpSW4T80-AmrtnHA-DDqorLsFrHOB8DuTjQo8SoCDBz9wq1Q0uT27ANqfs-Kt3CCFG2Y2NB70672tic45Bggt-Zi9gZ04NmLi3A&amp;token_type=bearer&amp;expires_in=900 响应直接返回了 access_token。从响应数据看，简化模式不能刷新 token！留意这里 url 使用了 #（hash，#fragment 部分），而非 ?。 下一步浏览器会访问 Location 指定的网址，但是 Hash 部分不会发送。接下来，服务提供商的资源服务器发送过来的代码，会提取出 Hash 中的令牌。密码模式 用户向客户端提供自己的用户名和密码。客户端使用这些信息，向”服务商提供商”索要授权。认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。向认证服务器发送请求 12345POST http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token HTTP/1.1Content-Type: application/x-www-form-urlencodedAuthorization: Basic cHJvZHVjdC1hcHA6ODFmZWFmNmItNDU0NC00Y2M5LWIwNWUtYWU1MDFmOTE4YTc4grant_type=password&amp;username=testuser&amp;password=123456&amp;scope=openid 请求头使用 Authorization 认证。使用 base64 解密之后 1product-app:81feaf6b-4544-4cc9-b05e-ae501f918a78product-app 是要访问的 client id，后面是 client secret。 字段说明：grant_type：表示授权类型，此处的值固定为”password”，必选项。username：表示用户名，必选项。password：表示用户的密码，必选项。scope：表示权限范围，可选项。响应是 json客户端模式 客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向”服务提供商”进行认证。请求 12345POST http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token HTTP/1.1Content-Type: application/x-www-form-urlencodedAuthorization: Basic cHJvZHVjdC1hcHA6ODFmZWFmNmItNDU0NC00Y2M5LWIwNWUtYWU1MDFmOTE4YTc4grant_type=client_credentials&amp;scope=openid 请求头使用 Authorization 认证。字段说明：grant_type：表示授权类型，此处的值固定为”clientcredentials”，必选项。scope：表示权限范围，可选项。响应为 json，不贴图了。刷新 tokenrefresh token 的初衷主要是为了用户体验不想用户重复输入账号密码来换取新 token，因而设计了 refresh token 用于换取新 token。在 refresh_token 过期之前，都可以使用 token 端点获取新的 access_token。发送请求 1234POST http://localhost:8080/auth/realms/SpringBoot/protocol/openid-connect/token HTTP/1.1Content-Type: application/x-www-form-urlencodedgrant_type=refresh_token&amp;refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJkZmMzOTNjOS01NjhjLTQ4OGUtODgyNy1iMGI4YTcyMThjOGIifQ.eyJleHAiOjE1OTk3MjE0NDIsImlhdCI6MTU5OTcxOTY0MiwianRpIjoiYzhkZWU5ZGUtMjYyOC00ODUxLWEzZDAtNDQ4NWU2N2Y3ZTMwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL1NwcmluZ0Jvb3QiLCJhdWQiOiJodHRwOi8vbG9jYWxob3N0OjgwODAvYXV0aC9yZWFsbXMvU3ByaW5nQm9vdCIsInN1YiI6IjA3OGQ5ODhjLTEyYjktNGFkOC1hMDZhLWFkMTFhYTM1MzczMyIsInR5cCI6IlJlZnJlc2giLCJhenAiOiJwcm9kdWN0LWFwcCIsIm5vbmNlIjoiN2NlNmMzNzAtMzdhOC00YjE1LWEyYTAtMThlYzc1ZmFmMjMzIiwic2Vzc2lvbl9zdGF0ZSI6IjFmMzk5NjQ2LWU5ZDMtNDM1OC1hNGM0LTFiNmQwMWVkYTY3MiIsInNjb3BlIjoib3BlbmlkIHByb2ZpbGUgZW1haWwifQ.xBJSxJEcGkTPRmKS5dKU7Ov8bPmuVoP5_ff4ISYhU5w&amp;client_id=product-app&amp;client_secret=81feaf6b-4544-4cc9-b05e-ae501f918a78 响应为 json。四种授权模式的对比 图片来源： https://blog.oauth.io/introduction-oauth2-flow-diagrams/授权码模式 先获取 code，再用 code 去换 access token。安全性高。市面上大多数第三方 app 都使用。支持 refresh_token。简化模式 不需要后端服务，直接在前端获取 token。没有 code，在 redirect_uri 直接返回 token。因为直接返回 token，所以安全性变低，token 的生命周期要尽量短。比如单页应用。不支持 refresh_token。密码模式 client 可能保存了用户密码，安全性低。 适合遗留项目升级。支持 refresh_token。客户端凭证模式 不需要用户参与，适合纯属两个服务之间的互操作。不支持 refresh_token（没有必要）。]]></content>
      <categories>
        <category>oauth2</category>
      </categories>
      <tags>
        <tag>oauth2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas 实战笔记]]></title>
    <url>%2Fp%2Fcas-in-action%2F</url>
    <content type="text"><![CDATA[CAS 使用和问题排查整理。传统 filter 接入 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.jasig.cas.client&lt;/groupId&gt; &lt;artifactId&gt;cas-client-core&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt;&lt;/dependency&gt; 在 web.xml 中添加 Cas Client 的拦截过滤配置：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;!-- 用于单点退出的 fiflter --&gt;&lt;filter&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.session.SingleSignOutFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;casServerUrlPrefix&lt;/param-name&gt; &lt;param-value&gt;http://cas-server.com:8080/cas&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt; &lt;listener&gt; &lt;listener-class&gt;org.jasig.cas.client.session.SingleSignOutHttpSessionListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;!-- 注意 casServerLoginUrl 指 cas server 服务器的 login 地址；而 serverName 指的是应用的地址 --&gt; &lt;filter&gt; &lt;filter-name&gt;CAS Authentication Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.authentication.AuthenticationFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;casServerLoginUrl&lt;/param-name&gt; &lt;param-value&gt;http://cas-server.com:8080/cas/login&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;serverName&lt;/param-name&gt; &lt;param-value&gt;http://client.app.com:8070&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;!-- 这个是对 st 票据的校验，其实 cas 中也就是通过这种方式来确定是否是同一个人 --&gt;&lt;filter&gt; &lt;filter-name&gt;CAS Validation Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;casServerUrlPrefix&lt;/param-name&gt; &lt;param-value&gt;http://cas-server.com:8080/cas&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;serverName&lt;/param-name&gt; &lt;param-value&gt;http://client.app.com:8070&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt; &lt;filter&gt; &lt;filter-name&gt;CAS HttpServletRequest Wrapper Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.util.HttpServletRequestWrapperFilter&lt;/filter-class&gt;&lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Validation Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS Authentication Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CAS HttpServletRequest Wrapper Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;这里没有 spring security，没有 shiro。如果有 shiro，那么 cas filter 要配置在 shiro 之前。参见：Shiro 与 CAS 整合实现单点登录 。No principal was found in the response from the CAS server1234org.jasig.cas.client.validation.TicketValidationException: No principal was found in the response from the CAS server.at org.jasig.cas.client.validation.Cas20ServiceTicketValidator.parseResponseFromServer(Cas20ServiceTicketValidator.java:82)at org.jasig.cas.client.validation.AbstractUrlBasedTicketValidator.validate(AbstractUrlBasedTicketValidator.java:197)at org.jasig.cas.client.validation.AbstractTicketValidationFilter.doFilter(AbstractTicketValidationFilter.java:164) 检查 Cas20ProxyReceivingTicketValidationFilter 配置 casServerUrlPrefix 服务地址。票根’xxx’不符合目标服务 1234567org.jasig.cas.client.validation.TicketValidationException: 票根 &apos;ST-1435-VDIliRQLn5rSsJw7Ldzc-cas01.example.org&apos; 不符合目标服务 at org.jasig.cas.client.validation.Cas20ServiceTicketValidator.parseResponseFromServer(Cas20ServiceTicketValidator.java:84) at org.jasig.cas.client.validation.AbstractUrlBasedTicketValidator.validate(AbstractUrlBasedTicketValidator.java:201) at org.jasig.cas.client.validation.AbstractTicketValidationFilter.doFilter(AbstractTicketValidationFilter.java:204) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)cas server 已经生成了 service ticket，当客户端拿着这张 ST 去服务端校验的时候出了问题。 常见的原因：AuthenticationFilter 配置的 serverName 和 Cas20ProxyReceivingTicketValidationFilter 配置的 serverName 不一致。这里是一个例子。用 postman 调用 cas server restful 接口获取 tgt、获取 st、使用 st。获取 tgt，使用的 service 是 http://xxx.xxx.xxx.xxx:8088 使用 st，使用的 service 却是 http://xxx.xxx.xxx.xxx:8088/dmserver/api/dataexplore/pcacas server 日志如下：1234567891011121314151617181920212223242020-09-09 13:47:41,611 INFO [org.jasig.inspektr.audit.support.Slf4jLoggingAuditTrailManager] - Audit trail record BEGIN=============================================================WHO: adminWHAT: ST-1540-dEoj7dd4z0VWke2K47s4-cas01.example.org for http://xxx.xxx.xxx.xxx:8088ACTION: SERVICE_TICKET_CREATEDAPPLICATION: CASWHEN: Wed Sep 09 13:47:41 GMT+08:00 2020CLIENT IP ADDRESS: 172.23.122.86SERVER IP ADDRESS: 172.25.21.205=============================================================2020-09-09 13:47:48,855 INFO [org.jasig.cas.support.rest.TicketsResource] - 调用查询 IP 地址的接口2020-09-09 13:48:04,200 ERROR [org.jasig.cas.CentralAuthenticationServiceImpl] - Service ticket [ST-1540-dEoj7dd4z0VWke2K47s4-cas01.example.org] with service [http://xxx.xxx.xxx.xxx:8088] does not match supplied service [http://xxx.xxx.xxx.xxx:8088/dmserver/api/dataexplore/pca]2020-09-09 13:48:04,201 INFO [org.jasig.inspektr.audit.support.Slf4jLoggingAuditTrailManager] - Audit trail record BEGIN=============================================================WHO: audit:unknownWHAT: ST-1540-dEoj7dd4z0VWke2K47s4-cas01.example.orgACTION: SERVICE_TICKET_VALIDATE_FAILEDAPPLICATION: CASWHEN: Wed Sep 09 13:48:04 GMT+08:00 2020CLIENT IP ADDRESS: xxx.xxx.xxx.xxxSERVER IP ADDRESS: 172.25.21.205============================================================= 未能够识别出目标 ‘xxx’票根 1234567javax.servlet.ServletException: org.jasig.cas.client.validation.TicketValidationException: 未能够识别出目标 &apos;ST-1291-vzevZgDVjhN77R0PNP0s-cas01.example.org&apos; 票根 org.jasig.cas.client.validation.AbstractTicketValidationFilter.doFilter(AbstractTicketValidationFilter.java:227) org.jasig.cas.client.session.SingleSignOutFilter.doFilter(SingleSignOutFilter.java:92) org.springframework.orm.jpa.support.OpenEntityManagerInViewFilter.doFilterInternal(OpenEntityManagerInViewFilter.java:178) org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 常见原因：客户端带着 ST 去服务器验证，但此时服务器端的 ST 已经失效。默认 ST 有效期是 10 秒。需要修改 cas server。参见 cas.properties：1234567### Service Ticket Timeout# Default sourced from WEB-INF/spring-configuration/ticketExpirationPolices.xml## Service Ticket timeout - typically kept short as a control against replay attacks, default is 10s. You'll want to# increase this timeout if you are manually testing service ticket creation/validation via tamperdata or similar tools# st.timeToKillInSeconds=10ST 已经验签过。cas server restful apicas server 引入 rest 组件12345&lt;dependency&gt; &lt;groupId&gt;org.apereo.cas&lt;/groupId&gt; &lt;artifactId&gt;cas-server-support-rest&lt;/artifactId&gt; &lt;version&gt;$&#123;cas.version&#125;&lt;/version&gt;&lt;/dependency&gt; 获取 TGT12POST /cas/v1/tickets HTTP/1.0username=xxx&amp;password=123456请求响应 12201 CreatedLocation: http://cas-server.com/cas/v1/tickets/&#123;TGT id&#125; 获取 ST12POST /cas/v1/tickets/&#123;TGT id&#125; HTTP/1.0service=&#123;form encoded parameter for the service url&#125;请求响应 12200 OKST-1572-kZsaCRRRFQGaKNmOw4Gr-cas01.example.org 校验 ST1GET /cas/p3/serviceValidate?service=&#123;service url&#125;&amp;ticket=&#123;service ticket&#125;请求响应 200，可能是成功，也可能是失败。登出 1DELETE /cas/v1/tickets/&#123;TGT&#125; HTTP/1.0 请求响应：返回注销的 TGT使用 ST访问受保护的资源加上 ?ticket={service ticket}。 单点登出不成功 单点登出不成功，常见原因有：没有正确配置 SingleSignOutFilterservice 地址为 localhost 或者 127.0.0.1，导致 cas server 不能正常通知 client 下线。]]></content>
      <categories>
        <category>CAS</category>
      </categories>
      <tags>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth2 系列 1：简介]]></title>
    <url>%2Fp%2Foauth2-p1-intro%2F</url>
    <content type="text"><![CDATA[整理 oauth2 认证资料。问题背景 在传统的客户端 - 服务器身份验证模式中，客户端请求服务器上限制访问的资源（受保护资源）时，需要使用资源所有者的凭据在服务器上进行身份验证。资源所有者为了给第三方应用提供受限资源的访问，需要与第三方共享它的凭据。 这造成一些问题和局限：需要第三方应用存储资源所有者的凭据，以供将来使用，通常是明文密码。需要服务器支持密码身份认证，尽管密码认证天生就有安全缺陷。第三方应用获得的资源所有者的受保护资源的访问权限过于宽泛，从而导致资源所有者失去对资源使用时限或使用范围的控制。资源所有者不能仅撤销某个第三方的访问权限而不影响其它，并且，资源所有者只有通过改变第三方的密码，才能单独撤销这第三方的访问权限。与任何第三方应用的让步导致对终端用户的密码及该密码所保护的所有数据的让步。软件开发中遇到的所有问题，都可以通过增加一层抽象而得以解决 OAuth 通过引入 授权层 以及分离客户端角色和资源所有者角色来解决这些问题。在 OAuth 中，客户端在请求受资源所有者控制并托管在资源服务器上的资源的访问权限时，将被颁发一组不同于资源所有者所拥有凭据的凭据。客户端获得一个访问令牌（一个代表特定作用域、生命期以及其他访问属性的字符串），用以代替使用资源所有者的凭据来访问受保护资源。访问令牌由授权服务器在资源所有者认可的情况下颁发给第三方客户端。客户端使用访问令牌访问托管在资源服务器的受保护资源。小结：OAuth 解决第三方授权问题：委托第三方来对既定的用户进行鉴定，鉴定成功之后，下发信任凭证，信任凭证和用户挂钩，同时可以使用此凭证来去第三方平台，获得该用户开放的部分信息。OAuth2 的角色 OAuth 2.0 主要有 4 类角色：resource owner：资源所有者，指终端的“用户”（user）。resource server：资源服务器，即服务提供商存放受保护资源。访问这些资源，需要获得访问令牌（access token）。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。client：客户端，代表向受保护资源进行资源请求的第三方应用程序。authorization server： 授权服务器， 在验证资源所有者并获得授权成功后，将发放访问令牌给客户端。OAuth2 的抽象流程1234567891011（A）客户端向从资源所有者请求授权。授权请求可以直接向资源所有者发起（如图所示），或者更可取的是通过作为中介的授权服务器间接发起。（B）客户端收到授权许可，这是一个代表资源所有者的授权的凭据，使用本规范中定义的四种许可类型之一或 者使用扩展许可类型表示。授权许可类型取决于客户端请求授权所使用的方式以及授权服务器支持的类型。（C）客户端与授权服务器进行身份认证并出示授权许可请求访问令牌。（D）授权服务器验证客户端身份并验证授权许可，若有效则颁发访问令牌。（E）客户端从资源服务器请求受保护资源并出示访问令牌进行身份验证。（F）资源服务器验证访问令牌，若有效则满足该请求。 其中，用户授权有四种模式：授权码模式（authorization code）简化模式（implicit）密码模式（resource owner password credentials）客户端凭证模式（client credentials）授权码模式 授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。简化模式 有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）”隐藏式”（implicit）。密码模式 如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。客户端凭证模式 最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。参考资料 OAuth2.0 RFC 6749 中文 理解 OAuth 2.0OAuth 2.0 的四种方式OAuth2.0 原理和验证流程分析An Introduction to OAuth 2]]></content>
      <categories>
        <category>oauth2</category>
      </categories>
      <tags>
        <tag>oauth2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAS 登录重定向和 ajax]]></title>
    <url>%2Fp%2Fcas-redirect-login-and-ajax%2F</url>
    <content type="text"><![CDATA[聊聊 CAS 登录过程的 302 重定向对 web 请求的影响。问题背景 CAS 协议登录流程涉及到 302 重定向： 未登录状态页面直接使用 XMLHttpRequest（即 ajax）发送请求，被 filter 拦截，app 服务器返回 302 重定向到登录页面。但是 ajax 并没有正常处理 302，没有跳到登录页面。问题分析 当服务器将 302 响应发给浏览器时，浏览器并不是直接进行 ajax 回调处理，而是先执行 302 重定向——从 Response Headers 中读取 Location 信息，然后向 Location 中的 Url 发出请求，在收到这个请求的响应后才会进行 ajax 回调处理。大致流程如下：ajax -&gt; browser -&gt; server -&gt; 302 -&gt; browser(redirect) -&gt; server -&gt; browser -&gt; ajax callback如果 302 返回的重定向 URL 在服务器上没有相应的处理程序，那么在 ajax 回调函数中得到的是 404 状态码。如果存在对应的 URL，得到的状态码就是 200。所以 ajax 请求得不到 302 响应码。解决 xhr web 重定向问题的几种思路：前后端不使用 302，自定义协议告诉前端要发生重定向 在重定向页面添加标记，xhr 在返回页面提取标记 不使用 xhr，使用 fetch api方案 1： 自定义协议 自定义协议很简单：302 换成 401、403。xhr 返回判断是否 error。302 换成 200，在 json 中返回跳转信息。ajax 增加全局 setup，处理自定义协议即可。虽然简单，但是对原生的 CAS 协议有入侵，抓包分析对不上。实现有 2 个选项：在 cas client在网关处理 这里使用 cas-client-autoconfig-support 接入 cas。引入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;net.unicon.cas&lt;/groupId&gt; &lt;artifactId&gt;cas-client-autoconfig-support&lt;/artifactId&gt; &lt;version&gt;2.3.0-GA&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.jasig.cas.client&lt;/groupId&gt; &lt;artifactId&gt;cas-client-core&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt;&lt;/dependency&gt;application.yml 增加配置12345cas: server-url-prefix: http://xxx/cas server-login-url: http://xxx/cas/login client-host-url: http://xxxx validation-type: cas 启动类增加 1@EnableCasClient 自定义 AuthenticationRedirectStrategy，从而处理 xhr 请求 12345678910111213141516171819202122public class CopeWithXhrRedirectStrategy implements AuthenticationRedirectStrategy &#123; @Override public void redirect(HttpServletRequest request, HttpServletResponse response, String potentialRedirectUrl) throws IOException &#123; String headerRequestedWith = request.getHeader("X-Requested-With"); // ajax 请求 if (!StringUtils.isEmpty(headerRequestedWith)) &#123; response.setStatus(200); response.setContentType("text/plain"); try &#123; response.getWriter().write(customRedirectUrl(potentialRedirectUrl)); &#125; catch (IOException e) &#123; &#125; &#125; else &#123; response.sendRedirect(potentialRedirectUrl); &#125; &#125; private String customRedirectUrl(String redirectUrl) &#123; return "&#123;\"status\":403,\"redirectURL\":\"" + redirectUrl + "\"&#125;"; &#125;&#125; 覆盖 CasClientConfigurerAdapter 配置 1234567891011@Configurationpublic class CasConfiguration extends CasClientConfigurerAdapter &#123; @Override public void configureAuthenticationFilter(FilterRegistrationBean authenticationFilter) &#123; // filter 参数的注入方式 super.configureAuthenticationFilter(authenticationFilter); authenticationFilter.getInitParameters().put("authenticationRedirectStrategyClass","xxxx.CopeWithXhrRedirectStrategy"); &#125;&#125; 相关入口在 AuthenticationFilter：123456789101112131415public class AuthenticationFilter extends AbstractCasFilter &#123; protected void initInternal(final FilterConfig filterConfig) throws ServletException &#123; if (!isIgnoreInitConfiguration()) &#123; // more codes final Class&lt;? extends AuthenticationRedirectStrategy&gt; authenticationRedirectStrategyClass = getClass(ConfigurationKeys.AUTHENTICATION_REDIRECT_STRATEGY_CLASS); if (authenticationRedirectStrategyClass != null) &#123; this.authenticationRedirectStrategy = ReflectUtils.newInstance(authenticationRedirectStrategyClass); &#125; &#125; &#125;&#125;在前端增加全局的 ajax setup，处理自定义协议。springboot 2.x securtiy 优先级比 cas filter 高，因此会拦截掉请求。这里改为全部放行，由 cas client 拦截。12345678910@Configurationpublic class BeanConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .anyRequest().permitAll().and().logout().permitAll();// 配置不需要登录验证 &#125;&#125; 方案 2： 增加响应头 xhr 能获取浏览器 302 重定向之后的响应码，这个没卵用。 但是 xhr 还能获取这个新页面的 response header。这里就可以做手脚。在跳转页面增加返回 header 标记。具体的流程：cas client 拦截到未登录请求，且为 ajax 请求，则在 redirect to cas server 的 url 增加标记，比如 x-from-ajax=1，再回复浏览器。 浏览器发现是 302，重定向到 cas server（这里还要考虑 cors 问题，此处不展开）。cas server 解析了 x-from-ajax=1，再在页面响应中增加头部X-LOGIN-PAGE-REDIRECT，值为当前 url。ajax 拿到浏览器加载完新页面的响应结果，status 是 200。 然后使用 xhr.getResponseHeader(&quot;X-LOGIN-PAGE-REDIRECT&quot;) 获取真实的重定向 url。ajax 处理后跳转到目标 urljquery 框架可以全局设置 123456789$(document).ajaxComplete(function(e, xhr, settings)&#123; if(xhr.status === 200)&#123; var loginPageRedirectHeader = xhr.getResponseHeader("X-LOGIN-PAGE-REDIRECT"); if(loginPageRedirectHeader &amp;&amp; loginPageRedirectHeader !== "")&#123; // 如果是 iframe，用 top window.location.replace(loginPageRedirectHeader); &#125; &#125;&#125;); 这个方案基本保持了 CAS 协议。cas client 对于 ajax 请求的重定向 url 增加标记。判断 ajax 请求根据 x-requested-with 请求头即可。1x-requested-with XMLHttpRequest如果使用标准 cas client 接入，那么 client 直接返回了。这里有 2 个选择：自行修改 cas client，加入上面的逻辑。在网关层处理 cas server 返回增加 header 标记，也是 2 种方法解决： 修改 cas server 源码 在网关层处理 方案 3： 使用 fetch 替换 xhrfetch 是浏览器提供的 api，功能强大，可以替代 XMLHttpRequest。1234567891011121314151617function postData(url, data) &#123; // Default options are marked with * return fetch(url, &#123; body: JSON.stringify(data), // must match 'Content-Type' header cache: 'no-cache', // *default, no-cache, reload, force-cache, only-if-cached credentials: 'same-origin', // include, same-origin, *omit headers: &#123; 'user-agent': 'Mozilla/4.0 MDN Example', 'content-type': 'application/json' &#125;, method: 'POST', // *GET, POST, PUT, DELETE, etc. mode: 'cors', // no-cors, cors, *same-origin redirect: 'follow', // manual, *follow, error referrer: 'no-referrer', // *client, no-referrer &#125;) .then(response =&gt; response.json()) // parses response to JSON&#125;fetch 的 options 配置里有一条叫做 redirectfollow 默认, 跟随跳转 error 阻止并抛出异常manual 阻止重定向 只需要在 cas server 配置好 cors 策略，则 fetch 可以顺利完成 302 重定向。使用 fetch 的响应结构 redirected 和url就可以方便在 web 端控制。fetch 问题在于兼容性。IE 全家桶不支持。另外，大量使用 xhr 类库也不兼容。参考资料 使用 Fetchfetch API 和 Ajax（XMLHttpRequest）的差异]]></content>
      <categories>
        <category>CAS</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas 单点登出分析]]></title>
    <url>%2Fp%2Fcas-single-sign-out%2F</url>
    <content type="text"><![CDATA[前台应用被客户发现单点退出服务不生效，又丢到架构部处理。结果发现了祖传代码的问题😂。基于 CAS server 4.1.x。问题排查 这是 tomcat war 应用。发现使用祖传的 tomcat cas client。祖传自定义的 logout handler：1234567891011121314public class SimpleURILogoutHandler extends AbstractLogoutHandler &#123; public SimpleURILogoutHandler(String casServer) &#123; this(casServer, "/logout", "/logout"); &#125; public void logout(HttpServletRequest request, HttpServletResponse response) &#123; HttpSession session = request.getSession(); if (session != null) &#123; session.invalidate(); &#125; AuthenticationUtil.removeAuthenticatedCookie(request, response); super.logout(request, response); &#125;失效 session 之后，使用父类的 logout 能力 org.jasig.cas.client.tomcat.AbstractLogoutHandler：123456789101112131415161718192021public abstract class AbstractLogoutHandler implements LogoutHandler &#123; public void logout(final HttpServletRequest request, final HttpServletResponse response) &#123; logger.debug("Processing logout request from CAS server."); final Assertion assertion; final HttpSession httpSession = request.getSession(false); if (httpSession != null &amp;&amp; (assertion = (Assertion) httpSession.getAttribute(AbstractCasFilter.CONST_CAS_ASSERTION)) != null) &#123; httpSession.removeAttribute(AbstractCasFilter.CONST_CAS_ASSERTION); logger.info("Successfully logged out &#123;&#125;", assertion.getPrincipal()); &#125; else &#123; logger.info("Session already ended."); &#125; final String redirectUrl = constructRedirectUrl(request); if (redirectUrl != null) &#123; logger.debug("Redirecting to &#123;&#125;", redirectUrl); CommonUtils.sendRedirect(response, redirectUrl); &#125; &#125; 从上面流程来看，自定义的 logout handler 占用了 /logout 入口。它的登出只做了 2 件事情：使 session 失效 发送重定向 少了通知 cas server 做单点登出动作！这就是没有实现单点登出的原因。CAS 单点登出原理 CAS 官方的单点登出流程图如下。 当一个 web 浏览器要求退出应用服务器，应用服务器 application 会把 url 重定向到 CAS server 上的 /cas/logout。 然后 cas server 向各个服务发出 logout 请求。 很明显要解决的问题：从应用 logout 怎么重定向到 cas server 的 logout？cas server 怎么知道要把这个 session 关联的其他应用也 logout？在深入之前，先介绍 2 个概念；CAS 定义了 2 种单点登出方式：Back ChannelCAS 服务端直接向各服务客户端发送 HTTP POST 消息。这是向服务执行通知的传统方式。Front ChannelCAS 通过一个 GET 请求，通过 JSONP 去验证各客户端服务，来使得各客户端的 session 无效。注意：该方式不一定适用所有客户端，必须要确保客户端支持该方式。CAS 单点注销请求默认是在后台通过 logoutType 的属性配置好了的，默认为 LogoutType.BACK_CHANNEL。解决方案 官方 SLO 方式，使用 SingleSignOutFilter。在 web.xml 增加：1234567891011&lt;filter&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;filter-class&gt;org.jasig.cas.client.session.SingleSignOutFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CAS Single Sign Out Filter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;listener&gt; &lt;listener-class&gt;org.jasig.cas.client.session.SingleSignOutHttpSessionListener&lt;/listener-class&gt;&lt;/listener&gt;SLO filter 要配置在 SSO filter 之前！SLO filter 要配置在 SSO filter 之前！SLO filter 要配置在 SSO filter 之前！SingleSignOutHttpSessionListener 则是清理 session，避免内存泄漏。源码分析 cas clientSingleSignOutFilter 的核心逻辑由 SingleSignOutHandler 负责。1234567891011121314151617public final class SingleSignOutHandler &#123; public boolean process(final HttpServletRequest request, final HttpServletResponse response) &#123; if (isTokenRequest(request)) &#123; logger.trace("Received a token request"); recordSession(request); return true; &#125; if (isLogoutRequest(request)) &#123; logger.trace("Received a logout request"); destroySession(request); return false; &#125; logger.trace("Ignoring URI for logout: &#123;&#125;", request.getRequestURI()); return true; &#125; 如果是 token request，则记录 token 和 session 的映射。12345678910111213141516171819private void recordSession(final HttpServletRequest request) &#123; final HttpSession session = request.getSession(this.eagerlyCreateSessions); if (session == null) &#123; logger.debug("No session currently exists (and none created). Cannot record session information for single sign out."); return; &#125; final String token = CommonUtils.safeGetParameter(request, this.artifactParameterName, this.safeParameters); logger.debug("Recording session for token &#123;&#125;", token); try &#123; this.sessionMappingStorage.removeBySessionById(session.getId()); &#125; catch (final Exception e) &#123; // ignore if the session is already marked as invalid. Nothing we can do! &#125; // 划重点 sessionMappingStorage.addSessionById(token, session);&#125; 如果是 logout request，则销毁 session。1234567891011121314151617181920212223242526272829303132private void destroySession(final HttpServletRequest request) &#123; // logoutRequest 是 post 请求，从 form body 解析。 String logoutMessage = CommonUtils.safeGetParameter(request, this.logoutParameterName, this.safeParameters); if (CommonUtils.isBlank(logoutMessage)) &#123; logger.error("Could not locate logout message of the request from &#123;&#125;", this.logoutParameterName); return; &#125; // logoutMessage 可能是 base64+deflate 处理过 if (!logoutMessage.contains("SessionIndex")) &#123; logoutMessage = uncompressLogoutMessage(logoutMessage); &#125; logger.trace("Logout request:\n&#123;&#125;", logoutMessage); // 解析 SessionIndex final String token = XmlUtils.getTextForElement(logoutMessage, "SessionIndex"); if (CommonUtils.isNotBlank(token)) &#123; final HttpSession session = this.sessionMappingStorage.removeSessionByMappingId(token); if (session != null) &#123; final String sessionID = session.getId(); logger.debug("Invalidating session [&#123;&#125;] for token [&#123;&#125;]", sessionID, token); try &#123; session.invalidate(); &#125; catch (final IllegalStateException e) &#123; logger.debug("Error invalidating session.", e); &#125; this.logoutStrategy.logout(request); &#125; &#125;&#125;cas server 向 client 发送的 logout request，是一个 POST 请求，并且是 form 编码，包含一个 key 为logoutRequest。 从抓包看就很清楚。这里 186 是 cas server，54 是 cas client。在 cas server 上执行（为了简化，只关注对 cas client 的出站流量）：1tcpdump -nn -A -i enp2s0f0 &apos;((tcp) and ((dst host 172.25.22.54) ))&apos; -w logout.capcas servercas server 使用 spring webflow 控制流程。从 logout-webflow.xml 看流程：第一个入口类是 LogoutAction。12345678910111213141516171819202122232425262728293031323334353637383940public final class LogoutAction extends AbstractLogoutAction &#123; protected Event doInternalExecute(final HttpServletRequest request, final HttpServletResponse response, final RequestContext context) throws Exception &#123; boolean needFrontSlo = false; putLogoutIndex(context, 0); final List&lt;LogoutRequest&gt; logoutRequests = WebUtils.getLogoutRequests(context); if (logoutRequests != null) &#123; for (final LogoutRequest logoutRequest : logoutRequests) &#123; // if some logout request must still be attempted // 只要有一个请求为未处理，则执行 SLO if (logoutRequest.getStatus() == LogoutRequestStatus.NOT_ATTEMPTED) &#123; needFrontSlo = true; break; &#125; &#125; &#125; // 从 logout 请求中提取 sevice 字段 final String service = request.getParameter("service"); if (this.followServiceRedirects &amp;&amp; service != null) &#123; // 从 service 找到对应应用 final Service webAppService = new SimpleWebApplicationServiceImpl(service); final RegisteredService rService = this.servicesManager.findServiceBy(webAppService); if (rService != null &amp;&amp; rService.getAccessStrategy().isServiceAccessAllowed()) &#123; context.getFlowScope().put("logoutRedirectUrl", service); &#125; &#125; // there are some front services to logout, perform front SLO if (needFrontSlo) &#123; // 需要 front logout return new Event(this, FRONT_EVENT); &#125; else &#123; // otherwise, finish the logout process return new Event(this, FINISH_EVENT); &#125; &#125;LogoutAction 的核心流程： 如果有一个 logout request 为未处理（LogoutRequestStatus.NOT_ATTEMPTED），则需要单点登出 SLO。从 request 中找到 service 参数，以及对应的应用。如果找到，则把 service 放进 webflow 的上下文变量 logoutRedirectUrl。 对于 front SLO，则执行 front logout 动作。FrontChannelLogoutAction 也是使用 SAML 协议发送 logoutRequest。server 端销毁 ticket 的入口在 CentralAuthenticationService：12345678910111213141516public final class CentralAuthenticationServiceImpl implements CentralAuthenticationService &#123; public List&lt;LogoutRequest&gt; destroyTicketGrantingTicket(@NotNull final String ticketGrantingTicketId) &#123; try &#123; logger.debug("Removing ticket [&#123;&#125;] from registry...", ticketGrantingTicketId); final TicketGrantingTicket ticket = getTicket(ticketGrantingTicketId, TicketGrantingTicket.class); logger.debug("Ticket found. Processing logout requests and then deleting the ticket..."); final List&lt;LogoutRequest&gt; logoutRequests = logoutManager.performLogout(ticket); this.ticketRegistry.deleteTicket(ticketGrantingTicketId); return logoutRequests; &#125; catch (final InvalidTicketException e) &#123; logger.debug("TicketGrantingTicket [&#123;&#125;] cannot be found in the ticket registry.", ticketGrantingTicketId); &#125; return Collections.emptyList(); &#125;LogoutManagerImpl 负责构造 logout 消息、执行 logout 动作。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public List&lt;LogoutRequest&gt; performLogout(final TicketGrantingTicket ticket) &#123; final Map&lt;String, Service&gt; services = ticket.getServices(); final List&lt;LogoutRequest&gt; logoutRequests = new ArrayList&lt;&gt;(); // if SLO is not disabled if (!this.singleLogoutCallbacksDisabled) &#123; // through all services for (final Map.Entry&lt;String, Service&gt; entry : services.entrySet()) &#123; // it's a SingleLogoutService, else ignore final Service service = entry.getValue(); if (service instanceof SingleLogoutService) &#123; final LogoutRequest logoutRequest = handleLogoutForSloService((SingleLogoutService) service, entry.getKey()); if (logoutRequest != null) &#123; LOGGER.debug("Captured logout request [&#123;&#125;]", logoutRequest); logoutRequests.add(logoutRequest); &#125; &#125; &#125; &#125; return logoutRequests;&#125;private LogoutRequest handleLogoutForSloService(final SingleLogoutService singleLogoutService, final String ticketId) &#123; if (!singleLogoutService.isLoggedOutAlready()) &#123; final RegisteredService registeredService = servicesManager.findServiceBy(singleLogoutService); if (serviceSupportsSingleLogout(registeredService)) &#123; final URL logoutUrl = determineLogoutUrl(registeredService, singleLogoutService); final DefaultLogoutRequest logoutRequest = new DefaultLogoutRequest(ticketId, singleLogoutService, logoutUrl); final LogoutType type = registeredService.getLogoutType() == null ? LogoutType.BACK_CHANNEL : registeredService.getLogoutType(); switch (type) &#123; // back_channel 是 http post 请求 case BACK_CHANNEL: if (performBackChannelLogout(logoutRequest)) &#123; logoutRequest.setStatus(LogoutRequestStatus.SUCCESS); &#125; else &#123; logoutRequest.setStatus(LogoutRequestStatus.FAILURE); LOGGER.warn("Logout message not sent to [&#123;&#125;]; Continuing processing...", singleLogoutService.getId()); &#125; break; default: // front_channel 是 http get 请求 logoutRequest.setStatus(LogoutRequestStatus.NOT_ATTEMPTED); break; &#125; return logoutRequest; &#125; &#125; return null;&#125; 在 client 源码分析看到，server 端构造一个 logoutRequest，携带要登出的 session id。构造 saml 格式的 logout message：12345678910111213141516171819202122232425public final class SamlCompliantLogoutMessageCreator implements LogoutMessageCreator &#123; /** The logger. */ private static final Logger LOGGER = LoggerFactory.getLogger(SamlCompliantLogoutMessageCreator.class); /** A ticket Id generator. */ private static final UniqueTicketIdGenerator GENERATOR = new DefaultUniqueTicketIdGenerator(); /** The logout request template. */ private static final String LOGOUT_REQUEST_TEMPLATE = "&lt;samlp:LogoutRequest xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\" ID=\"%s\" Version=\"2.0\" " + "IssueInstant=\"%s\"&gt;&lt;saml:NameID xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\"&gt;@NOT_USED@" + "&lt;/saml:NameID&gt;&lt;samlp:SessionIndex&gt;%s&lt;/samlp:SessionIndex&gt;&lt;/samlp:LogoutRequest&gt;"; @Override public String create(final LogoutRequest request) &#123; final String logoutRequest = String.format(LOGOUT_REQUEST_TEMPLATE, GENERATOR.getNewTicketId("LR"), new ISOStandardDateFormat().getCurrentDateAndTime(), request.getTicketId()); LOGGER.debug("Generated logout message: [&#123;&#125;]", logoutRequest); return logoutRequest; &#125; &#125;抠出来来看。1234&lt;samlp:LogoutRequest ID="[RANDOM ID]" Version="2.0" IssueInstant="[CURRENT DATE/TIME]"&gt;&lt;saml:NameID&gt;@NOT_USED@&lt;/saml:NameID&gt;&lt;samlp:SessionIndex&gt;[SESSION IDENTIFIER]&lt;/samlp:SessionIndex&gt;&lt;/samlp:LogoutRequest&gt;划重点：SessionIndex字段携带了 session 标记，实际是 request.getTicketId()，是 service ticket id。 对应于 client 端的 SingleSignOutHandler#destroySession()。 参考资料Java Apereo CAS Client。 后来发现这里有官方 cas client 的集成方式，包括单点登录和单点登出。]]></content>
      <categories>
        <category>cas</category>
      </categories>
      <tags>
        <tag>cas</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sprignboot 启动相关类分析]]></title>
    <url>%2Fp%2Fsprignboot-startup-p2%2F</url>
    <content type="text"><![CDATA[springboot 启动相关类的分析。SpringFactoriesLoaderSpringFactoriesLoader 工厂加载机制是 Spring 内部提供的一个约定俗成的加载方式，与 java spi 类似，只需要在模块的 META-INF/spring.factories 文件，这个 Properties 格式的文件中的 key 是接口、注解、或抽象类的全名，value 是以逗号 , 分隔的实现类，使用 SpringFactoriesLoader 来实现相应的实现类注入 Spirng 容器中。1234567public final class SpringFactoriesLoader &#123; /** * The location to look for factories. * &lt;p&gt;Can be present in multiple JAR files. */ public static final String FACTORIES_RESOURCE_LOCATION = "META-INF/spring.factories";举个例子，spring-boot-autoconfigure-2.2.9.RELEASE.jar!/META-INF/spring.factories：1234567891011121314151617181920# Initializersorg.springframework.context.ApplicationContextInitializer=\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener# Application Listenersorg.springframework.context.ApplicationListener=\org.springframework.boot.autoconfigure.BackgroundPreinitializer# Auto Configuration Import Listenersorg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener# Auto Configuration Import Filtersorg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\org.springframework.boot.autoconfigure.condition.OnBeanCondition,\org.springframework.boot.autoconfigure.condition.OnClassCondition,\org.springframework.boot.autoconfigure.condition.OnWebApplicationCondition# 以下省略 loadSpringFactories 利用 ClassLoader 获取指定路径下的 class url：123456789101112131415161718192021222324252627282930private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result = cache.get(classLoader); if (result != null) &#123; return result; &#125; try &#123; Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryTypeName = ((String) entry.getKey()).trim(); for (String factoryImplementationName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryTypeName, factoryImplementationName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException("Unable to load factories from location [" + FACTORIES_RESOURCE_LOCATION + "]", ex); &#125;&#125;然后使用反射工具实例化。12345678910111213141516@SuppressWarnings("unchecked")private static &lt;T&gt; T instantiateFactory(String factoryImplementationName, Class&lt;T&gt; factoryType, ClassLoader classLoader) &#123; try &#123; Class&lt;?&gt; factoryImplementationClass = ClassUtils.forName(factoryImplementationName, classLoader); if (!factoryType.isAssignableFrom(factoryImplementationClass)) &#123; throw new IllegalArgumentException( "Class [" + factoryImplementationName + "] is not assignable to factory type [" + factoryType.getName() + "]"); &#125; return (T) ReflectionUtils.accessibleConstructor(factoryImplementationClass).newInstance(); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException( "Unable to instantiate factory class [" + factoryImplementationName + "] for factory type [" + factoryType.getName() + "]", ex); &#125;&#125;@EnableAutoConfiguration 注解 自动配置是 springboot 非常强大的功能。默认的自动配置在 spring-boot-autoconfigure-2.2.9.RELEASE.jar!/META-INF/spring.factories：12345678910111213# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\org.springframework.boot.autoconfigure.cloud.CloudServiceConnectorsAutoConfiguration,\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\# 以下省略XXXAutoConfiguration 的实现，一般包含触发条件。12345678910111213141516171819@Configuration(proxyBeanMethods = false)@ConditionalOnProperty(prefix = "spring.aop", name = "auto", havingValue = "true", matchIfMissing = true)public class AopAutoConfiguration &#123; @Configuration(proxyBeanMethods = false) @ConditionalOnClass(Advice.class) static class AspectJAutoProxyingConfiguration &#123; @Configuration(proxyBeanMethods = false) @EnableAspectJAutoProxy(proxyTargetClass = false) @ConditionalOnProperty(prefix = "spring.aop", name = "proxy-target-class", havingValue = "false", matchIfMissing = false) static class JdkDynamicAutoProxyConfiguration &#123; &#125; // 省略一堆&#125;AutoConfigurationImportSelector 处理 @EnableAutoConfiguration。AutoConfigurationImportSelector 最核心的工作是配置去重、exclude 配置、filter 配置。12345678910111213141516protected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); // 调用 AutoConfigurationImportFilter configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);&#125;值得一提的是 filter，使用 XXXAutoConfiguration 的条件注解进行过滤。ApplicationContextInitializerCallback interface for initializing a Spring ConfigurableApplicationContext prior to being refreshed.Typically used within web applications that require some programmatic initialization of the application context.在 ConfigurableApplicationContext#refresh() 之前的钩子。通常也会实现 Ordered 接口。通常用于编程式初始化 applicationContext。SpringApplicationRunListenerSpringApplication#run()的监听器。由 SpringFactoriesLoader 加载。提供的钩子方法很多：ApplicationRunner 和 CommandLineRunnerSpringApplication#callRunners()：1234567891011121314private void callRunners(ApplicationContext context, ApplicationArguments args) &#123; List&lt;Object&gt; runners = new ArrayList&lt;&gt;(); runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); AnnotationAwareOrderComparator.sort(runners); for (Object runner : new LinkedHashSet&lt;&gt;(runners)) &#123; if (runner instanceof ApplicationRunner) &#123; callRunner((ApplicationRunner) runner, args); &#125; if (runner instanceof CommandLineRunner) &#123; callRunner((CommandLineRunner) runner, args); &#125; &#125;&#125;如果需要在 SpringApplication 启动后执行一些特殊的代码，你可以实现 ApplicationRunner 或 CommandLineRunner 接口。ApplicationRunner 中 run 方法的参数为 ApplicationArguments，而 CommandLineRunner 接口中 run 方法的参数为 String 数组。如果有多个 runner，则实现 Ordered 接口指定顺序。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot 启动流程]]></title>
    <url>%2Fp%2Fspringboot-startup%2F</url>
    <content type="text"><![CDATA[Springboot 的启动，负责创建配置环境 (environment)、事件监听(listeners)、应用上下文(applicationContext)，以及实例化 beans 并且注入到 applicationContext。 入口代码分析 脚手架生成的启动类代码：12345678@SpringBootApplicationpublic class ExchangerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ExchangerApplication.class, args); &#125;&#125;SpringApplication 设置：ApplicationContextInitializerApplicationListener123456789public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125;SpringApplication#run()分析 SpringApplication#run() 方法：12345678910111213141516171819202122232425262728293031323334353637383940public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context;&#125;画了一张图 准备 environment因为 spring 支持 web、reactive、非 web 环境，所以先确定要适配的运行环境。在 environment、context 首先要适配。12345678910111213private ConfigurableEnvironment getOrCreateEnvironment() &#123; if (this.environment != null) &#123; return this.environment; &#125; switch (this.webApplicationType) &#123; case SERVLET: return new StandardServletEnvironment(); case REACTIVE: return new StandardReactiveWebEnvironment(); default: return new StandardEnvironment(); &#125;&#125;之后向 property source 附加 environment。然后触发 SpringApplicationRunListeners#environmentPrepared() 事件。创建 ApplicationContext根据 web、reactive、非 web 环境，选择 contextClass 类型，再使用 BeanUtils 实例化。12345678910111213141516171819202122protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( "Unable create a default ApplicationContext, please specify an ApplicationContextClass", ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125;准备 context1234567891011121314151617181920212223242526272829private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; context.setEnvironment(environment); postProcessApplicationContext(context); applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) &#123; logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerSingleton("springApplicationArguments", applicationArguments); if (printedBanner != null) &#123; beanFactory.registerSingleton("springBootBanner", printedBanner); &#125; if (beanFactory instanceof DefaultListableBeanFactory) &#123; ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.lazyInitialization) &#123; context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, "Sources must not be empty"); load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context);&#125;postProcessApplicationContext 负责设置：beanNameGeneratorresourceLoaderaddConversionService应用 ApplicationContextInitializerTypically used within web applications that require some programmatic initialization of the application context.触发 SpringApplicationRunListeners#contextPrepared() 事件 BeanDefinitionLoader#load() 把 beans 装入 ApplicationContext触发 SpringApplicationRunListeners#contextLoaded() 事件 刷新 context调用 spring 框架的 AbstractApplicationContext#refresh()：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization -" + "cancelling refresh attempt:" + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125;afterRefresh() 钩子 提供子类覆盖的钩子。SpringApplicationRunListeners#start()钩子方法。callRunners()执行 ApplicationRunner 和 CommandLineRunner。SpringApplicationRunListeners#run()钩子方法。小结 springboot 最核心的启动流程：prepare environmentprepare contextrefresh context, AbstractApplicationContext#refresh()after refresh contxtcall runners (application or commandline) 穿插执行 SpringApplicationRunListeners 的钩子方法。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 springboot chaosmonkey 落地混沌工程]]></title>
    <url>%2Fp%2Fspring-boot-chaos-monkey%2F</url>
    <content type="text"><![CDATA[使用混沌工程进行自动化故障演练。配置 12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;chaos-monkey-spring-boot&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 配置类详见：ChaosMonkeyPropertiesAssaultPropertiesWatcherProperties一个 demo 如下：123456789101112chaos: monkey: enabled: true assaults: level: 10 latencyRangeStart: 500 latencyRangeEnd: 10000 exceptionsActive: true killApplicationActive: false watcher: repository: true restController: true其中 level 用于判断是否触发 request 级别攻击，见 ChaosMonkeyRequestScope：123private boolean isTrouble() &#123; return chaosMonkeySettings.getAssaultProperties().getTroubleRandom() &gt;= chaosMonkeySettings.getAssaultProperties().getLevel();&#125;level 取值区间为 [1, 10000]。Assault 攻击类型ChaosMonkeyAssault 是攻击类型的抽象，包含 2 个子类：ChaosMonkeyRuntimeAssault：运行时攻击，例如退出程序、内存飙升。ChaosMonkeyRequestAssault：请求级别攻击，例如延迟、请求异常。123456public interface ChaosMonkeyAssault &#123; boolean isActive(); // 攻击方法的实现 void attack();&#125;KillAppAssault 直接退出程序。1System.exit(exit)MemoryAssault消耗可用内存。方法很简单，往 Vector 填充 byte[] 数组，然后停顿一个间隔，再触发 gc。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private void eatFreeMemory() &#123; @SuppressWarnings("MismatchedQueryAndUpdateOfCollection") Vector&lt;byte[]&gt; memoryVector = new Vector&lt;&gt;(); long stolenMemoryTotal = 0L; while (isActive()) &#123; // overview of memory methods in java https://stackoverflow.com/a/18375641 long freeMemory = runtime.freeMemory(); long usedMemory = runtime.totalMemory() - freeMemory; if (cannotAllocateMoreMemory()) &#123; LOGGER.debug("Cannot allocate more memory"); break; &#125; LOGGER.debug("Used memory in bytes:" + usedMemory); stolenMemoryTotal = stealMemory(memoryVector, stolenMemoryTotal, getBytesToSteal()); waitUntil(settings.getAssaultProperties().getMemoryMillisecondsWaitNextIncrease()); &#125; // Hold memory level and cleanUp after, only if experiment is running if (isActive()) &#123; LOGGER.info("Memory fill reached, now sleeping and holding memory"); waitUntil(settings.getAssaultProperties().getMemoryMillisecondsHoldFilledMemory()); &#125; // clean Vector memoryVector.clear(); // quickly run gc for reuse runtime.gc(); long stolenAfterComplete = MemoryAssault.stolenMemory.addAndGet(-stolenMemoryTotal); metricEventPublisher.publishMetricEvent(MetricType.MEMORY_ASSAULT_MEMORY_STOLEN, stolenAfterComplete);&#125;private long stealMemory(Vector&lt;byte[]&gt; memoryVector, long stolenMemoryTotal, int bytesToSteal) &#123; memoryVector.add(createDirtyMemorySlice(bytesToSteal)); stolenMemoryTotal += bytesToSteal; long newStolenTotal = MemoryAssault.stolenMemory.addAndGet(bytesToSteal); metricEventPublisher.publishMetricEvent(MetricType.MEMORY_ASSAULT_MEMORY_STOLEN, newStolenTotal); LOGGER.debug("Chaos Monkey - memory assault increase, free memory:" + SizeConverter.toMegabytes(runtime .freeMemory())); return stolenMemoryTotal;&#125;private byte[] createDirtyMemorySlice(int size) &#123; byte[] b = new byte[size]; for (int idx = 0; idx &lt; size; idx += 4096) &#123; // 4096 // is commonly the size of a memory page, forcing a commit b[idx] = 19; &#125; return b;&#125;LatencyAssault给请求 RT 增加延迟。如果不配置延迟的实践区间，则使用随机数。123456789101112131415161718192021222324252627public void attack() &#123; LOGGER.debug("Chaos Monkey - timeout"); atomicTimeoutGauge.set(determineLatency()); // metrics if (metricEventPublisher != null) &#123; metricEventPublisher.publishMetricEvent(MetricType.LATENCY_ASSAULT); metricEventPublisher.publishMetricEvent(MetricType.LATENCY_ASSAULT, atomicTimeoutGauge); &#125; assaultExecutor.execute(atomicTimeoutGauge.get());&#125;private int determineLatency() &#123; final int latencyRangeStart = settings.getAssaultProperties().getLatencyRangeStart(); final int latencyRangeEnd = settings.getAssaultProperties().getLatencyRangeEnd(); if (latencyRangeStart == latencyRangeEnd) &#123; return latencyRangeStart; &#125; else &#123; return ThreadLocalRandom.current().nextInt(latencyRangeStart, latencyRangeEnd); &#125;&#125;请求的执行委派给 ChaosMonkeyLatencyAssaultExecutor。实际上就是 Thread.sleep()。12345678910public class LatencyAssaultExecutor implements ChaosMonkeyLatencyAssaultExecutor &#123; @Override public void execute(long durationInMillis) &#123; try &#123; Thread.sleep(durationInMillis); &#125; catch (InterruptedException e) &#123; // do nothing &#125; &#125;&#125;ExceptionAssault 抛出指定的异常。12345678public void attack() &#123; LOGGER.info("Chaos Monkey - exception"); AssaultException assaultException = this.settings.getAssaultProperties().getException(); // metrics if (metricEventPublisher != null) metricEventPublisher.publishMetricEvent(MetricType.EXCEPTION_ASSAULT); assaultException.throwExceptionInstance();&#125;metrics对接 io.micrometer，每个攻击都会发送 metrics，很方便在 dashboard 观察攻击效果。这里使用 spring 的事件机制。MetricEvent 转换为 ApplicationEvent 事件。1234567891011121314public class MetricEvent extends ApplicationEvent &#123; private final MetricType metricType; private final double metricValue; private final String methodSignature; private final String[] tags; public MetricEvent(Object source, MetricType metricType, long metricValue, String methodSignature, String... tags) &#123; super(source); this.metricType = metricType; this.tags = tags; this.methodSignature = methodSignature; this.metricValue = metricValue; &#125;&#125;MetricEventPublisher 向 spring 容器发送事件。1234public class MetricEventPublisher implements ApplicationEventPublisherAware &#123; private ApplicationEventPublisher publisher;&#125;每个攻击开始，都手动发送事件：12345678public void attack() &#123; LOGGER.info("Chaos Monkey - exception"); AssaultException assaultException = this.settings.getAssaultProperties().getException(); // metrics if (metricEventPublisher != null) metricEventPublisher.publishMetricEvent(MetricType.EXCEPTION_ASSAULT); assaultException.throwExceptionInstance();&#125;开启 actuator 端点，就可以在 dashboard 观察效果。12345678management: endpoint: chaosmonkey: enabled: true endpoints: web: exposure: include: health,info,chaosmonkey控制端点 提供了 jmx 和 rest 两种方式：ChaosMonkeyJmxEndpointChaosMonkeyRestEndpoint切面 通过 LTW 方式开启切面。123456789@Configuration@Profile("chaos-monkey")@EnableLoadTimeWeaving(aspectjWeaving= EnableLoadTimeWeaving.AspectJWeaving.ENABLED)public class ChaosMonkeyLoadTimeWeaving extends LoadTimeWeavingConfiguration &#123; @Override public LoadTimeWeaver loadTimeWeaver() &#123; return new ReflectiveLoadTimeWeaver(); &#125;&#125;watcher 包定义了几个切面，就不展开了。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>混沌工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud sidecar 集成非 jvm 微服务]]></title>
    <url>%2Fp%2Fspring-cloud-sidecar%2F</url>
    <content type="text"><![CDATA[现有几个 go、python 写的系统，需要提供微服务能力，很自然地想到使用 sidecar 模式集成到现有 java 技术栈。sidecar 模式 要将一个应用改成微服务架构，基本有两种方式:通过 SDK 、 Lib 等软件包的形式，在开发时引入该软件包依赖，使其与业务服务集成起来。这种方法可以与应用密切集成，提高资源利用率并且提高应用性能。但是这种方法是对代码有侵入的，受到编程语言和软件开发人员水平的限制，但当该依赖有 bug 或者需要升级时，业务代码需要重新编译和发布。以 Sidecar 的形式，在运维的时候与应用服务集成在一起。这种方式对应用服务没有侵入性，不受编程语言和开发人员水平的限制，做到了控制与逻辑分开部署。但是会增加应用延迟，并且管理和部署的复杂度会增加。好处：边车 (SideCar) 模式这种方式, 不仅对原来的应用代码零侵入，而且不限制原来应用的语言，特别适合这种异构微服务的场景！另外，以后你的边车 (SideCar) 要升级了，是可以独立升级的，不用重新打包原来的应用 坏处：服务之间的调用多了 一跳 ，性能有影响。spring-cloud-netflix-sidecarnetflix 提供的 sidecar 方案，使用 eureka 作为注册中心。 使用方式 第三方应用提供 health check 接口，返回格式：123&#123;"status":"UP"&#125;引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-sidecar&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;application.yml 增加配置12345678# 省略 eureka 配置部分# 配置异构服务sidecar: ip: localhost port: 8089 # 被包装应用提供 heath check url health-check-url: http://localhost:8089/health 启动类增加 @EnableSidecar 注解 12345678@SpringBootApplication@EnableSidecarpublic class SidecarApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SidecarApplication.class, args); &#125;&#125; 源码分析 内容很简单。主要包括：@EnableSidecar注解，表示开启 sidecar 能力 本地应用健康检查 sidecar 配置和 controller 本地应用健康检查由 handler 和 indicator 组成，可以集成 actuate 框架。sidecar controller 提供了几个工具方法：123456789101112131415@RequestMapping("/ping")public String ping() &#123; return "OK";&#125;@RequestMapping("/hosts/&#123;appName&#125;")public List&lt;ServiceInstance&gt; hosts(@PathVariable("appName") String appName) &#123; return hosts2(appName);&#125;@RequestMapping("/hosts")public List&lt;ServiceInstance&gt; hosts2(@RequestParam("appName") String appName) &#123; List&lt;ServiceInstance&gt; instances = this.discovery.getInstances(appName); return instances;&#125;spring-cloud-alibaba-sidecarspring-cloud-alibaba-sidecar 支持 nacos 和 consul 注册中心。使用方式和 netflix sidecar 基本相似。12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-sidecar&lt;/artifactId&gt; &lt;version&gt;2.2.0.RELEASE&lt;/version&gt;&lt;/dependency&gt;启动类增加 @EnableDiscoveryClient 注解。注意没有 @EnableSidecar 注解，也没有提供 controller 方法。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zuul filter 处理流程]]></title>
    <url>%2Fp%2Fzuul-filter-process-flow%2F</url>
    <content type="text"><![CDATA[顺便看下 zuul filter 的处理流程。ZuulServlet 和 ZuulServletFilterZuulServlet 和 ZuulServletFilter 的处理逻辑是相似的。ZuulServlet#service()：123456789101112131415161718192021222324252627282930313233343536public void service(javax.servlet.ServletRequest servletRequest, javax.servlet.ServletResponse servletResponse) throws ServletException, IOException &#123; try &#123; init((HttpServletRequest) servletRequest, (HttpServletResponse) servletResponse); // Marks this request as having passed through the "Zuul engine", as opposed to servlets // explicitly bound in web.xml, for which requests will not have the same data attached RequestContext context = RequestContext.getCurrentContext(); context.setZuulEngineRan(); try &#123; preRoute(); &#125; catch (ZuulException e) &#123; error(e); postRoute(); return; &#125; try &#123; route(); &#125; catch (ZuulException e) &#123; error(e); postRoute(); return; &#125; try &#123; postRoute(); &#125; catch (ZuulException e) &#123; error(e); return; &#125; &#125; catch (Throwable e) &#123; error(new ZuulException(e, 500, "UNHANDLED_EXCEPTION_" + e.getClass().getName())); &#125; finally &#123; RequestContext.getCurrentContext().unset(); &#125;&#125;ZuulServletFilter#doFilter()：123456789101112131415161718192021222324252627282930313233343536public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; try &#123; init((HttpServletRequest) servletRequest, (HttpServletResponse) servletResponse); try &#123; preRouting(); &#125; catch (ZuulException e) &#123; error(e); postRouting(); return; &#125; // Only forward onto to the chain if a zuul response is not being sent if (!RequestContext.getCurrentContext().sendZuulResponse()) &#123; filterChain.doFilter(servletRequest, servletResponse); return; &#125; try &#123; routing(); &#125; catch (ZuulException e) &#123; error(e); postRouting(); return; &#125; try &#123; postRouting(); &#125; catch (ZuulException e) &#123; error(e); return; &#125; &#125; catch (Throwable e) &#123; error(new ZuulException(e, 500, "UNCAUGHT_EXCEPTION_FROM_FILTER_" + e.getClass().getName())); &#125; finally &#123; RequestContext.getCurrentContext().unset(); &#125;&#125;当 zuul 以 filter 方式使用，则增加了 sendZuulResponse 处理逻辑。这样请求直接在 zuul server 处理完成，不会透传到后端服务。注意，即使设置了 sendZuulResponse，也要处理完当前后续 filter chain：12345// Only forward onto to the chain if a zuul response is not being sentif (!RequestContext.getCurrentContext().sendZuulResponse()) &#123; filterChain.doFilter(servletRequest, servletResponse); return;&#125;preRouting、routing、postRouting、error 是 zuul filter 的核心流程。都是调用 FilterProcessor#runFilters()。 sType 对应了定义 ZuulFilter 的filterType。1234567891011121314151617public Object runFilters(String sType) throws Throwable &#123; if (RequestContext.getCurrentContext().debugRouting()) &#123; Debug.addRoutingDebug("Invoking &#123;" + sType + "&#125; type filters"); &#125; boolean bResult = false; List&lt;ZuulFilter&gt; list = FilterLoader.getInstance().getFiltersByType(sType); if (list != null) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; ZuulFilter zuulFilter = list.get(i); Object result = processZuulFilter(zuulFilter); if (result != null &amp;&amp; result instanceof Boolean) &#123; bResult |= ((Boolean) result); &#125; &#125; &#125; return bResult;&#125;FilterProcessor#processZuulFilter() 执行 filter 方法，并且添加统计 metrics。1234// more codeZuulFilterResult result = filter.runFilter();ExecutionStatus s = result.getStatus();// more codeZuulFilter#runFilter()，熟悉的 template 模式：1234567891011121314151617181920212223public ZuulFilterResult runFilter() &#123; // 初始化结果为 ExecutionStatus.DISABLED ZuulFilterResult zr = new ZuulFilterResult(); if (!isFilterDisabled()) &#123; if (shouldFilter()) &#123; Tracer t = TracerFactory.instance().startMicroTracer("ZUUL::" + this.getClass().getSimpleName()); try &#123; // 子类实现 run 方法 Object res = run(); zr = new ZuulFilterResult(res, ExecutionStatus.SUCCESS); &#125; catch (Throwable e) &#123; t.setName("ZUUL::" + this.getClass().getSimpleName() + "failed"); zr = new ZuulFilterResult(ExecutionStatus.FAILED); zr.setException(e); &#125; finally &#123; t.stopAndLog(); &#125; &#125; else &#123; zr = new ZuulFilterResult(ExecutionStatus.SKIPPED); &#125; &#125; return zr;&#125;ZuulFilterzuul 1zuul 2 默认 zuul filter 顺序如下，图片来自网上 ServletDetectionFilter 用来检测当前请求是通过 Spring 的 DispatcherServlet 处理运行的，还是通过 Zuu1Servlet 来处理运行的 123456789101112131415161718public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); if (!(request instanceof HttpServletRequestWrapper) &amp;&amp; isDispatcherServletRequest(request)) &#123; ctx.set(IS_DISPATCHER_SERVLET_REQUEST_KEY, true); &#125; else &#123; ctx.set(IS_DISPATCHER_SERVLET_REQUEST_KEY, false); &#125; return null;&#125;private boolean isDispatcherServletRequest(HttpServletRequest request) &#123; return request.getAttribute( DispatcherServlet.WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null;&#125; 像上传文件之类的操作，可以直接交给 ZuulServlet 处理。相关的 filter 还有 FormBodyWrapperFilter。PreDecorationFilter转发前的前置处理，根据 RouteLocator 获取路由类型，再向 RequestContext 填充辅助标记。zuul 支持以下类型：http、httpsforwardserviceId工作流程：如果找不到路由，则设置为 forward 类型 找到路由，则：过滤敏感 header判断请求能否重试 如果是 http、https 请求，就设置 host 和 origin如果是 forward 类型，就设置规范化的 forward.to 标记 否则，是 serviceId 类型 如果是 proxy 请求，增加 x-forwarded-for、remote-addr 字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); final String requestURI = this.urlPathHelper .getPathWithinApplication(ctx.getRequest()); Route route = this.routeLocator.getMatchingRoute(requestURI); if (route != null) &#123; String location = route.getLocation(); if (location != null) &#123; ctx.put(REQUEST_URI_KEY, route.getPath()); ctx.put(PROXY_KEY, route.getId()); if (!route.isCustomSensitiveHeaders()) &#123; this.proxyRequestHelper.addIgnoredHeaders( this.properties.getSensitiveHeaders().toArray(new String[0])); &#125; else &#123; this.proxyRequestHelper.addIgnoredHeaders( route.getSensitiveHeaders().toArray(new String[0])); &#125; if (route.getRetryable() != null) &#123; ctx.put(RETRYABLE_KEY, route.getRetryable()); &#125; if (location.startsWith(HTTP_SCHEME + ":") || location.startsWith(HTTPS_SCHEME + ":")) &#123; ctx.setRouteHost(getUrl(location)); ctx.addOriginResponseHeader(SERVICE_HEADER, location); &#125; else if (location.startsWith(FORWARD_LOCATION_PREFIX)) &#123; ctx.set(FORWARD_TO_KEY, StringUtils.cleanPath( location.substring(FORWARD_LOCATION_PREFIX.length()) + route.getPath())); ctx.setRouteHost(null); return null; &#125; else &#123; // set serviceId for use in filters.route.RibbonRequest ctx.set(SERVICE_ID_KEY, location); ctx.setRouteHost(null); ctx.addOriginResponseHeader(SERVICE_ID_HEADER, location); &#125; if (this.properties.isAddProxyHeaders()) &#123; addProxyHeaders(ctx, route); String xforwardedfor = ctx.getRequest() .getHeader(X_FORWARDED_FOR_HEADER); String remoteAddr = ctx.getRequest().getRemoteAddr(); if (xforwardedfor == null) &#123; xforwardedfor = remoteAddr; &#125; else if (!xforwardedfor.contains(remoteAddr)) &#123; // Prevent duplicates xforwardedfor += "," + remoteAddr; &#125; ctx.addZuulRequestHeader(X_FORWARDED_FOR_HEADER, xforwardedfor); &#125; if (this.properties.isAddHostHeader()) &#123; ctx.addZuulRequestHeader(HttpHeaders.HOST, toHostHeader(ctx.getRequest())); &#125; &#125; &#125; else &#123; log.warn("No route found for uri:" + requestURI); String forwardURI = getForwardUri(requestURI); ctx.set(FORWARD_TO_KEY, forwardURI); &#125; return null;&#125; 对于 forward，通过 url 中使用 forward 来指定需要跳转的服务器资源路径（跳转到网关层）。来自官网的例子：1234567891011121314zuul: routes: first: path: /first/** url: http://first.example.com second: path: /second/** url: forward:/second third: path: /third/** url: forward:/3rd legacy: path: /** url: http://legacy.example.com则 /second/** 和/third/**跳转到网关层处理。RibbonRoutingFilter只处理 serviceId 转发，底层使用 ribbon、hystrix 和可配置的 http 客户端发送请求。其中使用工厂模式，RibbonCommandFactory 支持 okhttp、httpclient、restclient 等 http 客户端。SimpleHostRoutingFilter只处理具体的 url 转发，即 RequestContext 指定了 route host。SendForwardFilter只处理 forward 请求。使用了 servlet 规范的 RequestDispatcher 实现转发。12345678910111213141516171819public Object run() &#123; try &#123; RequestContext ctx = RequestContext.getCurrentContext(); String path = (String) ctx.get(FORWARD_TO_KEY); RequestDispatcher dispatcher = ctx.getRequest().getRequestDispatcher(path); if (dispatcher != null) &#123; // 已经转发过的标记 ctx.set(SEND_FORWARD_FILTER_RAN, true); if (!ctx.getResponse().isCommitted()) &#123; dispatcher.forward(ctx.getRequest(), ctx.getResponse()); ctx.getResponse().flushBuffer(); &#125; &#125; &#125; catch (Exception ex) &#123; ReflectionUtils.rethrowRuntimeException(ex); &#125; return null;&#125;SendErrorFilter 处理异常。把异常转发到网关处理（又是 RequestDispatcher）。123456789101112131415161718192021222324252627282930public Object run() &#123; try &#123; RequestContext ctx = RequestContext.getCurrentContext(); ExceptionHolder exception = findZuulException(ctx.getThrowable()); HttpServletRequest request = ctx.getRequest(); request.setAttribute("javax.servlet.error.status_code", exception.getStatusCode()); log.warn("Error during filtering", exception.getThrowable()); request.setAttribute("javax.servlet.error.exception", exception.getThrowable()); if (StringUtils.hasText(exception.getErrorCause())) &#123; request.setAttribute("javax.servlet.error.message", exception.getErrorCause()); &#125; // RequestDispatcher 是 servlet 规范定义的 RequestDispatcher dispatcher = request.getRequestDispatcher(this.errorPath); if (dispatcher != null) &#123; ctx.set(SEND_ERROR_FILTER_RAN, true); if (!ctx.getResponse().isCommitted()) &#123; ctx.setResponseStatusCode(exception.getStatusCode()); // 转发到网关层处理 dispatcher.forward(request, ctx.getResponse()); &#125; &#125; &#125; catch (Exception ex) &#123; ReflectionUtils.rethrowRuntimeException(ex); &#125; return null;&#125;SendResponseFilter 该过滤器会检查请求上下文中是否包含请求响应相关的头信息、响应数据流或是响应体，只要包含其中一个的时候执行处理逻辑。1234567public boolean shouldFilter() &#123; RequestContext context = RequestContext.getCurrentContext(); return context.getThrowable() == null &amp;&amp; (!context.getZuulResponseHeaders().isEmpty() || context.getResponseDataStream() != null || context.getResponseBody() != null);&#125;]]></content>
      <categories>
        <category>zuul</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>zuul</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zuul 和 spring 集成分析]]></title>
    <url>%2Fp%2Fzuul-spring-integration%2F</url>
    <content type="text"><![CDATA[zuul 和 spring 集成分析。zuul 和 spring 集成 Zuul is implemented as a Servlet. For the general cases, Zuul is embedded into the Spring Dispatch mechanism.This lets Spring MVC be in control of the routing. In this case, Zuul buffers requests.If there is a need to go through Zuul without buffering requests (for example, for large file uploads), the Servlet is also installed outside of the Spring Dispatcher.By default, the servlet has an address of /zuul. This path can be changed with the zuul.servlet-path property.zuul 以 servlet 形式设计，集成到 spring 的 DispatchServlet。 通过 handler mapping 形式，DispatchServlet 把请求分发到 zuul 处理，依赖 handler mapping。123456789101112protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); // handler 映射 initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context);&#125; 对于大文件上传这种服务，如果经过 DispatcherServlet，会影响性能。因为 DispatcherServlet 为了方便后续处理流程使用，会将 multipart/form 请求根据 RFC1867 规则进行统一分析处理，并且返回 MultipartHttpServletRequest 实例，通过它可以获取 file 和其他参数。网关通常来说不需要获取 MultipartHttpServletRequest，特别是大文件，这样会比较影响性能，可以直接用 ZuulServlet 处理，参见 ZuulServerAutoConfiguration：123456789101112131415161718192021222324252627282930@Bean@ConditionalOnMissingBean(name = "zuulServlet")@ConditionalOnProperty(name = "zuul.use-filter", havingValue = "false", matchIfMissing = true)public ServletRegistrationBean zuulServlet() &#123; ServletRegistrationBean&lt;ZuulServlet&gt; servlet = new ServletRegistrationBean&lt;&gt;( new ZuulServlet(), this.zuulProperties.getServletPattern()); // The whole point of exposing this servlet is to provide a route that doesn't // buffer requests. servlet.addInitParameter("buffer-requests", "false"); return servlet;&#125;@Bean@ConditionalOnMissingBean(name = "zuulServletFilter")@ConditionalOnProperty(name = "zuul.use-filter", havingValue = "true", matchIfMissing = false)public FilterRegistrationBean zuulServletFilter() &#123; final FilterRegistrationBean&lt;ZuulServletFilter&gt; filterRegistration = new FilterRegistrationBean&lt;&gt;(); // zuul.servletPath filterRegistration.setUrlPatterns( Collections.singleton(this.zuulProperties.getServletPattern())); filterRegistration.setFilter(new ZuulServletFilter()); // 优先级最低哦 filterRegistration.setOrder(Ordered.LOWEST_PRECEDENCE); // The whole point of exposing this servlet is to provide a route that doesn't // buffer requests. filterRegistration.addInitParameter("buffer-requests", "false"); return filterRegistration;&#125;zuul.use-filter 是控制和 spring 整合的方式：true： 默认配置 ，使用 zuulServletFilter，先创建为 FilterRegistrationBean，再注册到 spring 容器false：使用 zuulServlet，先创建为 ServletRegistrationBean，再注册到 servlet 容器 值得留意的是 buffer-requests。 不管是 ZuulServletFilter 还是 ZuulServlet，都由 ZuulRunner 对请求初始化。bufferRequests 决定是否包装 request 对象。1234567891011121314151617/** * * @param bufferRequests - whether to wrap the ServletRequest in HttpServletRequestWrapper and buffer the body. */public ZuulRunner(boolean bufferRequests) &#123; this.bufferRequests = bufferRequests;&#125;public void init(HttpServletRequest servletRequest, HttpServletResponse servletResponse) &#123; RequestContext ctx = RequestContext.getCurrentContext(); if (bufferRequests) &#123; ctx.setRequest(new HttpServletRequestWrapper(servletRequest)); &#125; else &#123; ctx.setRequest(servletRequest); &#125; ctx.setResponse(new HttpServletResponseWrapper(servletResponse));&#125; 前面讲到大文件请求不应该包装。FormBodyWrapperFilter 不对 form 和 multipart 包装：application/x-www-form-urlencodedmultipart/form-data1234567891011121314151617181920public boolean shouldFilter() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); String contentType = request.getContentType(); // Don't use this filter on GET method if (contentType == null) &#123; return false; &#125; // Only use this filter for form data and only for multipart data in a // DispatcherServlet handler try &#123; MediaType mediaType = MediaType.valueOf(contentType); return MediaType.APPLICATION_FORM_URLENCODED.includes(mediaType) || (isDispatcherServletRequest(request) &amp;&amp; MediaType.MULTIPART_FORM_DATA.includes(mediaType)); &#125; catch (InvalidMediaTypeException ex) &#123; return false; &#125;&#125;画了一张图，整理上面核心类的关系和流程：zuul 本地路由 把流量转发到本地 zuul 自定义的 controller 处理。使用 forward 路由：1234567zuul: sensitive-headers: routes: # 本地路由 login-service: path: /rest_login/** url: forward:/rest_login]]></content>
      <categories>
        <category>zuul</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>zuul</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring security 系列 5：CORS]]></title>
    <url>%2Fp%2Fspring-web-security-p5%2F</url>
    <content type="text"><![CDATA[CORS filter 源码分析。CorsConfigurationCorsConfiguration 是配置类。值得注意的是 checkOrigin 方法：12345678910111213141516171819202122public String checkOrigin(@Nullable String requestOrigin) &#123; if (!StringUtils.hasText(requestOrigin)) &#123; return null; &#125; if (ObjectUtils.isEmpty(this.allowedOrigins)) &#123; return null; &#125; if (this.allowedOrigins.contains(ALL)) &#123; if (this.allowCredentials != Boolean.TRUE) &#123; return ALL; &#125; else &#123; return requestOrigin; &#125; &#125; for (String allowedOrigin : this.allowedOrigins) &#123; if (requestOrigin.equalsIgnoreCase(allowedOrigin)) &#123; return requestOrigin; &#125; &#125; return null;&#125;当允许的 Origin 为 *，且 credentials 为true，则返回当前请求的 Origin、而不是*。CorsFilterCorsProcessor 是 CORS 处理器抽象，DefaultCorsProcessor。CorsFilter 是 CORS 的过滤器，调用 CorsProcessor 进行实际操作。123456789protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; CorsConfiguration corsConfiguration = this.configSource.getCorsConfiguration(request); boolean isValid = this.processor.processRequest(corsConfiguration, request, response); if (!isValid || CorsUtils.isPreFlightRequest(request)) &#123; return; &#125; filterChain.doFilter(request, response);&#125; 在 w3c 标准中，浏览器对复杂跨域请求，首先发送 options 请求判断能否执行。对应的是CorsUtils#isPreFlightRequest()12345public static boolean isPreFlightRequest(HttpServletRequest request) &#123; return (HttpMethod.OPTIONS.matches(request.getMethod()) &amp;&amp; request.getHeader(HttpHeaders.ORIGIN) != null &amp;&amp; request.getHeader(HttpHeaders.ACCESS_CONTROL_REQUEST_METHOD) != null);&#125;FilterComparator 定义了 filter 顺序。1234567891011121314151617final class FilterComparator implements Comparator&lt;Filter&gt;, Serializable &#123; private static final int INITIAL_ORDER = 100; private static final int ORDER_STEP = 100; private final Map&lt;String, Integer&gt; filterToOrder = new HashMap&lt;&gt;(); FilterComparator() &#123; Step order = new Step(INITIAL_ORDER, ORDER_STEP); put(ChannelProcessingFilter.class, order.next()); put(ConcurrentSessionFilter.class, order.next()); put(WebAsyncManagerIntegrationFilter.class, order.next()); put(SecurityContextPersistenceFilter.class, order.next()); put(HeaderWriterFilter.class, order.next()); // CORS filter here put(CorsFilter.class, order.next()); put(CsrfFilter.class, order.next()); put(LogoutFilter.class, order.next()); // more codes]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zuul 向后端服务传递 Principal]]></title>
    <url>%2Fp%2Fzuul-pass-principal-to-backend%2F</url>
    <content type="text"><![CDATA[在网关层对接 cas client 之后，后端服务不能正常获取当前登录用户信息。getUserPrincipal 和 getRemoteUserHttpServletRequest定义了获取登录用户的方法。123456789101112131415161718192021222324/** * Returns a &lt;code&gt;java.security.Principal&lt;/code&gt; object containing the name * of the current authenticated user. If the user has not been * authenticated, the method returns &lt;code&gt;null&lt;/code&gt;. * * @return a &lt;code&gt;java.security.Principal&lt;/code&gt; containing the name of the * user making this request; &lt;code&gt;null&lt;/code&gt; if the user has not * been authenticated */public java.security.Principal getUserPrincipal();/** * Returns the login of the user making this request, if the user has been * authenticated, or &lt;code&gt;null&lt;/code&gt; if the user has not been * authenticated. Whether the user name is sent with each subsequent request * depends on the browser and type of authentication. Same as the value of * the CGI variable REMOTE_USER. * * @return a &lt;code&gt;String&lt;/code&gt; specifying the login of the user making * this request, or &lt;code&gt;null&lt;/code&gt; if the user login is not known */public String getRemoteUser();不同的 servlet 容器实现会有差异。tomcat 对应实现类在 org.apache.catalina.connector.Request：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public Principal getUserPrincipal() &#123; if (userPrincipal instanceof TomcatPrincipal) &#123; GSSCredential gssCredential = ((TomcatPrincipal) userPrincipal).getGssCredential(); if (gssCredential != null) &#123; int left = -1; try &#123; left = gssCredential.getRemainingLifetime(); &#125; catch (GSSException e) &#123; log.warn(sm.getString("coyoteRequest.gssLifetimeFail", userPrincipal.getName()), e); &#125; if (left == 0) &#123; // GSS credential has expired. Need to re-authenticate. try &#123; logout(); &#125; catch (ServletException e) &#123; // Should never happen (no code called by logout() // throws a ServletException &#125; return null; &#125; &#125; return ((TomcatPrincipal) userPrincipal).getUserPrincipal(); &#125; return userPrincipal;&#125;public String getRemoteUser() &#123; if (userPrincipal == null) &#123; return null; &#125; return userPrincipal.getName();&#125;public void setUserPrincipal(final Principal principal) &#123; if (Globals.IS_SECURITY_ENABLED &amp;&amp; principal != null) &#123; if (subject == null) &#123; final HttpSession session = getSession(false); if (session == null) &#123; // Cache the subject in the request subject = newSubject(principal); &#125; else &#123; // Cache the subject in the request and the session subject = (Subject) session.getAttribute(Globals.SUBJECT_ATTR); if (subject == null) &#123; subject = newSubject(principal); session.setAttribute(Globals.SUBJECT_ATTR, subject); &#125; else &#123; subject.getPrincipals().add(principal); &#125; &#125; &#125; else &#123; subject.getPrincipals().add(principal); &#125; &#125; userPrincipal = principal;&#125; 可见，remoteUser 是从 userPrincipal#getName() 获取。思路：zuul filter 传递 principal 到某个 header后端服务解析该 header，得到登录的用户名 后端服务如果要更新 userPrincipal，则需要适配不同 servlet 容器，侵入性太强。更好的做法是，使用 filter + 自定义 request wrapper + 重载 getRemoteUser() 的方式。zuul filter12345678910111213141516171819202122232425262728293031323334353637383940/** * 向后端服务传递登录用户 */@Componentpublic class TransmitPrincipalFilter extends ZuulFilter &#123; public static final String PRINCIPAL_HEADER = "X-GATEWAY-PRINCIPAL-NAME"; @Override public String filterType() &#123; return FilterConstants.PRE_TYPE; &#125; /** * must after login filter * * @return */ @Override public int filterOrder() &#123; return 2000; &#125; @Override public boolean shouldFilter() &#123; return RequestContext.getCurrentContext().getRequest().getUserPrincipal() != null; &#125; @Override public Object run() throws ZuulException &#123; RequestContext ctx = RequestContext.getCurrentContext(); Principal principal = ctx.getRequest().getUserPrincipal(); if (principal != null) &#123; ctx.addZuulRequestHeader(PRINCIPAL_HEADER, principal.getName()); &#125; return null; &#125;&#125; 后端 filter 和 request wrapper 就不贴了。]]></content>
      <categories>
        <category>zuul</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>zuul</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring security 系列 4：Authentication 相关]]></title>
    <url>%2Fp%2Fspring-web-security-p4%2F</url>
    <content type="text"><![CDATA[AuthenticationEntryPoint 是不同类型验证方式的抽象入口。ExceptionTranslationFilter 根据 AuthenticationException 或者 AccessDeniedException 触发登录流程。AbstractAuthenticationProcessingFilter 和它的子类处理登录请求。AuthenticationProvider 用于处理 Authentication 请求，ProviderManager 是其中一个实现。AuthenticationEntryPoint 和 ExceptionTranslationFilterAuthenticationEntryPoint 是触发认证请求的入口，由 ExceptionTranslationFilter 使用。ExceptionTranslationFilter 会对 AuthenticationException，没有验证AccessDeniedException，没有访问权限 开始验证流程。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private void handleSpringSecurityException(HttpServletRequest request, HttpServletResponse response, FilterChain chain, RuntimeException exception) throws IOException, ServletException &#123; if (exception instanceof AuthenticationException) &#123; logger.debug( "Authentication exception occurred; redirecting to authentication entry point", exception); sendStartAuthentication(request, response, chain, (AuthenticationException) exception); &#125; else if (exception instanceof AccessDeniedException) &#123; Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); if (authenticationTrustResolver.isAnonymous(authentication) || authenticationTrustResolver.isRememberMe(authentication)) &#123; logger.debug( "Access is denied (user is" + (authenticationTrustResolver.isAnonymous(authentication) ? "anonymous" : "not fully authenticated") + "); redirecting to authentication entry point", exception); sendStartAuthentication( request, response, chain, new InsufficientAuthenticationException( messages.getMessage( "ExceptionTranslationFilter.insufficientAuthentication", "Full authentication is required to access this resource"))); &#125; else &#123; logger.debug( "Access is denied (user is not anonymous); delegating to AccessDeniedHandler", exception); accessDeniedHandler.handle(request, response, (AccessDeniedException) exception); &#125; &#125;&#125;protected void sendStartAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain, AuthenticationException reason) throws ServletException, IOException &#123; // SEC-112: Clear the SecurityContextHolder's Authentication, as the // existing Authentication is no longer considered valid SecurityContextHolder.getContext().setAuthentication(null); requestCache.saveRequest(request, response); logger.debug("Calling Authentication entry point."); authenticationEntryPoint.commence(request, response, reason);&#125;ExceptionTranslationFilter 会在目标 url 调用 commence 方法之前，向 HttpSession 增加属性 AbstractAuthenticationProcessingFilter.SPRING_SECURITY_SAVED_REQUEST_KEY。 实现类应该修改 ServletResponse 的 headers，用于开始验证流程。12345public interface AuthenticationEntryPoint &#123; void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException;&#125; 以基本的 http 验证为例：12345678910public class BasicAuthenticationEntryPoint implements AuthenticationEntryPoint, InitializingBean &#123; public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException &#123; response.addHeader("WWW-Authenticate", "Basic realm=\"" + realmName + "\""); response.sendError(HttpStatus.UNAUTHORIZED.value(), HttpStatus.UNAUTHORIZED.getReasonPhrase()); &#125;&#125;如果是 cas 协议，则发送重定向，跳转至登录页 123456789101112131415public class CasAuthenticationEntryPoint implements AuthenticationEntryPoint, InitializingBean &#123; public final void commence(final HttpServletRequest servletRequest, final HttpServletResponse response, final AuthenticationException authenticationException) throws IOException &#123; final String urlEncodedService = createServiceUrl(servletRequest, response); final String redirectUrl = createRedirectUrl(urlEncodedService); preCommence(servletRequest, response); response.sendRedirect(redirectUrl); &#125;&#125; 实际上会搭配 DelegatingAuthenticationEntryPoint 使用。RequestMatcher 使用模式匹配，找到不同请求使用的 AuthenticationEntryPoint。这样方便支持多种验证模式。12345678910111213141516171819202122232425262728public class DelegatingAuthenticationEntryPoint implements AuthenticationEntryPoint, InitializingBean &#123; public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; for (RequestMatcher requestMatcher : entryPoints.keySet()) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Trying to match using " + requestMatcher); &#125; if (requestMatcher.matches(request)) &#123; AuthenticationEntryPoint entryPoint = entryPoints.get(requestMatcher); if (logger.isDebugEnabled()) &#123; logger.debug("Match found! Executing " + entryPoint); &#125; entryPoint.commence(request, response, authException); return; &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("No match found. Using default entry point " + defaultEntryPoint); &#125; // No EntryPoint matched, use defaultEntryPoint defaultEntryPoint.commence(request, response, authException); &#125;&#125;PrincipalPrincipal 是 java.security 中定义的抽象，代表登录实体。123public interface Principal &#123; public String getName();AuthenticationAuthentication 以 token 的形式，代表认证请求、或者已经认证的 principal。Represents the token for an authentication request or for an authenticated principal once the request has been processed by the AuthenticationManager.authenticate(Authentication) method.123456789101112131415161718192021222324252627282930public interface Authentication extends Principal, Serializable &#123; /** * 由 AuthenticationManager 指定的授权 */ Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); /** * The credentials that prove the principal is correct. This is usually a password, * but could be anything relevant to the &lt;code&gt;AuthenticationManager&lt;/code&gt;. Callers * are expected to populate the credentials. * * @return the credentials that prove the identity of the &lt;code&gt;Principal&lt;/code&gt; */ Object getCredentials(); /** * Stores additional details about the authentication request. These might be an IP * address, certificate serial number etc. * * @return additional details about the authentication request, or &lt;code&gt;null&lt;/code&gt; * if not used */ Object getDetails(); Object getPrincipal(); boolean isAuthenticated(); void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException;&#125;Authentication 有不同的实现类 常见的有 UsernamePasswordAuthenticationToken：1234567891011public class UsernamePasswordAuthenticationToken extends AbstractAuthenticationToken &#123; private final Object principal; private Object credentials; public UsernamePasswordAuthenticationToken(Object principal, Object credentials) &#123; super(null); this.principal = principal; this.credentials = credentials; // 未验证，因此设置未 false setAuthenticated(false); &#125;紧密关联的是 AuthenticationManager。AuthenticationManagerAuthenticationManager 处理怎么验证 Authentication 对象。123456public interface AuthenticationManager &#123; // 输入 authentication 对象，计算完整的 authentication，包括授权（authorities） Authentication authenticate(Authentication authentication) throws AuthenticationException;&#125;AuthenticationProvider 和 ProviderManagerAuthenticationProvider 用于处理 Authentication 请求。ProviderManager 是 AuthenticationManager 的一个实现，顺序调用 AuthenticationProvider 列表，直到其中一个返回非 null。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class ProviderManager implements AuthenticationManager, MessageSourceAware, InitializingBean &#123; // 省略其他成员 private List&lt;AuthenticationProvider&gt; providers = Collections.emptyList(); private AuthenticationManager parent; public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; AuthenticationException parentException = null; Authentication result = null; Authentication parentResult = null; boolean debug = logger.isDebugEnabled(); for (AuthenticationProvider provider : getProviders()) &#123; if (!provider.supports(toTest)) &#123; continue; &#125; if (debug) &#123; logger.debug("Authentication attempt using " + provider.getClass().getName()); &#125; try &#123; result = provider.authenticate(authentication); if (result != null) &#123; copyDetails(authentication, result); break; &#125; &#125; catch (AccountStatusException | InternalAuthenticationServiceException e) &#123; prepareException(e, authentication); // SEC-546: Avoid polling additional providers if auth failure is due to // invalid account status throw e; &#125; catch (AuthenticationException e) &#123; lastException = e; &#125; &#125; if (result == null &amp;&amp; parent != null) &#123; // Allow the parent to try. try &#123; result = parentResult = parent.authenticate(authentication); &#125; catch (ProviderNotFoundException e) &#123; // ignore as we will throw below if no other exception occurred prior to // calling parent and the parent // may throw ProviderNotFound even though a provider in the child already // handled the request &#125; catch (AuthenticationException e) &#123; lastException = parentException = e; &#125; &#125; if (result != null) &#123; if (eraseCredentialsAfterAuthentication &amp;&amp; (result instanceof CredentialsContainer)) &#123; // Authentication is complete. Remove credentials and other secret data // from authentication ((CredentialsContainer) result).eraseCredentials(); &#125; // If the parent AuthenticationManager was attempted and successful then it will publish an AuthenticationSuccessEvent // This check prevents a duplicate AuthenticationSuccessEvent if the parent AuthenticationManager already published it if (parentResult == null) &#123; eventPublisher.publishAuthenticationSuccess(result); &#125; return result; &#125; // Parent was null, or didn't authenticate (or throw an exception). if (lastException == null) &#123; lastException = new ProviderNotFoundException(messages.getMessage( "ProviderManager.providerNotFound", new Object[] &#123; toTest.getName() &#125;, "No AuthenticationProvider found for &#123;0&#125;")); &#125; // If the parent AuthenticationManager was attempted and failed then it will publish an AbstractAuthenticationFailureEvent // This check prevents a duplicate AbstractAuthenticationFailureEvent if the parent AuthenticationManager already published it if (parentException == null) &#123; prepareException(lastException, authentication); &#125; throw lastException; &#125; 个人认为 AccountStatusException 这个抽象很好，在账号状态异常之后，避免后续 provider 继续检查。AuthenticationManagerBuilder 负责构建 ProviderManager12345678910111213141516protected ProviderManager performBuild() throws Exception &#123; if (!isConfigured()) &#123; logger.debug("No authenticationProviders and no parentAuthenticationManager defined. Returning null."); return null; &#125; ProviderManager providerManager = new ProviderManager(authenticationProviders, parentAuthenticationManager); if (eraseCredentials != null) &#123; providerManager.setEraseCredentialsAfterAuthentication(eraseCredentials); &#125; if (eventPublisher != null) &#123; providerManager.setAuthenticationEventPublisher(eventPublisher); &#125; providerManager = postProcess(providerManager); return providerManager;&#125;AbstractAuthenticationProcessingFilterAbstractAuthenticationProcessingFilter 是负责用户验证的 filter。Spring Security 提供了几个实现类：CasAuthenticationFilterOAuth2LoginAuthenticationFilterOpenIDAuthenticationFilterUsernamePasswordAuthenticationFilter作为抽象类，定义了用户验证的流程。提供几个子类可以覆盖的行为（典型的 template 模式）：requiresAuthenticationattemptAuthenticationunsuccessfulAuthenticationsuccessfulAuthentication123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public abstract class AbstractAuthenticationProcessingFilter extends GenericFilterBean implements ApplicationEventPublisherAware, MessageSourceAware &#123; private AuthenticationManager authenticationManager; public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (!requiresAuthentication(request, response)) &#123; chain.doFilter(request, response); return; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Request is to process authentication"); &#125; Authentication authResult; try &#123; authResult = attemptAuthentication(request, response); if (authResult == null) &#123; // return immediately as subclass has indicated that it hasn't completed // authentication return; &#125; sessionStrategy.onAuthentication(authResult, request, response); &#125; catch (InternalAuthenticationServiceException failed) &#123; logger.error( "An internal error occurred while trying to authenticate the user.", failed); unsuccessfulAuthentication(request, response, failed); return; &#125; catch (AuthenticationException failed) &#123; // Authentication failed unsuccessfulAuthentication(request, response, failed); return; &#125; // Authentication success if (continueChainBeforeSuccessfulAuthentication) &#123; chain.doFilter(request, response); &#125; successfulAuthentication(request, response, chain, authResult); &#125;注意，不管验证成功还是失败，都要更新 SecurityContext。验证成功 1SecurityContextHolder.getContext().setAuthentication(authResult); 验证失败1SecurityContextHolder.clearContext();AuthenticationConfigBuilderAuthenticationConfigBuilder 负责创建验证相关的 filter。123456789101112131415161718192021AuthenticationConfigBuilder(Element element, boolean forceAutoConfig, ParserContext pc, SessionCreationPolicy sessionPolicy, BeanReference requestCache, BeanReference authenticationManager, BeanReference sessionStrategy, BeanReference portMapper, BeanReference portResolver, BeanMetadataElement csrfLogoutHandler) &#123; // more codes createAnonymousFilter(); createRememberMeFilter(authenticationManager); createBasicFilter(authenticationManager); createBearerTokenAuthenticationFilter(authenticationManager); createFormLoginFilter(sessionStrategy, authenticationManager); createOAuth2LoginFilter(sessionStrategy, authenticationManager); createOAuth2ClientFilter(requestCache, authenticationManager); createOpenIDLoginFilter(sessionStrategy, authenticationManager); createX509Filter(authenticationManager); createJeeFilter(authenticationManager); createLogoutFilter(); createLoginPageFilterIfNeeded(); createUserDetailsServiceFactory(); createExceptionTranslationFilter();&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring security 系列 3：SecurityContext]]></title>
    <url>%2Fp%2Fspring-web-security-p3%2F</url>
    <content type="text"><![CDATA[了解 request 的安全上下文信息。SecurityContext 和 SecurityContextHolderSecurityContext 抽象是当前线程执行所需要的最小安全信息，提供了 Authentication 对象入口。123456public interface SecurityContext extends Serializable &#123; Authentication getAuthentication(); void setAuthentication(Authentication authentication);&#125;SecurityContext 保存在 SecurityContextHolder。123456789101112public class SecurityContextHolder &#123; public static final String MODE_THREADLOCAL = "MODE_THREADLOCAL"; public static final String MODE_INHERITABLETHREADLOCAL = "MODE_INHERITABLETHREADLOCAL"; public static final String MODE_GLOBAL = "MODE_GLOBAL"; public static final String SYSTEM_PROPERTY = "spring.security.strategy"; private static String strategyName = System.getProperty(SYSTEM_PROPERTY); private static SecurityContextHolderStrategy strategy; private static int initializeCount = 0; static &#123; initialize(); &#125;SecurityContextHolder 核心是怎样存储 SecurityContext。使用策略模式，交给 SecurityContextHolderStrategy 实现。SecurityContextHolderStrategy 有 3 种内置实现：ThreadLocalSecurityContextHolderStrategyGlobalSecurityContextHolderStrategyInheritableThreadLocalSecurityContextHolderStrategy很自然的一个问题，SecurityContextHolder 是怎么产生的？答案是 SecurityContextPersistenceFilter。SecurityContextPersistenceFilterSecurityContextPersistenceFilter 负责在 request 处理之前，填充 SecurityContextHolder；处理 request 之后，清理 SecurityContextHolder。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class SecurityContextPersistenceFilter extends GenericFilterBean &#123; public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; if (request.getAttribute(FILTER_APPLIED) != null) &#123; // ensure that filter is only applied once per request chain.doFilter(request, response); return; &#125; final boolean debug = logger.isDebugEnabled(); request.setAttribute(FILTER_APPLIED, Boolean.TRUE); if (forceEagerSessionCreation) &#123; HttpSession session = request.getSession(); if (debug &amp;&amp; session.isNew()) &#123; logger.debug("Eagerly created session:" + session.getId()); &#125; &#125; HttpRequestResponseHolder holder = new HttpRequestResponseHolder(request, response); SecurityContext contextBeforeChainExecution = repo.loadContext(holder); try &#123; // 在 request 处理之前填充 SecurityContextHolder SecurityContextHolder.setContext(contextBeforeChainExecution); // 继续 filter chain 处理 chain.doFilter(holder.getRequest(), holder.getResponse()); &#125; finally &#123; SecurityContext contextAfterChainExecution = SecurityContextHolder .getContext(); // Crucial removal of SecurityContextHolder contents - do this before anything // else. // request 处理之后清理 SecurityContextHolder SecurityContextHolder.clearContext(); repo.saveContext(contextAfterChainExecution, holder.getRequest(), holder.getResponse()); request.removeAttribute(FILTER_APPLIED); if (debug) &#123; logger.debug("SecurityContextHolder now cleared, as request processing completed"); &#125; &#125; &#125;&#125;SecurityContextRepositorySecurityContextRepository 负责抽象 SecurityContext 的存储。 主要实现是 HttpSessionSecurityContextRepository。12345678910111213141516171819202122public SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder) &#123; HttpServletRequest request = requestResponseHolder.getRequest(); HttpServletResponse response = requestResponseHolder.getResponse(); HttpSession httpSession = request.getSession(false); SecurityContext context = readSecurityContextFromSession(httpSession); if (context == null) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("No SecurityContext was available from the HttpSession:" + httpSession + "." + "A new one will be created."); &#125; context = generateNewContext(); &#125; SaveToSessionResponseWrapper wrappedResponse = new SaveToSessionResponseWrapper( response, request, httpSession != null, context); requestResponseHolder.setResponse(wrappedResponse); requestResponseHolder.setRequest(new SaveToSessionRequestWrapper( request, wrappedResponse)); return context;&#125;SaveToSessionResponseWrapper 需要注意 saveContext()，这里有些细节处理。123456789101112131415161718192021222324252627282930313233343536protected void saveContext(SecurityContext context) &#123; final Authentication authentication = context.getAuthentication(); HttpSession httpSession = request.getSession(false); // See SEC-776 if (authentication == null || trustResolver.isAnonymous(authentication)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("SecurityContext is empty or contents are anonymous - context will not be stored in HttpSession."); &#125; if (httpSession != null &amp;&amp; authBeforeExecution != null) &#123; // SEC-1587 A non-anonymous context may still be in the session // SEC-1735 remove if the contextBeforeExecution was not anonymous httpSession.removeAttribute(springSecurityContextKey); &#125; return; &#125; if (httpSession == null) &#123; httpSession = createNewSessionIfAllowed(context); &#125; // If HttpSession exists, store current SecurityContext but only if it has // actually changed in this thread (see SEC-37, SEC-1307, SEC-1528) if (httpSession != null) &#123; // We may have a new session, so check also whether the context attribute // is set SEC-1561 if (contextChanged(context) || httpSession.getAttribute(springSecurityContextKey) == null) &#123; httpSession.setAttribute(springSecurityContextKey, context); if (logger.isDebugEnabled()) &#123; logger.debug("SecurityContext'" + context + "'stored to HttpSession:'" + httpSession); &#125; &#125; &#125;&#125;SecurityContextConfigurer和 SecurityContext 相关的配置，都由 SecurityContextConfigurer 处理。123456789101112131415161718192021222324public final class SecurityContextConfigurer&lt;H extends HttpSecurityBuilder&lt;H&gt;&gt; extends AbstractHttpConfigurer&lt;SecurityContextConfigurer&lt;H&gt;, H&gt; &#123; public void configure(H http) &#123; SecurityContextRepository securityContextRepository = http .getSharedObject(SecurityContextRepository.class); if (securityContextRepository == null) &#123; securityContextRepository = new HttpSessionSecurityContextRepository(); &#125; SecurityContextPersistenceFilter securityContextFilter = new SecurityContextPersistenceFilter( securityContextRepository); SessionManagementConfigurer&lt;?&gt; sessionManagement = http .getConfigurer(SessionManagementConfigurer.class); SessionCreationPolicy sessionCreationPolicy = sessionManagement == null ? null : sessionManagement.getSessionCreationPolicy(); if (SessionCreationPolicy.ALWAYS == sessionCreationPolicy) &#123; securityContextFilter.setForceEagerSessionCreation(true); &#125; securityContextFilter = postProcess(securityContextFilter); http.addFilter(securityContextFilter); &#125;&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring security 系列 2：AbstractSecurityInterceptor]]></title>
    <url>%2Fp%2Fspring-web-security-p2%2F</url>
    <content type="text"><![CDATA[AbstractSecurityInterceptor 是安全拦截器的基类。FilterSecurityInterceptor 是鉴权相关的 filter。InterceptorStatusToken 是两者通信的渠道。AbstractSecurityInterceptor在理解 FilterSecurityInterceptor 之前，需要先深入 AbstractSecurityInterceptor。AbstractSecurityInterceptor 是安全拦截器的基类。涉及成员：AuthenticationManager：认证 AccessDecisionManager：鉴权SecurityMetadataSource：获取属性列表RunAsManager：替换认证用户AfterInvocationManager：鉴权完成后续处理 核心方法：beforeInvocationfinallyInvocationafterInvocation工作流程：从 SecurityContextHolder 获取 Authentication 对象 通过 SecurityMetadataSource 判断这个 secure object 是安全的还是公开的 对于安全请求：如果 Authentication.isAuthenticated()为 false，或者 alwaysReauthenticate 为 true，就交给 AuthenticationManager 验证。返回结果更新到 SecurityContextHolder。AccessDecisionManager 判断该请求的授权 RunAsManager 尝试切换用户执行 交给子类执行。子类执行完毕后，返回 InterceptorStatusToken 对象。AbstractSecurityInterceptor#finallyInvocation()用于清理现场。子类调用 AbstractSecurityInterceptor#afterInvocation()If the RunAsManager replaced the Authentication object, return the SecurityContextHolder to the object that existed after the call to AuthenticationManager.AfterInvocationManager 处理 对于公开请求：子类返回 InterceptorStatusToken 对象。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192protected InterceptorStatusToken beforeInvocation(Object object) &#123; Assert.notNull(object, "Object was null"); final boolean debug = logger.isDebugEnabled(); if (!getSecureObjectClass().isAssignableFrom(object.getClass())) &#123; throw new IllegalArgumentException( "Security invocation attempted for object" + object.getClass().getName() + "but AbstractSecurityInterceptor only configured to support secure objects of type:" + getSecureObjectClass()); &#125; Collection&lt;ConfigAttribute&gt; attributes = this.obtainSecurityMetadataSource() .getAttributes(object); if (attributes == null || attributes.isEmpty()) &#123; if (rejectPublicInvocations) &#123; throw new IllegalArgumentException( "Secure object invocation" + object + "was denied as public invocations are not allowed via this interceptor." + "This indicates a configuration error because the" + "rejectPublicInvocations property is set to'true'"); &#125; if (debug) &#123; logger.debug("Public object - authentication not attempted"); &#125; publishEvent(new PublicInvocationEvent(object)); return null; // no further work post-invocation &#125; if (debug) &#123; logger.debug("Secure object: " + object + "; Attributes: " + attributes); &#125; if (SecurityContextHolder.getContext().getAuthentication() == null) &#123; credentialsNotFound(messages.getMessage( "AbstractSecurityInterceptor.authenticationNotFound", "An Authentication object was not found in the SecurityContext"), object, attributes); &#125; Authentication authenticated = authenticateIfRequired(); // Attempt authorization try &#123; this.accessDecisionManager.decide(authenticated, object, attributes); &#125; catch (AccessDeniedException accessDeniedException) &#123; publishEvent(new AuthorizationFailureEvent(object, attributes, authenticated, accessDeniedException)); throw accessDeniedException; &#125; if (debug) &#123; logger.debug("Authorization successful"); &#125; if (publishAuthorizationSuccess) &#123; publishEvent(new AuthorizedEvent(object, attributes, authenticated)); &#125; // Attempt to run as a different user Authentication runAs = this.runAsManager.buildRunAs(authenticated, object, attributes); if (runAs == null) &#123; if (debug) &#123; logger.debug("RunAsManager did not change Authentication object"); &#125; // no further work post-invocation return new InterceptorStatusToken(SecurityContextHolder.getContext(), false, attributes, object); &#125; else &#123; if (debug) &#123; logger.debug("Switching to RunAs Authentication: " + runAs); &#125; SecurityContext origCtx = SecurityContextHolder.getContext(); SecurityContextHolder.setContext(SecurityContextHolder.createEmptyContext()); SecurityContextHolder.getContext().setAuthentication(runAs); // need to revert to token.Authenticated post-invocation return new InterceptorStatusToken(origCtx, true, attributes, object); &#125;&#125;12345678910111213141516171819202122232425262728protected Object afterInvocation(InterceptorStatusToken token, Object returnedObject) &#123; if (token == null) &#123; // public object return returnedObject; &#125; finallyInvocation(token); // continue to clean in this method for passivity if (afterInvocationManager != null) &#123; // Attempt after invocation handling try &#123; returnedObject = afterInvocationManager.decide(token.getSecurityContext() .getAuthentication(), token.getSecureObject(), token .getAttributes(), returnedObject); &#125; catch (AccessDeniedException accessDeniedException) &#123; AuthorizationFailureEvent event = new AuthorizationFailureEvent( token.getSecureObject(), token.getAttributes(), token .getSecurityContext().getAuthentication(), accessDeniedException); publishEvent(event); throw accessDeniedException; &#125; &#125; return returnedObject;&#125;finallyInvocation清理 AbstractSecurityInterceptor 产生的上下文。要在 secure object 调用之后、afterInvocation调用之前使用 finallyInvocation。12345678910protected void finallyInvocation(InterceptorStatusToken token) &#123; // token == null 为 public invocation if (token != null &amp;&amp; token.isContextHolderRefreshRequired()) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Reverting to original Authentication: " + token.getSecurityContext().getAuthentication()); &#125; SecurityContextHolder.setContext(token.getSecurityContext()); &#125;&#125;InterceptorStatusToken 是 security 拦截器的状态抽象。AbstractSecurityInterceptor 和它的子类通过 InterceptorStatusToken 来通信。12345public class InterceptorStatusToken &#123; private SecurityContext securityContext; private Collection&lt;ConfigAttribute&gt; attr; private Object secureObject; private boolean contextHolderRefreshRequired;FilterSecurityInterceptor 处理鉴权的子类，负责触发 secure object 调用。1public class FilterSecurityInterceptor extends AbstractSecurityInterceptor implements Filter &#123;&#125;有了 AbstractSecurityInterceptor 的基础，理解起来简单多。123456789101112131415161718192021222324252627282930313233public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; FilterInvocation fi = new FilterInvocation(request, response, chain); invoke(fi);&#125;public void invoke(FilterInvocation fi) throws IOException, ServletException &#123; if ((fi.getRequest() != null) &amp;&amp; (fi.getRequest().getAttribute(FILTER_APPLIED) != null) &amp;&amp; observeOncePerRequest) &#123; // filter already applied to this request and user wants us to observe // once-per-request handling, so don't re-do security checking fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; else &#123; // first time this request being called, so perform security checking if (fi.getRequest() != null &amp;&amp; observeOncePerRequest) &#123; fi.getRequest().setAttribute(FILTER_APPLIED, Boolean.TRUE); &#125; // AbstractSecurityInterceptor 前置操作 InterceptorStatusToken token = super.beforeInvocation(fi); // secure object invocation try &#123; fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; finally &#123; // AbstractSecurityInterceptor 清理操作 super.finallyInvocation(token); &#125; // AbstractSecurityInterceptor 后置操作 super.afterInvocation(token, null); &#125;&#125;AccessDecisionVoter 和 AccessDecisionManagerAccessDecisionVoter 是投票者，需要对 Authentication 请求投票，有 3 种结果：123int ACCESS_GRANTED = 1;int ACCESS_ABSTAIN = 0;int ACCESS_DENIED = -1;AccessDecisionVoter 有多种内置实现。RoleVoter 很简单，直接根据传入属性的前缀判断是否允许访问：123456789101112131415161718192021222324252627282930313233343536373839public class RoleVoter implements AccessDecisionVoter&lt;Object&gt; &#123; private String rolePrefix = "ROLE_"; public boolean supports(ConfigAttribute attribute) &#123; if ((attribute.getAttribute() != null) &amp;&amp; attribute.getAttribute().startsWith(getRolePrefix())) &#123; return true; &#125; else &#123; return false; &#125; &#125; public int vote(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; attributes) &#123; if (authentication == null) &#123; return ACCESS_DENIED; &#125; int result = ACCESS_ABSTAIN; Collection&lt;? extends GrantedAuthority&gt; authorities = extractAuthorities(authentication); for (ConfigAttribute attribute : attributes) &#123; if (this.supports(attribute)) &#123; result = ACCESS_DENIED; // Attempt to find a matching granted authority for (GrantedAuthority authority : authorities) &#123; if (attribute.getAttribute().equals(authority.getAuthority())) &#123; return ACCESS_GRANTED; &#125; &#125; &#125; &#125; return result; &#125;&#125;AccessDecisionManager 是判断有无权限的管理器。有 3 种实现：AffirmativeBased：任意一个 voter 返回同意则允许访问。ConsensusBased：多数服从少数。如果同意和拒绝票数一样，则根据allowIfEqualGrantedDeniedDecisions 判断。UnanimousBased：一票否决。如果 voter 都弃权，则根据 allowIfAllAbstainDecisions 判断。RunAsManagerRunAsManager 是个有意思的设计，可以 临时 替换 SecurityContext 中的 Authentication 对象。有了 RunAsManager，可以支持 2 层安全模式。一层是 public，面向外部调用者。另一层是 private，并且只提供给 public 层使用。private 层的方法也收到安全约束，因此需要配置特别的授权 GrantedAuthority（这些授权不会暴露到外部调用者）。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring security 系列 1：SpringSecurityFilterChain]]></title>
    <url>%2Fp%2Fspring-web-security-p1%2F</url>
    <content type="text"><![CDATA[SpringSecurityFilterChain 是 spring security 处理流程的重要链路。本文涉及：做什么？和 servlet 容器的关联？怎么创建？工作流程？DelegatingFilterProxyFilter 过滤器是 Servlet 容器来管理，一般我们需要 web.xml 文件中配置 Filter 链。题外话：直接注册到 tomcat 容器的过滤器会关联到 ApplicationFilterChain。DelegatingFilterProxy 是 servlet filter 的代理，通过 spring 容器来管理 filter 的生命周期。交由 spring 托管，可以简化 filter 的初始化。1234567891011121314151617public class DelegatingFilterProxy extends GenericFilterBean &#123; @Nullable private String contextAttribute; @Nullable private WebApplicationContext webApplicationContext; @Nullable private String targetBeanName; private boolean targetFilterLifecycle = false; @Nullable private volatile Filter delegate; private final Object delegateMonitor = new Object();作为 filter，核心方法是 doFilter，做了 lazy init，实际工作交给 delegate 处理。1234567891011121314151617181920212223public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; // Lazily initialize the delegate if necessary. Filter delegateToUse = this.delegate; if (delegateToUse == null) &#123; synchronized (this.delegateMonitor) &#123; delegateToUse = this.delegate; if (delegateToUse == null) &#123; WebApplicationContext wac = findWebApplicationContext(); if (wac == null) &#123; throw new IllegalStateException("No WebApplicationContext found:" + "no ContextLoaderListener or DispatcherServlet registered?"); &#125; delegateToUse = initDelegate(wac); &#125; this.delegate = delegateToUse; &#125; &#125; // Let the delegate perform the actual doFilter operation. invokeDelegate(delegateToUse, request, response, filterChain);&#125;AbstractSecurityWebApplicationInitializer 把 springSecurityFilterChain 以 DelegatingFilterProxy 的形式注册到 servlet 容器。123456789101112131415/** * Registers the springSecurityFilterChain * @param servletContext the &#123;@link ServletContext&#125; */private void insertSpringSecurityFilterChain(ServletContext servletContext) &#123; // 默认为 springSecurityFilterChain String filterName = DEFAULT_FILTER_NAME; DelegatingFilterProxy springSecurityFilterChain = new DelegatingFilterProxy( filterName); String contextAttribute = getWebApplicationContextAttribute(); if (contextAttribute != null) &#123; springSecurityFilterChain.setContextAttribute(contextAttribute); &#125; registerFilter(servletContext, true, filterName, springSecurityFilterChain);&#125;WebSecurityConfigurationWebSecurityConfiguration 负责创建 springSecurityFilterChain。123456789101112@Bean(name = AbstractSecurityWebApplicationInitializer.DEFAULT_FILTER_NAME)public Filter springSecurityFilterChain() throws Exception &#123; boolean hasConfigurers = webSecurityConfigurers != null &amp;&amp; !webSecurityConfigurers.isEmpty(); if (!hasConfigurers) &#123; WebSecurityConfigurerAdapter adapter = objectObjectPostProcessor .postProcess(new WebSecurityConfigurerAdapter() &#123; &#125;); webSecurity.apply(adapter); &#125; return webSecurity.build();&#125;WebSecurityWebSecurity 用来创建 FilterChainProxy（即 springSecurityFilterChain）。通过 WebSecurityConfigurer 或者 WebSecurityConfigurerAdapter 来自定义 WebSecurity。最核心的方法是 performBuild()：把 SecurityFilterChain 构建为 FilterChainProxy。123456789101112131415161718192021222324252627282930313233343536protected Filter performBuild() throws Exception &#123; Assert.state( !securityFilterChainBuilders.isEmpty(), () -&gt; "At least one SecurityBuilder&lt;? extends SecurityFilterChain&gt; needs to be specified." + "Typically this done by adding a @Configuration that extends WebSecurityConfigurerAdapter." + "More advanced users can invoke" + WebSecurity.class.getSimpleName() + ".addSecurityFilterChainBuilder directly"); int chainSize = ignoredRequests.size() + securityFilterChainBuilders.size(); List&lt;SecurityFilterChain&gt; securityFilterChains = new ArrayList&lt;&gt;( chainSize); for (RequestMatcher ignoredRequest : ignoredRequests) &#123; securityFilterChains.add(new DefaultSecurityFilterChain(ignoredRequest)); &#125; for (SecurityBuilder&lt;? extends SecurityFilterChain&gt; securityFilterChainBuilder : securityFilterChainBuilders) &#123; securityFilterChains.add(securityFilterChainBuilder.build()); &#125; FilterChainProxy filterChainProxy = new FilterChainProxy(securityFilterChains); if (httpFirewall != null) &#123; filterChainProxy.setFirewall(httpFirewall); &#125; filterChainProxy.afterPropertiesSet(); Filter result = filterChainProxy; if (debugEnabled) &#123; logger.warn("\n\n" + "********************************************************************\n" + "********** Security debugging is enabled. *************\n" + "********** This may include sensitive information. *************\n" + "********** Do not use in a production system! *************\n" + "********************************************************************\n\n"); result = new DebugFilter(filterChainProxy); &#125; postBuildAction.run(); return result;&#125;FilterChainProxyFilterChainProxy 的职责Delegates Filter requests to a list of Spring-managed filter beans.The FilterChainProxy is linked into the servlet container filter chain by adding a standard Spring DelegatingFilterProxy declaration in the application web.xml fileFilterChainProxy 负责委派 filter 请求到 spring 托管的 filter。FilterChainProxy 通过 spring DelegatingFilterProxy 和 servlet 容器集成。1234567public class FilterChainProxy extends GenericFilterBean &#123; private List&lt;SecurityFilterChain&gt; filterChains; private FilterChainValidator filterChainValidator = new NullFilterChainValidator(); private HttpFirewall firewall = new StrictHttpFirewall(); 作为 Filter，入口方法是 doFilter()：12345678910111213141516171819public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; // 检查标记字段 boolean clearContext = request.getAttribute(FILTER_APPLIED) == null; if (clearContext) &#123; try &#123; request.setAttribute(FILTER_APPLIED, Boolean.TRUE); doFilterInternal(request, response, chain); &#125; finally &#123; SecurityContextHolder.clearContext(); request.removeAttribute(FILTER_APPLIED); &#125; &#125; else &#123; doFilterInternal(request, response, chain); &#125;&#125; 核心逻辑在 doFilterInternal()：12345678910111213141516171819202122232425262728private void doFilterInternal(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; FirewalledRequest fwRequest = firewall .getFirewalledRequest((HttpServletRequest) request); HttpServletResponse fwResponse = firewall .getFirewalledResponse((HttpServletResponse) response); List&lt;Filter&gt; filters = getFilters(fwRequest); if (filters == null || filters.size() == 0) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(UrlUtils.buildRequestUrl(fwRequest) + (filters == null ? "has no matching filters" : "has an empty filter list")); &#125; fwRequest.reset(); chain.doFilter(fwRequest, fwResponse); return; &#125; // 参数: firewalledRequest, originalChain, additionalFilters VirtualFilterChain vfc = new VirtualFilterChain(fwRequest, chain, filters); vfc.doFilter(fwRequest, fwResponse);&#125; 实际是包装为 VirtualFilterChain，再做处理：1234567891011121314private static class VirtualFilterChain implements FilterChain &#123; private final FilterChain originalChain; private final List&lt;Filter&gt; additionalFilters; private final FirewalledRequest firewalledRequest; private final int size; private int currentPosition = 0; private VirtualFilterChain(FirewalledRequest firewalledRequest, FilterChain chain, List&lt;Filter&gt; additionalFilters) &#123; this.originalChain = chain; this.additionalFilters = additionalFilters; this.size = additionalFilters.size(); this.firewalledRequest = firewalledRequest; &#125;VirtualFilterChain 先处理 additionalFilters，到底后再处理 originalChain。1234567891011121314151617181920212223public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException &#123; if (currentPosition == size) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(UrlUtils.buildRequestUrl(firewalledRequest) + "reached end of additional filter chain; proceeding with original chain"); &#125; // Deactivate path stripping as we exit the security filter chain this.firewalledRequest.reset(); originalChain.doFilter(request, response); &#125; else &#123; currentPosition++; Filter nextFilter = additionalFilters.get(currentPosition - 1); if (logger.isDebugEnabled()) &#123; logger.debug(UrlUtils.buildRequestUrl(firewalledRequest) + "at position" + currentPosition + "of" + size + "in additional filter chain; firing Filter:'" + nextFilter.getClass().getSimpleName() + "'"); &#125; nextFilter.doFilter(request, response, this); &#125;&#125;官方的 filter 和配置顺序，参见 HttpSecurityBuilder#addFilter()：filter 配置顺序，定义在 FilterComparator12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final class FilterComparator implements Comparator&lt;Filter&gt;, Serializable &#123; private static final int INITIAL_ORDER = 100; private static final int ORDER_STEP = 100; private final Map&lt;String, Integer&gt; filterToOrder = new HashMap&lt;&gt;(); FilterComparator() &#123; Step order = new Step(INITIAL_ORDER, ORDER_STEP); put(ChannelProcessingFilter.class, order.next()); put(ConcurrentSessionFilter.class, order.next()); put(WebAsyncManagerIntegrationFilter.class, order.next()); put(SecurityContextPersistenceFilter.class, order.next()); put(HeaderWriterFilter.class, order.next()); put(CorsFilter.class, order.next()); put(CsrfFilter.class, order.next()); put(LogoutFilter.class, order.next()); filterToOrder.put( "org.springframework.security.oauth2.client.web.OAuth2AuthorizationRequestRedirectFilter", order.next()); filterToOrder.put( "org.springframework.security.saml2.provider.service.servlet.filter.Saml2WebSsoAuthenticationRequestFilter", order.next()); put(X509AuthenticationFilter.class, order.next()); put(AbstractPreAuthenticatedProcessingFilter.class, order.next()); filterToOrder.put("org.springframework.security.cas.web.CasAuthenticationFilter", order.next()); filterToOrder.put( "org.springframework.security.oauth2.client.web.OAuth2LoginAuthenticationFilter", order.next()); filterToOrder.put( "org.springframework.security.saml2.provider.service.servlet.filter.Saml2WebSsoAuthenticationFilter", order.next()); put(UsernamePasswordAuthenticationFilter.class, order.next()); put(ConcurrentSessionFilter.class, order.next()); filterToOrder.put( "org.springframework.security.openid.OpenIDAuthenticationFilter", order.next()); put(DefaultLoginPageGeneratingFilter.class, order.next()); put(DefaultLogoutPageGeneratingFilter.class, order.next()); put(ConcurrentSessionFilter.class, order.next()); put(DigestAuthenticationFilter.class, order.next()); filterToOrder.put( "org.springframework.security.oauth2.server.resource.web.BearerTokenAuthenticationFilter", order.next()); put(BasicAuthenticationFilter.class, order.next()); put(RequestCacheAwareFilter.class, order.next()); put(SecurityContextHolderAwareRequestFilter.class, order.next()); put(JaasApiIntegrationFilter.class, order.next()); put(RememberMeAuthenticationFilter.class, order.next()); put(AnonymousAuthenticationFilter.class, order.next()); filterToOrder.put( "org.springframework.security.oauth2.client.web.OAuth2AuthorizationCodeGrantFilter", order.next()); put(SessionManagementFilter.class, order.next()); put(ExceptionTranslationFilter.class, order.next()); put(FilterSecurityInterceptor.class, order.next()); put(SwitchUserFilter.class, order.next()); &#125;HttpSecurity 先对 filter 排序，再构建 SecurityFilterChain。1234protected DefaultSecurityFilterChain performBuild() &#123; filters.sort(comparator); return new DefaultSecurityFilterChain(requestMatcher, filters);&#125; 接下来的主角是 FilterSecurityInterceptor。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[API 网关 2：kong 概念]]></title>
    <url>%2Fp%2Fapi-gateway-p2-kong-notation%2F</url>
    <content type="text"><![CDATA[使用 kong 的几个基本概念。kong 的几个概念 UpstreamUpstream 对象表示虚拟主机名，可用于通过多个服务（目标）对传入请求进行负载均衡。Target 目标的 IP 地址 / 主机名，其端口表示后端服务的实例。每个上游都可以有多个 Target，并且可以动态添加 Target。Service服务实体是每个上游服务的抽象。服务的示例是数据转换微服务，计费 API 等。服务的主要属性是它的 URL（其中，Kong 应该代理流量），其可以被设置为单个串或通过指定其 protocol， host，port 和 path。服务与路由相关联（服务可以有许多与之关联的路由）。Service 可以是一个实际的地址，也可以是 Kong 内部提供的 Upstream 组件关联，由 Upstream 将请求转发到实际的服务。Route路由实体定义规则以匹配客户端的请求。每个 Route 与一个 Service 相关联，一个服务可能有多个与之关联的路由。在 kong 中，route 在 service 里面添加。ConsumerConsumer 对象表示服务的使用者或者用户。kong 相关概念的操作 Upstream 和 Upstream 相关的重要配置有：负载均衡 健康检查 参见：Loadbalancing reference。负载均衡 ring-balancer 是 kong 提供的负载均衡器。支持以下算法：round-robin (默认)consistent-hashingleast-connections 每个 upstream 都有自己的 ring-balancer。一致性哈希和 RR 算法，通过配置项切换：When using the consistent-hashing algorithm, the input for the hash can be either none, consumer, ip, header, or cookie. When set to none, the round-robin scheme will be used, and hashing will be disabled.slot 的配置，至少每个 target 分配 100 个 slots。slots 越多，随机分布效果越好，但是添加 / 删除 target 的维护成本越大。The number of slots to use per target should (at least) be around 100 to make sure the slots are properly distributed. Eg. for an expected maximum of 8 targets, the upstream should be defined with at least slots=800, even if the initial setup only features 2 targets.The tradeoff here is that the higher the number of slots, the better the random distribution, but the more expensive the changes are (add/removing targets)健康检查 健康检查：主动检查（active）：定期检查 target 中指定的 Http 或 Https 端点，并根据其响应确定 target 的运行状况 被动检查（passive）：也称为断路器，Kong 会分析正在运行的代理流量，并根据其响应请求行为确定 target 的运行状况 健康检查仅作用于活动状态的 target，但是不修改 Kong 数据库中 target 的活动状态。不健康的 target 不会从负载均衡器中移除，因此在使用 Hash 算法时，不会对平衡器布局造成任何影响（它们只是被跳过）。主动健康检查器可以在 target 恢复健康之后自动恢复流量；但是被动健康检查器不能。主动健康检查器需要一条 URL 路径可以访问，作为探测的端点（通常简单配置为”/“）；被动检查器不需要这样的配置 Targettarget 是 upstream 下真实的服务器端点，配置 ip 和 port。Serviceurl 是 protocol、host、port、path 的缩写。RouteRoute 要在 Service 中创建！Route 要在 Service 中创建！Route 要在 Service 中创建！ 这里有个操作上的坑，多选项的配置，要使用回车键才能正常录入！Route 上有 2 个配置项值得留意：Strip Path 和 Preserve Host，都会影响访问 upstream。Strip Path启用 strip_uri 属性来指示 Kong 在代理此 API 时，在上游请求的 URI 中不应包含匹配的 URI 前缀。12GET /service/path/to/resource HTTP/1.1Host:kong 代理到上游服务的真实 uri 为（此时的 uri 不包含 uris 中配置的内容，少了 /service 部分）12GET /path/to/resource HTTP/1.1Host: my-api.comPreserve HostPreserve Host：当启用代理时，KONG 默认将 API 的 upstream_url 的值配置为上游服务主机的 host 。客户端发送请求：12GET / HTTP/1.1Host: service.comPreserve Host=fase12345&#123; "name": "my-api", "upstream_url": "http://my-api.com", "hosts": ["service.com"],&#125;KONG 会从 API 的 upstream_url 中提取 HOST 值，在做代理时，会向上游服务发送类似的请求：12GET / HTTP/1.1Host: my-api.comPreserve Host=true123456&#123; "name": "my-api", "upstream_url": "http://my-api.com", "hosts": ["service.com"], "preserve_host": true&#125;KONG 将会保留客户端发送来的 HOST 值，在做代理时，会向上游服务发送以下的请求：12GET / HTTP/1.1Host: service.com验证 一个名为 my-local 的 Upstream，指向物理服务器，提供 hello222 接口的实现。一个名为 hello-service 的 Service；url 为http://my-local。 一个名为 hello-route 的 Route；设置 path 为/hello-service。kong 默认使用 8000 端口访问 http 服务：1curl http://&lt;kong ip&gt;:8000/hello-service/hello222]]></content>
      <categories>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>kong</tag>
        <tag>konga</tag>
        <tag>API网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgres 实战：日常问题记录]]></title>
    <url>%2Fp%2Fpostgres-in-action-p1%2F</url>
    <content type="text"><![CDATA[postgres 日常问题记录。认证配置 kong 2.1.0 整合 postgres 12.2，报错12[postgres error] could not retrieve current migrations: [postgres error]Error: /usr/local/share/lua/5.1/kong/cmd/start.lua:28: [postgres error] could not retrieve current migrations: [postgres error] 致命错误: 用户 &quot;kong&quot; Ident 认证失败postgres 常见的四种身份验证为：trust：凡是连接到服务器的，都是可信任的。只需要提供 psql 用户名，可以没有对应的操作系统同名用户；password 和 md5：对于外部访问，需要提供 psql 用户名和密码。对于本地连接，提供 psql 用户名密码之外，还需要有操作系统访问权。（用操作系统同名用户验证）password 和 md5 的区别就是外部访问时传输的密码是否用 md5 加密；ident：对于外部访问，从 ident 服务器获得客户端操作系统用户名，然后把操作系统作为数据库用户名进行登录对于本地连接，实际上使用了 peer；peer：通过客户端操作系统内核来获取当前系统登录的用户名，并作为 psql 用户名进行登录。postgres 的默认安全策略比较高，导致外部访问失败。 解决方法：12vi /var/lib/pgsql/12/data/pg_hba.conf# 把这个配置文件中的认证 METHOD 的 ident 修改为 trust，可以实现用账户和密码来访问数据库，template 数据库忙 创建 kong 数据库，遇到这样的问题：ERROR: source database “template1” is being accessed by other usersDETAIL: There is 1 other session using the database.CREATE DATABASE实际上通过拷贝一个已有数据库进行工作。默认情况下，它拷贝名为 template1 的标准系统数据库。如果你为 template1 数据库增加对象，这些对象将被拷贝到后续创建的用户数据库中。系统里还有名为 template0 的第二个标准系统数据库。这个数据库包含和 template1 初始内容一样的数据，也就是说，只包含你的 PostgreSQL 版本预定义的标准对象。在数据库集簇被初始化之后，不应该对 template0 做任何修改。从 template0 而不是 template1 复制的常见原因是， 可以在复制 template0 时指定新的编码和区域设置。也可以指定使用的 template1CREATE DATABASE dbname TEMPLATE template0;当源数据库被拷贝时，不能有其他会话连接到它！当源数据库被拷贝时，不能有其他会话连接到它！当源数据库被拷贝时，不能有其他会话连接到它！解决：关闭多余的连接 重启 pgsql或者，使用其他数据库作为模板 扩展阅读：模板数据库]]></content>
      <categories>
        <category>postgres</category>
      </categories>
      <tags>
        <tag>postgres</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[API 网关：kong 和 konga 安装记录]]></title>
    <url>%2Fp%2Fapi-gateway-p1-kong-konga-install%2F</url>
    <content type="text"><![CDATA[kong 2.1.0、konga 0.14.9 with postgresql 安装经历。postgres安装 pgsql 12.21docker run --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 -d postgres:12.3为 kong 和 konga 分配用户和数据库：12345678CREATE USER kong with password 'kong';CREATE DATABASE kong OWNER kong; grant all privileges on database kong to kong;CREATE USER konga with password 'konga';CREATE DATABASE konga OWNER konga; grant all privileges on database konga to konga;接下来就开始翻车了。kong参考官网：Install Kong Gateway安装和初始化 123456# 这里是 centos7# wget -O 指定输出的文件名wget https://bintray.com/kong/kong-rpm/download_file?file_path=centos/7/kong-2.1.0.el7.amd64.rpm -O kong-2.1.0.el7.amd64.rpm# localinstall 指定安装本地 rmp 包yum localinstall -y kong-2.1.0.el7.amd64.rpm 复制配置文件 1cp /etc/kong/kong.conf.default /etc/kong/kong.conf 执行 kong 命令，都可以使用 -c 指定使用的配置文件。kong 支持检查配置 1kong check &lt;path/to/kong.conf&gt; 放开管理端口 默认端口：Proxy 8000：接收客户端的 HTTP 请求，并转发到后端的 Upstream。Proxy 8443：接收客户端的 HTTPS 请求，并转发到后端的 Upstream。Admin 8001：接收管理员的 HTTP 请求，进行 Kong 的管理。Admin 8444：接收管理员的 HTTPS 请求，进行 Kong 的管理。默认情况下，admin 端口只允许本地访问（127.0.0.1）。根据实际需求修改 kong.conf 放开限制：12admin_listen = 0.0.0.0:8001 reuseport backlog=16384, 0.0.0.0:8444 http2 ssl reuseport backlog=16384#admin_listen = 127.0.0.1:8001 reuseport backlog=16384, 127.0.0.1:8444 http2 ssl reuseport backlog=16384修改后重启 kong 才生效。数据库 kong 也支持无数据库模式，但是 production 环境还是使用数据库靠谱。 修改 kong.conf12345678910111213database = postgres # Determines which of PostgreSQL or Cassandra # this node will use as its datastore. # Accepted values are `postgres`, # `cassandra`, and `off`.pg_host = xxx.xxx.xxx.xxx # Host of the Postgres server.pg_port = 5432 # Port of the Postgres server.#pg_timeout = 5000 # Defines the timeout (in ms), for connecting, # reading and writing.pg_user = kong # Postgres user.pg_password = kong # Postgres user's password.pg_database = kong # The database name to connect to.初始化数据库 1kong migrations bootstrap -c /etc/kong/kong.confkong 官网说是支持 postgres 9.5+，但是目前在 pg 12.2 上执行数据库初始化没有报错，也没有正常建表。 在 pg 11.8 是正常创建！启动和测试 启动 kong1kong start -c /etc/kong/kong.conf测试 kong，看到一堆输出，则正常启动了 1curl 127.0.0.1:8001konga 此处省略一堆手动安装步骤，因为最后使用了 docker 部署。这里只保留验证 postgres 相关步骤。konga 支持的数据库：mysql，postgres，mongo。konga 使用 sails 做 ORM 框架，不支持 postgres 12。2，经测试 11.8 是可以的。konga 需要初始化数据库 12# var conString ="postgres://username:password@localhost/database";node ./bin/konga.js prepare --adapter postgres --uri postgresql://localhost:5432/konga 然后就报错了 12error: A hook (`orm`) failed to load!error: Failed to prepare database: error: column r.consrc does not exist 通过 package.json，发现 sails-postgres 的版本是 0.11.4，手动升级到 1.0.2，结果 waterline 不兼容123456789101112131415161718192021222324252627282930[root@localhost konga]# npm install[root@localhost konga]# npm start&gt; kongadmin@0.14.9 start /data/konga&gt; node --harmony app.jsA hook (`orm`) failed to load!Error: -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-Cannot initialize Waterline.The installed version of adapter `sails-postgresql` is too new!Please try installing a version &lt; 1.0.-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- at /data/konga/node_modules/waterline/lib/waterline.js:90:15 at /data/konga/node_modules/waterline/node_modules/lodash/index.js:3073:15 at baseForOwn (/data/konga/node_modules/waterline/node_modules/lodash/index.js:2046:14) at /data/konga/node_modules/waterline/node_modules/lodash/index.js:3043:18 at Function.&lt;anonymous&gt; (/data/konga/node_modules/waterline/node_modules/lodash/index.js:3346:13) at module.exports.Waterline.initialize (/data/konga/node_modules/waterline/lib/waterline.js:86:7) at buildWaterlineOntology (/data/konga/node_modules/sails-hook-orm/lib/build-waterline-ontology.js:45:19) at Array.async.auto._buildOntology (/data/konga/node_modules/sails-hook-orm/lib/initialize.js:388:7) at listener (/data/konga/node_modules/sails-hook-orm/node_modules/async/lib/async.js:605:42) at /data/konga/node_modules/sails-hook-orm/node_modules/async/lib/async.js:544:17 at _arrayEach (/data/konga/node_modules/sails-hook-orm/node_modules/async/lib/async.js:85:13) at Immediate.taskComplete (/data/konga/node_modules/sails-hook-orm/node_modules/async/lib/async.js:543:13) at runCallback (timers.js:705:18) at tryOnImmediate (timers.js:676:5) at processImmediate (timers.js:658:5) at process.topLevelDomainCallback (domain.js:120:23) 于是改为 1.0.0 以下的最高版本，0.12.2，还是不行。1234567891011A hook (`orm`) failed to load!error: 字段 r.consrc 不存在 at Connection.parseE (/data/konga/node_modules/sails-postgresql/node_modules/pg/lib/connection.js:539:11) at Connection.parseMessage (/data/konga/node_modules/sails-postgresql/node_modules/pg/lib/connection.js:366:17) at Socket.&lt;anonymous&gt; (/data/konga/node_modules/sails-postgresql/node_modules/pg/lib/connection.js:105:22) at Socket.emit (events.js:189:13) at Socket.EventEmitter.emit (domain.js:441:20) at addChunk (_stream_readable.js:284:12) at readableAddChunk (_stream_readable.js:265:11) at Socket.Readable.push (_stream_readable.js:220:10) at TCP.onStreamRead [as onread] (internal/stream_base_commons.js:94:17)1234567891011121314151617181920212223A hook (`orm`) failed to load!/app/node_modules/sails-postgresql/lib/adapter.js:158 var collection = connectionObject.collections[table]; ^TypeError: Cannot read property &apos;collections&apos; of undefined at __DESCRIBE__ (/app/node_modules/sails-postgresql/lib/adapter.js:158:43) at after (/app/node_modules/sails-postgresql/lib/adapter.js:1292:7) at /app/node_modules/sails-postgresql/lib/adapter.js:1181:7 at /app/node_modules/sails-postgresql/node_modules/pg/lib/pool.js:84:11 at /app/node_modules/sails-postgresql/node_modules/pg/node_modules/generic-pool/lib/generic-pool.js:281:11 at /app/node_modules/sails-postgresql/node_modules/pg/lib/pool.js:58:20 at Connection.&lt;anonymous&gt; (/app/node_modules/sails-postgresql/node_modules/pg/lib/client.js:149:7) at Object.onceWrapper (events.js:417:26) at Connection.emit (events.js:322:22) at Connection.EventEmitter.emit (domain.js:482:12) at Socket.&lt;anonymous&gt; (/app/node_modules/sails-postgresql/node_modules/pg/lib/connection.js:109:12) at Socket.emit (events.js:310:20) at Socket.EventEmitter.emit (domain.js:482:12) at addChunk (_stream_readable.js:286:12) at readableAddChunk (_stream_readable.js:268:9) at Socket.Readable.push (_stream_readable.js:209:10) at TCP.onStreamRead (internal/stream_base_commons.js:186:23) 最后的存储方案：konga + mysql 5.7.30，适用于一般平台 konga + postgresql 11.8，针对特定平台konga 的数据库表： 为了简化部署，把这两个存储的数据库 ddl 都 dump 下来。因为 konga 的 npm 依赖包实在太多了，不适合做物理包部署，使用 docker 镜像部署：1docker run -d -p 1337:1337 -e &quot;TOKEN_SECRET=&#123;&#123;somerandomstring&#125;&#125;&quot; -e &quot;DB_ADAPTER=mysql&quot; -e &quot;DB_HOST=xxx.xxx.xxx.xxx&quot; -e &quot;DB_PORT=3306&quot; -e &quot;DB_USER=konga&quot; -e &quot;DB_PASSWORD=konga&quot; -e &quot;DB_DATABASE=konga&quot; -e &quot;NODE_ENV=production&quot; --name konga pantsel/kongaTOKEN_SECRET是 jwt 哈希时使用。对外暴露 1337 端口。然后打开 konga，新建 admin 账号。注意新建 kong 连接，填写的是 admin 端口（默认为 8001 / 8444）：]]></content>
      <categories>
        <category>API网关</category>
      </categories>
      <tags>
        <tag>kong</tag>
        <tag>konga</tag>
        <tag>API网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量删除 git 标签]]></title>
    <url>%2Fp%2Fgit-batch-delete-tags%2F</url>
    <content type="text"><![CDATA[很长时间没有搞过之前一个项目，拉下来发现很多自动化构建的标签。1234567C:\workspace\medical (master -&gt; origin)λ git pull originFrom code.aliyun.com:xxxxxx/medical * [new tag] tags/20190804115002698_medical -&gt; tags/20190804115002698_medical * [new tag] tags/20190820225718365_medical -&gt; tags/20190820225718365_medical * [new tag] tags/20190822224002996_medical -&gt; tags/20190822224002996_medical// 省略一大堆 尼玛，太多了，而且都没用了，于是清理一下。删除远程标签 123456789101112C:\workspace\medical (master -&gt; origin)λ git show-ref --tagd258885654a97e3f8816135f9e53394f2eaa30c3 refs/tags/tags/20190804115002698_medical29254f6b0452de31db7d15fa942cb8d095d9dd80 refs/tags/tags/20190820225718365_medicalC:\workspace\medical (master -&gt; origin)λ git show-ref --tag | awk &apos;&#123;print &quot;:&quot;$2&#125;&apos; | xargs git push originTo code.aliyun.com:xxxxxx/medical.git - [deleted] tags/20190804115002698_medical - [deleted] tags/20190820225718365_medical - [deleted] tags/20190822224002996_medical - [deleted] tags/20190829001121412_medical 在 tag 名前添加 :，标记为删除 删除本地标签1git tag -l | xargs git tag -d]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 内核 watchdog、NMI 和 soft lockup]]></title>
    <url>%2Fp%2Flinux-kernel-watchdog-soft-lockup%2F</url>
    <content type="text"><![CDATA[之前在 k8s 内核内存泄漏，接触到 kernel watchdog、soft lockup 等概念，整理笔记。相关文章：kubernetes 内存泄漏文章分享 linux watchdogwatchdog 是 linux 的一种监控机制，目的是监测系统运行的情况，一旦出现锁死，死机的情况，能及时重启机器（取决于设置策略），并收集 crash dump。 在了解 watchdog 之前，先要了解 NMI 和 lockup。NMIps. 此节内容摘抄自：NMI 是什么 。 中断分为可屏蔽中断和非可屏蔽中断（NMI）。NMI(non-maskable interrupt) ： 就是不可屏蔽的中断。产生 NMI 的方式：– NMI pin– delivery mode NMI messages through system bus or local APIC serial busNMI 通常用于通知操作系统发生了无法恢复的硬件错误。无法恢复的硬件错误通常包括：芯片错误、内存 ECC 校验错、总线数据损坏等等。当系统挂起，失去响应的时候，可以人工触发 NMI，使系统重置，如果早已配置好了 kdump，那么会保存 crash dump 以供分析。Linux 还提供一种称为”NMI watchdog“的机制，用于检测系统是否失去响应（也称为 lockup），可以配置为在发生 lockup 时自动触发 panic。中断是有优先级的：kernel 线程 &lt; 时钟中断 &lt; NMI 中断 其中，kernel 线程是可以被调度的，同时也是可以被中断随时打断的。lockupps. 此节内容摘抄自：Linux Watchdog 机制 。lockup，是指某段内核代码一直占用 CPU 资源、并且不释放。 只有内核代码才能引起 lockup，因为用户代码是可以被抢占的，不可能形成 lockup。其次内核代码必须处于禁止内核抢占的状态 (preemption disabled)，因为 Linux 是可抢占式的内核，只在某些特定的代码区才禁止抢占（例如 spinlock），在这些代码区才有可能形成 lockup。Lockup 分为两种：soft lockup 和 hard lockup。soft lockup 则是单个 CPU 被一直占用的情况（中断仍然可以响应）。hard lockup 发生在 CPU 屏蔽中断的情况下。soft lockupSoftLockup 检测首先需要对每一个 CPU core 注册叫做 watchdog 的 kernel 线程。即[watchdog/0]，[watchdog/1]，etc.12345[root@host143 ~]# ps aux | grep watchdogroot 11 0.0 0.0 0 0 ? S 7 月 15 0:01 [watchdog/0]root 12 0.0 0.0 0 0 ? S 7 月 15 0:01 [watchdog/1]root 17 0.0 0.0 0 0 ? S 7 月 15 0:01 [watchdog/2]root 22 0.0 0.0 0 0 ? S 7 月 15 0:01 [watchdog/3] 同时，系统会有一个高精度的计时器 hrtimer（一般来源于 APIC），该计时器能定期产生时钟中断。对应的中断处理例程是 kernel/watchdog.c: watchdog_timer_fn()，在该例程中： 要递增计数器 hrtimer_interrupts，这个计数器同时为 hard lockup detector 用于判断 CPU 是否响应中断；还要唤醒 [watchdog/x] 内核线程，该线程的任务是更新一个时间戳；soft lock detector 检查时间戳，如果超过 soft lockup threshold 一直未更新，说明 [watchdog/x] 未得到运行机会，意味着 CPU 被霸占，也就是发生了 soft lockup。注意，这里面的内核线程 [watchdog/x] 的目的是更新时间戳，该时间戳是被 watch 的对象。而真正的看门狗，则是由时钟中断触发的 watchdog_timer_fn()，这里面 [watchdog/x] 是被 scheduler 调用执行的，而 watchdog_timer_fn()则是被中断触发的。hard lockup发生 hard lockup 的时候，CPU 不仅无法执行其它进程，而且不再响应中断。检测 hard lockup 的原理利用了 PMU 的 NMI perf event，因为 NMI 中断是不可屏蔽的，在 CPU 不再响应中断的情况下仍然可以得到执行，它再去检查时钟中断的计数器 hrtimer_interrupts 是否在保持递增，如果停滞就意味着时钟中断未得到响应，也就是发生了 hard lockup检测 hard lockup 的机制：NMI watchdog 会利用到之前讲到的 hrtimer。修改 watchdoghard lockup 检查机制依赖 nmi_watchdog：1echo 1 &gt; /proc/sys/kernel/nmi_watchdogwatchdog_thresh 参数来定义发现 softlockup 以后系统 panic 的时间。watchdog 默认监控阈值是 10s。临时修改 1sysctl -w kernel.watchdog_thresh=60 永久修改 1echo 60 &gt; /proc/sys/kernel/watchdog_thresh 如果要在发生 lockup 的时候触发系统 panic 机制：123echo 1 &gt; /proc/sys/kernel/softlockup_panicecho 1 &gt; /proc/sys/kernel/hardlockup_panic扩展： 用户态 watchdog上面的 lockup 机制针对内核。事实上用户态 watchdog 可以监控用户程序，分为 software watchdog 和 hardware watchdog。software watchdog 要安装软件包 123yum install -y watchdogsystemctl start watchdoghardware watchdog 需要硬件支持。 扩展：linux panic 机制 Kernel panic 是内核错误，是系统内核遇到无法处理的致命错误时才会产生的异常。对应 windows 的蓝屏。 当内核发生 panic 时，linux 系统会默认立即重启系统，当然这只是默认情况，除非你修改了产生 panic 时重启定时时间，这个值默认情况下是 0，即立刻重启系统。参考Linux Watchdog 机制Linux 内核中断内幕linux panic 机制]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selinux 简介]]></title>
    <url>%2Fp%2Flinux-selinux-intro%2F</url>
    <content type="text"><![CDATA[最近在排查一个网络访问问题，接触到 SELinux。访问控制方式 为了避免恶意代码访问资源，要有一套访问控制方式，确定应用程序能够权限。DAC自主访问控制 Discretionary Access Control（DAC）。在这种形式下，一个软件或守护进程以 User ID（UID）或 Set owner User ID（SUID）的身份运行，并且拥有该用户的目标（文件、套接字、以及其它进程）权限。这使得恶意代码很容易运行在特定权限之下，从而取得访问关键的子系统的权限。MAC强制访问控制 Mandatory Access Control（MAC）。基于保密性和完整性强制信息的隔离以限制破坏。使用 最小特权原则 ：程序只能执行完成任务所需的操作。 该限制单元独立于传统的 Linux 安全机制运作，并且没有超级用户的概念。RBAC基于角色的访问控制（RBAC）。在 RBAC 中，权限是根据安全系统所授予的角色来提供的。角色的概念与传统的分组概念不同，因为一个分组代表一个或多个用户。一个角色可以代表多个用户，但它也代表一个用户集可以执行的权限。LSMLinux 内核继承了一种通用框架，将策略从实现中分离了出来，而不是采用单一的方法。该解决方案就是 Linux 安全模块（Linux Security Module，LSM）框架。LSM 提供了一种通用的安全框架，允许将安全模型实现为可载入内核模块。SELinux 介绍 SELinux 将 MAC 和 RBAC 都添加到了 GNU/Linux 操作系统中。SELinux 和 linux 内核的整体关系如下：SELinux 涉及几个概念： 主体 Subjects目标 Objects策略 Policy模式 Mode当一个主体 Subject（如一个程序）尝试访问一个目标 Object（如一个文件），SELinux 安全服务器 SELinux Security Server（在内核中）从策略数据库 Policy Database 中运行一个检查。基于当前的模式 mode，如果 SELinux 安全服务器授予权限，该主体就能够访问该目标。如果 SELinux 安全服务器拒绝了权限，就会在 /var/log/messages 中记录一条拒绝信息。在进程层面，SELinux 模块对调用的影响如下：这里有代码级别的分析例子，就不再展开了：安全增强 Linux (SELinux) 剖析 。 操作 SELinuxSELinux 有三个模式：Enforcing 强制 — SELinux 策略强制执行，基于 SELinux 策略规则授予或拒绝主体对目标的访问 Permissive 宽容 — SELinux 策略不强制执行，不实际拒绝访问，但会有拒绝信息写入日志Disabled 禁用 — 完全禁用 SELinux 默认情况下，大部分系统的 SELinux 设置为 Enforcing。查看 SELinux 模式，使用 getenforce：12[root@host143 ~]# getenforceEnforcingsestatus 查看详情：1234567891011121314151617181920212223242526[root@host143 ~]# sestatus -vSELinux status: enabledSELinuxfs mount: /sys/fs/selinuxSELinux root directory: /etc/selinuxLoaded policy name: targetedCurrent mode: enforcingMode from config file: enforcingPolicy MLS status: enabledPolicy deny_unknown status: allowedMax kernel policy version: 31Process contexts:Current context: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023Init context: system_u:system_r:init_t:s0/usr/sbin/sshd system_u:system_r:sshd_t:s0-s0:c0.c1023File contexts:Controlling terminal: unconfined_u:object_r:user_devpts_t:s0/etc/passwd system_u:object_r:passwd_file_t:s0/etc/shadow system_u:object_r:shadow_t:s0/bin/bash system_u:object_r:shell_exec_t:s0/bin/login system_u:object_r:login_exec_t:s0/bin/sh system_u:object_r:bin_t:s0 -&gt; system_u:object_r:shell_exec_t:s0/sbin/agetty system_u:object_r:getty_exec_t:s0/sbin/init system_u:object_r:bin_t:s0 -&gt; system_u:object_r:init_exec_t:s0/usr/sbin/sshd system_u:object_r:sshd_exec_t:s0设置 SELinux 模式，使用 setenforce：123# Use Enforcing or 1 to put SELinux in enforcing mode.# Use Permissive or 0 to put SELinux in permissive mode.[root@host143 ~]# setenforce 0 重启服务器后会恢复默认。在日常操作中，常见的是关闭 SELinux😂，永久关闭 SELinux：12sed -i -e "s/SELINUX=enforcing/SELINUX=disabled/" /etc/selinux/configreboot看下 /etc/selinux/config：12345678910111213[root@host143 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targetedSELINUXTYPE 的 targeted：Targeted 目标 — 只有目标网络进程（dhcpd，httpd，named，nscd，ntpd，portmap，snmpd，squid，以及 syslogd）受保护setroubleshoot 工具包setroubleshoot 提供 SELinux 审计日志分析、修复建议。12yum -y install setroubleshoot sealert -a /var/log/audit/audit.log 还有一个常用的命令：restorecon，用来恢复 SELinux 文件属性。一个文件受 selinux 策略配置，那么移动后可能不能正常访问，可以使用 restorecon 恢复。回到问题 在 Centos7.5.1804 上使用自动化脚本初始化节点，还有安装应用组件，其中包括关闭 selinux 步骤。遇到的问题是，关闭 selinux 步骤成功，但是安装的应用组件不能被正常访问。但是重启虚拟机后就正常。于是再找一台全新的机器，重复实验，保留现场。检查日志，发现 1[WARNING]: SELinux state change will take effect next reboot 执行的脚本是临时关闭 selinux，同时修改 /etc/selinux/config 永久关闭，但是没有重启。理论上是临时关闭成功，但是 status 还是 enabled。123456789101112[root@localhost ~]# getenforcePermissive[root@localhost ~]# sestatus SELinux status: enabledSELinuxfs mount: /sys/fs/selinuxSELinux root directory: /etc/selinuxLoaded policy name: targetedCurrent mode: permissiveMode from config file: disabledPolicy MLS status: enabledPolicy deny_unknown status: allowedMax kernel policy version: 31最后加上关闭 selinux 后的重启步骤。其他 和 SELinux 功能相似的是 AppArmor。SELinux 主要是红帽 Red Hat Linux 以及它的衍生发行版上使用。Ubuntu 和 SUSE（以及它们的衍生发行版）使用的是 AppArmor。参考 安全增强 Linux (SELinux) 剖析 SELinux 入门 在 centos7 安装完 mysql 5.7 之后，不能启动 mysql]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10 蓝屏处理 case]]></title>
    <url>%2Fp%2Fwin10-bluescreen-case%2F</url>
    <content type="text"><![CDATA[通过 minidump 排查故障 上周 bug 10 自动更新后，时不时蓝屏，一天发生几次，不能正常使用了。蓝屏代码是 WIN32K_POWER_WATCHDOG_TIMEOUT。 触发时间点是待机，再唤醒就蓝屏了。在巨硬官网没查到相应的处理方式。查资料发现 win10 崩溃的时候会写下转存，存放在 C:\Windows\Minidump，可以使用 BlueScreenView 这个软件查看。ntoskrnl.exe 故障导致崩溃。查资料了解是计划任务程序，在空闲时间做内存压缩的。平时确实会闲置一会电脑，就突然卡一下。参照 ntoskrnl.exe 占用 cpu 过高 操作，禁止该计划任务。但是蓝屏照旧。 还是要从 win10 自动更新列表入手。发现更新了显卡驱动：先尝试更新驱动，从 intel 官网下载了 27 开头的最新驱动。安装完后重启，目前 1 天多没再发生蓝屏。离线安装 win10 update cab更新于 2020.12.22。bug 10 自动安装了 2004 更新，之后待机蓝屏。网上查找到是通病，巨硬在 2020 年 11 月提供了可选的 kb4586853 补丁，还没推送到全渠道。不想开启 windows insider 做白老鼠，于是手动安装。下载安装包。通过关键字查找下载地址：https://www.catalog.update.microsoft.com/home.aspx下载的是 cab 文件，双击打开是压缩包，没有安装程序。可以通过命令行安装。1). 搜索 CMD 右键以管理员权限运行 2). 选择下面任意一个命令安装更新（文件名需要全目录）dism /online /add-package /packagepath: 文件名 或者start /w pkgmgr /ip /m: 文件名]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nacos 实战 2：配置隔离原理和实践]]></title>
    <url>%2Fp%2Fnacos-in-action-p2%2F</url>
    <content type="text"><![CDATA[引入配置中心后，需要解决多环境、多项目的配置隔离问题。配置隔离的原理和方案 nacos 的配置隔离支持单租户和多租户模式，很灵活。nacos 引入了几个概念来支持配置隔离：namespace: 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。group: 通过一个有意义的字符串（如 Buy 或 Trade ）对配置集进行分组，从而区分 Data ID 相同的配置集。dataId: Data ID 通常用于组织划分系统的配置集。一个系统或者应用可以包含多个配置集。 通常来说，namespace 可以灵活用于租户或者环境隔离；group 可以作为同一个环境不同项目的隔离。单租户模式 一个 nacos 集群只服务一个租户。配置隔离可以这样设计：namespace: 不同的环境，例如 dev、test、prod。group: 项目。dataId: xxx-service，不同服务的配置。(图片来源:https://www.cnblogs.com/larscheng/p/11411423.html)多租户模式 nacos 支持多租户、多环境的配置隔离，适合灵活扩展，也是官方推荐用法。配置隔离可以这样设计：namespace: 租户。group: 项目。dataId: xxx-service-[环境编号]，不同服务的配置。注意这里把环境加入到 dataId 中。(图片来源:https://www.cnblogs.com/larscheng/p/11411423.html) 其他 nacos 默认提供 public namespace，和默认分组 DEFAULT_GROUP。 如果不指定 namespace、group，就会使用默认配置。实践 当前部门应用服务和环境的情况是：开发、测试、生产环境 物理隔离 。不存在一个 nacos 集群服务多个租户的情况。 应用服务数量不多，未来一两年也不会有大量增加。另外，nacos v1.1.4 限制一个 namespace 最多 200 个 dataId。因此基调是使用 nacos 的单租户服务模式。public 保留不使用。DEFAULT_GROUP 保留不使用。namespace虽然是独立部署、物理隔离，但是依然使用 namespace 用于环境隔离，例如 DEV/TEST/PROD，但是实际上只应用使用 PROD 环境。保留 DEV、TEST 是为了方便开发人员临时用一下。出发点是和主流使用方式保持概念上的一致。注意: nacos client 要配置 namespace id，是一个 md5 字符串。因此每次创建 namespace 都不一样。于是创建了 DEV、TEST、PROD 三个 namespace，并且固化到 nacos 安装后的初始化脚本。所有应用提交的配置都使用固定的名空间，减少不必要的修改。12345678910111213spring: application: name: xxx profiles: active: prod cloud: nacos: config: server-addr: 127.0.0.1:30848 prefix: $&#123;spring.application.name&#125; file-extension: yml # 注意：这里使用的是 namespace id （是 md5） namespace: a85a37ef-5bec-478c-a60f-0b11f10b3da4group每个应用分配一个 group。一个 group 下面自由分配多个不同的 dataId。支持容器化和非容器化部署 应用部署分为容器化和非容器化两种模式。为了进一步减少配置修改，为 nacos 分配了域名，同时修改端口，从默认的 8848 改为 30848（因为 k8s 部署对外开放的端口范围为 30000 以上）。应用服务使用域名和固定端口访问 nacos，从而屏蔽容器化和非容器化的差异，减少一个配置修改点。题外话：公共配置 有些配置需要集中管理，并且被不同应用引用，例如中间件地址和端口、公共使用的第三方 key 等。于是设计了一个 group，专门存放这些公共配置，并且被其他应用引用。12345# ip:port，多个用逗号隔开 zookeeper.nodes=172.25.20.1:2181,172.25.20.2:2181,172.25.20.3:2181# 如果没有设置用户密码，可以不配置zookeeper.username=zookeeper.password= 应用服务应用了公共配置后，通过 ${zookeeper.nodes} 方式使用。nacos 配置有优先级，因此要把公共配置作为主配置，应用自身的配置放在 ext-config，才能正常解析（解析占位符的能力依赖 client 实现。java 客户端已经提供）。 例子如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# nacos 配置中心配置项 nacos: config: bootstrap: # 开启配置预加载功能 enable: true # 主配置服务器地址, 配置格式：ip:port，多个用英文逗号隔开，可配置 nacos 集群。 server-addr: 127.0.0.1:30848 # 主配置，命名空间，如果不填，默认为 public，非 public 命名空间，则需要配置命名空间 ID namespace: a85a37ef-5bec-478c-a60f-0b11f10b3da4 # 主配置，优先级高于 data-ids 配置，如果需要配置多个 data-id, 请在 data-ids 配置项配置，两者不能同时生效 data-id: xxx # 主配置 data-ids，可以配置多个 data-id, 多个配置之间用英文逗号隔开 data-ids: # 主配置 group-id, 默认值：DEFAULT_GROUP group: common-group # 主配置 开启自动刷新, 默认值：false auto-refresh: true # 主配置 配置文件类型 type: yaml # nacos 配置中心服务的项目名称 context-path: nacos # 获取配置重试时间 config-retry-time: 10 # 长轮询重试次数 max-retry: 3# 扩展配置 ext-config: - serverAddr : 127.0.0.1:30848 # 主配置，命名空间，如果不填，默认为 public，非 public 命名空间，则需要配置命名空间 ID namespace: a85a37ef-5bec-478c-a60f-0b11f10b3da4 # 主配置，优先级高于 data-ids 配置，如果需要配置多个 data-id, 请在 data-id 配置项配置，两者不能同时生效 data-id: # 主配置 data-ids，可以配置多个 data-id, 多个配置之间用英文逗号隔开 data-ids: xx, yy, zz # 主配置 group-id, 默认值：DEFAULT_GROUP group: my-group # 主配置 开启自动刷新, 默认值：false auto-refresh: true # 主配置 配置文件类型 type: yaml # nacos 配置中心服务的项目名称 context-path: nacos # 获取配置重试时间，可修改 config-retry-time: 10 # 长轮询重试次数，可修改 max-retry: 3 参考资料Nacos（六）：多环境下如何“管理”及“隔离”配置和服务Namespace, endpoint 最佳实践]]></content>
      <categories>
        <category>nacos</category>
      </categories>
      <tags>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 内存系列 4：zoneinfo 和水位]]></title>
    <url>%2Fp%2Flinux-mem-p4-zoneinfo%2F</url>
    <content type="text"><![CDATA[/proc/zoneinfozoneinfo 能够看到看到内存使用的细节。输出信息太多，这里只关注内存水位 (watermark)。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@host143 ~]# cat /proc/zoneinfoNode 0, zone DMA pages free 3960 min 16 low 20 high 24 scanned 0 spanned 4095 present 3997 managed 3976 nr_free_pages 3960 nr_alloc_batch 4 nr_inactive_anon 0 nr_active_anon 0 nr_inactive_file 0 nr_active_file 0 nr_unevictable 0 nr_mlock 0 nr_anon_pages 0 nr_mapped 0 nr_file_pages 0 nr_dirty 0 nr_writeback 0 nr_slab_reclaimable 0 nr_slab_unreclaimable 16 nr_page_table_pages 0 nr_kernel_stack 0 nr_unstable 0 nr_bounce 0 nr_vmscan_write 0 nr_vmscan_immediate_reclaim 0 nr_writeback_temp 0 nr_isolated_anon 0 nr_isolated_file 0 nr_shmem 0 nr_dirtied 0 nr_written 0 numa_hit 2 numa_miss 0 numa_foreign 0 numa_interleave 0 numa_local 2 numa_other 0 workingset_refault 0 workingset_activate 0 workingset_nodereclaim 0 nr_anon_transparent_hugepages 0 nr_free_cma 0# 和 lowmem_reserve_ratio 有关 protection: (0, 2830, 15850, 15850) pagesets cpu: 0 count: 0 high: 0 batch: 1 vm stats threshold: 6 cpu: 1 count: 0 high: 0 batch: 1 vm stats threshold: 6 cpu: 2 count: 0 high: 0 batch: 1 vm stats threshold: 6 cpu: 3 count: 0 high: 0 batch: 1 vm stats threshold: 6 all_unreclaimable: 0 start_pfn: 1 inactive_ratio: 1linux 内存使用水位方式控制：low：当剩余内存慢慢减少，触到这个水位时，就会触发 kswapd 线程的内存回收。min：如果剩余内存减少到触及这个水位，可认为内存严重不足，当前进程就会被堵住，kernel 会直接在这个进程的进程上下文里面做内存回收（direct reclaim）。high: 进行内存回收时，内存慢慢增加，触到这个水位时，就停止回收。min 下的内存是保留给内核使用的；当到达 min，会触发内存的 direct reclaim。 可以看到 kswapd 线程的工作区间是 min 和 low 之间。低于 low 启动，低于 min 触发 direct reclaim。每个 ZONE 都有这三个水位。如果 lowmem 被使用殆尽，触及 low 或 min 水位，内核的普通 kmalloc 就申请不到内存了，就会触发 cache/buffers 的回收和匿名页 swap，再不行就 OOM 了。lowmen_reserve参见 lowmem_reserve_ratio：So the Linux page allocator has a mechanism which prevents allocations which could use highmem from using too much lowmem. This means that a certain amount of lowmem is defended from the possibility of being captured into pinned user memory.The `lowmem_reserve_ratio’ tunable determines how aggressive the kernel is in defending these lower zones.lowmem_reserve 是给更高位的 zones 预留的内存，作用是防止高端 zone 在没内存的情况下过度使用低端 zone 的内存资源。。12[root@host143 ~]# cat /proc/sys/vm/lowmem_reserve_ratio256 256 32zone[i]’s protection[j] is calculated by following expression.12345678(i &lt; j): zone[i]-&gt;protection[j] = (total sums of managed_pages from zone[i+1] to zone[j] on the node) / lowmem_reserve_ratio[i];(i = j): (should not be protected. = 0;(i &gt; j): (not necessary, but looks 0)The default values of lowmem_reserve_ratio[i] are 256 (if zone[i] means DMA or DMA32 zone) 32 (others).预留内存值是 ratio 的倒数关系。/proc/sys/vm/min_free_kbytesvm.min_free_kbytes可以调节 watermark[min]。 内核对 min_free_kbytes 的解释如下：This is used to force the Linux VM to keep a minimum number of kilobytes free. The VM uses this number to compute a watermark[WMARK_MIN] value for each lowmem zone in the system. Each lowmem zone gets a number of reserved free pages based proportionally on its size.Some minimal amount of memory is needed to satisfy PF_MEMALLOC allocations; if you set this to lower than 1024KB, your system will become subtly broken, and prone to deadlock under high loads.Setting this too high will OOM your machine instantly. 划重点：不要随便调大 min_free_kbytes，容易导致 OOM。初始化条件，总的”min”值约等于所有 zones 可用内存的总和乘以 16 再开平方的大小，同时设有 min 和 max 的阈值。12345678910int __meminit init_per_zone_wmark_min(void)&#123; min_free_kbytes = int_sqrt(lowmem_kbytes * 16); if (min_free_kbytes &lt; 128) min_free_kbytes = 128; if (min_free_kbytes &gt; 65536) min_free_kbytes = 65536; ...&#125;/proc/sys/vm/swapnessTODO参考Describing Physical MemoryLinux 内存调节之 zone watermark]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 内存系列 3：匿名页和内存映射]]></title>
    <url>%2Fp%2Flinux-mem-p3-anon-page%2F</url>
    <content type="text"><![CDATA[anonymous pagesmeminfo 的 active/inactive 部分有 file/anon，具体是什么呢？12345678[root@host143 ~]# cat /proc/meminfo// 省略 Active: 3357056 kBInactive: 732448 kBActive(anon): 2815280 kBInactive(anon): 11608 kBActive(file): 541776 kBInactive(file): 720840 kB 用户进程的内存页分为两种：file-backed pages（与文件对应的内存页），和 anonymous pages（匿名页）。比如进程的代码、映射的文件都是 file-backed。进程的堆、栈都是不与文件相对应的、就属于匿名页。file-backed pages 在内存不足的时候可以直接写回对应的硬盘文件里，称为 page-out，不需要用到交换区 (swap)。 而 anonymous pages 在内存不足时就只能写到硬盘上的交换区 (swap) 里，称为 swap-out。pmappmap 命令输出进程的内存映射情况。123456789101112131415161718# pmap [options] &lt;pid&gt;[root@host143 ~]# pmap -dp 11: /usr/lib/systemd/systemd --switched-root --system --deserialize 22Address Kbytes Mode Offset Device Mapping00005578ffecb000 1408 r-x-- 0000000000000000 0fd:00000 /usr/lib/systemd/systemd000055790022a000 140 r---- 000000000015f000 0fd:00000 /usr/lib/systemd/systemd000055790024d000 4 rw--- 0000000000182000 0fd:00000 /usr/lib/systemd/systemd000055790046b000 1144 rw--- 0000000000000000 000:00000 [anon]00007fd810000000 164 rw--- 0000000000000000 000:00000 [anon]00007fd810029000 65372 ----- 0000000000000000 000:00000 [anon]00007fd818000000 164 rw--- 0000000000000000 000:00000 [anon]00007fd818029000 65372 ----- 0000000000000000 000:00000 [anon]// 中间省略一堆输出 00007fd821c9f000 4 rw--- 0000000000000000 000:00000 [anon]00007fffaf7c0000 132 rw--- 0000000000000000 000:00000 [stack]00007fffaf7e5000 8 r-x-- 0000000000000000 000:00000 [anon]ffffffffff600000 4 r-x-- 0000000000000000 000:00000 [anon]mapped: 191168K writeable/private: 18216K shared: 0K 参数：d: (device) 显示设备名 p: (path) 显示完整的文件路径pmap 命令的输出：Mapping: file backing the map , or ‘[anon]’ for allocated memory, or ‘[stack]’ for the program stack.Offset: offset into the fileDevice: device name (major:minor) 第一行是进程的启动参数。最后一行是进程的内存统计，其中:mapped: 该进程映射的虚拟地址空间大小，对应 top 的 VIRT、ps 的 VSZwriteable/private: 表示进程所占用的私有地址空间大小，也就是该进程实际使用的内存大小 shared: 和其他进程共享的内存大小/proc/pid/maps/proc/pid/maps 可以看到简单的进程内存区域使用状态。1234567[root@host143 ~]# cat /proc/1/maps5578ffecb000-55790002b000 r-xp 00000000 fd:00 17643364 /usr/lib/systemd/systemd55790022a000-55790024d000 r--p 0015f000 fd:00 17643364 /usr/lib/systemd/systemd55790024d000-55790024e000 rw-p 00182000 fd:00 17643364 /usr/lib/systemd/systemd55790046b000-557900589000 rw-p 00000000 00:00 0 [heap]7fd810000000-7fd810029000 rw-p 00000000 00:00 0 # 省略一堆输出 maps 的输出和内核每进程的 vm_area_struct 有对应关系。 第一列是虚拟内存地址的开始和结束。对应 vm_start 和 vm_end。第二列是这段内存的访问权限。对应 vm_flags。r 表示可读，w 表示可写，x 表示可执行，p 和 s 共用一个字段，互斥关系，p 表示私有段，s 表示共享段，如果没有相应权限，则用 - 代替。第三列是偏移地址。对应 vm_pgoff。对 file-back 映射，表示此段虚拟内存起始地址在文件中以页为单位的偏移。对匿名映射，它等于 0 或者 vm_start/PAGE_SIZE。第四列是映射文件所属设备号。对匿名映射来说，因为没有文件在磁盘上，所以没有设备号，始终为 00:00。对有名映射来说，是映射的文件所在设备的设备号.第五列是 inode 节点。第六列是映射文件。对 file-back 来说，是映射的文件名。对匿名映射来说，是此段虚拟内存在进程中的角色。[stack]表示在进程中作为栈使用，[heap]表示堆。其余情况则无显示。/proc/pid/smapssmaps 能够看到看到每个进程内存区域的使用详情。123456789101112131415161718[root@host143 ~]# cat /proc/1/smaps5578ffecb000-55790002b000 r-xp 00000000 fd:00 17643364 /usr/lib/systemd/systemdSize: 1408 kB # 虚拟内存大小 Rss: 1156 kBPss: 1156 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 1156 kBPrivate_Dirty: 0 kBReferenced: 1156 kBAnonymous: 0 kBAnonHugePages: 0 kBSwap: 0 kBKernelPageSize: 4 kBMMUPageSize: 4 kBLocked: 0 kBVmFlags: rd ex mr mw me dw sd # 以下省略VmFlags 含义：123456789101112131415161718192021222324252627rd - readablewr - writeableex - executablesh - sharedmr - may readmw - may writeme - may executems - may sharegd - stack segment growns downpf - pure PFN rangedw - disabled write to the mapped filelo - pages are locked in memoryio - memory mapped I/O areasr - sequential read advise providedrr - random read advise provideddc - do not copy area on forkde - do not expand area on remappingac - area is accountablenr - swap space is not reserved for the areaht - area uses huge tlb pagesar - architecture specific flagdd - do not include area into core dumpsd - soft-dirty flagmm - mixed map areahg - huge page advise flagnh - no-huge page advise flagmg - mergable advise flag/proc/pid/statusstatus 可以看到进程状态概览，包括内存部分。123456789101112131415161718192021222324252627282930313233343536373839404142[root@host143 ~]# cat /proc/1/statusName: systemdUmask: 0000State: S (sleeping)Tgid: 1Ngid: 0Pid: 1PPid: 0TracerPid: 0Uid: 0 0 0 0Gid: 0 0 0 0FDSize: 256Groups: VmPeak: 256700 kB# 虚拟内存大小，对应 top 的 VIRT，ps 的 VSZVmSize: 191164 kB# 进程锁住的物理内存大小，锁住的物理内存无法交换到硬盘# 和 mlock() 调用有关 VmLck: 0 kBVmPin: 0 kBVmHWM: 4204 kB# 使用的物理内存VmRSS: 4204 kB# 匿名内存使用的物理内存RssAnon: 1620 kB# file-back 映射使用的物理内存RssFile: 2584 kB# 共享区域使用的物理内存RssShmem: 0 kB# 数据段的虚拟内存VmData: 148760 kB# 用户态栈的虚拟内存VmStk: 132 kB# 代码段的虚拟内存VmExe: 1408 kB# 进程使用的库映射到虚拟内存空间的大小VmLib: 3728 kB# 进程页表大小VmPTE: 120 kB# 交换区的虚拟内存VmSwap: 0 kB# 以下省略/proc/pid/statmstatm 的输出好简洁，单位是 page。12[root@host143 ~]# cat /proc/1/statm47791 1051 646 352 0 37223 0 每列的含义如下：123456789size (1) total program size (same as VmSize in /proc/[pid]/status)resident (2) resident set size (same as VmRSS in /proc/[pid]/status)share (3) shared pages (i.e., backed by a file)text (4) text (code)lib (5) library (unused in Linux 2.6)data (6) data + stackdt (7) dirty pages (unused in Linux 2.6)参考Linux 内存管理 —— 文件系统缓存和匿名页的交换proc(5) - Linux man pagelinux proc maps 文件分析]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 内存系列 2：buddy 和 slab 简介]]></title>
    <url>%2Fp%2Flinux-mem-p2-slab%2F</url>
    <content type="text"><![CDATA[overviewlinux 内核内存分配方式，有 buddy system 和 slab 两大类。 从中可以看到：buddy 和 slab 都是工作在内核空间 slab 工作在 buddy 之上SLAB，SLOB，SLUB 是内核提供的分配器，其前端接口都是一致的，其中 SLAB 是通用的分配器，SLOB 针对微小的嵌入式系统，其算法较大简单（最先适应算法） ，SLUB 是针对配备大量物理内存的大规模并行系统，通过也减小中未使用的分区来管理页组，减少 SLUB 本身数据结构的内存占用。buddy systembuddy system 内存分配技术 它将内存划分为 2 的幂次方个分区，并使用 best-fit 方法来分配内存请求。当用户释放内存时，就会检查 buddy 块，查看其相邻的内存块是否也已经被释放。如果是的话，将合并内存块以最小化内存碎片。buddy system 的缺点是产生内存浪费。/proc/buddyinfoproc 文件系统提供了 buddyinfo 的诊断信息，可以看到内存碎片情况。12345# 64 bit linux[root@host143 ~]# cat /proc/buddyinfo Node 0, zone DMA 0 0 0 1 1 1 1 0 1 1 3 Node 0, zone DMA32 29 15 5 23 27 22 2 7 8 1 625 Node 0, zone Normal 54 20 27 103 85 37 33 12 27 43 2866linux 支持 numa 架构。Node 是处理器节点，每个节点都有自己的一块内存。Zone 是内存区域，代表不同范围的内存。DMA 是低 16MB 内存，出于历史原因保留。DMA32 只出现在 64bit linux，是低 4GB 内存（2^32字节）。Normal 在 32bit 和 64bit 所指向的内存空间不同。对于 32bit，是 16MB~896MB。对于 64bit，是所有内存。HighMem：只出现在 32bit。是高于 896MB 的内存区域。详情见 How the Linux kernel divides up your RAM。 关于 zone 的详细信息，参见/proc/zoneinfo，此处不展开。 后面跟着的列是各个大小的可用内存块 chunk 的统计 The size in bytes of a certain order is given by the formula:(2^order) * PAGE_SIZE 其中 order 从 0 开始 内存严重碎片化，则高位 order 的计数为 0。If the memory is heavily fragmented, the counters for higherorder chunks will be zero and allocation of large contiguousareas will fail.slab堆内存管理器，一般有 2 种搜索内存算法：first-fit（在堆中搜索到的第一个满足请求的内存块 ）best-fit（使用堆中满足请求的最合适的内存块）这种基于堆的分配策略的根本问题是碎片（fragmentation）。当内存块被分配后，它们会以不同的顺序在不同的时间返回。这样会在堆中留下一些洞，需要花一些时间才能有效地管理空闲内存。这种算法通常具有较高的内存使用效率（分配需要的内存），但是却需要花费更多时间来对堆进行管理。顶层是由 kmem_cache 组成的链表。每个 kmem_cache 包含三个成员：slabs_full：完全分配的 slabslabs_partial：部分分配的 slabslabs_empty：空 slab，或者没有对象被分配，是主要的候选回收对象。每个 slab 都是一个连续的内存块（一个或多个连续页）。slab 使用 cache 来存储不同的 kernel object。slab 的优点：slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。支持硬件缓存对齐和着色 发现一张很好的图，对初步了解 slab 有帮助。后续再深入 slab 源码。/proc/meminfo 提供了 slab 的基本情况 1234[root@localhost conf]# cat /proc/meminfoSlab: 237756 kBSReclaimable: 161696 kBSUnreclaim: 76060 kB 其中：SReclaimable: slab 中可回收的部分。调用 kmem_getpages()时加上 SLAB_RECLAIM_ACCOUNT 标记，表明是可回收的，计入 SReclaimable，否则计入 SUnreclaim。SUnreclaim: slab 中不可回收的部分。Slab: slab 中所有的内存，等于以上两者之和。/proc/slabinfo1234567[root@host143 ~]# cat /proc/slabinfoslabinfo - version: 2.1# name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;nf_conntrack_ffff8d7829fd0000 0 0 320 51 4 : tunables 0 0 0 : slabdata 0 0 0nf_conntrack_ffffffff9d4fc900 1428 1428 320 51 4 : tunables 0 0 0 : slabdata 28 28 0rpc_inode_cache 51 51 640 51 8 : tunables 0 0 0 : slabdata 1 1 0isofs_inode_cache 51 51 640 51 8 : tunables 0 0 0 : slabdata 1 1 0每行分为 3 个部分：statisticstunablesslabdatastatistics 各列的含义 12345678910111213141516active_objs The number of objects that are currently active (i.e., in use).num_objs The total number of allocated objects (i.e., objects that are both in use and not in use).objsize The size of objects in this slab, in bytes.objperslab The number of objects stored in each slab.pagesperslab The number of pages allocated for each slab.tunables 是当前 cache 的参数。当使用 SLUB 分配器，则 tunables 都是 0。12345678limit The maximum number of objects that will be cached.batchcount On SMP systems, this specifies the number of objects to trans‐ fer at one time when refilling the available object list.sharedfactor [To be documented]slabdata 的列：12345678active_slabs The number of active slabs.nums_slabs The total number of slabs.sharedavail [To be documented] 因为对象对齐（object alignment）和 slab cache overheadd，对象不会完全和 pages 适配。slubslab、slub、slob 都是内存分配器。slab 是传统的内存分配器，在实践中有如下不足（来自 Linux SLUB 分配器详解）： 较多复杂的队列管理。在 SLAB 分配器中存在众多的队列，例如针对处理器的本地对象缓存队列，slab 中空闲对象队列，每个 slab 处于一个特定状态的队列中，甚至缓冲区控制结构也处于一个队列之中。有效地管理这些不同的队列是一件费力且复杂的工作。slab 管理数据和队列的存储开销比较大。每个 slab 需要一个 struct slab 数据结构和一个管理所有空闲对象的 kmem_bufctl_t（4 字节的无符号整数）的数组。当对象体积较少时，kmem_bufctl_t 数组将造成较大的开销（比如对象大小为 32 字节时，将浪费 1/8 的空间）。为了使得对象在硬件高速缓存中对齐和使用着色策略，还必须浪费额外的内存。同时，缓冲区针对节点和处理器的队列也会浪费不少内存。测试表明在一个 1000 节点 / 处理器的大规模 NUMA 系统中，数 GB 内存被用来维护队列和对象的引用。缓冲区内存回收比较复杂。对 NUMA 的支持非常复杂。SLAB 对 NUMA 的支持基于物理页框分配器，无法细粒度地使用对象，因此不能保证处理器级缓存的对象来自同一节点。冗余的 Partial 队列。SLAB 分配器针对每个节点都有一个 Partial 队列，随着时间流逝，将有大量的 Partial slab 产生，不利于内存的合理使用。性能调优比较困难。针对每个 slab 可以调整的参数比较复杂，而且分配处理器本地缓存时，不得不使用自旋锁。调试功能比较难于使用。于是诞生了 slub。SLUB 分配器特点是简化设计理念，同时保留 SLAB 分配器的基本思想：每个缓冲区由多个小的 slab 组成，每个 slab 包含固定数目的对象。SLUB 分配器简化了 kmem_cache，slab 等相关的管理数据结构，摒弃了 SLAB 分配器中众多的队列概念，并针对多处理器、NUMA 系统进行优化，从而提高了性能和可扩展性并降低了内存的浪费。为了保证内核其它模块能够无缝迁移到 SLUB 分配器，SLUB 还保留了原有 SLAB 分配器所有的接口 API 函数。参考Allocating kernel memory (buddy system and slab system)Linux slab 分配器剖析proc(5) — Linux manual pageLinux SLUB 分配器详解]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>slab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 内存泄漏文章分享]]></title>
    <url>%2Fp%2Fkubernetes-mem-leak%2F</url>
    <content type="text"><![CDATA[背景 公司使用老版本的 linux 3.10 内核，使用 kubernetes 时不时遇到内存泄漏导致 pod 重启的问题。于是查了资料，分享几篇文章和笔记，后续再深入研究。强烈推荐：为什么容器内存占用居高不下，频频 OOM为什么容器内存占用居高不下，频频 OOM（续）how-much-is-too-much-the-linux-oomkiller-and-used-memorya-deep-dive-into-kubernetes-metrics-part-3-container-resource-metricsLinux Cgroup 系列（04）：限制 cgroup 的内存使用（subsystem 之 memory）cgroup 泄露 诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题 Cgroup 泄漏–潜藏在你的集群中kmem 泄漏现象 当 k8s 发生内存泄漏，dmesg 有一些特征：123[root@master-29 ~]# dmesg -T | grep SLUB | head[五 7 月 3 00:00:50 2020] SLUB: Unable to allocate memory on node -1 (gfp=0x80d0)[五 7 月 3 00:00:50 2020] SLUB: Unable to allocate memory on node -1 (gfp=0x80d0)123[root@master-29 ~]# dmesg -T | grep kmem[五 7 月 3 00:05:56 2020] kmem: usage 3868836kB, limit 9007199254740988kB, failcnt 0[五 7 月 3 00:11:10 2020] kmem: usage 3871544kB, limit 9007199254740988kB, failcnt 012345[root@master-29 ~]# dmesg -T | grep 'Memory cgroup' | head[五 7 月 3 00:05:57 2020] Memory cgroup stats for /kubepods/burstable/pod20edac81-191c-4bc6-b85f-e23a65bc7931: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB[五 7 月 3 00:05:57 2020] Memory cgroup stats for /kubepods/burstable/pod20edac81-191c-4bc6-b85f-e23a65bc7931/47fe2a20f8dabf213dbcd0995ab220c7df41eea7e5ac2f444e2d9dab2a49ed39: cache:0KB rss:44KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB active_file:0KB unevictable:0KB[五 7 月 3 00:05:57 2020] Memory cgroup stats for /kubepods/burstable/pod20edac81-191c-4bc6-b85f-e23a65bc7931/26879c55393e26dbccdf7dfba2fcba9d5be00cd92f1be6929c24d9725a2adcdf: cache:40KB rss:325384KB rss_huge:202752KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:325276KB inactive_file:4KB active_file:0KB unevictable:0KB[五 7 月 3 00:05:57 2020] Memory cgroup out of memory: Kill process 789343 (java) score 1074 or sacrifice child1234[root@master-29 ~]# dmesg -T | grep 'oom-killer' | head[五 7 月 3 00:05:56 2020] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=996[五 7 月 3 00:11:10 2020] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=996[五 7 月 3 00:16:19 2020] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=996k8s 会提示 no space left on device(注意是/sys/fs/cgroup/memory)12345Error response from daemon: oci runtime error: container_linux.go:247:starting container process caused &quot;process_linux.go:258: applyingcgroup configuration for process caused \&quot;mkdir/sys/fs/cgroup/memory/kubepods/burstable/podfxxxxx/xxxxxxx:no space left on device 因为不能释放内存，导致 cpu stuck12[root@master-29 ~]# dmesg -T | grep 'soft lockup'[五 7 月 3 00:05:57 2020] kernel:BUG: soft lockup - CPU#0 stuck for 38s! [kworker/0:1:25758]kmem accountingcgroup-v1 的说明：Memory Resource ControllerWith the Kernel memory extension, the Memory Controller is able to limit the amount of kernel memory used by the system. Kernel memory is fundamentally different than user memory, since it can’t be swapped out, which makes it possible to DoS the system by consuming too much of this precious resource.Kernel memory accounting is enabled for all memory cgroups by default. But it can be disabled system-wide by passing cgroup.memory=nokmem to the kernel at boot time. In this case, kernel memory will not be accounted at all.要点：kernel 内存不能被 swap默认对所有 memory cgrop 开启 kmem accounting引用煎鱼的博客:memcg 是 Linux 内核中管理 cgroup 内存的模块，但实际上在 Linux 3.10.x 的低内核版本中存在不少实现上的 BUG，其中最具代表性的是 memory cgroup 中 kmem accounting 相关的问题（在低版本中属于 alpha 特性）：slab 泄露：具体可详见该文章 SLUB: Unable to allocate memory on node -1 中的介绍和说明。memory cgroup 泄露：在删除容器后没有回收完全，而 Linux 内核对 memory cgroup 的总数限制是 65535 个，若频繁创建删除开启了 kmem 的 cgroup，就会导致无法再创建新的 memory cgroup。低版本的 3.10 内核，一旦开启了 kmem_limit 就可能会导致不能彻底删除 memcg 和对应的 cssid。查看内核相关 memcg 配置：123456# uname -r : 输出内核版本 [root@host143 ~]# cat /boot/config-`uname -r`|grep CONFIG_MEMCGCONFIG_MEMCG=yCONFIG_MEMCG_SWAP=yCONFIG_MEMCG_SWAP_ENABLED=yCONFIG_MEMCG_KMEM=ykubelet 和 runc 都会给 memory cgroup 开启 kmem accounting，所以要规避这个问题，就要保证 kubelet 和 runc 都关闭 kmem accounting。（但是不如直接升级内核利索） 解决方案有 2 个：修改内核启动参数，关闭 kmem accounting：1cgroup.memory=nokmem也可以通过 kubelet 的 nokmem Build Tags 来编译解决：1$ kubelet GOFLAGS=&quot;-tags=nokmem&quot;但需要注意，kubelet 版本需要在 v1.14 及以上。升级内核版本 升级 Linux 内核至 kernel-3.10.0-1075.el7 及以上就可以修复这个问题，详细可见 slab leak causing a crash when using kmem control group，其在发行版中 CentOS 7.8 已经发布。扩展：修改内核启动参数 上面 cgroup.memory=nokmem 需要修改内核启动参数。查看当前内核启动参数 12[root@localhost ~]# cat /proc/cmdline BOOT_IMAGE=/vmlinuz-3.10.0-1062.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=zh_CN.UTF-8 修改内核启动参数，通过修改启动器参数实现。在 centos 7 上默认使用 grub2 启动器，对应文件为 /boot/grub2/grub.cfg。 搜索上面输出的内容，在末尾增加即可123456# 省略一堆 else search --no-floppy --fs-uuid --set=root 6c8ad179-1280-424f-b0b0-bbb1760af720 fi linux16 /vmlinuz-3.10.0-1062.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=zh_CN.UTF-8 cgroup.memory=nokmem initrd16 /initramfs-3.10.0-1062.el7.x86_64.imgps. aliyun linux 的内核启动参数 扩展：container_memory_working_set_bytes 和 oom-killerkubernetes 有几个和内存相关的指标：container_memory_max_usage_bytes(最大可用内存) &gt;container_memory_usage_bytes(已经申请的内存 + 工作集使用的内存) &gt;container_memory_working_set_bytes(工作集内存) &gt;container_memory_rss(常驻内存集)container_memory_usage_bytes 包含了 cache，如 filesystem cache，当存在 mem pressure 的时候能够被回收。container_memory_working_set_bytes 更能体现出 mem usage，oom killer 也是根据 container_memory_working_set_bytes 来决定是否 oom kill 的。slub 内存泄漏 debug推荐大佬的几篇文章：怎样诊断 SLAB 泄露问题 如何诊断 SLUB 问题 用 KMEMLEAK 检测内核内存泄漏 内存泄漏的种类：内存泄露（leak），alloc 之后忘了 free，导致内存占用不断增长；越界（overrun），访问了 alloc 分配的区域之外的内存，覆盖了不属于自己的数据；使用已经释放的内存（use after free），正常情况下，已经被 free 释放的内存是不应该再被读写的，否则就意味着程序有 bug；使用未经初始化的数据（use uninitialised bytes），缺省模式下 alloc 分配的内存是不被初始化的，内存值是随机的，直接使用的话后果可能是灾难性的。slub 引入了 red zone 和 poisoning 机制，提供了更加方便的 debug 方式。引用大佬的文章：Red zone 来自于橄榄球术语，是指球场底线附近的区域，slub 通过在每一个对象后面额外添加一块 red zone 区域来帮助检测越界 (overrun) 问题，在 red zone 里填充了特征字符，如果代码访问到了 red zone 就意味着越界了。Poisoning 是通过往 slub 对象中填充特征字符的方式来检测 use-after-free、use-uninitialised 等问题，比如在分配 slub 对象时填充 0x5a，在释放时填充 0x6b，然后 debug 代码检查时如果看到本该是 0x6b 的位置变成了别的内容，就可能是发生了 use-after-free，而本该是 0x5a 的位置如果变成了其它内容就意味着可能是 use-uninitialised 问题。开启 slub debug 方式见：如何诊断 SLUB 问题 。 跟踪 slub 内核内存的另一个工具是 kmemleak。kmemleak 通过追踪 kmalloc(), vmalloc(), kmem_cache_alloc()等函数，把分配内存的指针和大小、时间、stack trace 等信息记录在一个 rbtree 中，等到调用 free 释放内存时就把相应的记录从 rbtree 中删除。通过 /sys/kernel/debug/kmemleak 查看信息。kmemleak 的扫描算法存在误报的可能。要启用 kmemleak，前提是内核编译时在“Kernel hacking”中开启了 CONFIG_DEBUG_KMEMLEAK 选项。123456[root@localhost bin]# cat /boot/config-`uname -r` | grep -i kmemCONFIG_MEMCG_KMEM=y# CONFIG_DEVKMEM is not setCONFIG_HAVE_DEBUG_KMEMLEAK=y# CONFIG_DEBUG_KMEMLEAK is not setCONFIG_HAVE_ARCH_KMEMCHECK=y挂载以下文件系统：1# mount -t debugfs nodev /sys/kernel/debug/默认每 10 分钟扫描内存一次，也可以手动触发1echo scan &gt; /sys/kernel/debug/kmemleak]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 内存系列 1：基本概念和工具]]></title>
    <url>%2Fp%2Flinux-mem-p1-intro%2F</url>
    <content type="text"><![CDATA[linux 内存相关比较复杂。我们从常见命令开始入手，逐步理解 linux 内存设计和管理。free 命令 free 命令可以简单看到机器整体内存情况。free 从/proc/meminfo 读取信息。123456# 默认是 kb# -h：以人类友好的方式展示数据 [root@host143 ~]# free -h total used free shared buff/cache availableMem: 15G 737M 14G 12M 655M 14GSwap: 0B 0B 0Bfree 输出的各个字段解释如下（来自man free）：1234567891011121314151617181920total Total installed memory (MemTotal and SwapTotal in /proc/meminfo)used Used memory (calculated as total - free - buffers - cache)free Unused memory (MemFree and SwapFree in /proc/meminfo)shared Memory used (mostly) by tmpfs (Shmem in /proc/meminfo, available on kernels 2.6.32, displayed as zero if not available)buffers Memory used by kernel buffers (Buffers in /proc/meminfo)cache Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)buff/cache Sum of buffers and cacheavailable Estimation of how much memory is available for starting new applications, withouswapping. Unlike the data provided by the cache or free fields, this field takes into account page cache and also that not all reclaimablmemory slabs will be reclaimed due to items being in use (MemAvailable in /proc/meminfo, available on kernels 3.14, emulated on kernels 2.6.27+otherwise the same as free)buff / cachelinux 文件系统做了优化，使用大量的 buff/cache 缓存访问过的文件，二次读取速度很快。buffers 是块设备的缓冲区。buffers 主要用于缓存文件系统中的元数据信息(dentries、inodes)，和另外一些不是文件数据的块，例如 metadata 和 raw block I/O。cached 包括 page cache 和 SReclaimable。 在 linux 文件系统中，buffer 加速对磁盘块的读写，page cache 加速文件 inode 的读写。这里扩展一下，dentry 的是目录项，是 Linux 文件系统中某个索引节点 (inode) 的链接。这个索引节点可以是文件的，也可以是目录的。大量使用 buff/cache，会导致 free 字段看起来很少。但是 buff 和 cache 作为缓存是可以释放的。手动清空 caches99.99% 的情况下，都没有必要去手动清理 caches，交给 linux 内存管理就好。有些场景可能需要，比如 IO benchmark。file: /proc/sys/vm/drop_cachesvariable: vm.drop_caches1234567891011# 清理 pagecache（页面缓存）# sysctl -w vm.drop_caches=1[root@localhost ~]# echo 1 &gt; /proc/sys/vm/drop_caches # 清理 dentries（目录缓存）和 inodes# sysctl -w vm.drop_caches=2[root@localhost ~]# echo 2 &gt; /proc/sys/vm/drop_caches # 清理 pagecache、dentries 和 inodes# sysctl -w vm.drop_caches=3[root@localhost ~]# echo 3 &gt; /proc/sys/vm/drop_cachesavailable通常关注的是 available 字段：Estimation of how much memory is available for starting new applications, without swapping.available 是估算值，大致为 1available = free + (buff/cache 可以回收的部分)top 命令，ps 命令top 命令能够进程级别的内存使用情况。1234567Tasks: 1 total, 0 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16249876 total, 14795000 free, 777544 used, 677332 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 15183136 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 191164 4200 2584 S 0.0 0.0 0:05.34 systemd 从 centos 7.5 的 man top 截取字段介绍：123456789101112131415161718192021222324 4. CODE -- Code Size (KiB) The amount of physical memory devoted to executable code, also known as the Text Resident Set size or TRS. 6. DATA -- Data + Stack Size (KiB) The amount of physical memory devoted to other than executable code, also known as the Data Resident Set size or DRS.17. RES -- Resident Memory Size (KiB) The non-swapped physical memory a task is using. （占用的物理内存，不包括 swap，包括共享内存和私有内存） （对应 ps 命令的 RSS）21. SHR -- Shared Memory Size (KiB) The amount of shared memory available to a task, not all of which is typically resident. It simply reflects memory that could be potentially shared with other processes.27. SWAP -- Swapped Size (KiB) The non-resident portion of a task&apos;s address space.34. USED -- Memory in Use (KiB) This field represents the non-swapped physical memory a task has used (RES) plus the non-resident portion of its address space (SWAP).36. VIRT -- Virtual Memory Size (KiB) The total amount of virtual memory used by the task. It includes all code, data and shared libraries plus pages that have been swapped out and pages that have been mapped but not used.ps 命令也可以看到进程级别的内存情况：123[root@host143 ~]# ps axu | head USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 191324 3416 ? Ss 3 月 23 27:29 /usr/lib/systemd/systemd --switched-root --system --deserialize 22top 和 ps 都能修改显示的列，这里就不展开了。RSS 和 REStop 命令的 %MEM 列显示的是 RES 内存使用率。top 命令的 RES 对应于 ps 命令的 RSS（RSS 是常驻内存集（Resident Set Size））。 两者都是读取 /proc/$(pidof process)/status。VIRTtop 命令的VIRT 对应于 ps 命令的 VSZ。 VSZ 包括进程可以访问的所有内存，包括进入交换分区的内容，以及共享库占用的内存。SHR 和 PSSSHR 内存被多个进程共享，可能导致各个进程加起来（RSS+SHR）比物理内存还要大。 因为 SHR 会被反复计算，因此引入了 PSS 内存的概念：PSS = SHR / (共享这份 SHR 的进程数)The “proportional set size” (PSS) of a process is the count of pages it has in memory, where each page is divided by the number of processes sharing it.So if a process has 1000 pages all to itself, and 1000 shared with one other process, its PSS will be 1500另外，线程共享同一个地址空间，所以一个进程内部的所有线程有相同的 RSS，VSZ 和 PSS。小结 内存方面，top 主要关注 RES，ps 关注 RSS。vmstatvmstat -s查看统计。数据同样来自 /proc/meminfo。12345678910111213141516171819202122232425262728[root@localhost ~]# vmstat -s 16247672 K total memory 1338220 K used memory# 这里增加了 active 和 inactive，后面再展开 2231860 K active memory 1760584 K inactive memory 11842600 K free memory 2660 K buffer memory 3064192 K swap cache 1679356 K total swap 0 K used swap 1679356 K free swap 420031 non-nice user cpu ticks 264 nice user cpu ticks 440930 system cpu ticks 743674626 idle cpu ticks 59055 IO-wait cpu ticks 0 IRQ cpu ticks 49343 softirq cpu ticks 0 stolen cpu ticks 259860 pages paged in 5564186 pages paged out 0 pages swapped in 0 pages swapped out 1136970454 interrupts 815767131 CPU context switches 1591807821 boot time 2345107 forks/proc/meminfo 文件/proc/meminfo 查看机器的内存（物理、虚拟）情况，是 free、vmstat 等工具的数据来源。信息很多，持续更新。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@localhost ~]# cat /proc/meminfo# 物理内存减去内核预留内存之后的总内存 MemTotal: 16247672 kB# 未被使用的内存MemFree: 11843704 kB# MemFree 加上 cache/buffer、slab 可以回收部分MemAvailable: 14567188 kB# buffers 是块设备的缓冲区。# buffers 主要用于缓存文件系统中的元数据信息(dentries、inodes)，和另外一些不是文件数据的块，例如 metadata 和 raw block I/O# cached 包括 page cache 和 SReclaimable 的 slab （来自 free 的说明）Buffers: 2660 kBCached: 2925600 kBSwapCached: 0 kB# page cache 相关# Active Memory 等于 ACTIVE_ANON 与 ACTIVE_FILE 之和# Inactive Memory 等于 INACTIVE_ANON 与 INACTIVE_FILE 之和Active: 2230508 kBInactive: 1761472 kBActive(anon): 1064524 kBInactive(anon): 19424 kBActive(file): 1165984 kBInactive(file): 1742048 kB# 不能 pageout/swapout 的内存页，包括 VM_LOCKED 的内存页、SHM_LOCK 的共享内存页Unevictable: 0 kB# 被 mlock() 系统调用锁定的内存大小。# 被锁定的内存因为不能 pageout/swapout，会从 Active/Inactive LRU list 移到 Unevictable LRU list 上 # “Mlocked”并不是独立的内存空间，它与以下统计项重叠：LRU Unevictable，AnonPages，Shmem，Mapped 等。Mlocked: 0 kBSwapTotal: 1679356 kBSwapFree: 1679356 kB# 等待被写回到磁盘的内存大小Dirty: 60 kB# 正在被写回的大小Writeback: 0 kB# 未映射的页的大小AnonPages: 1063740 kBMapped: 52360 kB# 共享页大小Shmem: 20228 kB# 内核数据结构缓存的大小Slab: 215648 kB# 可以回收的 slabSReclaimable: 138316 kB# 不可以回收的 slabSUnreclaim: 77332 kBKernelStack: 5232 kBPageTables: 8856 kB# NFS 相关NFS_Unstable: 0 kB# ??Bounce: 0 kBWritebackTmp: 0 kBCommitLimit: 9803192 kBCommitted_AS: 2764260 kBVmallocTotal: 34359738367 kBVmallocUsed: 206204 kBVmallocChunk: 34359310332 kBHardwareCorrupted: 0 kBAnonHugePages: 909312 kBCmaTotal: 0 kBCmaFree: 0 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBDirectMap4k: 173888 kBDirectMap2M: 8214528 kBDirectMap1G: 10485760 kB 参考/PROC/MEMINFO 之谜]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[arp 协议]]></title>
    <url>%2Fp%2Fnetwork-arp%2F</url>
    <content type="text"><![CDATA[概述 ARP（Address Resolution Protocol）即地址解析协议，用于实现从 IP 地址到 MAC 地址的映射，即询问目标 IP 对应的 MAC 地址。IPv4 需要 arp 协议，IPv6 使用 NDP 替代 arp 协议。 为什么需要 ARP 协议：IP 地址在 OSI 模型的第三层，MAC 地址在第二层，彼此不直接打交道。在通过以太网发送 IP 数据包时，需要先封装第三层（32 位 IP 地址）、第二层（48 位 MAC 地址）的报头，但由于发送时只知道目标 IP 地址，不知道其 MAC 地址，又不能跨第二、三层，所以需要使用地址解析协议。粗略来看，arp 协议流程是：主机发送信息时将包含目标 IP 地址的 ARP 请求广播到局域网络上的所有主机（ARP 广播请求），并接收返回消息，以此确定目标的物理地址。收到返回消息后将该 IP 地址和物理地址存入本机 ARP 缓存中并保留一定时间，下次请求时直接查询 ARP 缓存以节约资源。来一张 gif 动图加强印象（来源见水印）：对于网络协议，一般要思考工作在 osi 模型的哪一层：基于功能来考虑，ARP 是链路层协议；基于分层 / 包封装来考虑，ARP 是网络层协议。arp 流程 主机 A 的 IP 地址为 10.12.2.73，MAC 地址为 12:6e:eb:de:b3:ed；主机 B 的 IP 地址为 10.12.2.1，MAC 地址为 12:6f:56:c0:c4:c1；当主机 A 要与主机 B 通信时，地址解析协议可以将主机 B 的 IP 地址（10.12.2.1）解析成主机 B 的 MAC 地址。arp 工作流程：第 1 步：根据主机 A 上的路由表内容，IP 确定用于访问主机 B 的转发 IP 地址是 10.12.2.1。然后 A 主机在自己的本地 ARP 缓存中检查主机 B 的匹配 MAC 地址。第 2 步：如果主机 A 在 ARP 缓存中没有找到映射，它将询问 10.12.2.1 的硬件地址，从而将 ARP 请求帧广播到本地网络上的所有主机。源主机 A 的 IP 地址和 MAC 地址都包括在 ARP 请求中。本地网络上的每台主机都接收到 ARP 请求并且检查是否与自己的 IP 地址匹配。如果主机发现请求的 IP 地址与自己的 IP 地址不匹配，它将丢弃 ARP 请求。第 3 步：主机 B 确定 ARP 请求中的 IP 地址与自己的 IP 地址匹配，则将主机 A 的 IP 地址和 MAC 地址映射添加到本地 ARP 缓存中。第 4 步：主机 B 将包含其 MAC 地址的 ARP 回复消息直接发送回主机 A。第 5 步：当主机 A 收到从主机 B 发来的 ARP 回复消息时，会用主机 B 的 IP 和 MAC 地址映射更新 ARP 缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机 B 的 MAC 地址一旦确定，主机 A 就能向主机 B 发送 IP 通信了。arp 缓存 整理自百度百科 每个动态 ARP 缓存项的潜在生命周期是 10 分钟。新加到缓存中的项目带有时间戳，如果某个项目添加后 2 分钟内没有再使用，则此项目过期并从 ARP 缓存中删除；如果某个项目已在使用，则又收到 2 分钟的生命周期；如果某个项目始终在使用，则会另外收到 2 分钟的生命周期，一直到 10 分钟的最长生命周期。静态项目一直保留在缓存中，直到重新启动计算机为止。arp 报文格式 操作类型：用来表示这个报文的类型，ARP 请求为 1，ARP 响应为 2，RARP 请求为 3，RARP 响应为 4。linux arp 命令 ip neigh 命令可以替代 arp 命令：This program is obsolete. For replacement check ip neigh查看 arp 表 1234567[root@host143 ~]# arpAddress HWtype HWaddress Flags Mask Iface172.25.21.175 ether 0c:c4:7a:6c:40:06 C ens192172.25.20.20 (incomplete) ens192172.25.22.109 (incomplete) ens192172.25.23.21 ether 4c:ed:fb:3e:c4:70 C ens192172.25.21.4 ether ac:1f:6b:1a:e1:74 C ens192arp 表写在内存，可以通过/proc 路径访问：12345678[root@host143 ~]# cat /proc/net/arpIP address HW type Flags HW address Mask Device172.25.21.175 0x1 0x2 0c:c4:7a:6c:40:06 * ens192172.25.22.109 0x1 0x0 00:50:56:b0:80:c6 * ens192172.25.23.21 0x1 0x2 4c:ed:fb:3e:c4:70 * ens192172.25.21.4 0x1 0x2 ac:1f:6b:1a:e1:74 * ens192172.25.21.229 0x1 0x2 00:50:56:b0:7d:86 * ens192172.17.0.2 0x1 0x2 02:42:ac:11:00:02 * docker0手动删除条目 需要 root 或者 netadmin 权限 1[root@host143 ~]# arp -d 172.25.20.20 手动增加条目 12# arp -s address hw_addr [root@host143 ~]# arp -s 172.25.20.20 12:48:08:bb:a5:bb 批量添加多个 arp 映射 使用 arp -f，默认文件位置为/etc/ethers，文件 demo 如下:12:f7:fd:48:aa:79 10.12.2.4012:48:08:aa:a5:bb 10.12.2.43 执行 arp -f /etc/ethers 只显示某个网络接口 1234[root@host143 ~]# arp -i veth0Address HWtype HWaddress Flags Mask Ifacelocalhost (incomplete) veth0host143 ether c6:b2:b2:46:60:cc C veth0arp 参数调优1234567891011121314151617181920[root@host143 ~]# ls -1 /proc/sys/net/ipv4/neigh/defaultanycast_delayapp_solicitbase_reachable_timebase_reachable_time_msdelay_first_probe_timegc_intervalgc_stale_timegc_thresh1gc_thresh2gc_thresh3locktimemcast_solicitproxy_delayproxy_qlenretrans_timeretrans_time_msucast_solicitunres_qlenunres_qlen_bytes 这些参数的介绍，可以使用 man 7 arp 获取。几个核心参数 123456789101112131415161718192021delay_first_probe_time 发现某个相邻层记录无效(stale) 后，发出第一个探测要等待的时间。 缺省值是 5 秒。gc_interval 收集相邻层记录的无用记录的垃圾收集程序的运行周期，缺省为 30 秒。gc_stale_time 决定检查一次相邻层记录的有效性的周期。 当相邻层记录失效时，将在给它发送数据前，再解析一次。 缺省值是 60 秒。gc_thresh1 存在于 ARP 高速缓存中的最少层数，如果少于这个数， 垃圾收集器将不会运行。缺省值是 128。gc_thresh2 保存在 ARP 高速缓存中的最多的记录软限制。 垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。 缺省值是 512。gc_thresh3 保存在 ARP 高速缓存中的最多记录的硬限制， 一旦高速缓存中的数目高于此， 垃圾收集器将马上运行。缺省值是 1024。locktime ARP 记录保存在高速缓存内的最短时间（jiffy 数），以防止存在多个可能的映射 (potential mapping) 时，ARP 高速缓存系统的颠簸 (经常是由于网络的错误配置而引起)。 缺省值是 1 秒。 对应常见的错误是 arp 表溢出：Neighbour table overflow。linux arp 状态机 neighbour.h 定义了一堆状态，参见 neighbour.h123#define NUD_IN_TIMER (NUD_INCOMPLETE|NUD_REACHABLE|NUD_DELAY|NUD_PROBE)#define NUD_VALID (NUD_PERMANENT|NUD_NOARP|NUD_REACHABLE|NUD_PROBE|NUD_STALE|NUD_DELAY)#define NUD_CONNECTED (NUD_PERMANENT|NUD_NOARP|NUD_REACHABLE) 摘自参考资料：NUD_INCOMPLETE：未完成状态。当第一个包发送需要新的 ARP 缓存条目时 neigh_resolve_output()被调用。最后实际调用的是 neigh_event_send()函数, 它发送一个 ARP 请求并置传输状态为 NUD_INCOMPLETE。NUD_REACHABLE：可达状态。当收到 ARP 响应时, 邻居表进入 NUD_REACHABLE 状态 NUD_STALE：过期状态。当定时器触发 neigh_periodic_timer() 函数扫描到邻居表条目的最后时间超过可达时间参数时，它设置条目状态为过期状态 NUD_DELAY：延迟状态。当在 NUD_STALE 状态下有报文需要发送时，调用 neigh_resolve_output() 函数发送数据，在发送报文同时，将状态迁移到这个状态，同时启动一个定时器。NUD_PROBE：探测状态。NUD_DELAY 的定时器到时调用 neigh_timer_handler()迁移到这个状态，这个状态下，发送 ARP 请求并等待响应。NUD_FAILED： 失败状态。连续重发多次都没有响应。NUD_NOARP：不需要解析，如 PPP 接入。NUD_PERMANENT：静态 ARPNUD_IN_TIMER：定时器状态。表示邻居系统正在运行一个定时器 NUD_VALID：有效状态。表示邻居表有一个可用地址NUD_CONNECTED：连接状态。表示可以直接根据邻居系统发送数据包arp 缓存项的 reachable 状态对于外发包是可用的，对于 stale 状态的 arp 缓存项而言，它实际上是不可用的。 如果本地发出的包使用了这个 stale 状态的 arp 缓存表项，那么就将状态机推进到 delay 状态，如果在“垃圾收集”定时器到期后还没有人使用该邻居，那么就有可能删除这个表项了。linux arping 命令 向相邻主机发送 ARP 请求。1234567891011121314[root@localhost ~]# arping -hUsage: arping [-fqbDUAV] [-c count] [-w timeout] [-I device] [-s source] destination -f : quit on first reply -q : be quiet -b : keep broadcasting, don't go unicast -D : duplicate address detection mode -U : Unsolicited ARP mode, update your neighbours. No replies are expected. -A : ARP answer mode, update your neighbours -V : print version and exit -c count : how many packets to send -w timeout : how long to wait for a reply -I device : which ethernet device to use (默认是 eth0) -s source : source ip address destination : ask for what ip address-s指定源 ip 地址：12345-s source IP source address to use in ARP packets. If this option is absent, source address is: · In DAD mode (with option -D) set to 0.0.0.0. · In Unsolicited ARP mode (with options -U or -A) set to destination. · Otherwise, it is calculated from routing tables.向邻居发送 arp 请求 12345[root@host143 ~]# arping -f -I ens192 172.25.22.138ARPING 172.25.22.138 from 172.25.20.143 ens192Unicast reply from 172.25.22.138 [00:50:56:B0:82:3C] 0.852msSent 1 probes (1 broadcast(s))Received 1 response(s) 地址冲突检查 (DAD)Duplicate address detection mode (DAD). See RFC2131, 4.4.1. Returns 0, if DAD succeeded i.e. no replies are received相关资料见 RFC2131, 4.4.1TODO: 和 DHCP 相关，以后研究。123456[root@host143 ~]# arping -D -I ens192 172.25.22.138# 注意 -D 模式，默认的 source ip 是 0.0.0.0ARPING 172.25.22.138 from 0.0.0.0 ens192Unicast reply from 172.25.22.138 [00:50:56:B0:82:3C] 0.954msSent 1 probes (1 broadcast(s))Received 1 response(s) 免费 arpgratuitous ARP（GARP）是一种功能而非协议。普通 ARP 是请求对方 IP 地址对应的 MAC 地址。免费 ARP 是以源、目 IP 都是自己，源 MAC 也是自己，目标 MAC 是广播，即向自己所在网络请求自己的 MAC 地址。免费 ARP 的作用，主机可以用它来确定另一台主机是否设置了同样的 IP 地址。如果免费 ARP 请求收到了恢复说明广播域内存储 IP 地址冲突。1234567# 主机 host143 的 ip 地址为 172.25.20.143# 5s 内检查该地址是否有冲突 [root@host143 ~]# arping -w 5 -I ens192 172.25.20.143ARPING 172.25.20.143 from 172.25.20.143 ens192Sent 6 probes (6 broadcast(s))Received 0 response(s) 免费 ARP 的报文发出去是不希望收到回应的，只希望是起宣告作用；如果收到回应，则证明对方也使用自己目前使用的 IP 地址。所有网络设备（包括计 算机网卡）up 的时候，都会发送这样的免费 ARP 广播。arp 抓包 在 172.25.20.143 发送 arp 请求到 172.25.22.138。窗口 1：12345[root@host143 ~]# tcpdump -i ens192 -ennt'(dst 172.25.22.138 and src 172.25.20.143) or (dst 172.25.20.143 and src 172.25.22.138)'tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens192, link-type EN10MB (Ethernet), capture size 262144 bytes00:50:56:b0:b8:fd &gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 172.25.22.138 (ff:ff:ff:ff:ff:ff) tell 172.25.20.143, length 2800:50:56:b0:82:3c &gt; 00:50:56:b0:b8:fd, ethertype ARP (0x0806), length 60: Reply 172.25.22.138 is-at 00:50:56:b0:82:3c, length 46-i：指定网络接口。-e：显示链路层报头 -nn：不把地址转换为主机名-t：不打印时间 窗口 2：12345[root@host143 ~]# arping -I ens192 -f 172.25.22.138ARPING 172.25.22.138 from 172.25.20.143 ens192Unicast reply from 172.25.22.138 [00:50:56:B0:82:3C] 0.901msSent 1 probes (1 broadcast(s))Received 1 response(s)arp 欺骗 地址解析协议是建立在网络中各个主机互相信任的基础上的。ARP 地址转换表保存在缓存当中，并且周期性更新，这就给攻击者在更新表项之前修改地址转换表、达到攻击目标。ARP 欺骗会导致通信重定向，所有的数据都会通过攻击者的机器。防御措施：不要单独信任 ip 或者 mac。理想的关系应该建立在 IP+MAC 基础上。使用 ARP 服务器。通过该服务器查找自己的 ARP 转换表来响应其他机器的 ARP 广播。使用静态配置路由 ARP 条目。TODO: 补充 arp 欺骗实验。RARPReverse Address Resolution Protocol。从网关服务器的 ARP 表或者缓存上根据 MAC 地址请求 IP 地址的协议，其功能与地址解析协议相反。代理 ARP地址解析协议工作在一个网段中，而代理 ARP（Proxy ARP）工作在不同的网段间，其一般被像路由器这样的设备使用，用来代替处于另一个网段的主机回答本网段主机的 ARP 请求。NDP来自百度：在 IPv6 中，地址解析协议的功能将由 NDP（邻居发现协议，Neighbor Discovery Protocol）实现，它使用一系列 IPv6 控制信息报文（ICMPv6）来实现相邻节点（同一链路上的节点）的交互管理，并在一个子网中保持网络层地址和数据链路层地址之间的映射。TODO: 以后再深入。参考How Does ARP(Address Resolution Protocol) Work?ARP 状态Linux 实现的 ARP 缓存老化时间原理解析]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fiddler 例子：免费使用某笔记软件的 ocr]]></title>
    <url>%2Fp%2Ffiddler-case-try-free-ocr%2F</url>
    <content type="text"><![CDATA[贫穷使我掌握新技能系列。背景 最近看扫描版的 pdf，做笔记很不方便、效率低。需要一个简单、免费的 ocr 软件，截图后简单操作一下就能够把图片转为文字。候选软件 首先，所有需要上传图片的在线识别都 pass 掉，操作复杂。经过排查，有几个软件可以用。microsoft onenoteonenote 自带 ocr。但是，onenote for windows 10 版本阉割掉。需要重新安装 onenote 和激活。office 全家桶激活有点烦。先进入备选。capture2text这是个开源软件，托管在 sourceforge。需要下载中文包。识别比较慢。坑爹的是要每次截图后默认是用英语识别，等差不多 10 秒识别不出来后，手动选择中文，再识别，太反人类了（也许是我没认真找修改配置项？）。只能 pass 掉。某笔记软件 体验很好。截图后识别很方便，云计算识别速度快、精度高。但是。。。要开通会员才能使用。结论：如果能薅羊毛使用某笔记软件的 ocr 接口结果就好了。fiddler 拦截和分析 ocr 接口 于是看下 ocr 接口有没有被加密。因为是 https 接口，要先安装信任 fiddler 的根证书：然后就可以愉快的抓包了。发现 ocr 接口响应体没有加密 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&#123; "ctt": [ &#123; "isParam": false, "regions": [ &#123; "boundingBox": "10,11,341,11,341,34,10,34", "dir": "h", "lang": "zh", "lines": [ &#123; "boundingBox": "10,11,341,11,341,34,10,34", "text_height": 21, "words": [ &#123; "boundingBox": "10,13,25,13,25,34,10,34", "word": "元" &#125;, &#123; "boundingBox": "30,13,41,13,41,34,30,34", "word": "数" &#125;, &#123; "boundingBox": "46,13,56,13,56,34,46,34", "word": "据" &#125;, &#123; "boundingBox": "61,13,67,13,67,34,61,34", "word": "分" &#125;, &#123; "boundingBox": "72,13,82,13,82,34,72,34", "word": "类" &#125;, &#123; "boundingBox": "87,13,93,13,93,34,87,34", "word": ":" &#125;, &#123; "boundingBox": "98,13,108,13,108,34,98,34", "word": "技" &#125;, &#123; "boundingBox": "113,13,124,13,124,34,113,34", "word": "术" &#125;, &#123; "boundingBox": "129,13,139,13,139,34,129,34", "word": "元" &#125;, &#123; "boundingBox": "144,13,150,13,150,34,144,34", "word": "数" &#125;, &#123; "boundingBox": "155,13,165,13,165,34,155,34", "word": "据" &#125;, &#123; "boundingBox": "170,13,181,13,181,34,170,34", "word": "、" &#125;, &#123; "boundingBox": "186,13,191,13,191,34,186,34", "word": "业" &#125;, &#123; "boundingBox": "196,13,207,13,207,34,196,34", "word": "务" &#125;, &#123; "boundingBox": "212,13,222,13,222,34,212,34", "word": "元" &#125;, &#123; "boundingBox": "227,13,238,13,238,34,227,34", "word": "数" &#125;, &#123; "boundingBox": "243,13,248,13,248,34,243,34", "word": "据" &#125;, &#123; "boundingBox": "253,13,264,13,264,34,253,34", "word": "和" &#125;, &#123; "boundingBox": "269,13,279,12,279,33,269,34", "word": "管" &#125;, &#123; "boundingBox": "284,12,290,12,290,33,284,33", "word": "理" &#125;, &#123; "boundingBox": "295,12,305,12,305,33,295,33", "word": "元" &#125;, &#123; "boundingBox": "310,12,321,12,321,33,310,33", "word": "数" &#125;, &#123; "boundingBox": "326,11,331,11,331,32,326,32", "word": "据" &#125;, &#123; "boundingBox": "336,11,341,11,341,32,336,32", "word": "。" &#125; ], "text": "元数据分类: 技术元数据、业务元数据和管理元数据。", "lang": "zh" &#125; ] &#125; ], "width": 376, "height": 45 &#125; ], "failList": []&#125; 按区域来识别文字，字体大小也尝试识别。识别的文字按行放在 text 属性。到此已经基本可以使用。但是如果是多行文字，要在这一堆 json 中找到每行 text 字段，还是很低效。更好的方式是，fiddler 拦截响应，解析每行的 text，再拼凑出一个简单的响应体返回。fiddler 脚本修改响应体 查了一下，fiddler 脚本的入口在：要修改的函数在 剩下的就是在 OnBeforeResponse 增加解析 json 的代码。但是在这个步骤耗费了很多时间：一开始用 JavaScript 的语法，怎么都解析不了！后来网上查了才知道 Fiddler 脚本用的是 jscript.net，类似 c# 的语法。Fiddler 解析返回的 json 对象，非常弱鸡，不支持返回 json 数组，遍历麻烦。官网直接推荐用第三方 json 库处理，比如 JSON.net。于是下载 JSON.net，但是用起来也蛋疼。现在是卡在怎么方便的遍历。发现 responseJSON.JSONObject[&#39;ctt&#39;] 返回的是 Hashtable 对象，于是找 Hashtable 的 api：Hashtable Class，里面有 Count 字段，可以用来控制遍历次数，到此可以解决问题。1234567891011121314151617181920212223242526272829static function OnBeforeResponse(oSession: Session) &#123; if (m_Hide304s &amp;&amp; oSession.responseCode == 304) &#123; oSession[&quot;ui-hide&quot;] = &quot;true&quot;; &#125; if (oSession.host == &apos;xxx.com&apos; &amp;&amp; oSession.fullUrl.Contains(&apos;/yws/mapi/ocr/v2&apos;)) &#123; var responseStringOriginal = oSession.GetResponseBodyAsString(); //FiddlerObject.log(&apos;orginal string \n&apos; + responseStringOriginal); var responseJSON = Fiddler.WebFormats.JSON.JsonDecode(responseStringOriginal); //FiddlerObject.log(&apos;body: &apos;+responseJSON.JSONObject[&apos;ctt&apos;][0][&apos;regions&apos;][0][&apos;lines&apos;][0][&apos;text&apos;]); var str = &apos;&apos;; //FiddlerObject.log(&apos;xx:&apos; + responseJSON.JSONObject[&apos;ctt&apos;].Count); for (var i = 0; i &lt; responseJSON.JSONObject[&apos;ctt&apos;].Count; i++) &#123; var ctt = responseJSON.JSONObject[&apos;ctt&apos;][i]; for (var j = 0; j &lt; ctt[&apos;regions&apos;].Count; j++) &#123; var region = ctt[&apos;regions&apos;][j]; for (var k = 0; k &lt; region[&apos;lines&apos;].Count; k++) &#123; var text = region[&apos;lines&apos;][k][&apos;text&apos;]; str += &apos;\n&apos; + text; &#125; &#125; &#125; //FiddlerObject.log(&apos;&apos;+str); oSession.utilSetResponseBody(str); &#125;&#125;Fiddler 脚本的断点功能不是很好用，于是用 FiddlerObject.log() 在控制台打印日志。最后看下效果：也不知道能用多久，抓紧时间看书做笔记😂。ps. 有能力的话还是开个会员支持下。]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>fiddler</tag>
        <tag>ocr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux ip 命令]]></title>
    <url>%2Fp%2Flinux-ip-command%2F</url>
    <content type="text"><![CDATA[linux 的 ip 命令能够设置设备、ip 地址、路由，功能非常强大。ip 命令 ip [OPTIONS] OBJECT {COMMAND | help}OBJECT := {link | addr | addrlabel | route | rule | neigh | tunnel | maddr | mroute | monitor}OPTIONS := {-V[ersion] | -s[tatistics] | -r[esolve] | -f[amily] {inet | inet6 | ipx | dnet | link} | -o[neline] } 常见的 OBJECT 有：link ：设置设备 (device)，包括 MTU, MAC 地址等等 addr/address ：设置 IProute ：路由相关ip link12345678910111213141516171819202122232425262728293031ip link set &#123; DEVICE | group GROUP &#125; [&#123; up | down &#125;] # 停止 / 启动设备 [type ETYPE TYPE_ARGS ] [arp &#123; on | off &#125;] [dynamic &#123; on | off &#125;] # 动态 IP [multicast &#123; on | off &#125;] [allmulticast &#123; on | off &#125;] [promisc &#123; on | off &#125;] # 混杂模式，和 macvlan 有关 [trailers &#123; on | off &#125;] [txqueuelen PACKETS] [name NEWNAME] [address LLADDR] [broadcast LLADDR] [mtu MTU] [netns &#123; PID | NETNSNAME &#125;] [link-netnsid ID] [alias NAME ] [vf NUM [ mac LLADDR] [vlan VLANID [ qos VLAN-QOS] ] [rate TXRATE] [max_tx_rate TXRATE] [min_tx_rate TXRATE] [spoofchk &#123; on | off &#125;] [query_rss &#123; on | off &#125;] [state &#123; auto | enable | disable &#125; ] [trust &#123; on | off &#125;] ] [master DEVICE] [nomaster] [addrgenmode &#123; eui64 | none &#125;]ip link show [DEVICE] 支持的设备 type：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849TYPE := [bridge | bond | can | dummy | ifb | ipoib | macvlan | macvtap | vcan | veth | vlan | vxlan | ip6tnl | ipip | sit | gre | gretap | ip6gre | ip6gretap | vti | nlmon | geneve | macsec ]ETYPE := [TYPE | bridge_slave | bond_slave] bridge - Ethernet Bridge device bond - Bonding device dummy - Dummy network interface ifb - Intermediate Functional Block device ipoib - IP over Infiniband device macvlan - Virtual interface base on link layer address (MAC) macvtap - Virtual interface based on link layer address (MAC) and TAP. vcan - Virtual Controller Area Network interface veth - Virtual ethernet interface vlan - 802.1q tagged virtual LAN interface vxlan - Virtual eXtended LAN ip6tnl - Virtual tunnel interface IPv4|IPv6 over IPv6 ipip - Virtual tunnel interface IPv4 over IPv4 sit - Virtual tunnel interface IPv6 over IPv4 gre - Virtual tunnel interface GRE over IPv4 gretap - Virtual L2 tunnel interface GRE over IPv4 ip6gre - Virtual tunnel interface GRE over IPv6 ip6gretap - Virtual L2 tunnel interface GRE over IPv6 vti - Virtual tunnel interface nlmon - Netlink monitoring device geneve - GEneric NEtwork Virtualization Encapsulation macsec - Interface for IEEE 802.1AE MAC Security (MACsec)显示设备、统计 显示所有设备 12345[root@host143 ~]# ip link show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ffLOWER-UP: L1 是启动的，即网线是插着的。IFF_LOWER_UP Driver signals L1 up (since Linux 2.6.17) 一些字段说明：qdiscQDisc(排队规则)是 queueing discipline 的简写，它是理解流量控制 (traffic control) 的基础。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc(排队规则)把数据包加入队列。然后，内核会尽可能多地从 qdisc 里面取出数据包，把它们交给网络适配器驱动模块。TOOD：以后再单独深入 qdisc。state（主要是 UNKOWN）This can be DOWN (the network interface is not operational),UNKNOWN (the network interface is operational but nothing is connected),or UP (the network is operational and there is a connection)qlen最大传输队列长度 显示指定设备 123[root@host143 ~]# ip link show ens1922: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ff 显示指定类型设备 123[root@host143 ~]# ip link show type veth279: veth964027a@if278: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br-87716e5dd46e state UP mode DEFAULT group default link/ether ee:9d:49:30:fe:32 brd ff:ff:ff:ff:ff:ff link-netnsid 0 显示统计，使用 -s 参数 1234567[root@host143 ~]# ip -s link show ens1922: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 93515409047 1076693605 0 206749 0 11621833 TX: bytes packets errors dropped carrier collsns 77671901798 762758900 0 0 0 0 启动 / 停止设备 ip link set &lt;device&gt; {up|down}123456789101112131415[root@host143 ~]# ip link show br-87716e5dd46e 261: br-87716e5dd46e: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:62:ef:9a:d1 brd ff:ff:ff:ff:ff:ff[root@host143 ~]# ip link set br-87716e5dd46e down[root@host143 ~]# ip link show br-87716e5dd46e 261: br-87716e5dd46e: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:62:ef:9a:d1 brd ff:ff:ff:ff:ff:ff[root@host143 ~]# ip link set br-87716e5dd46e up[root@host143 ~]# ip link show br-87716e5dd46e 261: br-87716e5dd46e: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:62:ef:9a:d1 brd ff:ff:ff:ff:ff:ff 增加 / 删除设备 12345678ip link add [link DEVICE] [name] NAME [txqueuelen PACKETS] [address LLADDR] [broadcast LLADDR] [mtu MTU] [numtxqueues QUEUE_COUNT] [numrxqueues QUEUE_COUNT] type TYPE [ARGS]ip link delete &#123; DEVICE | group GROUP &#125; type TYPE [ARGS]123456[root@host143 ~]# ip link add veth0 type veth peer name veth1[root@host143 ~]# ip link show type veth314: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 7e:df:bd:e5:05:72 brd ff:ff:ff:ff:ff:ff315: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 6e:3d:10:37:9a:9a brd ff:ff:ff:ff:ff:ffveth 设备总是成对出现。 没找到 M-DOWN 的意义。删除其中一个 veth 设备，则 peer 也被自动删除：12[root@host143 ~]# ip link delete veth0[root@host143 ~]# ip link show type vethip addr显示设备的 ip 地址 12345678910111213[root@host143 ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ff inet 172.25.20.143/22 brd 172.25.23.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:feb0:b8fd/64 scope link valid_lft forever preferred_lft forever 分配新的 ip 地址 12345678910[root@host143 ~]# ip addr add 172.25.20.100 dev ens192 label ens192:100[root@host143 ~]# ip addr show ens1922: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ff inet 172.25.20.143/22 brd 172.25.23.255 scope global ens192 valid_lft forever preferred_lft forever inet 172.25.20.100/32 scope global ens192:100 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:feb0:b8fd/64 scope link valid_lft forever preferred_lft foreverlabel 是别名。scopescope 默认是 global。选项有：global ：允许来自所有来源的连线 site ：仅支持 IPv6 ，仅允许本主机的连接link ：仅允许本设备自我连接host ：仅允许本主机内部的连接 关于 site，只支持 ipv6：Site-local address are supposed to be used within a site.Routers will not forward any packet with site-local source or destination address outside the site.关于 link：Link-local address are supposed to be used for addressing nodes on a single link.Packets originating from or destined to a link-local address will not be forwarded by a router.两个 lft12valid_lft = Valid Lifetime 地址的有效使用期限 preferred_lft = Preferred Lifetime 地址的首选生存期 也可以使用 ifconfig 查看 1234[root@host143 ~]# ifconfig ens192:100ens192:100: flags=4419&lt;UP,BROADCAST,RUNNING,PROMISC,MULTICAST&gt; mtu 1500 inet 172.25.20.100 netmask 255.255.255.255 broadcast 0.0.0.0 ether 00:50:56:b0:b8:fd txqueuelen 1000 (Ethernet) 删除 ip 地址 1234567891011121314[root@host143 ~]# ip addr delete 172.25.20.100 dev ens192:100Warning: Executing wildcard deletion to stay compatible with old scripts. Explicitly specify the prefix length (172.25.20.100/32) to avoid this warning. This special behaviour is likely to disappear in further releases, fix your scripts!# 已经释放 172.25.20.100[root@host143 ~]# ip addr show ens1922: ens192: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:b0:b8:fd brd ff:ff:ff:ff:ff:ff inet 172.25.20.143/22 brd 172.25.23.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:feb0:b8fd/64 scope link valid_lft forever preferred_lft foreverip netns 先啰嗦下网络名空间：A network namespace is logically another copy of the network stack,with its own routes, firewall rules, and network devices.By default a process inherits its network namespace from its parent.Initially all the processes share the same default network namespacefrom the init process.每个新的 network namespace 默认有一个本地环回接口，除了 lo 接口外，所有的其他网络设备（物理 / 虚拟网络接口，网桥等）只能属于一个 network namespace。每个 socket 也只能属于一个 network namespace。查看、添加、删除 netns12345678910[root@host143 ~]# ip netns add ns1[root@host143 ~]# ip netns add ns2[root@host143 ~]# ip netns ns2ns1[root@host143 ~]# ip netns delete ns1[root@host143 ~]# ip netns delete ns2[root@host143 ~]# ip netnsnetns 会在 /var/run/netns/ 创建文件 123456[root@host143 ~]# ls -al /var/run/netns/ 总用量 0drwxr-xr-x 2 root root 80 6 月 18 16:12 .drwxr-xr-x 45 root root 1380 6 月 18 06:50 ..-r--r--r-- 1 root root 0 6 月 18 15:57 ns1-r--r--r-- 1 root root 0 6 月 18 15:57 ns2exec 在指定 netns 运行程序 新建的 netns 会自带 lo 设备，并且处于关闭状态：123[root@host143 ~]# ip netns exec ns1 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00因为 lo 没有起来，ping 不通本地地址 12345[root@host143 ~]# ip netns exec ns1 ping 127.0.0.1PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.^C--- 127.0.0.1 ping statistics ---5 packets transmitted, 0 received, 100% packet loss, time 3999msexec 后面可以执行任意合法的程序，比如 bash。pids 查看指定 netns 下面有哪些进程 pid12345678910# 第一个 shell 窗口[root@host143 ~]# ip netns exec ns1 sleep 100# 第二个 shell 窗口[root@host143 ~]# ip netns pids ns191560[root@host143 ~]# ps aux | grep 91560root 91560 0.0 0.0 107956 616 pts/2 S+ 16:04 0:00 sleep 100root 93332 0.0 0.0 112732 972 pts/1 S+ 16:06 0:00 grep --color=auto 91560monitor 监控 netns 的 add/delete 事件shell 1shell 2[root@host143 ~]# ip netns monitor ns3[root@host143 ~]# ip netns add ns3add ns3[root@host143 ~]# ip netns delete ns3delete ns3 连接两个 netns连接 2 个 netns，可以使用 veth 设备。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 创建 2 个 netns[root@host143 ~]# ip netns add ns1[root@host143 ~]# ip netns add ns2# 启动 netns 的 lo 设备 [root@host143 ~]# ip netns exec ns1 ip link set dev lo up# 因为没有分配 ip，所以是 UNKOWN 状态[root@host143 ~]# ip netns exec ns1 ip link 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00# 可以 ping 通 localhost[root@host143 ~]# ip netns exec ns1 ping 127.0.0.1PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.030 ms64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.045 ms^C--- 127.0.0.1 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 999msrtt min/avg/max/mdev = 0.030/0.037/0.045/0.009 ms[root@host143 ~]# ip netns exec ns2 ip link set dev lo up# 创建 veth pair[root@host143 ~]# ip link add veth0 type veth peer name veth1# 切换 netns[root@host143 ~]# ip link set dev veth0 netns ns1[root@host143 ~]# ip link set dev veth1 netns ns2# ns1 中已经增加 veth0 设备[root@host143 ~]# ip netns exec ns1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00317: veth0@if316: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether c6:dc:57:6a:cd:9c brd ff:ff:ff:ff:ff:ff link-netnsid 1# 启动 veth[root@host143 ~]# ip netns exec ns1 ip link set dev veth0 up[root@host143 ~]# ip netns exec ns1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00317: veth0@if316: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000 link/ether c6:dc:57:6a:cd:9c brd ff:ff:ff:ff:ff:ff link-netnsid 1[root@host143 ~]# ip netns exec ns2 ip link set dev veth1 up # 分配 ip 地址[root@host143 ~]# ip netns exec ns1 ip addr add dev veth0 192.168.1.10/24[root@host143 ~]# ip netns exec ns2 ip addr add dev veth1 192.168.1.20/24# ns1 联通 ns2[root@host143 ~]# ip netns exec ns1 ping 192.168.1.20PING 192.168.1.20 (192.168.1.20) 56(84) bytes of data.64 bytes from 192.168.1.20: icmp_seq=1 ttl=64 time=0.090 ms64 bytes from 192.168.1.20: icmp_seq=2 ttl=64 time=0.030 ms64 bytes from 192.168.1.20: icmp_seq=3 ttl=64 time=0.034 ms^C--- 192.168.1.20 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2000msrtt min/avg/max/mdev = 0.030/0.051/0.090/0.028 ms 一个设备只能属于一个 Network Namespace。如果转移了 netns，那么在原来 netns 就看不到该设备了。ip route路由相关。1234567[root@host143 ~]# ip route default via 172.25.23.254 dev ens192 169.254.0.0/16 dev ens192 scope link metric 1002 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 172.21.0.0/16 dev br-87716e5dd46e proto kernel scope link src 172.21.0.1 172.25.20.0/22 dev ens192 proto kernel scope link src 172.25.20.143 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1scope 定义和上面 ip addr 一样。link 表示连接到本设备 metric 路由距离，到达指定网络所需的中转数 proto：路由协议。可以是数字或者字符串，保存在/etc/iproute2/rt_protos1234567891011121314151617181920212223242526272829303132[root@host143 ~]# cat /etc/iproute2/rt_protos## Reserved protocols.#0 unspec1 redirect2 kernel3 boot4 static8 gated9 ra10 mrt11 zebra12 bird13 dnrouted14 xorp15 ntk16 dhcp42 babel## Used by me for gated#254 gated/aggr253 gated/bgp252 gated/ospf251 gated/ospfase250 gated/rip249 gated/static248 gated/conn247 gated/inet246 gated/default 常见的 proto 有：redirect - the route was installed due to an ICMP redirect.kernel - the route was installed by the kernel during autoconfiguration.boot - the route was installed during the bootup sequence. If a routing daemon starts, it will purge all of them.static - the route was installed by the administrator to override dynamic routing. Routing daemon will respect them and, probably, even advertise them to its peers.ra - the route was installed by Router Discovery protocol.也可以使用 route 命令123456789[root@host143 ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 0 0 0 ens192link-local 0.0.0.0 255.255.0.0 U 1002 0 0 ens192172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0172.21.0.0 0.0.0.0 255.255.0.0 U 0 0 0 br-87716e5dd46e172.25.20.0 0.0.0.0 255.255.252.0 U 0 0 0 ens192192.168.122.0 0.0.0.0 255.255.255.0 U 0 0 0 virbr0Flags 标记。一些可能的标记如下：U — 路由是活动的 H — 目标是一个主机 G — 路由指向网关 R — 恢复动态路由产生的表项 D — 由路由的后台程序动态地安装 M — 由路由的后台程序修改 ! — 拒绝路由 TODO：以后再补充其他路由操作ip neighborip neigh 命令可以替代 arp 命令，用于操作 arp 缓存。12345678[root@host143 ~]# ip neigh helpUsage: ip neigh &#123; add | del | change | replace &#125; &#123; ADDR [lladdr LLADDR] [nud STATE] | proxy ADDR &#125; [dev DEV] ip neigh &#123; show | flush &#125; [proxy] [to PREFIX] [dev DEV] [nud STATE] [vrf NAME]STATE := &#123; permanent | noarp | stale | reachable | none | incomplete | delay | probe | failed &#125; 显示 123456# 或者 ip neigh show[root@host143 ~]# ip neigh172.25.21.175 dev ens192 lladdr 0c:c4:7a:6c:40:06 REACHABLE172.25.22.109 dev ens192 FAILED172.25.23.21 dev ens192 lladdr 4c:ed:fb:3e:c4:70 REACHABLE172.25.21.4 dev ens192 lladdr ac:1f:6b:1a:e1:74 STALElladdr 是link layer address 的缩写。最后一列是 nud 状态，Neighbour Unreachability Detection：12345678910111213141516171819202122232425262728293031permanent the neighbour entry is valid forever and can be only be removed administratively.noarp the neighbour entry is valid. No attempts to validate this entry will be made but it can be removed when its lifetime expires.reachable the neighbour entry is valid until the reachability timeout expires.stale the neighbour entry is valid but suspicious. This option to ip neigh does not change the neighbour state if it was valid and the address is not changed by this command.none this is a pseudo state used when initially creating a neighbour entry or after trying to remove it before it becomes free to do so.incomplete the neighbour entry has not (yet) been validated/resolved.delay neighbor entry validation is currently delayed.probe neighbor is being probed.failed max number of probes exceeded without success, neighbor validation has ultimately failed.这里面最难理解的是 stale。arp 缓存项的 reachable 状态对于外发包是可用的，对于 stale 状态的 arp 缓存项而言，它实际上是不可用的。如果本地发出的包使用了这个 stale 状态的 arp 缓存表项，那么就将状态机推进到 delay 状态，如果在“垃圾收集”定时器到期后还没有人使用该邻居，那么就有可能删除这个表项了。添加、删除 so easy。123ip neigh add 192.168.0.1 dev ethXip neigh del 192.168.0.1 dev ethX 刷新缓存 ip neigh flush 必须要有参数，否则不执行。参数和 show 对应。1234[root@host143 ~]# ip neigh flushFlush requires arguments.[root@host143 ~]# ip neigh flush nud stale注意，不会刷新状态为 permanent、noarp 的缓存项。参考ip-link.htmlnetdevice.7.htmlip-address-scope-parameterLinux Namespace 系列（06）：network namespace (CLONE_NEWNET)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 网络虚拟化之 veth 和 tun/tap]]></title>
    <url>%2Fp%2Flinux-veth-tun-tap%2F</url>
    <content type="text"><![CDATA[整理 veth 和 tun/tap 的学习笔记。veth pairveth-pair 是成对出现的一种虚拟网络设备，一端连接着协议栈，一端连接着彼此，数据从一端出，从另一端进。常常用来连接不同的虚拟网络组件，构建大规模的虚拟网络拓扑，比如连接 Linux Bridge、OVS、LXC 容器等。veth 的特点:成对出现 工作在内核协议栈 数据从一个设备发出，在另一个设备接收 veth pair 使用很灵活。可以使用 veth pair 直连两个 netns；也可以挂载到 bridge（bridge 对接到物理网卡，则可以访问外网）。docker 使用 veth pair 的例子。容器内部的 eth0 和外部的 vethx 配对，再连接到 bridge 上（本质和上图一致）。 创建 veth pair123456# veth 成对出现 [root@host143 ~]# ip link add dev veth0 type veth peer name veth1# 启动设备[root@host143 ~]# ip link set dev veth0 up[root@host143 ~]# ip link set dev veth1 up 验证 veth 的联通特性 veth 的一个特性是：数据从一个设备发出，在另一个设备接收。 通过抓包体验：12345678910111213141516171819202122232425262728293031323334353637383940# 窗口 1tcpdump -n -i veth0# 窗口 2tcpdump -n -i veth1# 窗口 3# 指定使用 veth0 发送 [root@host143 ~]# ping -c 4 -I veth0 8.8.8.8 ping: Warning: source address might be selected on device other than veth0.PING 8.8.8.8 (8.8.8.8) from 172.25.20.143 veth0: 56(84) bytes of data.--- 8.8.8.8 ping statistics ---4 packets transmitted, 0 received, 100% packet loss, time 2999ms# 窗口 1[root@host143 ~]# tcpdump -n -i veth0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes20:42:47.401707 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 2820:42:48.403141 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 2820:42:49.405147 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 28^C3 packets captured3 packets received by filter0 packets dropped by kernel# 窗口 2[root@host143 ~]# tcpdump -n -i veth1tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes20:42:47.401716 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 2820:42:48.403146 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 2820:42:49.405151 ARP, Request who-has 8.8.8.8 tell 172.25.20.143, length 28^C3 packets captured3 packets received by filter0 packets dropped by kernel 从上面可见，从 veth0 发送的包，都流向了 veth1。ping 会构造 ICMP echo request，从 socket 发送到协议栈 因为本地路由表没有 8.8.8.8 的地址，因此要构造 ARP 请求，查询 mac 地址 协议栈知道从 veth0 发送的包，要流向 veth1veth1 收到 ARP 包，交给协议栈 协议栈发现本地没有 8.8.8.8 的地址，于是丢弃 ARP 包。所以没有应答包 为 veth pair 添加 ip，再 ping 对方 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 分配 ip[root@host143 ~]# ip addr add 192.168.1.10/24 dev veth0[root@host143 ~]# ip addr add 192.168.1.20/24 dev veth1# 留意 mac[root@host143 ~]# ip addr322: veth1@veth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c6:b2:b2:46:60:cc brd ff:ff:ff:ff:ff:ff inet 192.168.1.20/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::c4b2:b2ff:fe46:60cc/64 scope link valid_lft forever preferred_lft forever323: veth0@veth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 5a:64:c8:d3:28:a3 brd ff:ff:ff:ff:ff:ff inet 192.168.1.10/24 scope global veth0 valid_lft forever preferred_lft forever inet6 fe80::5864:c8ff:fed3:28a3/64 scope link valid_lft forever preferred_lft forever# veth0 ping veth1[root@host143 ~]# ping -c 4 -I veth0 192.168.1.20PING 192.168.1.20 (192.168.1.20) from 192.168.1.10 veth0: 56(84) bytes of data.--- 192.168.1.20 ping statistics ---4 packets transmitted, 0 received, 100% packet loss, time 2999ms[root@host143 ~]# tcpdump -n -i veth0tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes21:39:13.025468 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 1, length 6421:39:14.025172 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 2, length 6421:39:15.025190 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 3, length 6421:39:16.025165 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 4, length 6421:39:18.039146 ARP, Request who-has 192.168.1.20 tell 192.168.1.10, length 2821:39:18.039177 ARP, Reply 192.168.1.20 is-at c6:b2:b2:46:60:cc, length 28[root@host143 ~]# tcpdump -n -i veth1tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes21:39:13.025476 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 1, length 6421:39:14.025178 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 2, length 6421:39:15.025196 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 3, length 6421:39:16.025171 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 10835, seq 4, length 6421:39:18.039151 ARP, Request who-has 192.168.1.20 tell 192.168.1.10, length 2821:39:18.039176 ARP, Reply 192.168.1.20 is-at c6:b2:b2:46:60:cc, length 28 在 centos 7.5 上 ping 不通（内核 3.10，很老了）。网上找到的方法：12345echo 1 &gt; /proc/sys/net/ipv4/conf/veth1/accept_localecho 1 &gt; /proc/sys/net/ipv4/conf/veth0/accept_localecho 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/veth0/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/veth1/rp_filter还是 ping 不通。尴尬啊😥ps. 用不通的 network namespace 是可以 ping 通 veth pair。ps. 把 -I 的 veth0 换成对应 ip，可以 ping 通。但是抓包显示没有收到数据包 123456789101112131415161718[root@host143 ~]# ping -c 4 -I 192.168.1.10 192.168.1.20PING 192.168.1.20 (192.168.1.20) from 192.168.1.10 : 56(84) bytes of data.64 bytes from 192.168.1.20: icmp_seq=1 ttl=64 time=0.034 ms64 bytes from 192.168.1.20: icmp_seq=2 ttl=64 time=0.029 ms64 bytes from 192.168.1.20: icmp_seq=3 ttl=64 time=0.031 ms64 bytes from 192.168.1.20: icmp_seq=4 ttl=64 time=0.029 ms# 抓包没有数据，连 ICMP echo request 也没有# tcpdump -n -i veth0# tcpdump -n -i veth1# 流量直接经过 lo 设备。。。[root@host143 ~]# tcpdump -n -i lo21:52:24.716430 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 11671, seq 1, length 6421:52:24.716440 IP 192.168.1.20 &gt; 192.168.1.10: ICMP echo reply, id 11671, seq 1, length 6421:52:25.716256 IP 192.168.1.10 &gt; 192.168.1.20: ICMP echo request, id 11671, seq 2, length 6421:52:25.716266 IP 192.168.1.20 &gt; 192.168.1.10: ICMP echo reply, id 11671, seq 2, length 64TODO. 以后再解决 查看对端 veth 设备 123456789[root@host143 ~]# ip netns exec ns1 ethtool -S veth0NIC statistics: peer_ifindex: 318[root@host143 ~]# ip netns exec ns2 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00318: veth1@if319: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether fa:ad:be:ed:b0:29 brd ff:ff:ff:ff:ff:ff link-netnsid 0tun / taptap/tun 提供了一台主机内用户空间的数据传输机制。它虚拟了一套网络接口，这套接口和物理的接口无任何区别，可以配置 IP，可以路由流量，不同的是，它的流量只在主机内流通。TUN/TAP: The user-space application/VM can read or write an ethernet frame to the tap interface and it would reach the host kernel, where it would be handled like any other ethernet frame that reached the kernel via physical (e.g. eth0) ports. You can potentially add it to a software-bridge (e.g. linux-bridge)tun 是点对点的设备， 而 tap 是一个普通的以太网卡设备 。 也就是说 ，tun 设备其实完全不需要有物理地址的 。 它收到和发出的包不需要 arp， 也不需要有数据链路层的头 。 而 tap 设备则是有完整的物理地址和完整的以太网帧 。TUN（Tunel）设备模拟网络层设备，处理三层报文，如 IP 报文。TAP 设备模型链路层设备，处理二层报文，比如以太网帧。TUN 用于路由，而 TAP 用于创建网桥。TUN (tunnel) devices operate at layer 3, meaning the data (packets) you will receive from the file descriptor will be IP based. Data written back to the device must also be in the form of an IP packet.TAP (network tap) operates much like TUN however instead of only being able to write and receive layer 3 packets to/from the file descriptor it can do so with raw ethernet packets（layer 2）. You will typically see tap devices used by KVM/Qemu virtualization, where a TAP device is assigned to a virtual guests interface during creation. 来自 wiki，tun/tap 对应 osi 的层次：字符设备 /dev/net/tun 作为用户空间和内核空间交换数据的接口。当内核将数据包发送到虚拟网络设备时，数据包被保存在设备相关的一个队列中，直到用户空间程序通过打开的字符设备 tun 的描述符读取时，它才会被拷贝到用户空间的缓冲区中，其效果就相当于，数据包直接发送到了用户空间。tun/tap 驱动程序中包含两部分：字符设备驱动和网卡驱动。利用网卡驱动部分接受来自 tcp/ip 协议栈的网络分包并发送或者反过来将接收到的网络分包传给协议栈处理。而字符设备驱动部门将网络分包在内核与用户态之间传送，模拟物理链路的数据接受和发送。tun/tap 设备最常用的场景是 VPN，包括 tunnel 以及应用层的 IPSec 等。tap 通常用于创建虚拟机网卡。来自参考文章的例子： 小结 参考 TUN, TAP and Veth - Virtual Networking Devices ExplainedVirtual networking devices in LinuxLinux 上的基础网络设备详解Linux- 虚拟网络设备 -veth pairLinux- 虚拟网络设备 -tun/tap 云计算底层技术 - 虚拟网络设备(tun/tap,veth)Linux 虚拟网络设备之 vethveth.4.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux bridge 笔记]]></title>
    <url>%2Fp%2Flinux-bridge%2F</url>
    <content type="text"><![CDATA[bridge桥接就是把一台机器上的若干个网络接口“连接”起来。其结果是，其中一个网口收到的报文会被复制给其他网口并发送出去。以使得网口之间的报文能够互相转发。linux 中 bridge 是一种虚拟设备，通常作为 2 层交换机（链路层）。 Bridge 设备实例可以和 Linux 上其他网络设备实例连接。attach 一个从设备，类似于在现实世界中的交换机和一个用户终端之间连接一根网线。当有数据到达时，Bridge 会根据报文中的 MAC 信息进行广播、转发、丢弃处理。（当一个设备 attach 到 bridge 上时，该设备上的 IP 则变为无效，Linux 不在使用那个 IP 在三层接受数据。此时应该把该设备的 IP 赋值给 bridge 设备。）bridge 的一个作用是在容器化环境联通不同 network namespace：brctl 命令 brctl 是 linux 中操作网桥的命令。 首先安装 brctl。1yum install bridge-utils -y命令列表 123456789101112131415161718192021222324[root@host143 ~]# brctlUsage: brctl [commands]commands:# 增加 / 删除网桥 addbr &lt;bridge&gt; add bridge delbr &lt;bridge&gt; delete bridge# 绑定 / 解绑网卡 addif &lt;bridge&gt; &lt;device&gt; add interface to bridge delif &lt;bridge&gt; &lt;device&gt; delete interface from bridge# nat hairpin 模式 hairpin &lt;bridge&gt; &lt;port&gt; &#123;on|off&#125; turn hairpin on/off# mac 地址老化时间，超过则从 Forwarding DataBase 删除 setageing &lt;bridge&gt; &lt;time&gt; set ageing time setbridgeprio &lt;bridge&gt; &lt;prio&gt; set bridge priority setfd &lt;bridge&gt; &lt;time&gt; set bridge forward delay sethello &lt;bridge&gt; &lt;time&gt; set hello time setmaxage &lt;bridge&gt; &lt;time&gt; set max message age setpathcost &lt;bridge&gt; &lt;port&gt; &lt;cost&gt; set path cost setportprio &lt;bridge&gt; &lt;port&gt; &lt;prio&gt; set port priority show [&lt;bridge&gt;] show a list of bridges# 查看学习的 mac 地址，和老化时间 showmacs &lt;bridge&gt; show a list of mac addrs showstp &lt;bridge&gt; show bridge stp info stp &lt;bridge&gt; &#123;on|off&#125; turn stp on/offagingmac 地址学习和 mac 老化。通过反复这样的学习，交换机会构建一个基于所有端口的转发数据库，存储在交换机的内容可寻址存储器当中（CAM）。The bridge keeps track of ethernet addresses seen on each port. When it needs to forward a frame, and it happens to know on which port the destina‐tion ethernet address (specified in the frame) is located, it can ‘cheat’ by forwarding the frame to that port only, thus saving a lot of redundantcopies and transmits.However, the ethernet address location data is not static data. Machines can move to other ports, network cards can be replaced (which changes themachine’s ethernet address), etc. 相关参数 showmacs：查看学习的 mac 地址和老化时间setageing：如果一定时间内都没有流量经过 mac 地址，则把这个地址从 Forwarding DataBase 删除。setgcint：设置 brctl 每 N 秒检查一次 Forwarding DataBase。1234[root@host143 ~]# brctl showmacs virbr0port no mac addr is local? ageing timer 1 52:54:00:47:11:66 yes 0.00 1 52:54:00:47:11:66 yes 0.00 如果 aging 设置为 0，则退化为 hub（因为不记录学习到的 mac 地址）。stpSTP 协议（生成树协议）逻辑上断开环路，防止二层网络的广播风暴的产生。(TODO: 以后单独补充)showstp 可以看到所有相关参数。123456789101112root@host143 ~]# brctl showstp docker0docker0 bridge id 8000.0242a0f4db57 designated root 8000.0242a0f4db57 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 300.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 241.60 flags对比 桥接 / 交换 / 路由 桥接 / 交换工作在链路层（2 层）。路由工作在 IP 层（3 层）。路由划分的是独立的逻辑网段，每个所连接的网段都具有独立的网络 IP 地址信息，而不是以 MAC 地址作为判断路径的依据，这样路由便有隔离广播的能力。（通过判断目的 IP 与自己端口的 IP 是否一致，不一致就丢掉这个包，广播风暴就不会影响到其他网络里，只在自己小范围的网络里进行传递）而交换和桥接是划分物理网段，它们仅仅是将物理传输介质进行分段处理。参考Linux Bridge - how it worksHow to configure a Linux Bridge to act as a Hub instead of a SwitchVirtual switching technologies and Linux bridgeLinux 上的基础网络设备详解]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT hairpin 模式]]></title>
    <url>%2Fp%2Fnetwork-nat-hairpin%2F</url>
    <content type="text"><![CDATA[以下内容整理自：NAT 技术白皮书 。NAT hairpin 功能用于满足位于内网侧的用户之间或内网侧的用户与服务器之间通过 NAT 地址进行访问的需求。开启 NAT hairpin 的内网侧接口上会对报文同时进行源地址和目的地址的转换。 支持两种组网模式：P2P：位于内网侧的用户之间通过动态分配的 NAT 地址互访。C/S：位于内网侧的用户使用静态配置的 NAT 地址访问内网服务器。P2PP2P：位于内网侧的用户之间通过 NAT 地址互访。内网各主机首先向外网服务器注册自己 NAT 转换后的外网 IP 地址和端口号信息。当内网的一个客户端使用 NAT 地址访问内网的另一个客户端时，首先从服务器获取对方注册的 IP 地址和端口号信息，然后根据该信息与对方建立连接。 C/SC/S：位于内网侧的用户使用 NAT 地址访问内网服务器。内部网络中有一台 FTP 服务器，内网用户使用 NAT 地址访问该 FTP 服务器时，NAT 设备同时转换访问内网服务器的报文的源和目的 IP 地址，其中，目的 IP 地址转换通过匹配外网接口上的内部服务器配置来完成，源地址转换通过匹配内部服务器所在接口上的出方向动态地址转换或出方向静态地址转换来完成。 我的理解：host 访问内网 server 的公网 ip。router 对 host 出去的流量做 dnat 转换（把 server 公网 ip 替换为 server 内网 ip）。server 回流内网 host。router 对 host 入口流量做 snat 转换（因为 host 访问的公网 ip，如果回流 packet 是内网 ip，则被 host 丢弃）。参考NAT 技术白皮书]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux du df 命令]]></title>
    <url>%2Fp%2Flinux-du-df%2F</url>
    <content type="text"><![CDATA[一个常见的运维需求：看下当前目录的磁盘使用情况，并且排序。du 命令查看磁盘使用情况。-h：指定人类友好的输出，比如 K、M、G 等，而非以块为单位。-d：指定目录深度，不指定则递归遍历。排序使用 sort，没问题。因为 du 使用了 -h 选项，sort 默认不能很好处理带单位的排序，比如 1.4G 和 9M。sort 增加了 -h，支持人类友好的易读性数字(例如： 2K 1G) 排序（-h --human-numeric-sort）。12345678910111213141516171819202122[root@host143 ~]# du -hd 1 . | sort -hr1.4G .517M ./project246M ./frontend136M ./.cache123M ./standalone_143.etcd78M ./logs77M ./.pm2-dev74M ./.pm273M ./go59M ./.mozilla11M ./.npm1.5M ./.local672K ./.ansible132K ./nacos112K ./.config36K ./.vnc16K ./.ssh16K ./.dbus12K ./.node-diamond-client-cache4.0K ./.pip4.0K ./.oracle_jre_usage、df 查看磁盘剩余空间，就不多说了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vlan 学习笔记]]></title>
    <url>%2Fp%2Flinux-network-vlan%2F</url>
    <content type="text"><![CDATA[整理 vlan 学习笔记。子接口 子接口（subinterface）是通过协议和技术将一个物理接口（interface）虚拟出来的多个逻辑接口。对应的物理层接口成为主接口。子接口共用主接口的物理层参数，又可以分别配置各自的链路层和网络层参数。用户可以禁用或者激活子接口，这不会对主接口产生影响；但主接口状态的变化会对子接口产生影响，特别是只有主接口处于连通状态时子接口才能正常工作。优点：子接口打破了每个设备存在物理接口数量有限的局限性。缺点：多个子接口共用主接口，性能比单个物理接口差，负载大的情况下容易成为网络流量瓶颈。广播，组播，单播 广播 帧从单一的源发送到共享以太网上的所有主机。广播帧的目的 MAC 地址为十六进制的 FFFFFFFFFFFF，所有收到该广播帧的主机都要接收并处理这个帧。广播方式会产生大量流量，导致带宽利用率降低，进而影响整个网络的性能。单播 从单一的源端发送到单一的目的端。每个主机接口由一个 MAC 地址唯一标识，MAC 地址的 OUI 中，第一字节第 8 个比特表示地址类型。对于主机 MAC 地址，这个比特固定为 0，表示目的 MAC 地址为此 MAC 地址的帧都是发送到某个唯一的目的端。在冲突域中，所有主机都能收到源主机发送的单播帧，但是其他主机发现目的地址与本地 MAC 地址不一致后会丢弃收到的帧，只有真正的目的主机才会接收并处理收到的帧。组播 组播转发可以理解为选择性的广播，主机侦听特定组播地址，接收并处理目的 MAC 地址为该组播 MAC 地址的帧。组播 MAC 地址和单播 MAC 地址是通过第一字节中的第 8 个比特区分的。组播 MAC 地址的第 8 个比特为 1，而单播 MAC 地址的第 8 个比特为 0。广播域 vs 冲突域 冲突域：在同一个冲突域中的每一个节点都能收到所有被发送的帧。广播域：网络中能接收任一设备发出的广播帧的所有设备的集合。广播域可以跨网段，而冲突域只是发生的同一个网段的 。 冲突域：基于第一层（物理层）。广播域：基于第二层（数据链路层）。集线器（HUB） 所有端口都在同一个广播域，冲突域内。第二层交换机（Swith）所有端口都在同一个广播域内，而每一个端口就是一个冲突域。二层交换机，三层交换机 引用参考资料的图片：二层交换机工作在数据链路层，以太网交换机，依靠 MAC 表（MAC 和以太网端口映射表），交换机会记录每个端口连接的主机的 MAC。当交换机收到主机的数据包，提取目的 MAC 地址，查看目的 MAC 地址的主机连接哪个端口，然后往那个端口发送数据。当需要数据流量在 LAN 或 VLAN 之间交换时，则需要使用三层交换机。三层交换机工作在网络层，增加路由能力，根据 ip 地址转发；可配置不同 vlan 的 IP 地址，vlan 之间可通过三层路由实现不同 vlan 之间通讯。优势是当同样的数据流再次通过时，将此数据包直接从二层通过，这样减少了因为路由器的路由选择而造成网络的延迟，也可以大大提高转发效率。交换机和核心层、汇聚层、接入层 图片来源百度百科：核心层是互连网络的高速主干网，在设计中应增加冗余组件，使其具备高可靠性，能快速适应通信流量的变化。 设计核心层设备的功能时应避免使用数据包过滤、策略路由等降低转发速率的功能特性，使得核心层具有高速率、低延迟和良好的可管理性。 核心层设备覆盖的地理范围不宜过大，连接的设备不宜过多，否则会使得网络的复杂度增大，导致网络性能降低。 核心层应包括一条或多条连接外部网络的专用链路，使得可以高效地访问互联网。汇聚层是核心层与接入层之间的分界点，应实现资源访问控制和流量控制等功能。汇聚层应该对核心层隐藏接入层的详细信息，不管划分了多少个子网，汇聚层向核心路由器发布路由通告时，只通告各个子网汇聚后的超网地址 。如果局域网中运行了以太网和弹性分组环等不同类型的子网，或者运行了不同路由算法的区域网络，可以通过汇聚层设备完成路由汇总和协议转换功能。 接入层提供网络接入服务，并解决本地网段内用户之间互相访问的需求 ，要提供足够的带宽，使得本地用户之间可以高速访问； 接入层还应提供一部分管理功能，例如 MAC 地址认证、用户认证、计费管理等； 接入层要负责收集用户信息(例如用户 U 地址、MAC 地址、访问日志等)，作为计费和排错的依据。vlan 什么是 vlanVLAN 所指的 LAN 特指使用路由器分割的网络——也就是广播域。VLAN 是现代网络常用的网络虚拟化技术，它可以将物理的二层网络划分成多达 4094 个逻辑网络，这些逻辑网络在二层上是隔离的，每个逻辑网络（即 VLAN）由 VLAN ID 区分，VLAN ID 的取值为 1-4094。受主接口物理性能限制，实际中并无法完全达到 4096 个，数量越多，各子接口性能越差。为什么需要 vlan如果整个网络只有一个广播域，那么一旦发出广播信息，就会传遍整个网络，并且对网络中的主机带来额外的负担。因此，在设计 LAN 时，需要注意如何才能有效地分割广播域。二层交换机本来只能构建单一的广播域，不过使用 VLAN 功能后，它能够将网络分割成多个广播域。分割广播域时，一般都必须使用到路由器。使用路由器后，可以以路由器上的网络接口 (LAN Interface) 为单位分割广播域。使用路由器分割广播域的话，所能分割的个数完全取决于路由器的网络接口个数。VLAN 生成的逻辑上的交换机是互不相通的。因此，在交换机上设置 VLAN 后，如果未做其他处理，VLAN 间是无法通信的 。vlan 分类 静态 vlan基于端口的 VLAN(PortBased VLAN)。需要逐个端口指定配置，不适合那些需要频繁改变拓补结构的网络。动态 vlan动态 vlan 出发点是灵活配置网络拓扑结构。其细分类型可以对照 OSI 模型。基于 MAC 地址的 VLAN(MAC Based VLAN)如果计算机交换了网卡，还是需要更改设定。基于子网的 VLAN(Subnet Based VLAN)即使计算机因为交换了网卡或是其他原因导致 MAC 地址改变，只要它的 IP 地址不变，就仍可以加入原先设定的 VLAN。基于用户的 VLAN(User Based VLAN)根据交换机各端口所连的计算机上当前登录的用户，来决定该端口属于哪个 VLAN。这里的用户识别信息，一般是计算机操作系统登录的用户。vlan 和汇聚链接 汇聚链接（Trunk Link）指的是能够转发多个不同 VLAN 的通信的端口。汇聚链接背景：在现有网络基础上再新建 VLAN 时，为了让这个 VLAN 能够互通，就需要在交换机间连接新的网线，而建筑物楼层间的纵向布线比较麻烦。并且，VLAN 越多，楼层间（严格地说是交换机间）互联所需的端口也越来越多，交换机端口的利用率低是对资源的一种浪费、也限制了网络的扩展。为了避免这种低效率的连接方式，人们想办法让交换机间互联的网线集中到一根上，这时使用的就是 汇聚链接（Trunk Link）。在上面的组网，每个交换机分别处理 2 个 vlan：vlan10 和 vlan20。在没有 vlan trunk link 的情况下，如果要让两个交换机对应的 vlan10 和 vlan20 互通，则两个交换机需要分别占用 2 个端口：一个端口用于两侧交换机互通 vlan10一个端口用于两侧交换机互通 vlan20引入 vlan trunk link 之后，只需要使用一个端口，连同两个交换机即可。汇聚链路上流通的数据帧，都被附加了用于识别分属于哪个 VLAN 的特殊信息。对于复杂的组网，vlan 汇聚链接能够减少端口占用、减少布线、更加灵活。汇聚链接承载多个 vlan 的通信，负载较重，因此需要带宽更大。默认条件下，汇聚链接会转发交换机上存在的所有 VLAN 的数据。换一个角度看，可以认为汇聚链接 (端口) 同时属于交换机上所有的 VLAN。VLAN 的汇聚方式 —— IEEE802.1Q 与 ISL通过汇聚链路时附加的 VLAN 识别信息，常见的方式有：IEEE 802.1QCisco 产品独有的 ISL(Inter Switch Link)802.1q802.1q 协议在源地址和类型字段之间，增加了 4 个字节标记：2 字节的 TPID(Tag Protocol IDentifier)2 字节的 TCI(Tag Control Information)注意 vlan id 占用 12 bit，最多可供识别 4096 个 VLAN。因为 packet 多了 4 个字节，因此要更新末尾的 CRC 字段。而当数据帧离开汇聚链路时，TPID 和 TCI 会被去除，这时还会进行一次 CRC 的重新计算。cisco ISLISL 在数据帧头部增加了 26 字节的 ISL header，在数据帧尾部增加 4 字节的 CRC。总共增加了 30 字节。而当数据帧离开汇聚链路时，直接去掉头部的 ISL header 和尾部的 CRC，原来数据帧的 CRC 不用重新计算。路由器和三层交换机 非常值得去看 VLAN 基础知识 当中的 “VLAN 间路由” 章节，例子很清晰明了。以下内容来源于该文章：首先，先来复习一下为什么不同 VLAN 间不通过路由就无法通信。在 LAN 内的通信，必须在数据帧头中指定通信目标的 MAC 地址。而为了获取 MAC 地址，TCP/IP 协议下使用的是 ARP。ARP 解析 MAC 地址的方法，则是通过广播。也就是说，如果广播报文无法到达，那么就无从解析 MAC 地址，亦即无法直接通信。计算机分属不同的 VLAN，也就意味着分属不同的广播域，自然收不到彼此的广播报文。因此，属于不同 VLAN 的计算机之间无法直接互相通信。为了能够在 VLAN 间通信，需要利用 OSI 参照模型中更高一层——网络层的信息 (IP 地址) 来进行路由。解决 vlan 间的通信，有多种办法。使用路由器进行 VLAN 间路由 路由器和交换机的接线方式，大致有以下两种：(1)将路由器与交换机上的每个 VLAN 分别连接 (2) 不论 VLAN 有多少个，路由器与交换机都只用一条网线连接 实际上会使用方法 2。实施方案是：将用于连接路由器的交换机端口设为汇聚链接 (Trunk Link)，而路由器上的端口也必须支持汇聚链路。接着在路由器上定义对应各个 VLAN 的“子接口”(Sub Interface)。尽管实际与交换机连接的物理端口只有一个，但在理论上我们可以把它分割为多个虚拟端口。 收发信双方同属一个 VLAN 之内的通信，一切处理均在交换机内完成。不同 VLAN 间的通信，即使通信双方都连接在同一台交换机上，也必须经过：“发送方——交换机——路由器——交换机——接收方”这样一个流程。使用路由器进行 VLAN 间路由时的问题：路由器通常基于软件构建，处理数据能力低于基于硬件构建的交换机 流量会集中到路由器和交换机互联的汇聚链路部分，这一部分尤其特别容易成为速度瓶颈 于是出现三层交换机：交换模块硬件 + 路由模块硬件，并且二者通过内部高速链接互通。三层交换机则是在内部生成“VLAN 接口”(VLAN Interface)。VLAN 接口，是用于各 VLAN 收发数据的接口。加速 VLAN 间通信的手段 VLAN 间路由，必须经过外部的路由器或是三层交换机的内置路由模块。但是，有时并不是所有的数据都需要经过路由器(或路由模块)。 相同的 src ip、src port、dest ip、dest port 的一连串数据包被称为“流”(Flow)。只要将流最初的数据正确地路由以后，后继的数据理应也会被同样地路由。整个流的第一块数据，照常由交换机转发→路由器路由→再次由交换机转发到目标所连端口。这时，将第一块数据路由的结果记录到缓存里保存下来。需要记录的信息有：目标 IP 地址 源 IP 地址 目标 TCP/UDP 端口号 源 TCP/UDP 端口号 接收端口号 (交换机) 转发端口号 (交换机) 转发目标 MAC 地址 路由器和 L3 交换机的对比 路由器的必要性：连接 WAN网络安全性。路由器除了最基本的数据包过滤功能外，还能基于 IPSec 构建 VPN(VirtualPrivate Network)、利用 RADIUS 进行用户认证等等。支持除 TCP/IP 以外的异构网络架构 小结 VLAN 间路由，必须经过外部的路由器或是三层交换机的内置路由模块 虽然利用 VLAN 可以灵活地构建网络，但是同时，它也带来了网络结构复杂化的问题。参考OSI 七层模型与 TCP/IP 五层模型VLAN 基础知识]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>vlan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker macvlan network 实验]]></title>
    <url>%2Fp%2Fdocker-macvlan-network%2F</url>
    <content type="text"><![CDATA[整理 macvlan 学习笔记。混杂模式 默认情况下网卡只把发给本机的包（包括广播包）传递给上层程序，其它的包一律丢弃。混杂模式（Promiscuous Mode）是指一台机器能够接收所有经过它的数据流，而不论其目的地址是否是它。具体的地址转发则是在接受到数据后由 MAC 层来进行。使用 ifconfig 操作 promisc 模式：123456789101112131415161718192021222324252627282930313233(base) [root@publicService ~]# ifconfig ens192 ens192: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.25.22.109 netmask 255.255.252.0 broadcast 172.25.23.255 inet6 fe80::bca3:c77b:a9b:a75a prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:b0:80:c6 txqueuelen 1000 (Ethernet) RX packets 249274336 bytes 18440080984 (17.1 GiB) RX errors 0 dropped 101622 overruns 0 frame 0 TX packets 17261240 bytes 2476002584 (2.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 开启 promisc(base) [root@publicService ~]# ifconfig ens192 promisc(base) [root@publicService ~]# ifconfig ens192ens192: flags=4419&lt;UP,BROADCAST,RUNNING,PROMISC,MULTICAST&gt; mtu 1500 inet 172.25.22.109 netmask 255.255.252.0 broadcast 172.25.23.255 inet6 fe80::bca3:c77b:a9b:a75a prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:b0:80:c6 txqueuelen 1000 (Ethernet) RX packets 249275279 bytes 18440147356 (17.1 GiB) RX errors 0 dropped 101622 overruns 0 frame 0 TX packets 17261309 bytes 2476012173 (2.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 关闭 promisc(base) [root@publicService ~]# ifconfig ens192 -promisc(base) [root@publicService ~]# ifconfig ens192ens192: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.25.22.109 netmask 255.255.252.0 broadcast 172.25.23.255 inet6 fe80::bca3:c77b:a9b:a75a prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:b0:80:c6 txqueuelen 1000 (Ethernet) RX packets 249275622 bytes 18440170311 (17.1 GiB) RX errors 0 dropped 101622 overruns 0 frame 0 TX packets 17261333 bytes 2476015397 (2.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0或者使用 ip 命令 1234567891011121314151617181920212223# 开启 promisc(base) [root@publicService ~]# ip link set ens192 promisc on(base) [root@publicService ~]# ifconfig ens192ens192: flags=4419&lt;UP,BROADCAST,RUNNING,PROMISC,MULTICAST&gt; mtu 1500 inet 172.25.22.109 netmask 255.255.252.0 broadcast 172.25.23.255 inet6 fe80::bca3:c77b:a9b:a75a prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:b0:80:c6 txqueuelen 1000 (Ethernet) RX packets 249285891 bytes 18440882352 (17.1 GiB) RX errors 0 dropped 101622 overruns 0 frame 0 TX packets 17261762 bytes 2476085900 (2.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 关闭 promisc(base) [root@publicService ~]# ip link set ens192 promisc off(base) [root@publicService ~]# ifconfig ens192ens192: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.25.22.109 netmask 255.255.252.0 broadcast 172.25.23.255 inet6 fe80::bca3:c77b:a9b:a75a prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:b0:80:c6 txqueuelen 1000 (Ethernet) RX packets 249286554 bytes 18440926574 (17.1 GiB) RX errors 0 dropped 101622 overruns 0 frame 0 TX packets 17261792 bytes 2476091316 (2.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0macvlanmacvlan 是一种网卡虚拟化技术。允许在同一个物理网卡上配置多个 MAC 地址，即多个 interface，每个 interface 可以配置自己的 IP。macvlan 的最大优点是性能极好，相比其他实现，macvlan 不需要创建 Linux bridge。Macvlan 的缺点是需要将主机网卡（NIC）设置为混杂模式（Promiscuous Mode），这在大部分公有云平台上是不允许的。 工作在混乱模式下的物理网卡，其 MAC 地址会失效，所以，此模式中运行的容器并不能与外网进行通信。macvlan 接口会监听并接收链路上到达本 mac 地址的报文，因此 macvlan（除 bridge 外）仅能向外部网络发送报文，并接受目的为本机 mac 的报文。一个网卡只能创建一个 macvlan 网络, 如果要支持多个 macvlan 网络, 就要使用 vlan 子接口实现多 macvlan 网络。来自 docker 官网的示意图：macvlan 模式 macvlan 有四种模式：VEPA，bridge，Private 和 Passthru。VEPAVirtual Ethernet Port Aggregator。 在 VEPA 模式下，所有从 Macvlan 接口发出的流量，不管目的地全部都发送给父接口，即使流量的目的地是共享同一个父接口的其它 Macvlan 接口。不同子接口的通信，必须通过交换机实现，并且交换机要支持 802.1q。Linux 主机可以通过一种 Hairpin 模式的 Bridge 来让 VEPA 模式下的不同 Macvlan 接口通信。1brctl hairpin br0 eth0 on适合场景：因为所有流量都要经过交换机，适合在外部交换机上做流量进行优化设定。比如物理交换机和虚拟机 / 容器之间的通信。扩展：hairpin 技术 Hairpin 技术需要 NAT 网关支持，它能够让两台位于同一台 NAT 网关后面的主机，通过对方的公网地址和端口相互访问，NAT 网关会根据一系列规则，将对内部主机发往其 NAT 公网 IP 地址的报文进行转换，并从私网接口发送给目标主机。hairpin 在这篇文章进一步学习：NAT hairpin 模式bridgemacvlan 的默认模式。 拥有相同父接口的两块 Macvlan 虚拟网卡是可以直接通讯的，不需要把流量通过父网卡发送到外部网络。类似于 linux bridge，但是不需要 mac 地址学习、STP，因此性能更好。适合场景：让共享同一个父接口的 Macvlan 网卡进行直接通讯的场景 扩展知识：mac 地址学习，来自百度百科：描述学习网桥的服务，每个所接收的分组的源 MAC 地址存储在这个学习网桥里，以便将来送达该地址的分组只转发到此地址所在伪网桥接口。前往未识别地址的分组则转发到每个网桥接口上。这个方法有助于把相连局域网的业务量最小化。STP （生成树协议），来自百度百科：STP（Spanning Tree Protocol）是生成树协议的英文缩写，可应用于计算机网络中树形拓扑结构建立，主要作用是防止网桥网络中的冗余链路形成环路工作。生成树协议是 IEEE 802.1D 中定议的数据链路层协议，用于解决在网络的核心层构建冗余链路里产生的网络环路问题，通过在交换机之间传递网桥协议数据单元（Bridge Protocol Data Unit，简称 BPDU），通过采用 STA 生成树算法选举根桥、根端口和指定端口的方式，最终将网络形成一个树形结构的网络，其中，根端口、指定端口都处于转发状态，其他端口处于禁用状态。如果网络拓扑发生改变，将重新计算生成树拓扑。生成树协议的存在，既解决了核心层网络需要冗余链路的网络健壮性要求，又解决了因为冗余链路形成的物理环路导致“广播风暴”问题。由于协议机制本身的局限，STP 保护速度慢。private完全阻止共享同一父接口的 Macvlan 虚拟网卡之间的通讯，即使配置了 Hairpin 让从父接口发出的流量返回到宿主机，相应的通讯流量依然被丢弃。具体实现方式是丢弃广播 / 多播数据，这就意味着以太网地址解析 arp 将不可运行，除非手工探测 MAC 地址，否则通信将无法在同一宿主机下的多个 Macvlan 网卡间展开。passthru直接把父接口和相应的 MacVLAN 接口捆绑在一起，这种模式每个父接口只能和一个 Macvlan 虚拟网卡接口进行捆绑 ，并且 Macvlan 虚拟网卡接口继承父接口的 MAC 地址。 适用场景：需要在虚拟机 / 容器修改网卡的 mac 地址等参数。docker 创建 macvlan (bridge 模式)首先查找物理网卡对应网关地址 1234567(base) [root@publicService opt]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.25.23.254 0.0.0.0 UG 100 0 0 ens192172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0172.21.0.0 0.0.0.0 255.255.0.0 U 0 0 0 br-b0a8e75c0663172.25.20.0 0.0.0.0 255.255.252.0 U 100 0 0 ens192ens192 网卡 ip 地址为 172.25.20.143，对应的网关为 172.25.23.254。 创建 macvlan 网络。一个 docker host 只能创建一个 macvlan 类型的网络。12345[root@host143 ~]# docker network create -d macvlan mymacvlan \&gt; --subnet=172.25.20.0/24 \&gt; --gateway=172.25.23.254 \&gt; -o parent=ens192no matching subnet for gateway 172.25.23.254原因：A gateway address must be on the same subnet as an interface.解决；修改子网地址，与 gateway 在同一个子网。1234docker network create -d macvlan mymacvlan \--subnet=172.25.23.0/24 \--gateway=172.25.23.254 \-o parent=ens192创建容器 my-alpine-112345678910[root@host143 ~]# docker run --rm -it --network mymacvlan --name my-alpine-1 alpine:3.11.6 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever312: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 02:42:ac:19:17:01 brd ff:ff:ff:ff:ff:ff inet 172.25.23.1/24 brd 172.25.23.255 scope global eth0 valid_lft forever preferred_lft forever创建容器 my-alpine-212345678910[root@host143 ~]# docker run --rm -it --network mymacvlan --name my-alpine-2 alpine:3.11.6 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever313: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 02:42:ac:19:17:02 brd ff:ff:ff:ff:ff:ff inet 172.25.23.2/24 brd 172.25.23.255 scope global eth0 valid_lft forever preferred_lft forever观察这两个容器的 eth0 网卡 ip 地址，在同一个子网。因为是 macvlan 网络，并且默认是 bridge 模式，两个容器可以直接 ping 通。123456789/ # ping 172.25.23.2PING 172.25.23.2 (172.25.23.2): 56 data bytes64 bytes from 172.25.23.2: seq=0 ttl=64 time=0.096 ms64 bytes from 172.25.23.2: seq=1 ttl=64 time=0.060 ms64 bytes from 172.25.23.2: seq=2 ttl=64 time=0.057 ms^C--- 172.25.23.2 ping statistics ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max = 0.057/0.071/0.096 ms当使用 Macvlan 时，宿主机无法和 VM 或容器直接进行通讯。在容器 ping 宿主机，ping 不通。12345/ # ping 172.25.20.143 PING 172.25.20.143 (172.25.20.143): 56 data bytes^C--- 172.25.20.143 ping statistics ---41 packets transmitted, 0 packets received, 100% packet loss在宿主机 ping 容器，也不通。12345678910[root@host143 ~]# ping 172.25.23.2PING 172.25.23.2 (172.25.23.2) 56(84) bytes of data.From 172.25.20.143 icmp_seq=1 Destination Host UnreachableFrom 172.25.20.143 icmp_seq=2 Destination Host UnreachableFrom 172.25.20.143 icmp_seq=3 Destination Host UnreachableFrom 172.25.20.143 icmp_seq=4 Destination Host Unreachable^C--- 172.25.23.2 ping statistics ---4 packets transmitted, 0 received, +4 errors, 100% packet loss, time 3001mspipe 4参考Linux 虚拟网卡技术：MacvlanDocker Macvlan 网络驱动使用详解macvlan 和 pipeworkdocker 网络之 macvlanDocker 网络 host、bridge、macvlan 基本原理和验证]]></content>
      <categories>
        <category>vlan</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>vlan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker bridge network 实验]]></title>
    <url>%2Fp%2Fdocker-bridge-network%2F</url>
    <content type="text"><![CDATA[创建自定义 brdige，并且观察。123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@host143 ~]# docker network create --driver bridge my-net# 分配了子网网段和网关地址 # 新建容器的时候，可以使用 --ip 指定静态 ip，重启容器不会改变 ip[root@host143 ~]# docker network inspect my-net[ &#123; "Name": "my-net", # Id 也是 bridge id "Id": "c8a0eda8ec92485fbb60035ef38fd8a36b71adf0f0f7cc23832f1fe7a70168d8", "Created": "2020-05-26T19:57:15.748691755+08:00", "Scope": "local", "Driver": "bridge", "EnableIPv6": false, "IPAM": &#123; "Driver": "default", "Options": &#123;&#125;, "Config": [ &#123; "Subnet": "172.18.0.0/16", "Gateway": "172.18.0.1" &#125; ] &#125;, "Internal": false, "Attachable": false, "Ingress": false, "ConfigFrom": &#123; "Network": "" &#125;, "ConfigOnly": false, "Containers": &#123;&#125;, "Options": &#123;&#125;, "Labels": &#123;&#125; &#125;]# 新建的 bridge name 就是 br-[bridge id][root@host143 ~]# brctl showbridge name bridge id STP enabled interfacesbr-8fc7e4e9385e 8000.024215e9e52d no # c8a0eda8ec92 对应 bridge idbr-c8a0eda8ec92 8000.0242b1af64a8 no docker0 8000.0242a0f4db57 no virbr0 8000.525400471166 yes virbr0-nic 使用自定义 bridge：12345678[root@host143 ~]# docker create --rm --name my-alpine --network my-net alpine:latest topWARNING: IPv4 forwarding is disabled. Networking will not work.25c20e848724ce6623dbe05b488e5e738631852e66d990ecd934a19b9644d907[root@host143 ~]# docker start my-alpine[root@host143 ~]# docker exec -it my-alpine ping baidu.comping: bad address 'baidu.com'为什么 ping 不同外部网络呢？By default, traffic from containers connected to the default bridge network is not forwarded to the outside world.为了解决从 docker 容器到 bridge 网络不会转发到外部，要开启 forwarding：1234567891011121314# 查看 [root@host143 ~]# sysctl net.ipv4.conf.all.forwardingnet.ipv4.conf.all.forwarding = 0# 修改 forwarding[root@host143 ~]# sysctl net.ipv4.conf.all.forwarding=1net.ipv4.conf.all.forwarding = 1# 修改 forward 链[root@host143 ~]# iptables -P FORWARD ACCEPT[root@host143 ~]# docker exec -it my-alpine ping baidu.comPING baidu.com (220.181.38.148): 56 data bytes64 bytes from 220.181.38.148: seq=0 ttl=49 time=37.772 msdocker 不同 network 的隔离，表现在 2 条 isolation-stage 链：12345678910*filter# br-c8a0eda8ec92 是刚才建立 my-net 的网卡。# 来源于 br-c8a0eda8ec92、并且发送到 docker0 的包，都丢弃# 来源于 docker0、并且发送到 br-c8a0eda8ec92 的包，都丢弃-A DOCKER-ISOLATION-STAGE-1 -i br-c8a0eda8ec92 ! -o br-c8a0eda8ec92 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -j RETURN-A DOCKER-ISOLATION-STAGE-2 -o br-c8a0eda8ec92 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -j RETURN 如果要连同 2 个网络，使用 connect 命令：123456789101112131415161718192021222324252627282930313233343536373839404142[root@host143 ~]# docker network create --driver bridge my-net22e46e8012ef53d54f59467544f432989797989c1792dfc772812d71e114d3eb8[root@host143 ~]# docker network inspect my-net2[ &#123; "Name": "my-net2", "Id": "2e46e8012ef53d54f59467544f432989797989c1792dfc772812d71e114d3eb8", "Created": "2020-05-27T14:21:59.300644879+08:00", "Scope": "local", "Driver": "bridge", "EnableIPv6": false, "IPAM": &#123; "Driver": "default", "Options": &#123;&#125;, "Config": [ &#123; "Subnet": "172.20.0.0/16", "Gateway": "172.20.0.1" &#125; ] &#125;, "Internal": false, "Attachable": false, "Ingress": false, "ConfigFrom": &#123; "Network": "" &#125;, "ConfigOnly": false, "Containers": &#123;&#125;, "Options": &#123;&#125;, "Labels": &#123;&#125; &#125;][root@host143 ~]# brctl showbridge name bridge id STP enabled interfacesbr-2e46e8012ef5 8000.0242c0a774bf no br-8fc7e4e9385e 8000.024215e9e52d no br-c8a0eda8ec92 8000.0242b1af64a8 no docker0 8000.0242a0f4db57 no virbr0 8000.525400471166 yes virbr0-nic1234567891011121314151617181920212223242526272829303132333435363738394041[root@host143 ~]# docker create --rm --name my-alpine --network my-net alpine:latest top428ff5effcb821205a7a899235d399831e24db47b239919d12ed3feb5b1a6711[root@host143 ~]# docker network connect my-net2 my-alpine[root@host143 ~]# docker inspect --format='&#123;&#123;json .NetworkSettings.Networks&#125;&#125;' my-alpine # 省略其他输出 "Networks": &#123; "my-net": &#123; "IPAMConfig": null, "Links": null, "Aliases": null, "NetworkID": "", "EndpointID": "", "Gateway": "", "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "MacAddress": "", "DriverOpts": null &#125;, "my-net2": &#123; "IPAMConfig": &#123;&#125;, "Links": null, "Aliases": [ "428ff5effcb8" ], "NetworkID": "", "EndpointID": "", "Gateway": "", "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "MacAddress": "", "DriverOpts": &#123;&#125; &#125; &#125;iptables-save 查看规则变化。br-c8a0eda8ec92 对应 my-net。br-2e46e8012ef5 对应 my-net2。12345678*filter-A DOCKER-ISOLATION-STAGE-1 -i br-2e46e8012ef5 ! -o br-2e46e8012ef5 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i br-c8a0eda8ec92 ! -o br-c8a0eda8ec92 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -j RETURN-A DOCKER-ISOLATION-STAGE-2 -o br-2e46e8012ef5 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -j RETURN 更直观的是 ifconfig，注意两张网卡的网段。12345678910111213141516171819[root@host143 ~]# docker exec -it my-alpine sh/ # ifconfig# 对应 my-neteth0 Link encap:Ethernet HWaddr 02:42:AC:12:00:02 inet addr:172.18.0.2 Bcast:172.18.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:648 (648.0 B) TX bytes:0 (0.0 B)# 对应 my-net2eth1 Link encap:Ethernet HWaddr 02:42:AC:14:00:02 inet addr:172.20.0.2 Bcast:172.20.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:24 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2634 (2.5 KiB) TX bytes:0 (0.0 B)使用 docker network disconnect 可以实时取消自定义网络，且不需要重启容器。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker overlay network 实验]]></title>
    <url>%2Fp%2Fdocker-overlay-network%2F</url>
    <content type="text"><![CDATA[部署结构 两台机器 143、109，使用 2376 端口通信。使用 docker compose 搭建 etcd 集群，部署在 143。使用 docker compose 部署 etcd 集群 123456789101112131415161718192021222324252627282930313233343536373839404142434445version: '3'networks: myetcd: volumes: data-etcd1: data-etcd2: data-etcd3:services: etcd1: image: quay.io/coreos/etcd container_name: etcd1 command: etcd -name etcd1 -advertise-client-urls http://etcd1:12379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster "etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380" -initial-cluster-state new ports: - 12379:2379 - 12380:2380 volumes: - data-etcd1:/etcd-data networks: - myetcd etcd2: image: quay.io/coreos/etcd container_name: etcd2 command: etcd -name etcd2 -advertise-client-urls http://etcd2:22379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster "etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380" -initial-cluster-state new ports: - 22379:2379 - 22380:2380 volumes: - data-etcd2:/etcd-data networks: - myetcd etcd3: image: quay.io/coreos/etcd container_name: etcd3 command: etcd -name etcd3 -advertise-client-urls http://etcd3:32379 -listen-client-urls http://0.0.0.0:2379 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token etcd-cluster -initial-cluster "etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380" -initial-cluster-state new ports: - 32379:2379 - 32380:2380 volumes: - data-etcd3:/etcd-data networks: - myetcdetcd 使用 2 个端口，一个用于集群通信，一个用于客户端访问。 集群（cluster、peer）使用 2380。客户端（client）使用 2379。3 个实例的对外端口分别增加 1w、2w、3w，并且映射到宿主机，方便做实验。注意几个选项：-advertise-client-urls：通知其他 ETCD 节点，当前客户端接入本节点的监听地址。-listen-client-urls： 客户端请求监听地址。0.0.0.0为允许接收任意 ip 的请求。-listen-peer-urls： 本节点与其他节点进行数据交换 (选举，数据同步) 的监听地址。0.0.0.0为允许接收任意 ip 的请求。docker 实例必须能够访问 -advertise-client-urls，因此上面的 yaml 把该项设置为external 端口。（其实也可以使用 host 网络） 另外，etcd 规定，如果 --listen-client-urls 被设置了，那么就必须同时设置 --advertise-client-urls，所以即使设置和默认相同，也必须显式设置。etcd 集群部署在 143，为了让 109 机器正常识别 etcd1、etcd2、etcd3 主机名（-advertise-client-urls 选项），在 109 机器 hosts 文件增加映射到 143。启动上面的 etcd 集群 1docker-compose up -d 修改 docker servicenode143123456[root@host143 etcd]# vi /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=etcd://172.25.20.143:12379 --cluster-advertise=172.25.20.143:2376[root@host143 etcd]# systemctl daemon-reload[root@host143 etcd]# service docker restartRedirecting to /bin/systemctl restart docker.service注意选项：--cluster-store是分布式存储，支持 etcd、consul、zookeeper。--cluster-advertise是本 docker 实例对外公告的地址 这里挖了个坑：143 机器上使用 docker-compose 部署了 etcd 集群，那么 docker 重启并没有自动拉起 docker compose 的 etcd 集群，导致报错。使用 journalctl -fu docker 看到的日志：1Error response from daemon: pool configuration failed because of 100: Key not found (/docker/network/v1.0/ipam) [28]解决：修改 docker.service 文件，重启 docker 之后，再建立 etcd 集群。node109123456[root@host109 etcd]# vi /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=etcd://172.25.20.143:12379 --cluster-advertise=172.25.22.109:2376[root@host109 etcd]# systemctl daemon-reload[root@host109 etcd]# service docker restartRedirecting to /bin/systemctl restart docker.service开启 forwarding检查 forwarding 是否开启 12sysctl net.ipv4.conf.all.forwarding=1iptables -P FORWARD ACCEPT 创建 overlay 网络 在 node143 创建 overlay 网络 1docker network create --driver overlay myoverlay 在 node109 上可以看到 myoverlay，注意 scope 为 global。1234567(base) [root@publicService ~]# docker network lsNETWORK ID NAME DRIVER SCOPEe376d138725b bridge bridge localf0adf93c5be7 docker_gwbridge bridge local17ddb8efc8f4 host host local667bb5b0b972 myoverlay overlay global16ebe9d2b2e0 none null local观察容器网络连通性 在 143 执行 12345678910111213141516# --network 指定使用的网络docker run --network myoverlay --rm -it --name alpine_143 alpine:3.11.6 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever301: eth0@if302: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP link/ether 02:42:0a:00:00:02 brd ff:ff:ff:ff:ff:ff inet 10.0.0.2/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever303: eth1@if304: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff inet 172.19.0.2/16 brd 172.19.255.255 scope global eth1 valid_lft forever preferred_lft forever143 有 2 个网卡：eth0: 指向 overlay 网络，分配地址 10.0.0.2eth1: 私有网络，分配地址 172.19.0.2 在 109 执行 123456789101112131415docker run --network myoverlay --rm -it --name alpine_109 alpine:3.11.6 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever274: eth0@if275: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP link/ether 02:42:0a:00:00:03 brd ff:ff:ff:ff:ff:ff inet 10.0.0.3/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever276: eth1@if277: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1 valid_lft forever preferred_lft forever109 有 2 个网卡：eth0: 指向 overlay 网络，分配地址 10.0.0.3eth1: 私有网络，分配地址 172.18.0.2 同时在 143 或者 109 观察 overlay 网络 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647(base) [root@publicService ~]# docker network inspect myoverlay[ &#123; &quot;Name&quot;: &quot;myoverlay&quot;, &quot;Id&quot;: &quot;667bb5b0b97288d859292ba35378d3b927c2bfda9ec39e7e0a49d1d6f927d33e&quot;, &quot;Created&quot;: &quot;2020-06-02T17:01:15.959652724+08:00&quot;, &quot;Scope&quot;: &quot;global&quot;, &quot;Driver&quot;: &quot;overlay&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;10.0.0.0/24&quot;, &quot;Gateway&quot;: &quot;10.0.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;95686ec61aa208df6b51cb5b07296e7971c780746cd3414716d81f14292047a3&quot;: &#123; &quot;Name&quot;: &quot;alpine_109&quot;, &quot;EndpointID&quot;: &quot;40eac211fea5351a39d5c0874c106c752f9aea3db76cb393772d8023d0ef7e4c&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:03&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.3/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;ep-646ed81e8f2c0a527389e0b44ec9087436634d1a9f0710900efaa5ffd3bf94f7&quot;: &#123; &quot;Name&quot;: &quot;alpine_143&quot;, &quot;EndpointID&quot;: &quot;646ed81e8f2c0a527389e0b44ec9087436634d1a9f0710900efaa5ffd3bf94f7&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:02&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.2/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;]overlay 网络分配了子网10.0.0.0/24。 测试网络联通性，在 alpine_143 ping alpine_109:123456789/ # ping 10.0.0.3PING 10.0.0.3 (10.0.0.3): 56 data bytes64 bytes from 10.0.0.3: seq=0 ttl=64 time=0.440 ms64 bytes from 10.0.0.3: seq=1 ttl=64 time=0.338 ms64 bytes from 10.0.0.3: seq=2 ttl=64 time=0.234 ms^C--- 10.0.0.3 ping statistics ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max = 0.234/0.337/0.440 ms重温题图：]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables docker]]></title>
    <url>%2Fp%2Fiptables-docker%2F</url>
    <content type="text"><![CDATA[docker 增加的 chaindocker v18 以后有 4 个 iptables chain：DOCKERDOCKER-USERDOCKER-ISOLATION-STAGE-1DOCKER-ISOLATION-STAGE-2DOCKER 链和 DOCKER-USER 链 DOCKER 链处理从宿主机到 docker0 的 IP 数据包。 如果有自定义的规则，并且在 DOCKER chain 之前生效，则使用DOCKER-USER。12345678[root@master-29 ~]# iptables --list DOCKERChain DOCKER (1 references)target prot opt source destination[root@master-29 ~]# iptables --list DOCKER-USERChain DOCKER-USER (1 references)target prot opt source destination RETURN all -- anywhere anywhereDOCKER-ISOLATION-STAGE 链2 条 DOCKER-ISOLATION-STAGE 链是为了隔离在不同的 bridge 网络之间的容器。 摘抄自《基于 iptables 的 Docker 网络隔离与通信详解 》DOCKER-ISOLATION-STAGE-1 链过滤源地址是 bridge 网络（默认 docker0）的 IP 数据包，匹配的 IP 数据包再进入 DOCKER-ISOLATION-STAGE-2 链处理，不匹配就返回到父链 FORWARD。 在 DOCKER-ISOLATION-STAGE-2 链中，进一步处理目的地址是 bridge 网络的 IP 数据包，匹配的 IP 数据包表示该 IP 数据包是从一个 bridge 网络的网桥发出，到另一个 bridge 网络的网桥，这样的 IP 数据包来自其他 bridge 网络，将被直接 DROP；不匹配的 IP 数据包就返回到父链 FORWARD 继续进行后续处理。DOCKER-ISOLATION-STAGE-1 处理 source 为 docker0 的流量。DOCKER-ISOLATION-STAGE-2 处理 dest 为 docker0 的流量。123456789101112[root@master-29 ~]# iptables -nvL DOCKER-ISOLATION-STAGE-1Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 1543K 582M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 [root@master-29 ~]# iptables -nvL DOCKER-ISOLATION-STAGE-2Chain DOCKER-ISOLATION-STAGE-2 (1 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0也可以用 iptables-save 观察，nat 表规则：1234567891011121314*nat:PREROUTING ACCEPT [14084225:999405411]:INPUT ACCEPT [14083892:999374948]:OUTPUT ACCEPT [82455:5057925]:POSTROUTING ACCEPT [82455:5057925]:DOCKER - [0:0]# 1 对于发向本地的包，都经过 DOCKER 链 -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER# 2 目标地址不是 127.0.0.0/8、且发向本地的包，都经过 DOCKER 链-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER# 3 源地址为 172.17.0.0/16 （docker 容器）、并且目标接口不是 docker0（即发向外部流量），进行 MASQUERADE（源地址转换成主机网卡的地址）。-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE# 4 来自 docker0 的流量，返回上一级链处理-A DOCKER -i docker0 -j RETURNfilter 表规则：123456789101112131415161718192021222324252627282930313233343536373839*filter:INPUT ACCEPT [587512631:125743990396]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [423226964:67793677714]:DOCKER - [0:0]:DOCKER-ISOLATION-STAGE-1 - [0:0]:DOCKER-ISOLATION-STAGE-2 - [0:0]:DOCKER-USER - [0:0]-A INPUT -i virbr0 -p udp -m udp --dport 53 -j ACCEPT-A INPUT -i virbr0 -p tcp -m tcp --dport 53 -j ACCEPT-A INPUT -i virbr0 -p udp -m udp --dport 67 -j ACCEPT-A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT-A FORWARD -j DOCKER-USER-A FORWARD -j DOCKER-ISOLATION-STAGE-1-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o docker0 -j DOCKER-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A FORWARD -o br-8fc7e4e9385e -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o br-8fc7e4e9385e -j DOCKER-A FORWARD -i br-8fc7e4e9385e ! -o br-8fc7e4e9385e -j ACCEPT-A FORWARD -i br-8fc7e4e9385e -o br-8fc7e4e9385e -j ACCEPT-A FORWARD -d 192.168.122.0/24 -o virbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -s 192.168.122.0/24 -i virbr0 -j ACCEPT-A FORWARD -i virbr0 -o virbr0 -j ACCEPT-A FORWARD -o virbr0 -j REJECT --reject-with icmp-port-unreachable-A FORWARD -i virbr0 -j REJECT --reject-with icmp-port-unreachable-A OUTPUT -o virbr0 -p udp -m udp --dport 68 -j ACCEPT# 来自 docker0、并且不是发向 docker0 的包，执行 DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i br-8fc7e4e9385e ! -o br-8fc7e4e9385e -j DOCKER-ISOLATION-STAGE-2# 返回上一层链处理-A DOCKER-ISOLATION-STAGE-1 -j RETURN# stage-2 中，发向 docker0 的包都丢弃-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o br-8fc7e4e9385e -j DROP# 返回上一层链处理-A DOCKER-ISOLATION-STAGE-2 -j RETURN-A DOCKER-USER -j RETURN 限制访问 docker host默认情况下，所有流量都可以访问 docker 主机。以下是限制只允许 192.168.1.1 访问 docker host：1iptables -I DOCKER-USER -i ext_if ! -s 192.168.1.1 -j DROP允许 docker 主机转发流量 docker 默认设置 FORWARD 链为 DROP。 如果 docker 主机恰好是 router，则需要设置：1iptables -I DOCKER-USER -i src_if -o dst_if -j ACCEPT参考 Docker and iptables 基于 iptables 的 Docker 网络隔离与通信详解 理解 Docker 网络(一) – Docker 对宿主机网络环境的影响Use bridge networksdocker 的几种原生网络模式]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables 笔记]]></title>
    <url>%2Fp%2Flinux-iptables%2F</url>
    <content type="text"><![CDATA[整理 iptables 学习笔记，内容基本复制粘贴自参考资料整理。工作流程 iptables 能够访问底层的 netfilter 模块。 找到一张图，比较精简的阐述 netfilter 处理包的流程：下面这张图就更加详细：(图片来源：iptables-processing-flowchart)表 tablesraw 表：关闭 nat 表上启用的连接追踪机制；mangle 表：拆解报文，做出修改，并重新封装 的功能；nat 表：network address translation，网络地址转换功能；filter 表：负责过滤功能，防火墙；security 用于强制访问控制网络规则（例如：SELinux )优先级次序（由高而低）：1raw --&gt; mangle --&gt; nat --&gt; filter大部分情况仅需要使用 filter 和 nat。链 chains链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一条或数条规则。PREROUTING —— 进入 netfilter 后的数据包在进入路由判断前执行的规则 FORWARD —— 经过路由判断后，目的地不是本机的数据包执行的规则。与 nat 和 mangle 表相关联很高，与本机没有关联。INPUT —— 要进入本机的数据包执行的规则OUTPUT —— 由本机产生，需向外发的数据包执行的规则POSTROUTING —— 数据包准备离开 netfilter 时执行的规则 本机路由转发的时候，才配置 FORWARD 转发链。(/proc/sys/net/ipv4/ip_forward，0 表示禁止数据包转发，1 表示允许)链的默认规则通常设置为 ACCEPT，如果想确保任何包都不能通过规则集，那么可以重置为 DROP。表和链的关系 1234567891011filter FORWARD、INPUT、 OUTPUTnat PREROUTING、POSTROUTING、OUTPUTmangle PREROUTING、POSTROUTING、OUTPUT、INPUT、FORWARDraw PREROUTING、OUTPUT 规则 rules &amp; 目标 target数据包的过滤基于 规则 。规则由一个 目标 （数据包包匹配所有条件后的动作）和很多匹配（导致该规则可以应用的数据包所满足的条件）指定。 目标使用 -j 或者 --jump 选项指定。目标可以是用户定义的链（例如，如果条件匹配，跳转到之后的用户定义的链，继续处理）、一个内置的特定目标或者是一个目标扩展。内置目标 是 ACCEPT， DROP， QUEUE 和 RETURN，目标扩展 有 REJECT 、 LOG 等。如果目标是内置目标，数据包的命运会立刻被决定并且在当前表的数据包的处理过程会停止。如果目标是用户定义的链，并且数据包成功穿过第二条链，目标将移动到原始链中的下一个规则。目标扩展可以被终止（像内置目标一样）或者不终止（像用户定义链一样）。常见的 target：1234567891011121314151617181920212223242526272829ACCEPT：允许数据包通过。DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。LOG：在 /var/log/messages 文件中记录日志信息，然后将数据包传递给下一条规则。REDIRECT：在本机做端口映射。只适用于 nat 表的 PREROUTING 和 OUTPUT 链，和只调用它们的用户自定义链。DNAT：This target is only valid in the nat table, in the PREROUTING and OUTPUT chains, and user-defined chains which are only called from those chains.SNAT：This target is only valid in the nat table, in the POSTROUTING and INPUT chains, and user-defined chains which are only called from those chains. MASQUERADE：This target is only valid in the nat table, in the POSTROUTING chain. RETURN：在自定义链执行完毕后使用返回，来返回原规则链 MARK： 增加防火墙标记 3 种基本路由情况 入站数据流向 从外界到达防火墙的数据包，先被 PREROUTING 规则链处理（是否修改数据包地址等），之后会进行路由选择（判断该数据包应该发往何处），如果数据包的目标主机是防火墙本机（比如说 Internet 用户访问防火墙主机中的 web 服务器的数据包），那么内核将其传给 INPUT 链进行处理（决定是否允许通过等），通过以后再交给系统上层的应用程序（比如 Apache 服务器）进行响应。转发数据流向 来自外界的数据包到达防火墙后，首先被 PREROUTING 规则链处理，之后会进行路由选择，如果数据包的目标地址是其它外部地址（比如局域网用户通过网 关访问 QQ 站点的数据包），则内核将其传递给 FORWARD 链进行处理（是否转发或拦截），然后再交给 POSTROUTING 规则链（是否修改数据包的地 址等）进行处理。出站数据流向 防火墙本机向外部地址发送的数据包（比如在防火墙主机中测试公网 DNS 服务器时），首先被 OUTPUT 规则链处理，之后进行路由选择，然后传递给 POSTROUTING 规则链（是否修改数据包的地址等）进行处理。iptables 命令和例子 常见参数 1234567891011121314151617181920212223242526272829303132333435363738394041-P 设置默认策略:iptables -P INPUT (DROP|ACCEPT)-F 清空规则链-L 查看规则链-A 在规则链的末尾加入新规则-I num 在规则链的头部加入新规则-D num 删除某一条规则-s 匹配来源地址 IP/MASK，加叹号 &quot;!&quot; 表示除这个 IP 外。-d 匹配目标地址-i 网卡名称 匹配从这块网卡流入的数据-o 网卡名称 匹配从这块网卡流出的数据-p 匹配协议, 如 tcp,udp,icmp--dport num 匹配目标端口号--sport num 匹配来源端口号Options: --ipv4 -4 Nothing (line is ignored by ip6tables-restore) --ipv6 -6 Error (line is ignored by iptables-restore)[!] --protocol -p proto protocol: by number or name, eg. `tcp&apos;[!] --source -s address[/mask][...] source specification[!] --destination -d address[/mask][...] destination specification[!] --in-interface -i input name[+] network interface name ([+] for wildcard)--jump -j target target for rule (may load target extension)--goto -g chain jump to chain with no return-match -m match extended match (may load extension)--numeric -n Numeric output. IP addresses and port numbers will be printed in numeric format. By default, the program will try to display them as host names, network names, or services (whenever applicable).[!] --out-interface -o output name[+] network interface name ([+] for wildcard) --table -t table table to manipulate (default: `filter&apos;) --verbose -v verbose mode --wait -w [seconds] maximum wait to acquire xtables lock before give up --wait-interval -W [usecs] wait time to try to acquire xtables lock default is 1 second --line-numbers print line numbers when listing --exact -x expand numbers (display exact values) 输出一个链的详情：1iptables -nvL &lt;chain&gt;iptables match / target 有一堆扩展，参见：iptables-extensions。 可以简单查看 match / target 的参数：123456789101112[root@host143 ~]# iptables -m iprange --helpiprange match options:[!] --src-range ip[-ip] Match source IP in the specified range[!] --dst-range ip[-ip] Match destination IP in the specified range[root@host143 ~]# iptables -j DNAT --helpDNAT target options: --to-destination [&lt;ipaddr&gt;[-&lt;ipaddr&gt;]][:port[-port]] Address to map destination to.[--random] [--persistent]使用 iptables，要确定操作的 chain、过滤条件（ip、端口、协议）、目标动作。前置操作 清除所有规则 1tables -F 设置链的默认策略 (默认是 ACCEPT)1iptables -P FORWARD DROP 保存规则 1iptables-save &gt; /etc/sysconfig/iptables 或者 1service iptables save 导入 iptables-save 保存的规则 1iptables-restore &lt; files_saved_by_iptables-save 禁止转发 禁止转发源 IP 地址为 192.168.1.20-192.168.1.99 的 TCP 数据包 123456# 因为是“转发”，使用 FORWARD 链# -A 在某个 chain 后面增加规则# -p 协议# -j 跳转目标# -m match 匹配，iprange 是扩展，支持 --dst-range 和 --src-rangeiptables -A FORWARD -m iprange --src-range 192.168.0.20-192.168.0.99 -p tcp -j DROP 这里使用了 iptables match 扩展的 iprange。MAC 和地址绑定1iptables –A FORWARD -s 192.168.30.20 –m mac --mac-source 00:11:5B:EF:7A:D8 -j DROP 阻止某个 ip 入站 1iptables -A INPUT -s x.x.x.x -j DROP 允许所有外部 http 的连接请求 1234iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT# 只对已经建立连接的出站# 如果是 ftp，则还要增加 RELATEDiptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT 这里使用了 match 的 state 模块。为了解决一个安全问题：怎样判断这些报文是为了回应我们之前发出的报文，还是主动向我们发送的报文呢 ？ (因为发送过来的报文并不总是为了响应我们，也有可能是为了主动攻击我们。)state 是 conntrack 模块的子集，支持INVALID, ESTABLISHED, NEW, RELATED or UNTRACKED。The “state” extension is a subset of the “conntrack” module. “state” allows access to the connection tracking state for this packet.[!] –state stateWhere state is a comma separated list of the connection states to match. Only a subset of the states unterstood by “conntrack” are recognized: INVALID, ESTABLISHED, NEW, RELATED or UNTRACKED. For their description, see the “conntrack” heading in this manpage.state 状态如下：NEW：连接中的第一个包ESTABLISHED：把 NEW 状态包后面的包的状态理解为 ESTABLISHED，表示连接已建立RELATED：正在建立一个新的连接，这个连接是和一个已建立的连接相关的。比如，FTP data transfer，ICMP error 和一个 TCP 或 UDP 连接相关INVALID：如果一个包没有办法被识别，或者这个包没有任何状态UNTRACKED：当报文的状态为 Untracked 时通常表示无法找到相关的连接。multiport 合并多个端口的规则match 扩展支持multiport，使用--dports 和--sports。这样多个端口能够合并为一条规则。1234# 允许外部的多个端口访问本机 iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPTiptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT 允许从本地发起的 HTTPS 连接请求 因为出站规则，先设置 OUPUT 链，对应的连接状态是 NEW、ESTABLISHED。然后设置 INPUT 链，对应的连接状态是 ESTABLISHED。123iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPTiptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPTDNAT扩展 target。防火墙收到来自外网的数据包后，会将该数据包的目的 IP 地址进行替换（源 IP 地址不变），重新转发到内网的主机。修改目的 ip 地址的原因一般就是为了改变包发送的目的地，让包走出去，而不是留下来，所以在 iptables 中，DNAT 是在入口，也即 PREROUTING 链中发挥作用，以便让包进入 FORWARD 表。12# 把所有指向 11.22.33.44 的 http 流量转到 192.168.99.99iptables -t nat -A PREROUTING -d 11.22.33.44 -p tcp --dport 80 -j DNAT --to-destination 192.168.99.99:8080参数如下：–to-destination [ipaddr[-ipaddr]][:port[-port]]which can specify a single new destination IP address, an inclusive range of IP addresses. Optionally a port range, if the rule also specifies one of the following protocols: tcp, udp, dccp or sctp. If no port range is specified, then the destination port will never be modified. If no IP address is specified then only the destination port will be modified. In Kernels up to 2.6.10 you can add several –to-destination options. For those kernels, if you specify more than one destination address, either via an address range or multiple –to-destination options, a simple round-robin (one after another in cycle) load balancing takes place between these addresses. Later Kernels (&gt;= 2.6.11-rc1) don’t have the ability to NAT to multiple ranges anymore.–randomIf option –random is used then port mapping will be randomized (kernel &gt;= 2.6.22).–persistentGives a client the same source-/destination-address for each connection. This supersedes the SAME target. Support for persistent mappings is available from 2.6.29-rc2.作用：没有公网 IP 而对外提供服务场景，相当于端口映射 保护真实主机 ip 的作用 SNAT 扩展 target。可以代理上网的功能。当内网数据包到达防火墙后，防火墙会使用外部地址替换掉数据包的源 IP 地址（目的 IP 地址不变），使网络内部主机能够与网络外部主机通信。修改源 ip 地址的目的一般都是为了让这个包能再回到自己这里，所以在 iptables 中，SNAT 是在出口，也即 POSTROUTING 链发挥作用。假设防火墙两张网卡，一张 eth0 对接外网，ip 为 1.1.1.1。 另一张 eth1，对接内网。内网网段192.168.100.0/24 都使用这个网关访问互联网：1iptables -t nat -A PREROUTING -s 192.168.100.0/24 -j SNAT --to-source 1.1.1.1贴一张网上对比 snat 和 dnat 的表格：MASQUERADE扩展 target。适用于外网 ip 地址非固定的情况。用发送数据的网卡上的 IP 来替换源 IP，因此，对于那些 IP 不固定的场合，比如拨号网络或者通过 dhcp 分配 IP 的情况。例如上面 SNAT 的例子，如果 eth0 使用 adsl 方式上网：1iptables -t nat -A PREROUTING -s 192.168.100.0/24 -j MASQUERADE端口转发 假设对外使用 30022 作为 ssh 端口（默认为 22）12345iptables -t nat -A PREROUTING -p tcp -d 192.168.100.1 --dport 30022 -j DNAT --to 192.168.100.1:22iptables -A INPUT -i eth0 -p tcp --dport 30022 -m state --state NEW,ESTABLISHED -j ACCEPTiptables -A OUTPUT -o eth0 -p tcp --sport 30022 -m state --state ESTABLISHED -j ACCEPTping 相关 允许内网 ping 外网 12iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPTiptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT 允许外网 ping 内网 12iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPTiptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT 负载均衡 把 80 端口的流量负载均衡到 3 台机器上：12345678910# -m nth：使用统计模块，并且是轮询模式，需要指定两个参数：# --every n：每 n 个 packet 轮询一次 # --packet p：设置初始化计数# 从第 p 个包开始，每 n 个包执行该规则iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.100.101:8080iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.100.102:8080iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.100.103:8080 这里使用了 statistic 模块。支持两种负载均衡模式：随机和轮询。–mode modeSet the matching mode of the matching rule, supported modes are random and nth.[!] –probability pSet the probability for a packet to be randomly matched. It only works with the random mode. p must be within 0.0 and 1.0. The supported granularity is in 1/2147483648th increments.[!] –every nMatch one packet every nth packet. It works only with the nth mode (see also the –packet option).–packet pSet the initial counter value (0 &lt;= p &lt;= n-1, default 0) for the nth mode.docker iptables 使用 snat 访问外网 网上看到的图，形象生动： 参考iptablesiptables 详解A Deep Dive into Iptables and Netfilter Architecture25 个 iptables 常用示例LINUX FIREWALL]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防火墙和 ftp]]></title>
    <url>%2Fp%2Flinux-firewall-ftp%2F</url>
    <content type="text"><![CDATA[概述 FTP 是基于 TCP 的。FTP 使用 2 个端口，一个数据端口（使用 20 端口）和一个命令端口（也可叫做控制端口，使用 21 端口）。FTP 有 2 种工作模式：被动和主动。 主动和被动是针对服务器视角的。主动模式：由 ftp server 发起建立 data channel。被动模式：由 ftp client 发起建立 data channel。主动模式 PORT交互流程：client 使用普通端口 N（大于 1024 的端口号）向 server 的 21 端口发送指令 PORT N+1，表示 client 将会使用端口号为 N+1 的端口作为数据端口。server 回复OK。server 使用 20 端口向 client 的 N+1 端口发起连接。client 在数据端口回复OK。 对于 server 前面的防火墙，需要配置：允许远程任意大于 1024 端口连接到 server 的 21 端口 允许 server 的 21 端口向所有远程大于 1024 的端口建立连接 允许 server 的 20 端口向所有远程大于 1024 的端口建立连接 允许远程任意大于 1024 端口连接到 server 的 20 端口 被动模式 PASV交互流程：client 使用普通端口 N（大于 1024 的端口号）向 server 的 21 端口发送指令 PASVserver 回复OK X，表示使用 X 作为数据端口（X &gt; 1024）client 使用端口 N+1，向 server 的 X 端口（X &gt; 1024）建立连接server 在 X 端口的连接回复OK 对于 server 前面的防火墙，需要配置：允许远程任意大于 1024 端口连接到 server 的 21 端口 允许 server 的 21 端口向所有远程大于 1024 的端口建立连接 允许远程任意大于 1024 端口连接到 server 的大于 1024 端口 小结 PORT 模式，server 只要开放 21 和 20 端口。PASV 模式，server 要开放 21 端口和所有非特权端口。 理论上 PORT 模式更加安全。但是实际上因为端口固定，会导致端口 sniffer 更加容易。从安全性角度看，PASV 模式好一些。PORT 模式的另一个问题是，客户端都是在路由器后面，没有独立的公网 IP 地址，服务器想要主动连接客户端比较困难。其中涉及 NAT 设备地址转换。但是 NAT 一般只针对 packet header 的 ip、port 进行转换 。 但是 PORT 模式中，要建立 data channel 的具体端口是在 packet body，因此 NAT 设备可能不能正确进行转换。（TODO： NAT ALG 能够正确转换） 关于 FTP NAT ALG 转换的例子，参见：NAT ALG 原理与应用 ，引用其中的两幅图： 还有一个方面，client 端的防火墙通常是禁止远程主动建立连接。因此 PASV 模式使用得多。客户端可以选择 PORT 模式还是 PASV 模式建立数据连接。但是是否支持在于 server 端。参考FTP 主动模式 与被动模式FTP 协议报文详解及 FTP 穿越 NAT]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防火墙概念笔记]]></title>
    <url>%2Fp%2Flinux-firewall-notes%2F</url>
    <content type="text"><![CDATA[整理的学习防火墙笔记。概念 无状态防火墙就是基于静态数值来过滤或阻拦网络数据包。例如基于地址，端口，协议等等。无状态指的就是，防火墙本身不关心当前的网络连接状态。有状态防火墙能区分出网络连接的状态。例如 TCP 连接，有状态防火墙可以知道当前是连接的哪个阶段。也就是说，有状态防火墙可以在静态数值之外，再通过连接状态来过滤或阻拦网络数据包。无状态防火墙只进行地址，端口，协议等规则匹配，实现比较简单，消耗资源较少，也可以做成硬件进一步加速。适用于负载很高、规则简单的场景。无状态防火墙的不足：如果规则很多，则检索表（lookup table）就会很大，交换机、路由器等内存有限的硬件会很敏感 大规模部署，管理这些规则也是问题 没有充分利用连接的上下文信息 ct 状态机 网络连接的状态，通常是由 connection tracker 来实现。可以简称为 CT 模块。在 linux 内核中，CT 是由 conntrack 实现，其内部实际上是对各个网络连接实现了状态机。CT 状态机跟具体通信协议无关 。CT 状态机管理的 connection，有 2 个关注点： 怎样才算建立了连接?怎样才算连接断开？对于建立连接：只要是第一个 packet，就认为 connection 是 NEW，收到了第一个合法的返回，就认为 connection 是 ESTABLISHED。对于连接断开：依赖 connection 的 TTL。（以下几张图片来源于参考文章）tcp 收到 SYN/ACK 就认为 connection 是 ESTABLISHED。12[root@host143 ~]# conntrack -L -p tcp -f ipv4 --dport 3306 -o timestamptcp 6 431915 ESTABLISHED src=172.25.21.29 dst=172.25.20.143 sport=57764 dport=3306 src=172.25.20.143 dst=172.25.21.29 sport=3306 dport=57764 [ASSURED] mark=0 use=1udp第一个 UDP 包创建的 NEW 状态的 UDP connection，TTL 是 30；当 connection 变成 ESTABLISHED，其 TTL 默认值是 180。12[root@host143 ~]# conntrack -L -p udp -f ipv4 -o timestampudp 17 5 src=172.25.23.15 dst=255.255.255.255 sport=46705 dport=1947 [UNREPLIED] src=255.255.255.255 dst=172.25.23.15 sport=1947 dport=46705 mark=0 use=1icmpicmp 不熟悉，以后再补充。12[root@host143 ~]# conntrack -L -p icmp -f ipv4 -o timestampicmp 1 28 src=172.28.51.6 dst=172.25.20.143 type=8 code=0 id=3 src=172.25.20.143 dst=172.28.51.6 type=0 code=0 id=3 mark=0 use=1参考Stateful firewall in OpenFlow based SDN]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 查看 os 版本]]></title>
    <url>%2Fp%2Flinux-os-version%2F</url>
    <content type="text"><![CDATA[几个查看 liunx 系统的方法。优先使用 lsb_release。lsb_releaseThe lsb_release command displays LSB (Linux Standard Base) information about your specific Linux distribution, including version number, release codename, and distributor ID. 所有 linux 发行版都适用，但是要安装对应的 lsb 模块。centos:12345678[root@host143 ~]# yum install -y redhat-lsb-core[root@host143 ~]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.5.1804 (Core) Release: 7.5.1804Codename: Coreubuntu:12345678ubuntu@VM-0-2-ubuntu:~$ apt-get install lsb-coreubuntu@VM-0-2-ubuntu:~$ lsb_release -aLSB Version: core-9.20170808ubuntu1-noarch:security-9.20170808ubuntu1-noarchDistributor ID: UbuntuDescription: Ubuntu 18.04.3 LTSRelease: 18.04Codename: bionicuname 能看到内核版本。123# ubuntuubuntu@VM-0-2-ubuntu:~$ uname -aLinux VM-0-2-ubuntu 4.15.0-66-generic #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux/proc/version 能够看到内核版本，不一定能够看到 os version。1234567# ubuntuubuntu@VM-0-2-ubuntu:~$ cat /proc/version Linux version 4.15.0-66-generic (buildd@lgw01-amd64-044) (gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)) #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019# centos[root@host143 ~]# cat /proc/version Linux version 3.10.0-862.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC) ) #1 SMP Fri Apr 20 16:44:24 UTC 2018/etc/issue/etc/issue 文件是 Linux 系统开机启动时在命令行界面弹出的欢迎语句文件 123# 可能看到 os versionubuntu@VM-0-2-ubuntu:~$ cat /etc/issueUbuntu 18.04.3 LTS \n \lcentos &amp; redhat 专属方式 123456# centos[root@host143 ~]# rpm -q centos-releasecentos-release-7-5.1804.el7.centos.x86_64# redhat# rpm -q redhat-release12[root@host143 ~]# cat /etc/system-releaseCentOS Linux release 7.5.1804 (Core)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 nsenter 访问 docker 容器]]></title>
    <url>%2Fp%2Fdocker-nsenter%2F</url>
    <content type="text"><![CDATA[nsenter 工具 nsenter 可以在某个 namespace 下执行程序。默认执行的程序是${SHELL}。 常见参数有：12345678910111213141516171819202122232425262728293031323334353637383940-t, --target pid Specify a target process to get contexts from. The paths to the contexts specified by pid are&gt; /proc/pid/ns/mnt the mount namespace /proc/pid/ns/uts the UTS namespace /proc/pid/ns/ipc the IPC namespace /proc/pid/ns/net the network namespace /proc/pid/ns/pid the PID namespace /proc/pid/ns/user the user namespace /proc/pid/root the root directory /proc/pid/cwd the working directory respectively-m, --mount[=file] Enter the mount namespace. If no file is specified, enter the mount namespace of the target process. If file is specified, enter the mount namespace specified by file-u, --uts[=file] Enter the UTS namespace. If no file is specified, enter the UTS namespace of the target process. If file is specified, enter the UTS names‐ pace specified by file-i, --ipc[=file] Enter the IPC namespace. If no file is specified, enter the IPC namespace of the target process. If file is specified, enter the IPC names‐ pace specified by file-n, --net[=file] Enter the network namespace. If no file is specified, enter the network namespace of the target process. If file is specified, enter the net‐ work namespace specified by file-p, --pid[=file] Enter the PID namespace. If no file is specified, enter the PID namespace of the target process. If file is specified, enter the PID names‐ pace specified by file-U, --user[=file] Enter the user namespace. If no file is specified, enter the user namespace of the target process. If file is specified, enter the user namespace specified by file. See also the --setuid and --setgid options.``` 使用例子 ```shPID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container_name_or_ID&gt;)nsenter --target $PID --mount --uts --ipc --net --pid 使用 nsenter 对 dns 容器抓包 dns 容器通常不带 shell 环境，不能直接使用 docker exec 进入容器抓包。 可以使用 nsenter 切换到对应 namespace，再使用 tcpdump 抓包。12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 窗口 1# 先找到容器 id[root@master-29 ~]# docker ps | grep coredns67d08deccd40 7987f0908caf "/coredns -conf /etc…" 8 days ago Up 8 days k8scoredns_coredns-7c945697f5-8dd5n_kube-system_aa257a19-31a6-4bbb-8d8b-05804cf5f7a6_0729bffc11526 mirrorgooglecontainers/pause-amd64:3.1 "/pause" 8 days ago Up 8 days k8s_POD_coredns-7c945697f5-8dd5n_kube-system_aa257a19-31a6-4bbb-8d8b-05804cf5f7a6_0# 窗口 1# 使用 docker inspect 反查得到对应的 PID[root@master-29 ~]# docker inspect --format "&#123;&#123;.State.Pid&#125;&#125;" 67d08deccd40176016# 窗口 1[root@master-29 ~]# nsenter -n -t 176016# 窗口 2，切换到其他容器 # 因为 dns 容器有多个，nslookup 指定使用的 dns 地址# 此容器的 resolv.conf 配置为 options ndots:5bash-5.0# nslookup ycwu314.top 10.244.154.1 Server: 10.244.154.1Address: 10.244.154.1#53Non-authoritative answer:Name: ycwu314.topAddress: 104.24.99.9Name: ycwu314.topAddress: 104.24.98.9Name: ycwu314.topAddress: 2606:4700:3033::6818:6309Name: ycwu314.topAddress: 2606:4700:3033::6818:6209# 窗口 1# 抓包。dns 默认使用 udp 协议、53 端口[root@master-29 ~]# tcpdump -i eth0 udp dst port 53 | grep ycwu314.toptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes# 先根据 options ndots 配置，尝试发起本地 dns 查询21:53:14.567215 IP 10.244.154.23.46946 &gt; master-29.domain: 14770+ A? ycwu314.top.prophet.svc.cluster.local. (55)21:53:14.568171 IP 10.244.154.23.36680 &gt; master-29.domain: 49422+ A? ycwu314.top.svc.cluster.local. (47)21:53:14.568910 IP 10.244.154.23.55443 &gt; master-29.domain: 1651+ A? ycwu314.top.cluster.local. (43)21:53:14.569494 IP 10.244.154.23.37613 &gt; master-29.domain: 50249+ A? ycwu314.top. (29)# 本地 dns 查询失败，发起外部 dns 查询# 注意发起了 A 和 AAAA 记录查询21:53:14.569812 IP master-29.36226 &gt; xxx.com.domain: 50249+ A? ycwu314.top. (29)21:53:14.571655 IP 10.244.154.23.34600 &gt; master-29.domain: 19908+ AAAA? ycwu314.top. (29)21:53:14.571867 IP master-29.36226 &gt; xxx.com.domain: 19908+ AAAA? ycwu314.top. (29)options ndots 选项 从上面可以看到，k8s 容器查询 ycwu314.top，实际上依次发起了多次请求：ycwu314.top.prophet.svc.cluster.local.ycwu314.top.svc.cluster.local.ycwu314.top.cluster.local.ycwu314.top. 先发起了 3 次本地 dns 查询（master-29.domain），查询不到后再向外部 dns 服务器发起请求 (xxx.com.domain)。因此整体查询 dns 延迟可能比较高。options ndots:5 表示域名包含 . 数量小于等于 5 个（并且不以 . 结尾），则认为是 PQDN，会结合 domain 或者 search 的域名进行补全，再作为 FQDN 查询。对应的性能优化：使用 FQDN 查询。域名以 . 结尾，则 resolver 认为是 FQDN，直接查询，而不使用 resolv.conf 中domain或者 search 选项进行补全。例如：ycwu314.top是 PQDN，ycwu314.top.是 FQDN。修改 ndots比如 options ndots:1。12345678910111213apiVersion: v1kind: Podmetadata: namespace: default name: dns-examplespec: containers: - name: test image: nginx dnsConfig: options: - name: ndots value: "1" 参考Debugging DNS Resolution]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 主机解析相关配置文件]]></title>
    <url>%2Fp%2Flinux-hostname-dns-config-files%2F</url>
    <content type="text"><![CDATA[基于 centos7.x 系统。FQDN 和 PQDN首先分清楚 2 个概念，摘抄自《理解 OpenShift（2）：网络之 DNS（域名服务）》域名（Domain Name）分为两种，一种是绝对域名（Absolute Domain Name，也称为 Fully-Qualified Domain Name，简称 FQDN），另一种是相对域名（Relative Domain Name，也称为 Partially Qualified Domain Name，简称 PQDN）。FQDN 能被直接到 DNS 名字服务器中查询；而 PQDN 需要先转化为 FQDN 再进行查询。其做法是将 PQDN 附加一个搜索域名（search domain）来生成一个 FQDN。(对应于 resolv.conf 的domain或者 search 选项)。在域名系统中，域名结尾是否是 . 被用来区分 FQDN 和 PQDN。/etc/hostshosts：the static table lookup for host name（主机名查询静态表）在没有域名服务器的情况下，系统上的所有网络程序都通过查询该文件来解析对应于某个主机名的 IP 地址。k8s 上容器的 hosts 文件 123456789[root@nacos-0 nacos]# cat /etc/hosts# Kubernetes-managed hosts file.127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetfe00::0 ip6-mcastprefixfe00::1 ip6-allnodesfe00::2 ip6-allrouters10.244.197.129 nacos-0.nacos-center.v-base.svc.cluster.local. nacos-0hosts 文件的格式如下：1234IP 地址 主机名 / 域名 第一部份： 网络 IP 地址 第二部份： 主机名或域名 第三部份： 主机名别名 (可选) 主机名 (hostname) 和域名 (domain）的区别： 主机名通常在局域网内使用，通过 hosts 文件，主机名就被解析到对应 ip；域名通常在 internet 上使用，但如果本机不想使用 internet 上的域名解析，这时就可以更改 hosts 文件，加入自己的域名解析。/etc/hostname主机名。相关工具是 hostname。1234567891011121314151617[root@host143 ~]# cat /etc/hostnamehost143# 也可以使用 hostname 命令查看 [root@host143 ~]# hostnamehost143# 显示主机的 ip 地址（组）[root@host143 ~]# hostname -i172.25.20.143# 临时修改主机名，重启就会恢复为 /etc/hostname[root@host143 ~]# hostname aaa[root@host143 ~]# hostnameaaa[root@host143 ~]# cat /etc/hostnamehost143/etc/host.conf 当系统中同时存在 DNS 域名解析和 /etc/hosts 主机表机制时，由该 /etc/host.conf 确定主机名解释顺序。作用于 resolver 库：The file /etc/host.conf contains configuration information specific to the resolver library.123order hosts,bind,nis #名称解释顺序 multi on #允许主机拥有多个 IP 地址，on / off，默认是 off。nospoof on #禁止 IP 地址欺骗 参见 HOST.CONF，这里列出一些配置项内容。order 搜索顺序，支持的查找方法为 bind、hosts 和nis，分别代表 DNS、/etc/hosts和 NIS。multi表示是否运行 /etc/hosts 文件允许主机指定多个多个地址。返回一台主机在 /etc/hosts 文件中出现的的所有有效地址, 而不只是第一个。如果 hosts 文件很大，可能导致性能问题，因此默认是 off。nospoof是否允许服务器对 ip 地址进行其欺骗。尝试增强 rlogin 和 rsh 的安全性。工作原理：It works as follows: after performing a host address lookup, the resolver library willperform a hostname lookup for that address. If the two hostnames do not match, the query fails.默认 off。reorder对主机地址重排序，优先本地地址 (即在同一子网中的地址)。If set to on, the resolver library will attempt to reorder host addresses so that local addresses (i.e., on the same subnet) are listed first when a gethostbyname(3) is performed.trim在查寻之前将被从主机名中删去。可以配置多个 trim123456trim This keyword may be listed more than once. Each time it should be followed by a list of domains, separated by colons (&apos;:&apos;), semicolons (&apos;;&apos;) or commas (&apos;,&apos;), with the leading dot. When set, the resolver library will automatically trim the given domain name from the end of any hostname resolved via DNS. This is intended for use with local hosts and domains./etc/resolv.conf用于设置 DNS 服务器的 IP 地址及 DNS 域名。以下是 k8s nacos 的配置 demo1234[root@nacos-0 nacos]# cat /etc/resolv.confnameserver 10.68.0.2search v-base.svc.cluster.local. svc.cluster.local. cluster.local.options ndots:5resolv.conf 的配置项有 nameserver # 定义 DNS 服务器的 IP 地址domain # 定义本地域名search # 定义域名的搜索列表options # 选项sortlist # 对返回的域名进行排序resolv.conf 对于这些配置项的数量限制：The resolv.conf file can contain one domain entry or one search entry, a maximum of three nameserver entries, a sortlist entry, and any number of options entries.nameserverDNS 服务器的 IP 地址。一行一个 ip，可以有多行。按照 nameserver 在文件中出现顺序查询。只有当第一个 nameserver 没有反应时才查询下面的 nameserver。domainA domain entry tells the resolver routines which default domain name to append to names that do not end with a . (period).search 当要查询没有域名的主机，主机将在由 search 声明的域中分别查找。domain 和 search 不能共存；如果同时存在，后面出现的将会被使用。domain 和 search 作用是类似的。当只有一个默认域名，使用 domain；有多个默认域名，则使用 search。ps. 基于 dns 的服务发现、例如 k8s，使用服务名、而不是 ip 加端口的形式访问服务，跟 search 选项有关。许多 DNS 解析器如果发现被解析的域名中有任何的点（.）就把它当做一个 FQDN 来解析；如果域名中没有任何点，就把它当做 PQDN 来处理，并且会加上系统的默认 domain name 和最后的点，来组成 FQDN。在 resolv.conf 增加配置 1search ycwu314.top 然后用 host 命令查询。注意默认的是 options ndots:1：只要被解析域名中有任何一个点（.），那么它就会被当做 FQDN。 例子 1因为 svc 没有 .，所以分别使用resolv.conf 中 search 的主机名补全为 FQDN 来查询。12345678910111213[root@host143 ~]# host -a svcTrying "svc.ycwu314.top";; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 63724;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;svc.ycwu314.top. IN ANY;; ANSWER SECTION:svc.ycwu314.top. 3600 IN RRSIG HINFO 13 3 3789 20200519125111 20200517105111 34505 ycwu314.top. FgUcebSliovgQpCJz88ynUM5YoTGNFQGhSTUQ1FOPW/vsrbt8uebNYEB ugH7qxVy1WYU3bJcvm/mVZWVowlbuA==svc.ycwu314.top. 3600 IN HINFO "RFC8482" ""Received 161 bytes from 10.200.100.211#53 in 1025 ms例子 2a.svc中间有 .（不是末尾），先当作是个 FQDN 进行查询，查询失败再分别使用resolv.conf 中 search 的主机名补全为 FQDN 来查询。12345[root@host143 ~]# host -a a.svcTrying "a.svc"Received 98 bytes from 10.200.100.211#53 in 3088 msTrying "a.svc.ycwu314.top";; connection timed out; no servers could be reached例子 3svc.末尾为 .，于是作为 FQDN 查询，即使查询失败也不会使用resolv.conf 中 search 的主机名补全。123456[root@host143 ~]# host -a svc.Trying "svc"Received 21 bytes from 10.200.100.211#53 in 1 msTrying "svc"Host svc not found: 2(SERVFAIL)Received 21 bytes from 10.200.100.211#53 in 2 mssortlist调整 gethostbyname() 的返回顺序。参数是 IP/ 子网掩码对，最多 10 个。1sortlist 130.155.160.0/255.255.240.0 130.155.0.0optionsoptions 中常见的有下面几个，完整参见 RESOLV.CONF。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 使用 tcp 而非 udp 来进行 dns 查询# 会导致性能下降use-vc (since glibc 2.14) Sets RES_USEVC in _res.options. This option forces the use of TCP for DNS resolutions.# 默认 glibc 并发查询 A 记录和 AAAA 记录，# 会导致部分 DNS 解析器不能正确处理并且查询超时。# 开启这个选项，改为串行方式查询 A 和 AAAA 记录。single-request (since glibc 2.10) Sets RES_SNGLKUP in _res.options. By default, glibc performs IPv4 and IPv6 lookups in parallel since version 2.9. Some appliance DNS servers cannot handle these queries properly and make the requests time out. This option disables the behavior and makes glibc perform the IPv6 and IPv4 requests sequentially (at the cost of some slowdown of the resolving process).# 默认 glibc 使用同一个端口发送 A 和 AAAA 查询，# 部分已经只返回其中一个查询响应，导致等待超时。# 修改此项，使用不同的端口查询 A 和 AAAA 记录。single-request-reopen (since glibc 2.9) Sets RES_SNGLKUPREOP in _res.options. The resolver uses the same socket for the A and AAAA requests. Some hardware mistakenly sends back only one reply. When that happens the client system will sit and wait for the second reply. Turning this option on changes this behavior so that if two requests from the same port are not handled correctly it will close the socket and open a new one before sending the second request.# 当只要被解析域名中包含不超过五个点（不以点号结尾），# 该域名就会被当做 PQDN，使用 search 配置拼接成 FQDN。# k8s 默认为 5ndots:n Sets a threshold for the number of dots which must appear in a name given to res_query(3) (see resolver(3)) before an initial absolute query will be made. The default for n is 1, meaning that if there are any dots in a name, the name will be tried first as an absolute name before any search list elements are appended to it. The value for this option is silently capped to 15.# dns 查询 5s 超时timeout:n Sets the amount of time the resolver will wait for a response from a remote name server before retrying the query via a different name server. This may not be the total time taken by any resolver API call and there is no guarantee that a single resolver API call maps to a single timeout. Measured in seconds, the default is RES_TIMEOUT (currently 5, see &lt;resolv.h&gt;). The value for this option is silently capped to 30.# 轮询 nameserverrotate sets RES_ROTATE in _res.options, which causes round-robin selection of nameservers from among those listed. This has the effect of spreading the query load among all listed servers, rather than having all clients try the first listed server first every time. 关于 single-request-reopen 和single-request使用相同端口并发查询 A 记录和 AAAA 记录，理论上性能更好、节约资源。但是实际上可能有问题：相同的 src ip，相同的 dest port，同样的第 4 层协议的连接会被防火墙看成是同一个会话，因此会存在返回包被丢弃现象。上图中同一个端口并发查询 A 和 AAAA 记录，并且 AAAA 记录先返回。于是防火墙拦截了随后相同端口返回的 A 记录，导致系统等待 A 记录返回超时。于是系统使用新的、不同 端口再次查询 A、AAAA 记录。/etc/hosts.allow and /etc/hosts.deny基于 tcp wrappers 的 ACL 机制。 /etc/hosts.deny优先级更高。检查应用是否支持 tcp wrappers包含有 libwrap.so 库文件的的程序就可以受 TCP_Wrappers 的安全控制。12# strings 命令在对象文件或二进制文件中查找可打印的字符串。strings /usr/sbin/sshd | grep hosts_access或者 1ldd /usr/sbin/sshd | grep libwrap 配置选项 12daemon 要监控的服务，如 telnetd、ftpd、sshdclient 主机名、IP 地址 /IP 范围，或域名 选项有：123allow 允许对客户端的访问 deny 拒绝对客户端的访问except 会匹配第一个列表中所有项，除非匹配第二个列表。例如，允许 domainA 中所有项，除了 hostX.domainA 和 hostY.domanA。 配置例子参见：TCP_Wrappers 简介 /etc/nsswitch.conf/etc/nsswitch.conf(name service switch configuration，名字服务切换配置)，指定了”数据库” 及其查找顺序。 作用于 GNU library C。配置项和方法 nsswitch 支持多种配置项12345678910111213141516171819202122232425automount：自动挂载（/etc/auto.master 和 /etc/auto.misc）bootparams：无盘引导选项和其他引导选项（参见 bootparam 的手册页）ethers：MAC 地址group：用户所在组（/etc/group),getgrent() 函数使用该文件 hosts：主机名和主机号（/etc/hosts)，gethostbyname() 以及类似的函数使用该文件 networks：网络名及网络号（/etc/networks)，getnetent() 函数使用该文件 passwd：用户口令（/etc/passwd)，getpwent() 函数使用该文件 protocols：网络协议（/etc/protocols），getprotoent() 函数使用该文件 publickey：NIS+ 及 NFS 所使用的 secure_rpc 的公开密钥rpc：远程过程调用名及调用号（/etc/rpc），getrpcbyname() 及类似函数使用该文件 services：网络服务（/etc/services），getservent() 函数使用该文件 shadow：映射口令信息（/etc/shadow），getspnam() 函数使用该文件 aliases：邮件别名，sendmail() 函数使用该文件 控制搜索信息类型的方法:1234567files：搜索本地文件，如 /etc/passwd 和 /etc/hostsnis：搜索 NIS 数据库，nis 还有一个别名，即 ypdns：查询 DNS（只查询主机）compat：passwd、group 和 shadow 文件中的±语法 题外话： NIS网络信息服务 (Network Information Service, NIS)NIS 也曾被称 YP (Yellow Pages)，它是一个基于 RPC (Remote Procedure Call Protocol) 的客户机 / 服务器系统，允许一个 NIS 域中的一组机器共享一系列配置文件。在 NIS 环境中，有主服务器、从服务器和客户机三种类型的主机。服务器的作用是充当主机配置信息的中央数据库。主服务器上保存着这些信息的权威副本，而从服务器则是保存这些信息的冗余副本。客户机依赖于服务器向它们提供这些信息。NIS 详细参见：【NIS】深入了解 NIS 例子 1hosts: files dns myhostname 其中 myhostname 是一个 nss 插件，返回 local、localhost、gateway 等值。具体参见：NSS-MYHOSTNAME。TODO：疑问： host.conf中的 order 和 nsswitch.conf 的 hosts 都能控制解析顺序，哪个优先级更高？getent 命令 getent 可以检索 nsswitch 的配置。123456[root@master-29 ~]# getent hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4[root@master-29 ~]# getent hosts ycwu314.top2606:4700:3033::6818:6209 ycwu314.top2606:4700:3033::6818:6309 ycwu314.topalpine linuxmusl 实现的 DNS resolver 行为和 glic 有差异。 参照官网 Functional differences from glibc 的”Name Resolver/DNS”章节：glibc 分别使用 nameserver 进行查询。musl 并发向所有 nameserver 查询。musl 不支持 single-request 和 single-request-reopen （划重点）。musl v1.1.13 之前不支持 domain 和search选项。musl v1.1.13 以后支持 domain 和search选项。但是行为和 glibc 有差异：.数量小于 ndots，则先使用search 拼接查询，再进行字面查询，和 glibc 一样 . 数量等于 ndots，则不使用search，直接进行全局 dns 查询，和 glibc 不一样musl 不支持 IDN (non-ASCII name lookups via DNS)。 参考资料resolv.conf File Format for TCP/IP/etc/resolv.conf 文件中的 search 项作用CoreDNS 系列 1：Kubernetes 内部域名解析原理、弊端及优化方式Kubernetes pods /etc/resolv.conf ndots:5 option and why it may negatively affect your application performancesDNS 解析超时排查 /etc/resolv.conf single-request-reopen 参数说明nsswitch.conf 文件详解]]></content>
      <categories>
        <category>liunx</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
        <tag>dns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome 不同方式查看 cookies 的坑]]></title>
    <url>%2Fp%2Fchrome-diff-ways-to-see-cookies%2F</url>
    <content type="text"><![CDATA[背景 前后端联调 cas 系统，前端应该写入 TGC cookies，但是说有时候没有写入成功，没有查看到对应的 cookies。扯来扯去，发现是 chrome 中不同查看 cookies 的方式导致。需要区分的是：本次页面设置的 cookies，以及站点存储 cookies。页面刷新之后，当前 cookies 可能变化 方式 1：地址栏查看 cookies“查看网页时设置”：是当前页面写入的 cookies。对于同一个网页，如果第一次访问写入 cookies，那么 刷新 当前页面，显示的 cookies 可能变少，有可能产生误解（cookies 丢失、写入失败）。方式 2：debug console 查看 cookies这里查看的是当前页面使用到的 cookies。方式 3：chrome://settings/siteData 查看 cookies1chrome://settings/siteData站点级别存储的 cookies，不受刷新影响。例子 清空站点 cookies，再进行实验。打开网页，登录成功后：（这里接入了 cas）地址栏方式：debug console：siteData：点击页面刷新之后 地址栏方式：debug console：siteData（不变）：回到案例 前端、现场习惯使用地址栏、debug console 观察 cookies，因此看到的是本次页面所写入的 cookies。会有一种错觉，cas 的 TGC cookies 丢失了。后端习惯在 siteData 看 cookies，因此看到 cookies 正确写入。结论：以后统一在 siteData 查看 cookies（调试当前页面写入 cookies 的除外）。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 容器进程和宿主机进程映射]]></title>
    <url>%2Fp%2Fdocker-find-container-process-on-host-machine%2F</url>
    <content type="text"><![CDATA[容器内进程和宿主机进程的映射 docker 容器内的一个进程对应于宿主机器上的一个进程。 容器内的进程，与相对应的宿主进程，由相同的 uid、gid 拥有。在 k8s 上部署 nacos 容器，下面演示怎么找到对应的宿主机进程。12345678910111213141516171819202122232425262728293031323334# 找到 nacos pod 部署的机器 [root@master-29 ~]# kubectl describe pod nacos-0 -n v-baseName: nacos-0Namespace: v-basePriority: 0Node: 172.25.23.19/172.25.23.19# ssh 到目标宿主机# 找到具体 container id[root@node-19 ~]# docker ps --filter name=nacosCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd12fef39614c ac34e13f83a8 "bin/docker-startup.…" 15 hours ago Up 15 hours k8s_k8snacos_nacos-0_v-base_f310006e-0798-42ca-852c-73a3784a9b47_00f885e4061d3 mirrorgooglecontainers/pause-amd64:3.1 "/pause" 15 hours ago Up 15 hours k8s_POD_nacos-0_v-base_f310006e-0798-42ca-852c-73a3784a9b47_0 k8s_POD_nacos-0_v-base_f310006e-0798-42ca-852c-73a3784a9b47_0# docker top 查看 PID 和 PPID# PID 是容器内进程在宿主机上的 pid# PPID 是容器内进程在宿主机上的父进程 pid# 注意和 exec 进入容器再执行 top 命令是不同的[root@node-19 ~]# docker top d12fef39614cUID PID PPID C STIME TTY TIME CMDroot 13747 13727 0 5 月 12 ? 00:00:00 /bin/bash bin/docker-startup.shroot 14001 13747 1 5 月 12 ? 00:15:03 /usr/lib/jvm/java-1.8.0-openjdk/bin/java -Xms512m -Xmx512m -Xmn256m -Dnacos.standalone=true -Dnacos.preferHostnameOverIp=true -Djava.ext.dirs=/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext:/usr/lib/jvm/java-1.8.0-openjdk/lib/ext:/home/nacos/plugins/cmdb:/home/nacos/plugins/mysql -Xloggc:/home/nacos/logs/nacos_gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dnacos.home=/home/nacos -jar /home/nacos/target/nacos-server.jar --spring.config.location=classpath:/,classpath:/config/,file:./,file:./config/,file:/home/nacos/conf/,/home/nacos/init.d/ --spring.config.name=application,custom --logging.config=/home/nacos/conf/nacos-logback.xml --server.max-http-header-size=524288root 26753 13727 0 10:32 pts/0 00:00:00 bash# 在宿主机上找到对应进程，验证 PID 和 PPID# ps -O 指定输出列[root@node-19 ~]# ps ax -O uid,uname,gid,group,ppid | grep nacos# 手动贴上 header，方便比较# PID UID USER GID GROUP PPID S TTY TIME COMMAND14001 0 root 0 root 13747 S ? 00:15:04 /usr/lib/jvm/java-1.8.0-openjdk/bin/java -Xms512m -Xmx512m -Xmn256m -Dnacos.standalone=true -Dnacos.preferHostnameOverIp=true -Djava.ext.dirs=/usr/lib/jvm/java-1.8.0-openjdk/jre/lib/ext:/usr/lib/jvm/java-1.8.0-openjdk/lib/ext:/home/nacos/plugins/cmdb:/home/naco/plugins/mysql -Xloggc:/home/nacos/logs/nacos_gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dnacos.home=/home/nacos -jar /home/nacos/target/nacos-server.jar --spring.config.location=classpath:/,classpath:/config/,file:./,file:./config/,file:/home/nacos/conf/,/home/nacos/init.d/ --spring.config.name=application,custom --logging.config=/home/nacos/conf/nacos-logback.xml --server.max-http-header-size=52428815857 0 root 0 root 30535 S pts/1 00:00:00 grep --color=auto nacos 在宿主机上根据进程 PID 查找归属容器 ID一台 docker 主机上跑了多个容器，可能其中一个容器里的进程导致了整个宿主机 load 很高。先在宿主机上使用 ps 命令找到可疑的进程 PID，然后执行：1for i in `docker ps -q`; do docker top $i | xargs -i echo CONTAINER_ID=$i &#123;&#125; ; done | grep &lt;PID&gt;其中：docker ps -q: 列出当前正在运行的容器 IDdocker top &lt;container_id&gt;: 容器内 top 情况 xargs -i echo CONTAINER_ID=$i {}: 给每一行开头增加容器 ID，方便grep &lt;PID&gt; 后直接看到容器 ID扩展: ps 命令的参数 ps 命令看到的 UID、GID，对应资料:0 用户 ID(UID)：每个用户必需指定 UID。UID 0 是保留给 root 用户的，UID 1-99 是保留给其它预定义用户的， UID 100-999 是保留给系统用户的；0 组 ID(GID)：主组 ID(保存在 /etc/group 文件中)；ps a 显示现行终端机下的所有程序，包括其他用户的程序。ps f 用 ASCII 字符显示树状结构，表达程序间的相互关系。ps -H 显示树状结构，表示程序间的相互关系。ps e 列出程序时，显示每个程序所使用的环境变量。ps x 显示所有程序，不以终端机来区分。 参考Understanding how uid and gid work in Docker containers]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker & 僵尸进程 & tini]]></title>
    <url>%2Fp%2Fdocker-init-and-zombie-reaping%2F</url>
    <content type="text"><![CDATA[僵尸进程、PID 1 进程 推荐阅读这篇文章，快速了解僵尸进程、pid 1 进程：Docker and the PID 1 zombie reaping problem在 unix 系列系统，父进程应该等待子进程结束（waitpid 系统调用）：Unix is designed in such a way that parent processes must explicitly “wait” for child process termination, in order to collect its exit status.当子进程结束，会向父进程发送 SIGCHLD 信号，通知父进程回收 会有一种异常情况，父进程在子进程结束之前被杀死，那么这个子进程会被 pid 1 进程管理：在 docker 中，如果使用 bash 作为 pid 1 进程，那么向容器发送 SIGTERM 信号，bash 退出了，但是没有向子进程传递信号、等待子进程关闭，导致形成僵尸进程 PID 1 有一个独特的职责，那就是收割“僵尸进程”。init 进程周期执行 wait 系统调用 reap 其所收养的所有僵尸进程。 那么 what is 僵尸进程：已经退出 父进程也退出 没有被其父进程 wait（wait 是指 syscall 父进程用于检索其子进程的退出代码）from a Unix operating system point of view – zombie processes have a very specific definition. They are processes that have terminated but have not (yet) been waited for by their parent processes.init 进程的职责：向子进程发送信号，同时等待子进程结束：Sending signals to child processes is not enough: the init process must also wait for child processes to terminate, before terminating itself. If the init process terminates prematurely then all children are terminated uncleanly by the kernel.僵尸进程的危害：消耗 PID 空间。僵尸进程被 reap 后，其进程号与在进程表中的表项都可以被系统重用。但如果父进程没有调用 wait，僵尸进程将保留进程表中的表项，导致资源泄漏。如何处理僵尸进程 父进程捕获 SIGCHLD 信号，并调用 wait 或 waitpid。什么是 SIGCHLD？来自百度：在一个进程终止或者停止时，将 SIGCHLD 信号发送给其父进程。按系统默认将忽略此信号。如果父进程希望被告知其子系统的这种状态，则应捕捉此信号。信号的捕捉函数中通常调用 wait 函数以取得进程 ID 和其终止状态。如何避免僵尸进程？（来自文章：linux 下的僵尸进程处理 SIGCHLD 信号 ） 通过 signal(SIGCHLD, SIG_IGN)通知内核对子进程的结束不关心，由内核回收。如果不想让父进程挂起，可以在父进程中加入一条语句：signal(SIGCHLD,SIG_IGN); 表示父进程忽略 SIGCHLD 信号，该信号是子进程退出的时候向父进程发送的。父进程调用 wait/waitpid 等函数等待子进程结束，如果尚无子进程退出 wait 会导致父进程阻塞。waitpid 可以通过传递 WNOHANG 使父进程不阻塞立即返回。如果父进程很忙可以用 signal 注册信号处理函数，在信号处理函数调用 wait/waitpid 等待子进程退出。通过两次调用 fork。父进程首先调用 fork 创建一个子进程然后 waitpid 等待子进程退出，子进程再 fork 一个孙进程后退出。这样子进程退出后会被父进程等待回收，而对于孙子进程其父进程已经退出所以孙进程成为一个孤儿进程，孤儿进程由 init 进程接管，孙进程结束后，init 会等待回收。SIG_IGN 又是什么？signal.h中的宏定义 SIG_DFL 及SIG_IGNSIG_DFL, SIG_IGN 分别表示无返回值的函数指针，指针值分别是 0 和 1，这两个指针值逻辑上讲是实际程序中不可能出现的函数地址值。SIG_DFL：默认信号处理程序 SIG_IGN：忽略信号的处理程序ps. defunct: 死了的；不存在的docker 容器和信号处理Docker 的 stop 和 kill 命令都是用来向容器发送信号的。注意，只有容器中的 1 号进程能够收到信号。stop 命令会首先发送 SIGTERM 信号，并等待应用优雅的结束。默认是等待 10s，可以通过-t &lt;seconds&gt; 设置等待时间；如果发现应用没有结束 (用户可以指定等待的时间)，就再发送一个 SIGKILL 信号强行结束程序。kill 命令默认发送的是 SIGKILL 信号，当然你可以通过 -s 选项指定任何信号。 如果 PID 1 不处理信号传递，那么由 PID 1 启动的子进程将不能接收 docker 发送的信号（比如 bash 作为 ENTRYPOINT 入口）。案例一： 非 PID 1 的 ENTRYPOINT 进程不能正确处理 SIGTERMDockerfile1234567FROM alpine:3.11.6MAINTAINER ycwuADD t.sh .RUN chmod u+x t.sh# 使用 ENTRYPOINT ["./t.sh"] 其中 t.sh 文件 1234567#! /bin/shfor x in `seq 1 1000`do echo $x sleep 1done 然后构建镜像。第一个窗口执行 12345[root@host143 ycwu]# docker run -it --name test ycwu/ycwu-alpine:v1 1234 第二个窗口执行 123456789101112131415[root@host143 ~]# docker exec -it test topMem: 16025852K used, 224024K free, 54120K shrd, 40K buff, 4391664K cachedCPU: 7% usr 10% sys 0% nic 82% idle 0% io 0% irq 0% sirqLoad average: 0.02 0.04 0.05 3/1192 66 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root S 1584 0% 0 0% &#123;t.sh&#125; /bin/sh ./t.sh 61 0 root R 1568 0% 1 0% top 66 1 root S 1560 0% 2 0% sleep 1# shell 脚本以 /bin/sh 解析[root@host143 ~]# docker stop test# stop 命令会首先发送 SIGTERM 信号，并等待应用优雅的结束。如果发现应用没有结束(用户可以指定等待的时间)，就再发送一个 SIGKILL 信号强行结束程序。# 在此期间窗口一继续输出数字。# sleep 本身支持响应 SIGTERM 信号，但是外层的 sh 并没有传递信号。tinitini 是一个极简的 init 程序。使用 tini 的好处： 避免产生僵尸进程 为 docker 容器提供默认的信号处理器，能够处理 SIGTERMtini 原理：After spawning your process, Tini will wait for signals and forward those to the child process, and periodically reap zombie processes that may be created within your container.When the “first” child process exits (/your/program in the examples above), Tini exits as well, with the exit code of the child process (so you can check your container’s exit code to know whether the child exited successfully).tini 使用 tips：Tini 在退出时会重复使用子进程的退出代码。可以使用 -e 把任意子进程的 exit code 重新映射为 0。1ENTRYPOINT ["/tini", "-v", "-e", "143", "--", "your_app"]Tini 可以使用 -v、-vv、-vvv 输出更多的 debug 信息。Tini 可以结合 docker-entrypoint.sh 使用。1ENTRYPOINT ["/tini", "--", "/docker-entrypoint.sh"]Subreaping 问题。默认情况下 tini 应该作为 PID 1 进程，这样能够回收僵尸进程。如果 tini 不能作为 PID 1 进程，则可以使用 -s 参数：tini -s --tini 和 docker --init 关系 NOTE: If you are using Docker 1.13 or greater, Tini is included in Docker itself. This includes all versions of Docker CE. To enable Tini, just pass the –init flag to docker run.--init: Run an init inside the container that forwards signals and reaps processesdocker 1.13 以后内置了 tini，通过--init 启动。用 docker 时，如果不加 --init 参数，容器中的 1 号进程就是所给的 ENTRYPOINT，而加上 --init 之后，1 号进程就会是 tini。tini 进程能够将终止信号转发给其子进程, 同时能 reap 子进程, 不会出现因孤儿进程导致的线程句柄无法回收情形。案例二： docker init 和信号处理 这里使用 docker CE 19.03.5。还是上面的镜像，但是启动时候加上 --init:12345678[root@host143 ~]# docker run -it --init ycwu/ycwu-alpine:v1[root@host143 ~]# docker exec -it 9da ps auxPID USER TIME COMMAND 1 root 0:00 /sbin/docker-init -- ./t.sh 6 root 0:00 &#123;t.sh&#125; /bin/sh ./t.sh 66 root 0:00 sleep 1 67 root 0:00 ps aux 加上 --init 之后，PID 1 进程变成了 /sbin/docker-init，并且由它来执行 ENTRYPOINT 指定的脚本。 第二个窗口执行：12[root@host143 ~]# docker stop 9da 9da第一个窗口立即退出了。因此 SIGTERM 信号能够正确传播到 ./t.sh。 使用 tini 向前兼容 为 docker 1.13 以前版本增加 tini 支持，比如在 alpine 基础镜像：12RUN apk add --no-cache tini bashENTRYPOINT ["/tini", "--", "/docker-entrypoint.sh"]案例三： java 容器化 在容器中通过自定义脚本启动 java 应用，1ENTRYPOINT ["./run.sh"]init 进程是 sh 或者 bash，处理了 SIGTERM，但是没有传播到里面启动的 java 进程；这就导致了 java 不能正常接收 docker 发出的 SIGTERM，不能实现优雅关闭。如果直接使用 java 作为 init 进程，则可以接收 docker 发送的信号：1ENTRYPOINT ["java", "-jar", ...]但是 openjdk 对于 java 作为 init 进程有点 bug：jmap not happy on alpine #76。现在可以简单地使用 docker --init 或者 tini，避免 java 作为 PID 1 进程；解决了僵尸进程、信号处理、openjdk 工具 bug 问题。其他 init 进程方案 例如 sysvinit、upstart、systemd、supervisord。但是相比 tini，都很重型。TODO：后续了解这些方案。小结 docker 1.13 以后，集成了 tini。docker --init 把 PID 1 进程修改为 /sbin/docker-init --docker 1.13 以前，可以手动在镜像安装 tini 参考 Docker and the PID 1 zombie reaping problemlinux 下的僵尸进程处理 SIGCHLD 信号 在 entrypoint.sh 中使用 Tini 的优势是什么？krallin / tini在 docker 容器中捕获信号docker reap 僵尸进程问题]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerfile 使用经验]]></title>
    <url>%2Fp%2Fdocker-dockerfile-in-practise%2F</url>
    <content type="text"><![CDATA[docker build 命令 在 Dockerfile 所在目录 123456# -t Name and optionally a tag in the 'name:tag' format# -f Name of the Dockerfile (Default is 'PATH/Dockerfile')# --force-rm Always remove intermediate containers# --no-cache Do not use cache when building the image (很慢)docker build -t ycwu/ycwu-alpine:v1 . 最后的 . 表示使用当前目录作为工作目录，ADD、COPY 等命令以此作为操作的上下文。如果文件名不是 Dockerfile，要 -f 指定。1$ docker build --no-cache=true -f /path/to/Dockerfile -t some_tag -t image_name:image_version /path/to/buildENV通过 ENV 定义的环境变量，可以在 dockerfile 被后面的所有指令中使用。1234ENV TZ=Asia/Shanghai# 使用等号，可以在一个 ENV 指定设置多个环境变量 ENV a=1 b=2 ENV c 3ENV 的内容会写入层。但是经过测试，非等号语法方式设置环境变量，偶尔不能正确持久化到层，有些奇怪。 在docker run命令中通过 -e 标记来传递环境变量，这样容器运行时就可以使用该变量。1docker run --env &lt;key&gt;=&lt;value&gt;如果只需要对一条指令设置环境变量，可以使用这种方式：RUN &lt;key&gt;=&lt;value&gt; &lt;command&gt;。WORKDIR指定工作目录。如果不存在，则自动创建。复制文件 对于 COPY 和 ADD 命令来说，如果要把本地的文件拷贝到镜像中，那么本地的文件必须是在上下文目录中的文件。因为在执行 build 命令时，docker 客户端会把上下文中的所有文件发送给 docker daemon。COPY 和 ADD 都只复制目录中的内容而不包含目录自身 。 举个例子，Dockerfile 所在上下文目录情况：123456789101112[root@host143 tmp]# ls -al总用量 4drwxr-xr-x 3 root root 36 5 月 9 11:30 .drwxr-xr-x. 7 root root 138 5 月 9 11:27 ..-rw-r--r-- 1 root root 80 5 月 9 11:33 Dockerfiledrwxr-xr-x 2 root root 32 5 月 9 11:28 haha[root@host143 tmp]# ls -al haha总用量 8drwxr-xr-x 2 root root 32 5 月 9 11:28 .drwxr-xr-x 3 root root 36 5 月 9 11:30 ..-rw-r--r-- 1 root root 5 5 月 9 11:28 1.txt-rw-r--r-- 1 root root 8 5 月 9 11:28 2.txt执行如下的构建 12345678FROM alpine:3.11.6MAINTAINER ycwuWORKDIR /data# 把 haha 目录下内容复制到 /data 下面，不包括 haha 目录本身ADD haha .ENTRYPOINT ["sh"] 进去容器看看，没有 haha 目录。123456/data # ls -altotal 8drwxr-xr-x 1 root root 32 May 9 03:33 .drwxr-xr-x 1 root root 18 May 9 03:33 ..-rw-r--r-- 1 root root 5 May 9 03:28 1.txt-rw-r--r-- 1 root root 8 May 9 03:28 2.txt如果要保留目录名，只需要在 &lt;dest&gt; 加上目录名：1ADD haha ./haha进入容器看看：1234/data # lshaha/data # ls haha1.txt 2.txtADD1ADD &lt;src&gt;... &lt;dest&gt;必须是在上下文目录和子目录中，无法添加 ../a.txt 这样的文件。如果 &lt;src&gt; 是个目录，则复制的是目录下的所有内容，但不包括该目录。&lt;dest&gt;可以是绝对路径，也可以是相对 WORKDIR 目录的相对路径。ADD 命令支持下载远程文件，但是官方例子建议使用 RUN curl 或者 RUN wget 替代，因为可以直接删除源文件。Because image size matters, using ADD to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead. That way you can delete the files you no longer need after they’ve been extracted and you don’t have to add another layer in your image. For example, you should avoid doing things like:123ADD http://example.com/big.tar.xz /usr/src/things/RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/thingsRUN make -C /usr/src/things allAnd instead, do something like:1234RUN mkdir -p /usr/src/things \ &amp;&amp; curl -SL http://example.com/big.tar.xz \ | tar -xJC /usr/src/things \ &amp;&amp; make -C /usr/src/things allADD 支持自动解压缩文件，比如 tar。COPYCOPY 最重要的功能是 multi-stage build 中复制产物。技巧 如果要把多个文件复制到容器，其中有不经常变更、经常变更的文件，那么应该分开写，加速镜像构建。假设 a.txt（不经常变更）、b.txt（经常变更）、c.txt（经常变更）。123# a.txt 不经常变更，单独占一层，大概率可以重用缓存。ADD a.txt .ADD b.txt c.txt .1234# change apk source mirrorsRUN echo http://mirrors.ustc.edu.cn/alpine/v3.11/main &gt; /etc/apk/repositories &amp;&amp; \ echo http://mirrors.ustc.edu.cn/alpine/v3.11/community &gt;&gt; /etc/apk/repositoriesRUN apk update &amp;&amp; apk upgradeshell 模式和 exec 模式 Shell 格式：&lt;instruction&gt; &lt;command&gt;。Exec 格式：&lt;instruction&gt; [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;, ...]。exec 模式的参数要使用双括号：The exec form is parsed as a JSON array, which means that you must use double-quotes (“) around words not single-quotes (‘).shell 模式 使用 shell 模式时，docker 会以 /bin/sh -c &quot;task command&quot; 的方式执行任务命令。也就是说容器中的 1 号进程不是任务进程而是 bash 进程。shell 模式可以解析变量。exec 模式 使用 exec 模式时，容器中的任务进程就是容器内的 1 号进程。（如果执行 shell 脚本，则依然是 sh）因为 exec 模式不启动 shell，因此默认情况下缺少环境变量解析的能力。如果要解析环境变量，可以：1ENTRYPOINT ["/bin/bash", "-c", "echo", "$HOME"]ETNTRYPOINT &amp; CMDENTRYPOINT指定镜像的执行程序，只有最后一条 ENTRYPOINT 指令有效。如果想要覆盖 ENTRYPOINT 命令，需要在 docker run -it [image]后面添加–entrypoint string 参数 每个 Dockerfile 只能有一个 ENTRYPOINT 命令，如果存在多个 ENTRYPOINT 命令，则执行最后一个 ENTRYPOINT。CMDCMD 指令允许用户指定容器的默认执行的命令。此命令会在容器启动且 docker run 没有指定其他命令时运行。每个 Dockerfile 只能有一个 CMD 命令，如果存在多个 CMD 命令，则执行最后一个 CMD。区别 ENTRYPOINT 中的参数始终会被使用。CMD 设置的命令能够被 docker run 命令后面的命令行参数替换。docker entryfile sh 一些软件提供提供的 dockerfile，入口的 docker-entrypoint.sh 脚本值得看看。以 mysql 5.7 docker-entrypoint.sh 为例子。1234567set -eo pipefailshopt -s nullglob# if command starts with an option, prepend mysqldif ["$&#123;1:0:1&#125;" = '-' ]; then set -- mysqld "$@"fisetset -e: 后续脚本执行遇到非 0 返回值就退出。好处遇到执行错误就退出，避免往后产生更多错误。123Exit immediately if a pipeline (which may consist of a single simple command), a subshell command enclosed in parentheses, or one of the commands executed as part of a command listenclosed by braces (see SHELL GRAMMAR above) exits with a non-zero status.set -o pipefail：管道模式的命令，遇到错误就退出执行。1If set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status,or zero if all commands in the pipeline exit successfully. This option is disabled by default.shoptshopt 命令用于显示和设置 shell 中的行为选项。shopt -s nullglob：如果 nullglob 选项被设置，并且没有找到任何匹配，这个单词被删除。if [&quot;${1:0:1}&quot; = &#39;-&#39; ];${1:0:1}是 bash 的语法。从第 N 个参数截取字符串。第一个参数：命令行传入的第 N 个参数。第二个参数：截取的开始索引。第三个参数：截取的结束索引。${1:0:1}是指从 $1 截取 0-1 的字符串。整个 if 判断看是不是 - 开头的选项。set -- mysqld &quot;$@&quot;set --会将他后面所有以空格区分的字符串, 按顺序分别存储 $1，$2，… ，$@。12If no arguments follow this option, then the positional parameters are unset. Otherwise, thepositional parameters are set to the args, even if some of them begin with a -. 当 $* 和 $@ 不被双引号 &quot; &quot; 包围时，它们之间没有任何区别，都是将接收到的每个参数看做一份数据，彼此之间以空格来分隔。但是当它们被双引号 &quot; &quot; 包含时，就会有区别了：&quot;$*&quot;会将所有的参数从整体上看做一份数据，而不是把每个参数都看做一份数据。$@&quot;仍然将每个参数都看作一份数据，彼此之间是独立的。1234567891011121314151617#! /bin/bashecho "param:" $*echo 'loop"$@"'for x in "$@"do echo $xdoneechoecho 'loop "$*"'for x in "$*"do echo $xdone12345678[root@host143 ycwu]# ./t2.sh 123 456param: 123 456loop "$@"123456loop "$*"123 456exec使用 exec command 方式，会用 command 进程替换当前 shell 进程，并且保持 PID 不变。执行完毕，直接退出，不回到之前的 shell 环境。exec &quot;$@&quot;: 作为 entrypoint 的兜底，执行用户传入的命令。参考 深入 Dockerfile（一）: 语法指南 Dockerfile 中的 COPY 与 ADD 命令Best practices for writing Dockerfilesdocker entrypoint 入口文件详解 分析 Mysql 5.6 的 Dockerfile【exec】shell 脚本中的 exec 命令]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux localtime 和 timezone]]></title>
    <url>%2Fp%2Flinux-localtime-and-timezone%2F</url>
    <content type="text"><![CDATA[折腾 alpine，遇到时区问题，于是整理 linux 的 localtime 和 timezone。localtime vs timezone/etc/localtime 是用来描述本机时间，而 /etc/timezone 是用来描述本机所属的时区。修改时区 1echo &apos;Asia/Shanghai&apos; &gt; /etc/timezonelocaltime 影响 linux date 命令。timezone 影响 java Date 类型。 修改时间，建议使用软链接方式：1ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimejava TimeZone上面提到 /etc/timezone 影响 java 的 Date 行为。java 获取 timezone 的优先级，参见 TimeZone.java： 用户指定的 user.timezone 系统的 timezone，通过 navtive 方法获取。访问 TZ 环境变量、/etc/timezone等 GMT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Gets the platform defined TimeZone ID. **/private static native String getSystemTimeZoneID(String javaHome);private static synchronized TimeZone setDefaultZone() &#123; TimeZone tz; // get the time zone ID from the system properties String zoneID = AccessController.doPrivileged( new GetPropertyAction("user.timezone")); // if the time zone ID is not set (yet), perform the // platform to Java time zone ID mapping. if (zoneID == null || zoneID.isEmpty()) &#123; String javaHome = AccessController.doPrivileged( new GetPropertyAction("java.home")); try &#123; zoneID = getSystemTimeZoneID(javaHome); if (zoneID == null) &#123; zoneID = GMT_ID; &#125; &#125; catch (NullPointerException e) &#123; zoneID = GMT_ID; &#125; &#125; // Get the time zone for zoneID. But not fall back to // "GMT" here. tz = getTimeZone(zoneID, false); if (tz == null) &#123; // If the given zone ID is unknown in Java, try to // get the GMT-offset-based time zone ID, // a.k.a. custom time zone ID (e.g., "GMT-08:00"). String gmtOffsetID = getSystemGMTOffsetID(); if (gmtOffsetID != null) &#123; zoneID = gmtOffsetID; &#125; tz = getTimeZone(zoneID, true); &#125; assert tz != null; final String id = zoneID; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; @Override public Void run() &#123; System.setProperty("user.timezone", id); return null; &#125; &#125;); defaultTimeZone = tz; return tz;&#125; 对应的源文件 src/share/native/java/util/TimeZone.c：1234567891011121314151617181920212223242526272829303132333435363738Java_java_util_TimeZone_getSystemTimeZoneID(JNIEnv *env, jclass ign, jstring java_home, jstring country)&#123; const char *cname; const char *java_home_dir; char *javaTZ; if (java_home == NULL) return NULL; java_home_dir = JNU_GetStringPlatformChars(env, java_home, 0); if (java_home_dir == NULL) return NULL; if (country != NULL) &#123; cname = JNU_GetStringPlatformChars(env, country, 0); /* ignore error cases for cname */ &#125; else &#123; cname = NULL; &#125; /* * Invoke platform dependent mapping function */ javaTZ = findJavaTZ_md(java_home_dir, cname); free((void *)java_home_dir); if (cname != NULL) &#123; free((void *)cname); &#125; if (javaTZ != NULL) &#123; jstring jstrJavaTZ = JNU_NewStringPlatform(env, javaTZ); free((void *)javaTZ); return jstrJavaTZ; &#125; return NULL;&#125; 关键是 findJavaTZ_md()，是一个平台相关的实现。 打开一个看看：/src/solaris/native/java/util/TimeZone_md.c123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/* * findJavaTZ_md() maps platform time zone ID to Java time zone ID * using &lt;java_home&gt;/lib/tzmappings. If the TZ value is not found, it * trys some libc implementation dependent mappings. If it still * can't map to a Java time zone ID, it falls back to the GMT+/-hh:mm * form. `country', which can be null, is not used for UNIX platforms. *//*ARGSUSED1*/char *findJavaTZ_md(const char *java_home_dir, const char *country)&#123; char *tz; char *javatz = NULL; char *freetz = NULL; tz = getenv("TZ");#ifdef __linux__ if (tz == NULL) &#123;#else#ifdef __solaris__ if (tz == NULL || *tz == '\0') &#123;#endif#endif tz = getPlatformTimeZoneID(); freetz = tz; &#125; if (tz != NULL) &#123; if (*tz == ':') &#123; tz++; &#125;#ifdef __linux__ /* * Ignore "posix/" prefix. */ if (strncmp(tz, "posix/", 6) == 0) &#123; tz += 6; &#125;#endif javatz = strdup(tz); if (freetz != NULL) &#123; free((void *) freetz); &#125; &#125; return javatz;&#125;/* * Performs libc implementation specific mapping and returns a zone ID * if found. Otherwise, NULL is returned. */static char *getPlatformTimeZoneID()&#123; struct stat statbuf; char *tz = NULL; FILE *fp; int fd; char *buf; size_t size; /* * Try reading the /etc/timezone file for Debian distros. There's * no spec of the file format available. This parsing assumes that * there's one line of an Olson tzid followed by a '\n', no * leading or trailing spaces, no comments. */ if ((fp = fopen(ETC_TIMEZONE_FILE, "r")) != NULL) &#123; char line[256]; if (fgets(line, sizeof(line), fp) != NULL) &#123; char *p = strchr(line, '\n'); if (p != NULL) &#123; *p = '\0'; &#125; if (strlen(line) &gt; 0) &#123; tz = strdup(line); &#125; &#125; (void) fclose(fp); if (tz != NULL) &#123; return tz; &#125; &#125; /* * Next, try /etc/localtime to find the zone ID. */ if (lstat(DEFAULT_ZONEINFO_FILE, &amp;statbuf) == -1) &#123; return NULL; &#125; /* * If it's a symlink, get the link name and its zone ID part. (The * older versions of timeconfig created a symlink as described in * the Red Hat man page. It was changed in 1999 to create a copy * of a zoneinfo file. It's no longer possible to get the zone ID * from /etc/localtime.) */ if (S_ISLNK(statbuf.st_mode)) &#123; char linkbuf[PATH_MAX+1]; int len; if ((len = readlink(DEFAULT_ZONEINFO_FILE, linkbuf, sizeof(linkbuf)-1)) == -1) &#123; jio_fprintf(stderr, (const char *) "can't get a symlink of %s\n", DEFAULT_ZONEINFO_FILE); return NULL; &#125; linkbuf[len] = '\0'; tz = getZoneName(linkbuf); if (tz != NULL) &#123; tz = strdup(tz); &#125; return tz; &#125; /* * If it's a regular file, we need to find out the same zoneinfo file * that has been copied as /etc/localtime. */ size = (size_t) statbuf.st_size; buf = (char *) malloc(size); if (buf == NULL) &#123; return NULL; &#125; if ((fd = open(DEFAULT_ZONEINFO_FILE, O_RDONLY)) == -1) &#123; free((void *) buf); return NULL; &#125; if (read(fd, buf, size) != (ssize_t) size) &#123; (void) close(fd); free((void *) buf); return NULL; &#125; (void) close(fd); tz = findZoneinfoFile(buf, size, ZONEINFO_DIR); free((void *) buf); return tz;&#125;zdump 命令Zdump 对命令行中的每一个 zonename 输出其当前时间。123[root@host143 Asia]# zdump America/New_York PRCAmerica/New_York Fri May 8 09:39:35 2020 EDTPRC Fri May 8 21:39:35 2020 CSTtzselect 命令tzselect 提供交互引导方式设置时区。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@host143 Asia]# tzselectPlease identify a location so that time zone rules can be set correctly.Please select a continent or ocean. 1) Africa 2) Americas 3) Antarctica 4) Arctic Ocean 5) Asia 6) Atlantic Ocean 7) Australia 8) Europe 9) Indian Ocean10) Pacific Ocean11) none - I want to specify the time zone using the Posix TZ format.#? 5Please select a country. 1) Afghanistan 18) Israel 35) Palestine 2) Armenia 19) Japan 36) Philippines 3) Azerbaijan 20) Jordan 37) Qatar 4) Bahrain 21) Kazakhstan 38) Russia 5) Bangladesh 22) Korea (North) 39) Saudi Arabia 6) Bhutan 23) Korea (South) 40) Singapore 7) Brunei 24) Kuwait 41) Sri Lanka 8) Cambodia 25) Kyrgyzstan 42) Syria 9) China 26) Laos 43) Taiwan10) Cyprus 27) Lebanon 44) Tajikistan11) East Timor 28) Macau 45) Thailand12) Georgia 29) Malaysia 46) Turkmenistan13) Hong Kong 30) Mongolia 47) United Arab Emirates14) India 31) Myanmar (Burma) 48) Uzbekistan15) Indonesia 32) Nepal 49) Vietnam16) Iran 33) Oman 50) Yemen17) Iraq 34) Pakistan#? 9Please select one of the following time zone regions.1) Beijing Time2) Xinjiang Time#? 1The following information has been given: China Beijing TimeTherefore TZ=&apos;Asia/Shanghai&apos; will be used.Local time is now: Fri May 8 21:42:41 CST 2020.Universal Time is now: Fri May 8 13:42:41 UTC 2020.Is the above information OK?1) Yes2) No#?]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 alpine 基础镜像]]></title>
    <url>%2Fp%2Fdocker-alpine%2F</url>
    <content type="text"><![CDATA[最近使用 alpine 制作镜像，记录使用经验。Alpine Linux is a Linux distribution built around musl libc and BusyBox. The image is only 5 MB in size.glibc vs musl libcglibc 是 linux 下面 c 标准库的实现，即 GNU C Library。glibc 本身是 GNU 旗下的 C 标准库，后来逐渐成为了 Linux 的标准 c 库。其实现了常见的 C 库的函数，支持很多种系统平台，功能很全，但是也相对比较臃肿和庞大。Musl 是一个轻量级的 C 标准库，主要目标是跨平台，减少底层依赖，比如移植到新的 os，支持嵌入式操作系统和移动设备。uClibc 一个小型的 C 语言标准库，主要用于嵌入式。和 glibc 在源码结构和二进制上，都不兼容。Eglibc = Embedded GLIBC 。glibc 的原创作组织 FSF 推出的 glibc 的一种变体，目的在于将 glibc 用于嵌入式系统。保持源码和二进制级别的兼容于 Glibc 源代码架构和 ABI 层面兼容。之前用 glibc 编译的程序，可以直接用 eglibc 替换，而不需要重新编译。 这样就可以复用之前的很多的程序了。 Eglibc 的最主要特点就是可配置，这样对于嵌入式系统中，你所不需要的模块，比如 NIS，locale 等，就可以裁剪掉，不把其编译到库中，使得降低生成的库的大小了。alpine 和 centos 的重要差别是：libc 从 glibc 换成了 musl。因此很多软件包不能直接使用。为了能够正常使用这些基于 glibc 的软件，可以使用 alpine-pkg-glibc：123456# -q: 安静模式，不显示进度条# -O: 保存到文件wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pubwget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-2.31-r0.apkadk add glibc-2.31-r0.apkrm -f glibc-2.31-r0.apk 包管理工具： apkalpine 使用 apk 作为包管理工具，功能对应 yum、apt。时区 alpine 默认把时区相关部分也去掉了。需要安装 tzdata。1234ENV TZ=Asia/ShanghaiRUN apk add -U tzdata \ &amp;&amp; ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \ &amp;&amp; echo &apos;$TZ&apos; &gt; /etc/timezone 国际化支持 i18n 支持也是被删掉的。If you are using tools like localedef you will need the glibc-bin and glibc-i18n packages in addition to the glibc package.12345wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-bin-2.31-r0.apkwget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-i18n-2.31-r0.apkapk add glibc-bin-2.31-r0.apk glibc-i18n-2.31-r0.apk/usr/glibc-compat/bin/localedef -i en_US -f UTF-8 en_US.UTF-8rm -f glibc-bin-2.31-r0.apk glibc-i18n-2.31-r0.apk 安装 JDKJava 是基于 GUN Standard C library(glibc)。apk 支持安装 openjdk、openjre。1234567891011121314151617181920212223242526272829303132333435363738394041FROM amd64/alpine:3.11.6 MAINTAINER ycwuENV LANG=en_US.UTF-8ENV LANGUAGE=en_US:enENV LC_ALL=en_US.UTF-8ENV TZ=Asia/ShanghaiENV JAVA_HOME=/usr/lib/jvm/default-jvmENV PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin:/usr/glibc-compat/bin# change apk source mirrorsRUN echo http://mirrors.ustc.edu.cn/alpine/v3.11/main &gt; /etc/apk/repositories &amp;&amp; \ echo http://mirrors.ustc.edu.cn/alpine/v3.11/community &gt;&gt; /etc/apk/repositoriesRUN apk update &amp;&amp; apk upgrade# install glibcRUN wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \ &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-2.31-r0.apk \ &amp;&amp; apk add glibc-2.31-r0.apk &amp;&amp; rm -rf glibc-2.31-r0.apkRUN apk add --no-cache tini bash# install i18nRUN wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-bin-2.31-r0.apk \ &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-i18n-2.31-r0.apk \ &amp;&amp; apk add glibc-bin-2.31-r0.apk glibc-i18n-2.31-r0.apk \ &amp;&amp; /usr/glibc-compat/bin/localedef -i en_US -f UTF-8 en_US.UTF-8 \ &amp;&amp; rm -rf glibc-bin-2.31-r0.apk glibc-i18n-2.31-r0.apk # setup localtime &amp; timezoneRUN apk add -U tzdata \ &amp;&amp; echo '$TZ' &gt; /etc/timezone \ &amp;&amp; ln -snf /usr/share/zoneinfo/$TZ /etc/localtime # install openjdk8RUN apk add openjdk8# other toolsRUN apk add tcpdumpENTRYPOINT ["sh"]1docker build -t ycwu/ycwu-alpine:v1 .镜像大小 165MB。注意：alpine 容器中通过 env 或者 printenv 查看环境变量 这里还有压缩空间，优化 i18n 和 locale：只保留 utf8 和 asia/shanghai。123456/ # /usr/glibc-compat/bin/localedef --usageSystem&apos;s directory for character maps : /usr/glibc-compat/share/i18n/charmaps repertoire maps: /usr/glibc-compat/share/i18n/repertoiremaps locale path :/usr/glibc-compat/lib/locale:/usr/glibc-compat/share/i18n1234RUN apk add -U tzdata \ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai tz &amp;&amp; rm -rf /usr/share/zoneinfo &amp;&amp; mkdir -p /usr/share/zoneinfo/Asia &amp;&amp; cp tz /usr/share/zoneinfo/Asia/Shanghai &amp;&amp; rm -f tz \ &amp;&amp; echo &apos;$TZ&apos; &gt; /etc/timezone \ &amp;&amp; ln -snf /usr/share/zoneinfo/$TZ /etc/localtime还有更加直接粗暴的做法：把文件 ADD 进去。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nacos 实战 1：排错指南]]></title>
    <url>%2Fp%2Fnacos-in-action-p1%2F</url>
    <content type="text"><![CDATA[整理 nacos 使用中遇到的问题。nacos server 地址配置问题，缺少必要的日志提示 背景：使用 k8s nacos，为了兼容部分物理机部署应用，把 nacos 端口容器内外端口统一为 30848。但是部分容器内应用访问 nacos 端口忘记相应改变，导致没有访问到 nacos 服务器。启动的时候，报错不能解析 placeholder:123452020-04-27 20:41:32.894 INFO 11088 --- [restartedMain] c.a.b.n.c.u.NacosConfigPropertiesUtils : nacosConfigProperties : com.alibaba.boot.nacos.config.properties.NacosConfigProperties@59d702db2020-04-27 20:41:37.098 INFO 11088 --- [restartedMain] c.a.b.n.config.util.NacosConfigUtils : load config from nacos, data-id is : test, group is : ycwu// 省略一堆无关日志 Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'msg.text' in value "$&#123;msg.text&#125;" 一开始没想到是服务器端口改变，查了好一会。最终同事发现是服务端口对不上，修改后就正确了。对应问题：nacos 没有输出不能连接服务器的错误提示（即使修改日志级别为 debug 也没有！），不方便排查问题。处理方案：Could not resolve placeholder可能是 nacos 没有正常连接到 nacos server，导致 placeholder 解析失败。检查配置是否正确 nacos-config-spring-boot-starter 和 @NacosRefresh 版本支持 问题：使用 nacos-config-spring-boot-starter 接入 nacos。最初使用 0.2.4，后来升级为 0.2.6。升级之后，发现 @NacosRefresh 不见了，编译报错。@NacosRefresh支持对 spring 的 @Value 注解实现热更新。nacos-config-spring-boot-starter 会引入 nacos-spring-context 依赖，对应关系为：0.2.4，对应 0.3.40.2.6，对应 0.3.6 对比发现在新版本 nacos-spring-context，以下类被删掉了：NacosRefreshSpringValueAnnotationBeanPostProcessorValueAnnotationBeanPostProcessor于是通过 @NacosRefresh 对 spring 的 @Value 支持就没了。在官方 github 找到这个文章：support @Value auto refresh #160。这。。。有点无力吐槽。为了支持热更新，对应的解决方案：使用 @NacosValue 替换 spring 的 @Value 或者使用 spring cloud nacos，以及 spring cloud 的 @RefreshScope 或者，把 nacos-spring-context 0.3.4 中的几个 java 文件收到拷贝到项目，并且 @Component 在 spring 中激活：nacos client 日志和数据缓存 nacos client 会在启动应用的用户目录创建日志和数据缓存，可以方便排查问题123456# 日志~/logs/nacos/config.log~/logs/nacos/naming.log# 数据缓存~/nacos/config/&lt; 配置中心 &gt;-&lt; 名空间 &gt;/snapshot-tenant/&lt; 名空间 &gt;/&lt;group&gt;/&lt;data id&gt; 日志文件可以看到 nacos client 访问 nacos server 的情况 12345678910111213142020-07-15 15:53:19.279 ERROR [com.alibaba.nacos.client.Worker.longPolling.fixed-nacos-center.v-base_30848-a85a37ef-5bec-478c-a60f-0b11f10b3da4:c.a.n.c.java.net.ConnectException: no available server, currentServerAddr : http://nacos-center.v-base:30848 at com.alibaba.nacos.client.config.http.ServerHttpAgent.httpPost(ServerHttpAgent.java:178) ~[nacos-client-1.2.0.jar!/:na] at com.alibaba.nacos.client.config.http.MetricsHttpAgent.httpPost(MetricsHttpAgent.java:64) ~[nacos-client-1.2.0.jar!/:na] at com.alibaba.nacos.client.config.impl.ClientWorker.checkUpdateConfigStr(ClientWorker.java:386) [nacos-client-1.2.0.jar!/:na] at com.alibaba.nacos.client.config.impl.ClientWorker.checkUpdateDataIds(ClientWorker.java:354) [nacos-client-1.2.0.jar!/:na] at com.alibaba.nacos.client.config.impl.ClientWorker$LongPollingRunnable.run(ClientWorker.java:521) [nacos-client-1.2.0.jar!/:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_221] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_221] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_221] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_221] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_221] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_221] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_221] 数据目录可以看到是否拉取到最新数据。]]></content>
      <categories>
        <category>nacos</category>
      </categories>
      <tags>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka：解决 too many open files]]></title>
    <url>%2Fp%2Fkafka-open-files-limits%2F</url>
    <content type="text"><![CDATA[在一台机器上 kafka server.log 发现一堆”too many open files”异常：12345678[2020-04-16 15:20:39,782] ERROR Error while accepting connection (kafka.network.Acceptor)java.io.IOException: 打开的文件过多 at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250) at kafka.network.Acceptor.accept(SocketServer.scala:337) at kafka.network.Acceptor.run(SocketServer.scala:280) at java.lang.Thread.run(Thread.java:745)ps. 关联的 WARN 日志：1Attempting to send response via channel for which there is no open connection, connection idulimit -n 发现 openfiles 是 1024，实在太小了。解决：临时修改：ulimit -n 102400。重启 kafka server。永久修改/etc/security/limits.conf1234vim /etc/security/limits.conf # 在最后加入 kafka soft nofile 102400 kafka hard nofile 102400ulimit 扩展：12-H ：hard limit ，严格的设定，必定不能超过这个设定的数值-S ：soft limit ，警告的设定，可以超过这个设定值，但是若超过则有警告信息]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka rebalance 系列：incremental cooperative rebalancing]]></title>
    <url>%2Fp%2Fkafka-rebalance-p4%2F</url>
    <content type="text"><![CDATA[Incremental Cooperative RebalancingThe key idea behind all the designs proposed here is to change the assumption we’ve always made with groups and their resources until now:when you go into a JoinGroup, it is expected that you give up control of all resources that you control and get them into a clean state for handoff.在 kafka 2.3 以前，rebalance 对 kafka consumer 集群性能的影响，体现在一旦进入 JoinGroup 则立即放弃控制资源，其中涉及状态初始化、offset 提交、释放 partition，是耗时的操作。KIP-429 这个 KIP 在原有的紧迫再平衡协议（eager rebalance protocol）的基础上，增加了消费者增量平衡协议（Incremental Rebalance Protocol）。与 eager 协议不同，eager 协议总是在重新平衡之前撤销所有已分配的分区，然后尝试重新分配它们。而 incremental 协议允许消费者在重新平衡事件期间保留其分区，从而尽量减少消费者组成员之间的分区迁移。因此，通过 scaling out/down 操作触发的端到端重新平衡时间更短，这有利于重量级、有状态的消费者，比如 Kafka Streams 应用程序。Incremental because the final desired state of rebalancing is reached in stages. A globally balanced final state does not have to be reached at the end of each round of rebalancing. A small number of consecutive rebalancing rounds can be used in order for the group of Kafka clients to converge to the desired state of balanced resources. In addition, you can configure a grace period to allow a departing member to return and regain its previously assigned resources.“增量”是指最终平衡状态经历多个阶段实现，而不需要在一次全局 stop-the-world 平衡中实现。可以设置一个 grace period，方便 consumer 重新加入和再次拿到之前分配的资源。Cooperative because each process in the group is asked to voluntarily release resources that need to be redistributed. These resources are then made available for rescheduling given that the client that was asked to release them does so on time.“协同”是指 group 内的进程自愿释放需要被重新分发的资源。Incremental Cooperative Rebalancing 从以下 3 个方面进行优化：Design I: Simple Cooperative RebalancingDesign II: Deferred Resolution of ImbalanceDesign III: Incremental Resolution of ImbalanceDesign I: Simple Cooperative Rebalancingmember 进程:member 在 JoinGroup 中附带订阅的 topics，以及分配的 partitions 列表。在 JoinGroup 过程中，memeber 继续持有已有资源。（对比原来的设计：一旦进入 JoinGroup 则立即放弃控制资源）处理 assignment 中新分配的分区 如果 assignment 包含 RevokePartitions，则立即停止处理对应的分区 ，commit，并且立即初始化另一轮 join groupleader 进程： 为所有 member 计算分区；另外，通过 member 上报的信息，计算 RevokePartitions。leader 要为 group 内失联的 topic partition 负责（持有这些分区的 member 可能不再返回）。解决方式是在上边步骤，分配分区的时候要包含所有分区（而不是只局限于上报的分区）这一版本的优化只对 RevokePartitions 发生影响。对比以前的设计，一旦进入 JoinGroup，所有 consumer 都要停止处理、commit offset。Design II: Deferred Resolution of Imbalancewe should schedule another rebalance instead of always trying to resolve imbalance immediately.在 Design 1 的基础上，在 assignment 增加 ScheduledRebalanceTimeout，延迟处理不平衡状态。1234567891011Assignment (Leader → Member):Assignment =&gt; Version AssignedTopicPartitions RevokeTopicPartitions ScheduledRebalanceTimeout Version =&gt; int16 AssignedTopicPartitions =&gt; [Topic Partitions] Topic =&gt; String Partitions =&gt; [int32] RevokeTopicPartitions =&gt; [Topic Partitions] Topic =&gt; String Partitions =&gt; [int32] ScheduledRebalanceTimeout =&gt; int32新增配置：scheduled.rebalance.max.delay.ms(默认 5min)。对应上面的 ScheduledRebalanceTimeout。member 进程：如果 ScheduledRebalanceTimeout &gt; 0，则在超时之后尽快 rejoin（RevokePartitions 字段为空才设置该字段）。As long as this delay is active, the lost tasks remain unassigned. This gives the departing worker (or its replacement) some time to return to the group. Once that happens, a second rebalance is triggered, but the lost tasks remain unassigned until the scheduled rebalance delay expires.scheduled.rebalance.max.delay.ms 允许延迟若干时间才触发 rebalance，在这期间部分任务未被分配。离开的 worker 有机会在这时间内重新加入 group。当超时发生，再触发一次 rebalance。这样设计是期望原来 worker 能够及时 rejoin，再下一次 rebalance 就不需要 revoke partition。kafka 0.11 增加了配置：group.initial.rebalance.delay.ms，作用是类似的，但是只应用于空组。ScheduledRebalanceTimeout 应用场景更广。Design III: Incremental Resolution of Imbalance在 Design 2 的基础上，允许 leader 以多次迭代、每次只重新分配部分分区的方式、实现 rebalance。协议配置选项 增加 connect.protocol 选项：Values: eager, compatibleDefault: compatibleThis property defines which Connect protocol is enabled.eager corresponds to the initial non-cooperative protocol that resolves imbalance with an immediate redistribution of connectors and tasks (version 0).compatible corresponds to both eager (protocol version 0) and incremental cooperative (protocol version 1 or higher) protocols being enabled with the incremental cooperative protocol being preferred if both are supported (version 1 or version 0).小结 kafka 2.3 的增量协同平衡优化：member 进入 JoinGroup 状态，继续持有资源。只对 RevokePartitions 释放资源。 对于 RevokePartitions，不是立即释放，而是等待 scheduled.rebalance.max.delay.ms，让离开的 worker 有机会 rejoin。发生超时后再触发 rebalance。scheduled.rebalance.max.delay.ms 是group.initial.rebalance.delay.ms的升级 参考Incremental Cooperative Rebalancing: Support and PoliciesKIP-415: Incremental Cooperative Rebalancing in Kafka ConnectApache Kafka Rebalance Protocol, or the magic behind your streams applications]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka rebalance 系列：static membership 优化]]></title>
    <url>%2Fp%2Fkafka-rebalance-p3%2F</url>
    <content type="text"><![CDATA[rebalance 流程 在了解 static membership 优化之前，先简单学习 rebalance 流程（kafka 2.3 以前）。这篇文章对理解 rebalance 流程很有帮助，推荐阅读：Apache Kafka Rebalance Protocol, or the magic behind your streams applications。下面图片都是来自这篇文章。JoinGroup当一个 consumer 启动，通过向 kafka broker coordinator 发送 FindCoordinator 请求找到 group coordinator，然后发送 JoinGroup。 注意请求带上了 session.timeout.ms 和max.poll.interval.ms，协助 coordinator 踢出超时的 client。JoinGroup请求，使得 coordinator 进入屏障，在 min(rebalance timeout, group.initial.rebalance.delay.ms) 时间内，等待收集其他 consumer 发送的 JoinGroup 请求。group 内第一个 consumer 被选作 group leader。coordinator 向 leader 发送 JoinGroup 响应，包含当前活跃成员列表。其他成员收到空响应。group leader 负责 partition 分配。SyncGroup所有成员向 coordinator 发送 SyncGroup 请求。其中，group leader 的 SyncGroup 请求包含了 partition 分配结果。其他成员的 SyncGroup 请求是空请求。coordinator 响应所有的 SyncGroup 请求。每个 consumer 收到响应后，知道自己分配到的 partition，监听分区并且拉取消息。Heartbeatconsumer 后台线程发送心跳：heartbeat.interval.ms。在 rebalance 阶段，coordinator 收到心跳信息，则认为这个 consumer 需要 rejoin。coordinator 通知其他 consumer，在下一个 heartbeat 周期，进行 JoinGroup、SyncGroup 操作。在整个 rebalance 阶段，在重新分区之前，consumer 都不会再处理任何消息。默认的 rebalance timeout (max.poll.interval.ms)为 5min，是非常长的，会导致产生很大的 consumer-lag。consumer 滚动更新的问题 Transient failures are those that occur temporarily and for a short period of time. 并非所有的 consumer 异常都需要触发 rebalance。它们可能稍后就会重新加入 group，比如滚动升级。而当一个新的成员加入 group，请求里面不包含任何 membership 信息。Coordinator 分配一个 UUID 作为此成员 id，并且缓存起来。这个 consumer 后续的生命周期内，都会使用这个 member id。那么这个 consumer rejoin 的时候不会触发 rebalance。但是，一个刚刚重启的 consumer，本地内存里面没有 member id 或者 generation id。因此它重新加入 group，会触发 rebalance。GroupCoordinator 也不保证原来这个 member 处理的分区会被重新分配回去。于是问题变为：如何在重启之后，还能正常识别 consumer。KIP-345 static membership为了解决这个问题，KIP-345 增加 static membership 特性：增加 group.instance.id 选项（client 端）。Group instance id 是用户指定的、区分不同 client 的标识。现在 GroupCoordinator 识别一个 consumer，可以通过 coordinator-assigned member ID （client 重启后丢失）group.instance.id （client 重启后不丢失） 有了 static membership 之后，触发 consumer group rebalace 的条件:A new member joinsA leader rejoins (possibly due to topic assignment change)An existing member offline time is over session timeoutBroker receives a leave group request containing a list of group.instance.ids为了使用 static membership 配置，需要 server 和 client 都升级到 kafka 2.3 版本。开启 static membership 之后，还要考虑 session.timeout.ms 是否足够大。When using static membership, it’s recommended to increase the consumer property session.timeout.ms large enough so that the broker coordinator will not trigger rebalance too frequently.参考KIP-345: Introduce static membership protocol to reduce consumer rebalancesApache Kafka Rebalance Protocol, or the magic behind your streams applicationsApache Kafka Rebalance Protocol for the Cloud: Static Membership]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka rebalance 系列：空消费者组优化]]></title>
    <url>%2Fp%2Fkafka-rebalance-p2%2F</url>
    <content type="text"><![CDATA[背景 上次讲到，消费者的重启 / 关闭会也是 rebalance 的原因之一。大量消费者不是同一个时间完成启动，导致反复进行 rebalance。这次记录 consumer group 状态和 rebalance 流程的关系，以及 kafka 做出的优化。 consumer group 状态和状态转移 GroupCoordinator 负责组状态维护。kafka 0.11 之前，Server 端 Consumer 的 Group 共定义了五个状态：Empty：Group has no more members, but lingers until all offsets have expired. This state also represents groups which use Kafka only for offset commits and have no members.PreparingRebalance：Group is preparing to rebalance.AwaitingSync：Group is awaiting state assignment from the leader.Stable：Group is stable.Dead：Group has no more members and its metadata is being removed. 完整的状态定义和状态转移见 GroupMetadata.scala。这里引用网上找到一张状态机图，非常值得仔细学习：（图片来源：https://matt33.com/2017/01/16/kafka-group/） 现有问题 kafka 0.11 之前，一个新 consumer group 的建立，要经过至少 2 次 rebalance（假设这个组有不只一个成员）。rebalance 是一个耗时的操作，涉及状态的持久化、offset 提交等。如果涉及移出 partition，则成本更高。 产生多次 rebalance 的一个重要原因是，consumer 通常不是在同一个时间启动。每次有 consumer 加入 / 离开 group，都要触发 rebalance，如果有相当数量的 consumers，那么要经历比较长的时间才能进入 stable 状态。KIP-134增加新的状态：InitialRebalance。增加新的 broker 配置项：group.initial.rebalance.delay.ms。InitialRebalance 发生在 Empty 和 PreparingRebalance 之间。当一个空组收到第一个 JoinGroupRequest，那么：Group 的状态：Empty =&gt; InitialRebalance最长等待 t=min(rebalanceTimeout, group.initial.rebalance.delay.ms)(rebalanceTimeout对应 max.poll.interval.ms，默认 5min。) 若等待期间，有新的 consumer 加入，则 Group 依然处于 InitialRebalance 状态，且更新等待时间为 min(remainingRebalanceTimeout, group.initial.rebalance.delay.ms) 若等待超时，那么 JoinGrou 完成，且 Group 状态：InitialRebalance =&gt; PreparingRebalance。我的理解：InitialRebalance 阶段增加了单次 rebalance 的时间。但是一次 rebalance 能够收集更多的 JoinGroupRequest，避免反复进入 rebalance，从而减少整体总的 rebalance 时间。参考KIP-134: Delay initial consumer group rebalance]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka rebalance 系列：KIP-62 max.poll.interval.ms]]></title>
    <url>%2Fp%2Fkafka-rebalance-p1%2F</url>
    <content type="text"><![CDATA[背景 最近处理线上 kafka rebalance 问题，顺便整理笔记。 这次讨论的是 consumer 不能及时发送心跳，导致 rebalance 的问题。kafka 0.10.0 以前的消费者心跳问题 kafka 官方推荐在 consumer thread 中完成拉取数据和消费数据，简化编程。 但是，kafka 0.10.0 以前，kafka consumer thread 在单个线程内既负责消费 records 数据，又负责维护和 kafka server 的心跳。如果消费者消耗大量时间处理 records，超过了 session timeout 还没上报心跳，那么这个 consumer 会被 group 移出，它所拥有的 partition 被指定到其他成员（触发 rebalance）。实际上，消费者并不是死掉，而是正在消费 records。因此这个 reblance 是多余的。最好的情况，这个消费者重新加入。最坏的情况，因为消费者被踢出了 group，不能正常提交 offset，那么这些消息在 rebalance 之后又被再次消费。but if this takes longer than the configured session timeout, the consumer is removed from the group and its partitions are assigned to other members.In the best case, this causes unneeded rebalancing when the consumer has to rejoin.In the worst case, the consumer will not be able to commit offsets for any messages which were handled after the consumer had fallen out of the group, which means that these messages will have to be processed again after the rebalance.If a message or set of messages always takes longer than the session timeout, then the consumer will not be able to make progress until the user notices the problem and makes some adjustment.在 0.10.0.0 以前，解决这个问题的方式：增加 session timeout 时间 减少 max.poll.records 对于 1)，尽管避免了 consumer thread 长时间消费 records、未能及时上报心跳的问题。但是，一旦进程崩溃，kafka 要等待更长时间才能识别这个故障的 consumer。对于 2)，会对某些批处理应用性能有影响。rebalance 是一个耗时的操作。一旦开始 rebalance，consumer 要完成正在处理事情，提交 offset，在 session 超时之前重新加入 group。即使一个消费者正常发送心跳，如果在 rebalance 期间不能及时加入，也会被剔除掉。When a rebalance begins, the consumer must finish whatever processing it is currently doing, commit offsets, and rejoin the group before the session timeout expires. In other words, the session timeout is also used as a rebalance timeout.Even if a consumer continues sending heartbeats to the coordinator, it will be removed from the group if it cannot rejoin fast enough when a rebalance begins.KIP-62根本原因是，session timeout 的使用范围膨胀了：标记应用失败或者不可访问 consumer 处理一批 records 的最大时间 一个 group 完成 rebalance 的最大时间 KIP-62 进行了优化，把这些 timeout 解耦： 由后台线程发送心跳 增加了 max.poll.interval.ms 配置项。增加 rebalance timeout后台线程单独发送心跳，则不会占用 consumer thread 的时间。解耦消费者处理 records 的最大 timeout增加 max.poll.interval.ms，作为 consumer 处理一批 records 的 timeout。这是两次连续调用poll() 的最大间隔。如果发生 timeout，那么 consumer 停止发送心跳，并且显式发送 LeaveGroup 请求 。 当 consumer 再次调用poll()，消费者会 rejoin the group。 那么 max.poll.interval.ms 要怎么配置？In practice, we expect the process timeout to always be at least as large as the session timeout.解耦 rebalance timeout在 JoinGroup Request 增加 rebalance timeout 字段：We propose to add a separate rebalance timeout, which is passed to the coordinator in the JoinGroup request.As with the session timeout, the broker will use the maximum rebalance timeout across all members in the group那么这个值设置多少？官方建议是 max.poll.interval.msSince we give the client as much as max.poll.interval.ms to handle a batch of records, this is also the maximum time before a consumer can be expected to rejoin the group in the worst case.We therefore propose to set the rebalance timeout in the Java client to the same value configured with max.poll.interval.ms. 新版本的 JoinGroup Request12345678GroupId =&gt; stringSessionTimeout =&gt; int32RebalanceTimeout =&gt; int32 ;;; this is newMemberId =&gt; stringProtocolType =&gt; stringGroupProtocols =&gt; [ProtocolName ProtocolMetadata] ProtocolName =&gt; string ProtocolMetadata =&gt; bytes小结 KIP-62 优化（0.10.0）： 单独的后台线程发送心跳 max.poll.interval.ms 表示单个处理线程的最大超时。它是 client 两次主动发起 poll() 的最大间隔。如果超出这个时间，client 会发送 LeaveGroup 命令。同时用于 rebalance timeout。session.timeout.ms表示进程级别的超时 123 参数名 --&gt; MemberMetadata 字段 session.timeout.ms --&gt; MemberMetadata.sessionTimeoutMsmax.poll.interval.ms --&gt; MemberMetadata.rebalanceTimeoutMs 默认 5min 非常值得阅读 KIP-62 原文。参考KIP-62: Allow consumer to send heartbeats from a background threadDifference between session.timeout.ms and max.poll.interval.ms for Kafka 0.10.0.0 and later versions]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql 性能优化 case 3]]></title>
    <url>%2Fp%2Fsql-performance-tuning-case-p3%2F</url>
    <content type="text"><![CDATA[背景 一个 count 查询耗时超过 1 秒。使用了 pagehelper 计算 count。 中间是一堆上万个的 ID 列表。这个 sql 贴到文本文件，有 200KB😂。优化过程 套路：子查询的 ORDER BY ID DESC 可以去掉，节省时间。 子查询 SELECT A.* 对于 count 来说是多余的，去掉。1SELECT COUNT(0) FROM VIDEODEV_INFO_VIEW A WHERE (A.ADMINAREA_GB_CODE LIKE concat(44, '%') ) OR A.BITMAP_ID in (4970,3640) // in 是一堆 ID 列表 速度是很快很多，从 1.2s 降低到 0.7s。但是，根据 count 函数的语义，上面 sql 应该一个返回一行，数值是行数 N，而不是不是返回 N 多行 1。于是把 ID 列表删除只剩下几个，发现 count 函数返回又符合预期。这期间犯傻了，没有认证对比原来的 sql，还以为是触发了 mysql 的什么 bug，绕了很大的弯路。最后在无意中才发现了 id 太长，里面还有 group by 语句 12SELECT COUNT(0) FROM VIDEODEV_INFO_VIEW A WHERE (A.ADMINAREA_GB_CODE LIKE concat(44, '%') ) OR A.BITMAP_ID in (4970,3640)GROUP BY A.BITMAP_ID HAVING A.BITMAP_ID in (3640,3638) 因为 bitmap_id 是唯一的，导致 group by 之后，每个 bitmap 分组只有一条数据，最后执行 count 对每个 bitmap_id 分组计数，当然返回了一堆 1 了。这个 group by + having 过滤太骚了。这时候找了源码来看 123456789101112131415161718192021222324252627282930&lt;select id="getCameraViewByOrgCodes" resultMap="DeviceResultMap"&gt; SELECT A.* FROM VIDEODEV_INFO_VIEW A &lt;where&gt; &lt;if test="@com.xxxx.common.util.Ognl@isNotEmpty(deviceName)"&gt; ( A.DEVICE_NAME LIKE concat('%', #&#123;deviceName, jdbcType=VARCHAR&#125;, '%') OR A.VIDEODEV_GB_ID LIKE concat('%', #&#123;deviceName, jdbcType=VARCHAR&#125;, '%') ) &lt;/if&gt; &lt;if test="@com.xxxx.common.util.Ognl@isNotEmpty(listAdminAreaGbCode)"&gt; &lt;foreach item="item" index="index" collection="listAdminAreaGbCode" open="AND (" separator="OR" close=")"&gt; A.ADMINAREA_GB_CODE LIKE concat(#&#123;item&#125;, '%') &lt;/foreach&gt; &lt;/if&gt; &lt;if test="@com.xxxx.common.util.Ognl@isNotEmpty(deviceParam)"&gt; // 1 OR A.BITMAP_ID in $&#123;deviceParam&#125; &lt;/if&gt; &lt;if test="@com.xxxx.common.util.Ognl@isNotEmpty(userDeviceParam)"&gt; // 2 GROUP BY A.BITMAP_ID HAVING A.BITMAP_ID in $&#123;userDeviceParam&#125; &lt;/if&gt; &lt;if test="@com.xxxx.common.util.Ognl@isNotDefault(deviceSpeicaltypeDict)"&gt; AND A.DEVICE_SPEICALTYPE_DICT = #&#123;deviceSpeicaltypeDict,jdbcType=INTEGER&#125; &lt;/if&gt; &lt;/where&gt; ORDER BY A.ID DESC&lt;/select&gt; 注意 1）和 2）都是过滤 bitmap_id 字段，但是传入列表不一样。2）的操作结果，约等于使用指定 bitmap 列表做了过滤。问了同事为什么在 2）不用 and 过滤，一定要 group by + having，没有回答清楚，说之前有 bug。 本质问题是，业务含义（不展开了）理解和 sql 操作符号优先级。or优先级比 and 低，如果 2）直接用 and，那么and 先执行，再执行上面的 or，导致比目标结果集大（之前版本的 bug）。 为什么 group by 的结果就对呢？因为 group by 优先级比 or 低，后面再执行，实现了“过滤”。至此，定位慢 sql 的原因了。解决问题 去掉 group by + having，改用and 适配优先级，上面几个条件用 () 括住。涉及到 mybatis if 标签嵌套 更优化的改动是，根据两处 bitmap_id 的条件，分别拆开 sql，但是导致应用复杂一些 1234SELECT count(0) FROM VIDEODEV_INFO_VIEW A WHERE ( (A.ADMINAREA_GB_CODE LIKE concat(44, '%') ) OR A.BITMAP_ID in (4970,364) // 这些条件被一对括号括住，保证优先级)AND A.BITMAP_ID in (4970,364) 这样修改后，原有数据量，count 在 0.5s - 0.6s 返回（因为有个 like 匹配，快不了）。小结 这次 sql 优化经历有点粗心，中间浪费了不少时间。认证对比 sql直接看源文件 都可以避免掉。]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>性能优化</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 消费者停机重启后重新消费 offset 的 case]]></title>
    <url>%2Fp%2Fkafka-consumers-reconume-offsets-after-long-time-shutdown%2F</url>
    <content type="text"><![CDATA[背景 线上一个项目使用了 kafka。因为要做维护，停机了一天多 ，再次重启应用（kafka 消费者），发现并没有从昨天提交的 offset 消费（对比应用日志文件），出现重复消费。 这个问题主要是同事排查，收获比较大，于是记录下来。排查过程 显然是消费者 commit offset 没了。但是为什么没呢？另外，以前停机维护时间不超过一天，并没有发生这个问题。kafka 的 consumer offset 保存在 __consumer_offsets 这个 topic（since kafka 0.9）。这里有一篇重要的官方文章讲述相关 offset retention 问题：Adjust default values of log.retention.hours and offsets.retention.minutes。为什么要设置 offsets.retention.minutes? kafka 使用__consumer_offsets 保存消费者 offset。注意这是个 topic，每次消费者 commit offsets 会写日志落盘。如果不清理日志，就会一直消耗磁盘直至撑爆。因此提供了选项来控制 retention 时间。另外，如果一个消费者只出现一下，然后消失很长时间，也需要有机制清理掉日志。消费者不下线的话，就会一直保持最近提交的消费吗？不会。如果消费者在线但是不消费消息、不提交 offset，那么受 offsets.retention.minutes 影响，超过时间 offset 会被重置；与消费者是否在线无关。log.retention.hours和 offsets.retention.minutes 的关系？log.retention.hours日志消息保留时间。offsets.retention.minutes是消费者 offset 保留时间。如果 log.retention.hours &lt; offsets.retention.minutes，那么可能会出现消费者 offset 被清理、重复消费旧的日志。 因此，offsets.retention.minutes &gt; log.retention.hours。kafka 2.0 以前，这两者的默认值是有问题的 1default values of log.retention.hours (168 hours = 7 days) and offsets.retention.minutes (1440 minutes = 1 day) 官网提到因为这 2 个默认组合的常见问题（对应开篇的问题）：12345678We have observed the following scenario and issue:- Producing of data to a topic was disabled two days ago by producer update, topic wasn&apos;t deleted.- Consumer consumed all data and properly committed offsets to Kafka.- Consumer made no more offset commits for that topic because there was no more incoming data and there was nothing to confirm. (We have auto-commit disabled, I&apos;m not sure how behaves enabled auto-commit.)- After one day: Kafka cleared too old offsets according to offsets.retention.minutes.- After two days: Long-term running consumer was restarted after update, it didn&apos;t find any committed offsets for that topic since they were deleted by offsets.retention.minutes so it started consuming from the beginning.- The messages were still in Kafka due to larger log.retention.hours, about 5 days of messages were read again.解决问题 确认现场没有配置 offsets.retention.minutes，根据当前版本（0.9），默认值是 1440 分钟，即 1 天。 而停机维护时间恰好大于 1 天，导致消费者 offset 丢失。修复：server.properties增加 offsets.retention.minutes，配置为 10 天。 重启 kafka server。扩展话题 __consumer_offsetskafka 0.9 之前，消费者 offset 是保存在 zookeeper，但是 zooKeeper 并不适合大批量的频繁写入操作，会引发写入性能问题（因为 zk 的模型，写由一个 master 负责，并且同步到各个 slave。offset 提交性能受限于 master 的写入能力）。 因此 0.9 开始，默认把 offset 保存在 kafka 内部 topic：__consumer_offsets。频繁提交 offsets 可能引发的问题 消费者提交的 offsets 作为一条日志写入到 __consumer_offsets。 系统默认每 60s 为 consumer 提交一次 offset commit 请求（由auto.commit.interval.ms, auto.commit.enable 两个参数决定）。如果每次消费一条消息就手动提交一次 offsets，那么会产生大量的日志，迅速消耗磁盘空间。解决：使用异步提交，由 offset manager 管理 或者，同步提交，设置批量提交的最小处理消息数量 设置日志的清理策略]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis pagehelper 插件自定义 count sql]]></title>
    <url>%2Fp%2Fmybatis-pagehelper-customized-count-sql%2F</url>
    <content type="text"><![CDATA[背景 一个分页接口比较慢，需要返回当前页面数据和 count 总数。系统中使用了 pagehelper 插件，这 2 个查询是串行执行，理想情况下是 50:50 的时间消耗。于是想看下改成并发查询的效果。pagehelper 分析 pagehelper 新增支持自定义 count sqlpagehelper 默认 count sql，是在原来 sql 的基础上，套上 count（具体看下面的源码分析）12select count(0) from(&lt; 原来的sql&gt;) tmp_count 这样的问题是，内部 sql select 了一堆列，对于 count 来说是多余的。pagehelper 5.0.4 增加自定义 count sql 支持：增加 countSuffix count 查询后缀配置参数，该参数是针对 PageInterceptor 配置的，默认值为 _COUNT。来自官网的例子：123456789&lt;select id="selectLeftjoin" resultType="com.github.pagehelper.model.User"&gt; select a.id,b.name,a.py from user a left join user b on a.id = b.id order by a.id&lt;/select&gt;&lt;select id="selectLeftjoin_COUNT" resultType="Long"&gt; select count(distinct a.id) from user a left join user b on a.id = b.id&lt;/select&gt;PageInterceptor count 分析 pagehelper 的分页入口在 PageInterceptor：1234567891011121314151617181920212223public Object intercept(Invocation invocation) throws Throwable &#123; try &#123; // more code if (!this.dialect.skip(ms, parameter, rowBounds)) &#123; if (this.dialect.beforeCount(ms, parameter, rowBounds)) &#123; // 计算总数 Long count = this.count(executor, ms, parameter, rowBounds, resultHandler, boundSql); if (!this.dialect.afterCount(count, parameter, rowBounds)) &#123; Object var12 = this.dialect.afterPage(new ArrayList(), parameter, rowBounds); return var12; &#125; &#125; resultList = ExecutorUtil.pageQuery(this.dialect, executor, ms, parameter, rowBounds, resultHandler, boundSql, cacheKey); &#125; else &#123; resultList = executor.query(ms, parameter, rowBounds, resultHandler, cacheKey, boundSql); &#125; // more code &#125; finally &#123; this.dialect.afterAll(); &#125;&#125;123456789101112131415161718private Long count(Executor executor, MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; // 优先定制化 count sql String countMsId = ms.getId() + this.countSuffix; MappedStatement countMs = ExecutorUtil.getExistedMappedStatement(ms.getConfiguration(), countMsId); Long count; if (countMs != null) &#123; count = ExecutorUtil.executeManualCount(executor, countMs, parameter, boundSql, resultHandler); &#125; else &#123; countMs = (MappedStatement)this.msCountMap.get(countMsId); if (countMs == null) &#123; countMs = MSUtils.newCountMappedStatement(ms, countMsId); this.msCountMap.put(countMsId, countMs); &#125; // 自动拼装 sql count = ExecutorUtil.executeAutoCount(this.dialect, executor, countMs, parameter, boundSql, rowBounds, resultHandler); &#125; return count;&#125; 自动拼装 count sql 的工具类在 ExecutorUtil：1234567891011121314151617public static Long executeAutoCount(Dialect dialect, Executor executor, MappedStatement countMs, Object parameter, BoundSql boundSql, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; Map&lt;String, Object&gt; additionalParameters = getAdditionalParameter(boundSql); CacheKey countKey = executor.createCacheKey(countMs, parameter, RowBounds.DEFAULT, boundSql); // 依赖不同数据库方言 String countSql = dialect.getCountSql(countMs, boundSql, parameter, rowBounds, countKey); BoundSql countBoundSql = new BoundSql(countMs.getConfiguration(), countSql, boundSql.getParameterMappings(), parameter); Iterator var11 = additionalParameters.keySet().iterator(); while(var11.hasNext()) &#123; String key = (String)var11.next(); countBoundSql.setAdditionalParameter(key, additionalParameters.get(key)); &#125; Object countResultList = executor.query(countMs, parameter, RowBounds.DEFAULT, resultHandler, countKey, countBoundSql); Long count = (Long)((List)countResultList).get(0); return count;&#125; 针对大部分数据库的实现 CountSqlParser：12345678910111213public String getSimpleCountSql(String sql) &#123; return this.getSimpleCountSql(sql, "0");&#125;public String getSimpleCountSql(String sql, String name) &#123; StringBuilder stringBuilder = new StringBuilder(sql.length() + 40); stringBuilder.append("select count("); stringBuilder.append(name); stringBuilder.append(") from ("); stringBuilder.append(sql); stringBuilder.append(") tmp_count"); return stringBuilder.toString();&#125;当主 sql 查询大量的列，那么会影响 count sql 性能。解决问题 方案 1：提供 _COUNT sql，由 pagehelper 自动触发 二次开发 paghelper，在 intercept() 实现并发查询 方案 2：提供 _COUNT sql 手写分页 sql在慢接口（2 个）手动包装 FutureTask 查询 count 和分页内容。显然方案 1 侵入性更少，只修改一个地方，但是对全局有影响。先测试性能，发现性能提升大概 20%-30% 左右。耗时大的 sql 是底层数据库执行 count，并行查询优化有限。综合考虑，使用方案 2，尽量减少二次开发的维护成本。回到这个问题本身，现在考虑使用 elasticsearch 对查询字段落盘，查询 count 单独走 ES 返回。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[低版本 jdk 不能正常识别 pod 内存限制的 case]]></title>
    <url>%2Fp%2Fkubernetes-java-pod-memory-case%2F</url>
    <content type="text"><![CDATA[背景 把 cas server 4.x （运行在 tomcat 中）塞到 k8s 运行，经常被吐槽消耗大量内存。排查过程 1kubectl exec -it -n prophet cas-tomcat-deployment-xxx-xxx bash 启动 jmap 失败，根本没有 jmap 命令😥。问了容器组，当时为了精简镜像，把 jdk 工具都干掉了。。。r u kidding me。。。于是从物理机 kubectl cp 一份 jdk 1.8.0_242 进去（注意版本）。1234bash-4.3# ./jmap 8Attaching to process ID 8, please wait...Error attaching to process: sun.jvm.hotspot.runtime.VMVersionMismatchException: Supported versions are 25.242-b08. Target VM is 25.60-b23sun.jvm.hotspot.debugger.DebuggerException: sun.jvm.hotspot.runtime.VMVersionMismatchException: Supported versions are 25.242-b08. Target VM is 25.60-b23这里有 2 个要点：jdk instrument 工具有版本兼容性，包括 minor 版本（以前很少留意这一点）容器使用的 java 版本是 1.8.0_60 （划重点）于是又从物理机 cp 一份 jdk 1.8.0_60 到容器。了解到 tomcat 启动时候并没有设置 jvm 内存参数 (catalina.sh)。使用jinfo 看下默认 jvm 申请内存大小 12VM Flags:Non-default VM flags: -XX:CICompilerCount=15 -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=32210157568 -XX:MaxNewSize=10736369664 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=715653120 -XX:OldSize=1431830528 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseFastUnorderedTimeStamps -XX:+UseParallelGC 初始化 heap 就申请了 2G，最大 heap 是 32G，肯定有问题。（期间容器又被重启过）gc 之前（使用 jmap -heap &lt;pid&gt;）12345678910111213141516171819202122Heap Usage:PS Young GenerationEden Space: capacity = 6462373888 (6163.0MB) used = 1415239592 (1349.6776504516602MB) free = 5047134296 (4813.32234954834MB) 21.89968603685965% usedFrom Space: capacity = 71303168 (68.0MB) used = 0 (0.0MB) free = 71303168 (68.0MB) 0.0% usedTo Space: capacity = 99614720 (95.0MB) used = 0 (0.0MB) free = 99614720 (95.0MB) 0.0% usedPS Old Generation capacity = 1700265984 (1621.5MB) used = 118246632 (112.7687759399414MB) free = 1582019352 (1508.7312240600586MB) 6.954596111004712% usedgc 之后(通过jmap -histo:live 触发)：12345678910111213141516171819202122Heap Usage:PS Young GenerationEden Space: capacity = 7146569728 (6815.5MB) used = 75274344 (71.7872085571289MB) free = 7071295384 (6743.712791442871MB) 1.0532933542238854% usedFrom Space: capacity = 2097152 (2.0MB) used = 0 (0.0MB) free = 2097152 (2.0MB) 0.0% usedTo Space: capacity = 102760448 (98.0MB) used = 0 (0.0MB) free = 102760448 (98.0MB) 0.0% usedPS Old Generation capacity = 2156396544 (2056.5MB) used = 82477096 (78.65628814697266MB) free = 2073919448 (1977.8437118530273MB) 3.8247648017005913% used其实内存使用率相当的低。ps. 想研究 heap 内容，可以这么 dump，再从容器中拷贝到物理机。1jmap -dump:live,format=b,file=cas_mem_8G.dumpk8s resources 配置 再看下对应 deployment 中的资源设置。1234567resources: limits: cpu: 1000m memory: 10000Mi requests: cpu: 10m memory: 100MiRequests: 容器使用的最小资源需求，作为容器调度时资源分配的判断依赖。只有当节点上可分配资源量 &gt;= 容器资源请求数时才允许将容器调度到该节点。但 Request 参数不限制容器的最大可使用资源。Limits: 容器能使用资源的资源的最大值，设置为 0 表示使用资源无上限。Request 能够保证 Pod 有足够的资源来运行，而 Limit 则是防止某个 Pod 无限制地使用资源，导致其他 Pod 崩溃。两者之间必须满足关系: 0&lt;=Request&lt;=Limit&lt;=Infinity (如果 Limit 为 0 表示不对资源进行限制，这时可以小于 Request)显然，pod 中的 java 并没有正确识别 resources.limits.memory，直接读取了物理机的内存大小。 问题回顾 前面的整理步骤有点凌乱。k8s deployment 的资源限制不合理 java 启动命令行没有做内存限制，并且 java 版本过低，对于 cgroups 隔离支持有问题。导致直接使用整个物理机内存作为计算申请内存的基础（应该使用 cgroups 限制的内存作为计算依据）。 之前整理过资料，但是都忘记了：java 支持 docker 的资源限制 解决 重新设置 k8s yaml 的 resources limits，改为 2G。原来 java 底包是 jdk_1.8.0_60，不能正确识别 cgroups 配置。现在升级为 jdk_1.8.0_221。增加 xmx 配置。因为默认使用 1/4 可见内存。检查其他容器是否有相同类型问题。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解除静态网站的关注公众号弹窗]]></title>
    <url>%2Fp%2Fjavascript-hack-web-unlock-code%2F</url>
    <content type="text"><![CDATA[搜索资料，导航到某些网站，“阅读更多”提示要“关注公众号输入验证码”。 这种引流方式挺烦人的。关注后还不是要取消。 想了想，这些静态模板网站，不太可能做动态验证码。应该是静态验证码。chrome 中打开调试面板，关注 XHR 请求，随便输入一堆字符提交，提示验证码错误，且没有发送网络请求。 嗯，验证码基本就是在页面里面，或者引用的 js 里面。 打开 element tab，找到弹窗，记下验证码文本框的 id(“unlockCode”)。 在页面、引用的 js 中查找“unlockCode”，终于找到了。 输入验证码后，保存了 cookie，避免再出现弹窗。 搞定！]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch match vs term, filter vs query]]></title>
    <url>%2Fp%2Felasticsearch-match-vs-term-and-filter-vs-query%2F</url>
    <content type="text"><![CDATA[elasticsearch match 和 term、 filter 和 query 笔记。match vs termmatchmatch 是全文检索。传入的文本，先被分析（analyzed），再匹配索引。termterm 是精确匹配。term query 不会对传入的文本分析（analyzed）。不要在 text 类型上使用 term query。因为 text 类型落盘的时候会被分析 (analyzed)，导致 term query 很难匹配上。ps. 标准分析器（standard analyzer）对 text 类型的分析步骤：Removes most punctuation (标点)Divides the remaining content into individual words, called tokens （分词）Lowercases the tokensquery vs filterqueryquery 会对检索文档进行相关性打分(涉及 TF/IDF)，结果保存在_score 字段，单精度浮点数。并且排序。query 结果不会缓存。filterfilter 用于过滤文档（是 / 否 满足过滤条件），不会对文档打分。通常比 query 快。filter 结果可以使用缓存。filter cache 是节点层面的缓存设置，每个节点上所有数据在响应请求时，是共用一个缓存空间的。当空间用满，按照 LRU 策略淘汰掉最冷的数据。ES 2.0 以后把 filter 和 query 合并。constant_score当我们不关心检索词频率 TF（Term Frequency）对搜索结果排序的影响时，可以使用 constant_score 将查询语句 query 或者过滤语句 filter 包装起来。1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot;&#125; &#125;, &quot;boost&quot; : 1.2 &#125; &#125;&#125;小结 text 类型使用 match 全文检索。query 是要相关性评分的，filter 不要；query 结果无法缓存，filter 可以。 全文搜索、评分排序，使用 query；是非过滤，精确匹配，使用 filter。参考query-dsl-term-queryqueries_and_filters]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java validation groups 分组校验]]></title>
    <url>%2Fp%2Fjava-validator-with-groups%2F</url>
    <content type="text"><![CDATA[背景 使用 java validation 框架，其中一个 DTO 对象的字段，在不同场景下校验规则不一样。例如，比如 update 的时候 id 字段不能为空，而插入的时候不对 id 校验。validation 框架的 groups 属性 validation 框架支持 groups 字段，解决这类问题。 新建接口类，用于 validator 分组。123public interface Insert &#123;&#125;public interface Update &#123;&#125;对 validator 配置 groups 属性 12345678@Datapublic class CameraDTO &#123; @NotBlank(groups=&#123;Update.class&#125;) private String id; // more codes&#125; 应用层使用 @Validated 和对应的 groups。spring 应用 1@Validated(Update.class) CameraDTO cameraDTO 普通 java 应用 12345678@Testpublic void test() &#123; Validator validator = Validation .buildDefaultValidatorFactory().getValidator(); CameraDTO cameraDTO = new CameraDTO(); Set&lt;ConstraintViolation&lt;CameraDTO&gt;&gt; constraintViolations = validator.validate(cameraDTO, Update.class); Assertions.assertEquals(1, constraintViolations.size());&#125;@Valid 和 @Validated 这 2 个容易混淆。@Valid是 java validation 标准。1234567package javax.validation;@Target(&#123;ElementType.METHOD, ElementType.FIELD, ElementType.CONSTRUCTOR, ElementType.PARAMETER, ElementType.TYPE_USE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Valid &#123;&#125;@Validated是 spring validation 框架。12345678package org.springframework.validation.annotation;@Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Validated &#123; Class&lt;?&gt;[] value() default &#123;&#125;;&#125;对比两者的 @Target 注解属性，@Valid应用范围更广。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 arthas 做性能优化，case 2]]></title>
    <url>%2Fp%2Farthas-debug-case-p2%2F</url>
    <content type="text"><![CDATA[最近指导年轻同事做性能优化的经过。retainAll 对集合求交集优化 最先处理的是一个 java 方法调用很慢。具体参见：java retainAll 求交集 批量操作优化 定位入口方法 redis hash 批量读读取 从耗时大头的 getDevicesBitMapIdByOrgCode 入手 里面的大头是 getDevicesByVirtualOrgCode 里面的大头是 getDeviceByGroupOrgCodeWithString。这个方法被循环调用次数非常多。 整个操作里面，访问 redis 是耗时操作。解决思路是使用批量读取。redis 本来就支持一次读取 hash 的多个 field：12345678/** * Get values for given &#123;@code hashKeys&#125; from hash at &#123;@code key&#125;. * * @param key must not be &#123;@literal null&#125;. * @param hashKeys must not be &#123;@literal null&#125;. * @return &#123;@literal null&#125; when used in pipeline / transaction. */List&lt;HV&gt; multiGet(H key, Collection&lt;HK&gt; hashKeys);一顿修改之后，访问 redis 耗时降低到 1ms 左右。但是接口整体还是很慢。多次全表扫描 继续调查，发现了坑爹的事情。这是从数据库捞取所有行政区划，再组装为树。问题是，这个接口被反复调用。每次都 full table scan，多次捞取全量数据，不慢才怪。其实外层一次操作之中，这个树结构应该看成固定不变的。优化：只计算一次树结构 新增接口，接受 1)的树作为参数，避免反复读取数据库计算 使用 2)的接口，重构原有调用 这下耗时进一步缩减，非常可观（当时忘了截图）。增加必要的索引 捞取一批行政区划，还是很慢，最后发现是漏掉索引。潜在的并发优化 之前的图发现 getUserInfo() 有点慢，有时候要 300ms。这个接口要串行读取 3 次 db。继续 trace（不贴图了），发现 3 个请求耗时比较相近。另外这个接口流量很少（因此做并发优化对 db 影响很少）。因此可以考虑做个并发优化，同时发起这 3 个 db 请求，再汇总结果。去掉频繁访问 List.removeAll()这个接口功能很简单，把一堆节点构建成一棵树，结果用 8000 个节点测试就产生严重的性能问题。 代码简单。先找到并且标记一级节点。然后再遍历找到各级节点。因为传入的列表可能很大，原来的作者做了一个“优化”：找到一颗子树之后，把这些节点从原来列表删除，减少下次遍历的数量。看起来合理，但是 List 类型要删除一个节点，先要遍历检查，标记，然后新建数组复制目标结果（见 batchRemove 的实现）。优化：去掉删除 removeAll()调用 效果：相同数据量，原来 30s+ 返回，现在 0.5s 返回 小结 到此接口整体性能在 500ms~1000ms，对于管理系统来说，已经够用了。优化过程：trace 定位耗时操作 求交集，把 List 改成 Set 数据结构再操作 redis hash 支持批量读取 field。（但是要控制一次读取 field 数量）。 对于一次调用不变的数据，可以在外围计算一次，并且作为参数，给其他接口使用（避免反复计算产生 IO、cpu 消耗）。建立必要的索引。并发优化访问数据库（未实施）减少操作的数据量 背景：在页面上修改角色和权限的映射，保存的时候很慢。先 trace 入口方法：最后发现是批量插入很慢：对于 mysql 批量写入操作很慢，条件反射是一条一条数据操作，即 12for x in batch_data: mysql.insert(x); 但确认是真正的 batch insert：12insert into table_name (c1, c2, .., cn) values(x1), (x2), .., (xn);于是查了一下数据量 这个角色管关联 8000 多设备权限。按照 mysql 单节点 8K~12K insert tps 来算，这个时间是正常的。但问题是，如果我只增加、删除一个设备权限，还是要这么慢，还是操作了这么多数据。这就有问题了。打开源码一看：后端直接接受前端传来的全量权限映射，哪怕修改一个设备权限，都要全量操作数据库。这是设计上不合理：前后端应该只关注本次操作权限的差集部分 后端对差集部分进行落盘 但是 API 接口改动涉及前后端，这次可以只修改后端实现。考虑到现场操作场景，没有并发修改。因此可以先 load 数据，找到差集，再修改。这样真正落盘的数据就会减少很多。（其实这个走了弯路。一开始直接看源码都可以解决问题）优化结果：一个角色第一次大量关联设备权限还是会很慢。（通过告诉交付人员分批操作解决）后续关联操作就很快。递归改非递归 背景：目录树返回，需要 70+s。目录项有 7w 多。采用递归方式构建目录树。 这个功能原本设计是给几百、最多几千个目录项使用，最初使用递归方式构建树。但是实际上现场已经有几万数据。从 trace 图可以看到，递归调用了 140+W 次，非常缓慢。解决方法是，采用非递归方式构建，思路是先构建顶层树，然后为每个节点增加 id 索引、并且添加到 map，通过 map 查找对应节点，避免递归查找。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public List&lt;SceneNode&gt; createTree(List&lt;Scene&gt; list) &#123; List&lt;SceneNode&gt; sceneNodeList = new ArrayList&lt;&gt;(); // 根据 parent scene id 分组 // key: parent scene id // value: scene // 注意：groupingBy 方法不支持 null key Map&lt;String, List&lt;Scene&gt;&gt; sceneGroupByMap= list.stream().collect(Collectors.groupingBy(Scene::getParentSceneId)); if(sceneGroupByMap==null || sceneGroupByMap.isEmpty())&#123; return sceneNodeList; &#125; // 辅助索引，快速找到 // key: scene id // val: scene node Map&lt;String, SceneNode&gt; indexMap=new HashMap&lt;&gt;(1024); // 顶级节点 if(sceneGroupByMap.get("")!=null)&#123; sceneGroupByMap.get("").forEach(scene-&gt;&#123; SceneNode t=new SceneNode(scene, 0); sceneNodeList.add(t); indexMap.put(scene.getSceneId(), t); &#125;); sceneGroupByMap.remove(""); &#125; // 各级节点 List&lt;String&gt; cleanList=new ArrayList&lt;&gt;(); while(!sceneGroupByMap.isEmpty())&#123; sceneGroupByMap.entrySet().stream().forEach(item-&gt;&#123; if(indexMap.containsKey(item.getKey()))&#123; SceneNode parentNode= indexMap.get(item.getKey()); for(Scene childScene: item.getValue())&#123; SceneNode child= parentNode.setChild(childScene); indexMap.put(child.getId()+"", child); &#125; cleanList.add(item.getKey()); &#125; &#125;); cleanList.forEach(i-&gt; sceneGroupByMap.remove(i)); cleanList.clear(); &#125; return sceneNodeList;&#125; 效果：7w 数据，后端优化后耗时 2s。勉强能接受。但是前端 dom 解析撑不住了。后续：根据现场反馈，是数据会增长到百万级别😥。只能重新设计：根据父节点，返回直接子节点，分层展示。]]></content>
      <categories>
        <category>arthas</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>arthas</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java retainAll 求交集]]></title>
    <url>%2Fp%2Fjava-retainAll-interset%2F</url>
    <content type="text"><![CDATA[背景 现场出现性能问题。用 arthas trace 发现两个列表求交集慢成狗： 优化过程 guava Sets.intersetion 话说第一次见到用 List 求交集，以前都是直接操作 Set，自己遍历一遍。retainAll 是 Collection 接口提供的方法，可以求交集（retain：保留）。集合操作，先转换为 Set 再 Sets.intersetion(s1, s2) 操作。整体增加的成本是创建并用原来 List 的数据初始化 HashSet。这个问题就这么解决了😁。但是感兴趣的是求集合的实现和对比。List vs Set retainAllCollection 接口定义了 retainAll：12// @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the callboolean retainAll(Collection&lt;?&gt; c)因为 Set 也是 Collection 接口一个实现。对比下两者 retainAll 性能，看下有无惊喜。123456789101112131415161718192021222324252627282930313233343536@Testpublic void test2() &#123; Random random = new Random(); Set&lt;String&gt; s1 = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; s1.add(random.nextInt(1000000) + ""); Set&lt;String&gt; s2 = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 1000000; i++) &#123; s2.add(random.nextInt(1000000) + ""); for (int i = 0; i &lt; 1000; i++) &#123; String x = random.nextInt(1000000) + ""; s1.add(x); s2.add(x); List&lt;String&gt; a1 = s1.stream().collect(Collectors.toList()); List&lt;String&gt; a2 = s1.stream().collect(Collectors.toList()) long start = System.currentTimeMillis(); a1.retainAll(a2); long end = System.currentTimeMillis(); System.out.println("list retainAll, cost= " + (end - start) + " ms") a1 = s1.stream().collect(Collectors.toList()); a2 = s1.stream().collect(Collectors.toList()); start = System.currentTimeMillis(); ((AbstractCollection) a1).retainAll(a2); end = System.currentTimeMillis(); System.out.println("abstract list retainAll, cost= " + (end - start) + " ms") start = System.currentTimeMillis(); s1.retainAll(s2); end = System.currentTimeMillis(); System.out.println("set retainAll, cost= " + (end - start) + " ms");&#125;执行结果 123list retainAll, cost= 325 msabstract list retainAll, cost= 263 msset retainAll, cost= 5 ms 小结：java 的 Set.retainAll 也能用。接下来看具体实现。AbstractList 的实现 12345678910111213public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); boolean modified = false; Iterator&lt;E&gt; it = iterator(); while (it.hasNext()) &#123; // 注意不同的底层数据结构，contains、remove 方法成本不一样 if (!c.contains(it.next())) &#123; it.remove(); modified = true; &#125; &#125; return modified;&#125;AbstractList 的实现方式很朴素。直接遍历，找不到相同对象就删掉。 这也是 Set 的对应 retainAll 实现。对于 List 类型，contains 要遍历，O(n)每次 remove 后要重新复制数据。对于 Set 类型，contians 查找比较快（例如 HashSet 实现）删除后调整内部节点数量少。ArrayList 的实现 12345678910111213141516171819202122232425262728293031323334353637public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true);&#125;// complement 可以控制获取交集还是差集// - true: 交集// - false: 差集private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; try &#123; for (; r &lt; size; r++) // contains 要遍历，O(n) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; &#125; finally &#123; // Preserve behavioral compatibility with AbstractCollection, // even if c.contains() throws. if (r != size) &#123; System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; &#125; if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true; &#125; &#125; return modified;&#125; 先遍历数据，找到所有的目标位置，再复制。ArrayList 相对 AbstractCollection 的优化是，避免 remove，减少反复伸缩数据的成本。然而，上面测试被打脸了。AbstractCollection 的实现比 ArrayList 更快。。。数据样本原因？guava Sets 的实现 guava 的Sets.intersection() 返回一个不可修改的 Set 视图。通常把数量少的集合作为第一个参数，数量大的集合作为第二个参数，性能好些。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public final class Sets &#123; /** * Returns an unmodifiable &lt;b&gt;view&lt;/b&gt; of the intersection of two sets. The * returned set contains all elements that are contained by both backing sets. * The iteration order of the returned set matches that of &#123;@code set1&#125;. * * &lt;p&gt;Results are undefined if &#123;@code set1&#125; and &#123;@code set2&#125; are sets based * on different equivalence relations (as &#123;@code HashSet&#125;, &#123;@code TreeSet&#125;, * and the keySet of an &#123;@code IdentityHashMap&#125; all are). * * &lt;p&gt;&lt;b&gt;Note:&lt;/b&gt; The returned view performs slightly better when &#123;@code * set1&#125; is the smaller of the two sets. If you have reason to believe one of * your sets will generally be smaller than the other, pass it first. * Unfortunately, since this method sets the generic type of the returned set * based on the type of the first set passed, this could in rare cases force * you to make a cast, for example: &lt;pre&gt; &#123;@code * * Set&lt;Object&gt; aFewBadObjects = ... * Set&lt;String&gt; manyBadStrings = ... * * // impossible for a non-String to be in the intersection * SuppressWarnings("unchecked") * Set&lt;String&gt; badStrings = (Set) Sets.intersection( * aFewBadObjects, manyBadStrings);&#125;&lt;/pre&gt; * * &lt;p&gt;This is unfortunate, but should come up only very rarely. */ public static &lt;E&gt; SetView&lt;E&gt; intersection(final Set&lt;E&gt; set1, final Set&lt;?&gt; set2) &#123; checkNotNull(set1, "set1"); checkNotNull(set2, "set2"); final Predicate&lt;Object&gt; inSet2 = Predicates.in(set2); return new SetView&lt;E&gt;() &#123; @Override public UnmodifiableIterator&lt;E&gt; iterator() &#123; // 核心 return Iterators.filter(set1.iterator(), inSet2); &#125; @Override public Stream&lt;E&gt; stream() &#123; return set1.stream().filter(inSet2); &#125; @Override public Stream&lt;E&gt; parallelStream() &#123; return set1.parallelStream().filter(inSet2); &#125; @Override public int size() &#123; return Iterators.size(iterator()); &#125; @Override public boolean isEmpty() &#123; return !iterator().hasNext(); &#125; @Override public boolean contains(Object object) &#123; return set1.contains(object) &amp;&amp; set2.contains(object); &#125; @Override public boolean containsAll(Collection&lt;?&gt; collection) &#123; return set1.containsAll(collection) &amp;&amp; set2.containsAll(collection); &#125; &#125;; &#125;&#125;1234567891011121314151617181920212223public final class Iterators &#123; /** * Returns a view of &#123;@code unfiltered&#125; containing all elements that satisfy * the input predicate &#123;@code retainIfTrue&#125;. */ public static &lt;T&gt; UnmodifiableIterator&lt;T&gt; filter( final Iterator&lt;T&gt; unfiltered, final Predicate&lt;? super T&gt; retainIfTrue) &#123; checkNotNull(unfiltered); checkNotNull(retainIfTrue); return new AbstractIterator&lt;T&gt;() &#123; @Override protected T computeNext() &#123; while (unfiltered.hasNext()) &#123; T element = unfiltered.next(); if (retainIfTrue.apply(element)) &#123; return element; &#125; &#125; return endOfData(); &#125; &#125;; &#125;&#125; 小结 求交集，使用 Set 性能比 List 好太多了。通常使用 Java 原生的 Set.retainAll() 即可。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql 性能优化 case 2]]></title>
    <url>%2Fp%2Fsql-performance-tuning-case-p2%2F</url>
    <content type="text"><![CDATA[背景 设备管理页面支持对摄像头、WiFi、电子围栏等设备进行管理。现场投诉打开很慢。不管是几十万、还是几千的接入设备类型，都慢成狗。优化过程 首先观察现状：页面上可以选择不同的设备类型进行操作。不同设备类型的数据不一样，有的几十万，有的几千条。但是即使选择接入数量少的设备类型，接口返回都非常缓慢。打开接口，发现每次查询都从一个视图捞取数据：12select * from vw_deviceswhere device_type = 'xxx' // 设备类型 打开 vw_devices 视图一看，震惊了：1234567select id, 'camera' as device_type, // 省略一堆属性，把个别属性转换 from cameraUNION ALLselect id, 'wifi' as device_type, // 省略一堆属性，把个别属性转换from wifiUNION ALL// 省略多个类型的设备表 这个视图暴露了两个问题：操作一个类型的设备，却硬生生要 UNION ALL 读取其他不相关的设备类型数据。不慢才怪。这个视图是为了上层应用写的方便，把不同类型的设备进行抽象。根本问题是最初设计，前人对设备的抽象不够。摄像头、wifi 的属性肯定不太一样。但是都可以进行抽象，比如 id、设备类型、标准编码等。这些记录在主表。不同类型设备的个性化属性记录在各自的扩展表。其实就是面向对象设计、抽取领域模型。扯远了。要重新设计抽象套回去，牵连太多，只能就着现状修改：原来对不同设备 UNION ALL 的视图不能再使用了。应用层根据不同设备，直接读取不同的设备表。小结 解决这个慢查询问题很简单。但是背后隐藏的是领域理解不深才是大问题😥。]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>性能优化</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql 性能优化 case 1]]></title>
    <url>%2Fp%2Fsql-performance-tuning-case-p1%2F</url>
    <content type="text"><![CDATA[背景 线上一个 sql 发生性能问题，炸锅了。超过 40s 都不能返回。背景介绍：VIDEODEV_INFO_VIEW 是一张视图，转换底层的摄像机表 摄像机使用“乐观锁”设计，包含一个 version 字段，对于 update 操作，不是修改原来记录，而是新增一条记录，且 version 自增。VIDEODEV_INFO_VIEW 的关键字段：ID：自增主键 UUID：每个设备的 UUIDVIDEODEV_GB_ID：国标 idADMINAREA_GB_CODE：行政区划VERSION：当前版本号sql 的语义是：根据权限，拿到对应的设备信息。123456789101112131415161718SELECT A.* FROM VIDEODEV_INFO_VIEW A, (SELECT ID, Max(VERSION ) AS VERSION FROM VIDEODEV_INFO_VIEW GROUP BY ID ) B WHERE ADMINAREA_GB_CODE LIKE CONCAT (44, '%' ) OR A.ID IN ( 8 // 此处省略5000 个id ) AND A.ID = B.ID AND A.VERSION = B.VERSION GROUP BY A.UUID ORDER BY ID DESC LIMIT 20优化经历 step 1 无脑 explain 一下：12345| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------------+------------+-------+---------------------------------------------------------------------------------------------+---------+---------+------+-------+----------+-------------------------------------------------+| 1 | PRIMARY | &lt;derived2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | 29096 | 100.00 | Using temporary; Using filesort || 1 | PRIMARY | RM_VIDEODEV_INFO | NULL | ALL | PRIMARY,idx_RM_VIDEODEV_INFO_ver,index_ADMINAREA_GB_CODE | NULL | NULL | NULL | 29096 | 100.00 | Range checked for each record (index map: 0x13) || 2 | DERIVED | RM_VIDEODEV_INFO | NULL | index | PRIMARY,idx_RM_VIDEODEV_INFO_ver,index_VIDEODEV_GB_ID,idx_bitmap_id,index_ADMINAREA_GB_CODE | PRIMARY | 4 | NULL | 29096 | 100.00 | NULL |29000 条数据，但是 filesort、temp table。看到 IN 后面一堆 id（四五千个），怀疑是随机读太多，IO 吃不消。但是 40s 也不返回，说不过去。于是先把 IN 改成一个 id 试试看，还是慢成狗。step 2算了一下，前面的笛卡尔积，29000 * 29000 = 841,000,000。 尼玛，不跪才怪。12345SELECT A.* FROM VIDEODEV_INFO_VIEW A, (SELECT ID, Max(VERSION ) AS VERSION FROM VIDEODEV_INFO_VIEW GROUP BY ID ) B这里有 2 个疑问：为什么要搞 B 表呢？B 表写的好别扭 针对 1），因为一个设备每个 update 修改版本都产生一条记录，因此要想办法拿到最新版本的那条记录。这是 B 表的出发点。然而，B 表这样写是有问题的 1(SELECT ID, Max(VERSION ) AS VERSION FROM VIDEODEV_INFO_VIEW GROUP BY ID ) BID 是自增主键，肯定是不同的，GROUP BY ID 这个操作完全没有意义！设备的唯一标识不是 ID，而是 VIDEODEV_GB_ID，或者 UUID。这个错误的 sql 已经存在半年以上了。它虽然是错的，但是不影响最终结果😥。正确写法是：1SELECT VIDEODEV_GB_ID, Max(VERSION ) AS VERSION FROM VIDEODEV_INFO_VIEW GROUP BY VIDEODEV_GB_ID另外，外围查询的 group by UUID 也是多余的😀。step 3显然瓶颈在于关联，找到最大版本的记录。那么为什么要搞多个 version 呢？问到原来的架构设计，说是 避免并发插入的时候同时发生关联查询、导致锁表；因此引入 version，，把 update 改写为 insert。于是问了了解现场情况的同事，真实使用情况是，基本就是一个人操作，一次导入几千条数据。以 mysql 的性能，单机 1W 左右的 insert tps 还是可以的。version 机制产生多条记录，可以通过定时器清除，减少数据量。但问题是，定时器不清理之前，会出现一个设备存在多条记录的问题。还是需要分组查询每个设备的最新版本信息。个人觉得，version 机制对于 99% 真实场景太重了。那么，接下来解决问题的思路就有 2 个：整体代码去掉 version 机制，需要修改所有关联模块。兼容现有 version 机制 方案 1方案 1 的好处是一步到位，把过度设计扭回来。方案 1 的困难点在于牵扯了五六个模块，且开放了视图给外部系统使用。方案 1 的改动：应用层重写 修改这个 sql对外开放视图做兼容适配。保留 version 字段，但是永远为 0。因为 VIDEODEV_INFO_VIEW 每个设备只有一个记录，直接过滤就好。12345678910111213SELECT A.*FROM VIDEODEV_INFO_VIEW AWHERE ADMINAREA_GB_CODE LIKE CONCAT (44, '%' ) OR A.ID IN ( 8// 省略 4000 个id ) ORDER BY A.ID DESC LIMIT 20非常快，通常 0.1s 就能返回。方案 2方案 2 不改动现在有问题的设计，只在 sql 上做优化。假设 VIDEODEV_INFO_VIEW 上一个设备有多条记录。但是，不需要每次都找出所有设备的最新版本 ，只关注当前分页的设备即可。这样可以大大减少中间表数据，提升效率。 思路：根据权限过滤 VIDEODEV_GB_ID去重，分页；得到最终的设备 VIDEODEV_GB_ID (可能有多个 version 数据)，这是个小表。再和原来的设备表做 group by，得到最大 version 的行（即 ID）最后从 VIDEODEV_INFO_VIEW 表捞取对应表的记录。123456789101112131415SELECT A.*FROM (SELECT A.VIDEODEV_GB_ID,ba MAX(ID ) AS ID FROM (SELECT DISTINCT VIDEODEV_GB_ID FROM VIDEODEV_INFO_VIEW A WHERE ADMINAREA_GB_CODE LIKE CONCAT (44, '%' ) OR A.ID IN ( 8, 10 // 省略一堆 ID ) ORDER BY VIDEODEV_GB_ID LIMIT 100, 20 ) C, VIDEODEV_INFO_VIEW A // 分页 WHERE C.VIDEODEV_GB_ID = A.VIDEODEV_GB_ID GROUP BY A.VIDEODEV_GB_ID ) D, VIDEODEV_INFO_VIEW A WHERE D.ID = A.ID 针对 29000 条数据，分页通常 0.1s - 0.2s 就能返回。不分页查询，要 5s 才能返回。小结 最终使用方案 1。虽然改动大，但是一步到位修正旧的设计。前人不考虑应用场景，引入 sexy 的过度设计，真是坑死后人。]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>性能优化</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch keyword vs text]]></title>
    <url>%2Fp%2Felasticsearch-keyword-type%2F</url>
    <content type="text"><![CDATA[背景 问题背景：项目中使用 ES 存储用户信息，查询身份证末位包含 X 的，不能正常返回。在 elasticsearch 中，文本类型主要有 string、text、keyword。其中，在 es 2.x 版本只有 string 类型。5.x 以后把 string 字段设置为了过时字段，引入 text，keyword 类型。keyword使用场景 来自官网的介绍（keyword datatype）：A field to index structured content such as IDs, email addresses, hostnames, status codes, zip codes or tags.They are typically used for filtering (Find me all blog posts where status is published), for sorting, and for aggregations. Keyword fields are only searchable by their exact value.If you need to index full text content such as email bodies or product descriptions, it is likely that you should rather use a text field.keyword 查询，是精确值匹配。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 基于 ES 7.x 环境 # 创建索引PUT my_person&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;ID_NUMBER&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;# 插入数据POST /my_person/_doc&#123; &quot;ID_NUMBER&quot;: &quot;61032219001017552X&quot;&#125;# term 是精确匹配，直接从反向索引查询GET my_person/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;ID_NUMBER&quot;: &quot;61032219001017552X&quot; &#125; &#125;&#125;# 留意这里使用了小写 x# match 全文检索，对 item 进行相同处理后再查询倒排表。因此查询不出来。GET my_person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;ID_NUMBER&quot;: &quot;61032219001017552x&quot; &#125; &#125;&#125;# 留意这里使用了大写 X# 转换后的 item 和倒排表一致，可以返回结果。GET my_person/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;ID_NUMBER&quot;: &quot;61032219001017552X&quot; &#125; &#125;&#125;texttext 是全文检索类型，会被 analyzer 解析，转换为 norm 之后再建立索引。 解决问题 因为使用 keyword 类型存储身份证，且写入的时候是大写 X。未配置 normalizer。那么 ES 把完整的身份证创建索引。使用 term query 查询，传入身份证参数为小写的 x。导致匹配不上。解决方法有 2 个：前台接口都增加转换为大写操作；每个接口都修改一遍。keyword 字段，增加 normalizer 属性，对传入查询的关键字进行预处理。扩展： normalizer 属性 翻译自官网 normalizer：normalizer 属性，和 analyzer 类似，但是 normalizer 只会产生单个 token。normalizer 在索引 keyword 之前使用，并且在查询时对输入关键字使用相同的 normalizer 转换，再去检索索引。这个行为对match 或者 term 查询有影响。normalizer 的用途是，输入的关键字和存储的索引，都经过相同的规范化处理。因为 term query 是精确匹配查询索引。因此输入关键字和索引存在差异将不能匹配（比如大小写）。match 是全文检索，也是类似。小结keyword：存储数据时候，不会分词，存储空间相对少些。通常用于过滤、排序、聚合。text：存储数据时候，会自动分词。text 类型的字段不能用于排序, 也很少用于聚合。]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apdex 分数：评估应用性能]]></title>
    <url>%2Fp%2Fapdex-score%2F</url>
    <content type="text"><![CDATA[Prometheus 中遇到了 apdex 分数的例子，记录学习笔记。12345( sum(rate(http_request_duration_seconds_bucket&#123;le=&quot;0.3&quot;&#125;[5m])) by (job)+ sum(rate(http_request_duration_seconds_bucket&#123;le=&quot;1.2&quot;&#125;[5m])) by (job)) / 2 / sum(rate(http_request_duration_seconds_count[5m])) by (job)Apdex 分数是一个评估应用性能的方式。Apdex 定义了应用响应时间的最优门槛为 T，另外根据应用响应时间结合 T 定义了三种不同的性能表现：Satisfied（满意）：应用响应时间低于或等于 T（T 由性能评估人员根据预期性能要求确定）。Tolerating（可容忍）：应用响应时间大于 T，但同时小于或等于 4T。Frustrated（烦躁期）：应用响应时间大于 4T。 计算公式 1Apdex = (Satisfied Count + Tolerating Count / 2) / Total SamplesT 值的选择对于最终的 Apdex 值也会有直接影响，越大的 T 值理论上来说会有更大的 Apdex 得分。Apdex = 1 可以只是一个不断优化的方向，却不一定是要成为优化的目标。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus PromQL 入门]]></title>
    <url>%2Fp%2Fprometheus-promql-intro%2F</url>
    <content type="text"><![CDATA[Prometheus 官网中 PromQL 学习笔记。PromeQL 支持的数据类型 Instant vector - 瞬时向量，相同 timestamp 的时序集合 （a set of time series containing a single sample for each time series, all sharing the same timestamp）Range vector - 范围向量，一个时间范围内不同时序的集合 （a set of time series containing a range of data points over time for each time series）Scalar - 标量，即浮点数String - 字符串Instant vector 选择器 通过向 {} 里附加一组标签来进一步过滤时间序列。例如jvm_memory_used_bytes{id=~&quot;PS.*&quot;}1234= : 选择与提供的字符串完全相同的标签。!= : 选择与提供的字符串不相同的标签。=~ : 选择正则表达式与提供的字符串（或子字符串）相匹配的标签。!~ : 选择正则表达式与提供的字符串（或子字符串）不匹配的标签。Range Vector 选择器 时间范围通过时间范围选择器 [] 进行定义。jvm_memory_used_bytes{id=~&quot;PS.*&quot;} [5m]123456s - secondsm - minutesh - hoursd - daysw - weeksy - years还支持 offset，表示最近过去时间：1jvm_memory_used_bytes&#123;id=~&quot;PS.*&quot;&#125; offset 10m范围向量的表达式不能直接绘图 。 函数和例子 为了得到一堆采样数据，在 Windows 系统本地安装了 wmi_exporter 插件，指标通过 http://localhost:9182/metrics 暴露。prometheus.yml增加配置:123456scrape_configs: - job_name: 'local-windows' scrape_interval: 5s metrics_path: '/metrics' static_configs: - targets: ['127.0.0.1:9182']以网络发送流量做例子（wmi_net_bytes_sent_total{nic=&quot;Intel_R__Wireless_AC_9560_160MHz&quot;}）。increase()increase(v range-vector) 函数获取区间向量中的第一个和最后一个样本并返回其增长量, 它会在单调性发生变化时 (如由于采样目标重启引起的计数器复位) 自动中断。rate()rate(v range-vector) 函数可以直接计算区间向量 v 在时间窗口内平均增长速率 ，它会在单调性发生变化时(如由于采样目标重启引起的计数器复位) 自动中断。该函数的返回结果不带有度量指标，只有标签列表。当时间区间小于等于 scrape 频率，则 rate 函数无法返回 。 例如我的 scrape 频率是 5s 一次，则 1rate(wmi_net_bytes_sent_total&#123;nic=&quot;Intel_R__Wireless_AC_9560_160MHz&quot;&#125; [5s] ) 不返回数据。rate() 函数返回值类型只能用计数器，在长期趋势分析或者告警中推荐使用这个函数。irate()irate(v range-vector) 函数用于计算区间向量的增长率，但是其反应出的是 瞬时增长率 。irate 函数是通过 区间向量中最后两个两本数据 来计算区间向量的增长速率，它会在单调性发生变化时 (如由于采样目标重启引起的计数器复位) 自动中断。rate vs irateirate 只能用于绘制快速变化的计数器，在长期趋势分析或者告警中更推荐使用 rate 函数。Percona 讲述 Prometheus 这 2 个函数问题的文章值得看下：Better Prometheus rate() Function with VictoriaMetrics。源码实现 rate 等函数的实现见functions.go 关于 rate 实现的讨论，有个好 TMD 长的讨论，以后再分析：rate()/increase() extrapolation considered harmful #3746123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132// extrapolatedRate is a utility function for rate/increase/delta.// It calculates the rate (allowing for counter resets if isCounter is true),// extrapolates if the first/last sample is close to the boundary, and returns// the result as either per-second (if isRate is true) or overall.func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper, isCounter bool, isRate bool) Vector &#123; ms := args[0].(*parser.MatrixSelector) vs := ms.VectorSelector.(*parser.VectorSelector) var ( samples = vals[0].(Matrix)[0] rangeStart = enh.ts - durationMilliseconds(ms.Range+vs.Offset) rangeEnd = enh.ts - durationMilliseconds(vs.Offset) ) // No sense in trying to compute a rate without at least two points. Drop // this Vector element. if len(samples.Points) &lt; 2 &#123; return enh.out &#125; var ( counterCorrection float64 lastValue float64 ) for _, sample := range samples.Points &#123; if isCounter &amp;&amp; sample.V &lt; lastValue &#123; counterCorrection += lastValue &#125; lastValue = sample.V &#125; resultValue := lastValue - samples.Points[0].V + counterCorrection // Duration between first/last samples and boundary of range. durationToStart := float64(samples.Points[0].T-rangeStart) / 1000 durationToEnd := float64(rangeEnd-samples.Points[len(samples.Points)-1].T) / 1000 // 区间长度 sampledInterval := float64(samples.Points[len(samples.Points)-1].T-samples.Points[0].T) / 1000 averageDurationBetweenSamples := sampledInterval / float64(len(samples.Points)-1) if isCounter &amp;&amp; resultValue &gt; 0 &amp;&amp; samples.Points[0].V &gt;= 0 &#123; // Counters cannot be negative. If we have any slope at // all (i.e. resultValue went up), we can extrapolate // the zero point of the counter. If the duration to the // zero point is shorter than the durationToStart, we // take the zero point as the start of the series, // thereby avoiding extrapolation to negative counter // values. durationToZero := sampledInterval * (samples.Points[0].V / resultValue) if durationToZero &lt; durationToStart &#123; durationToStart = durationToZero &#125; &#125; // If the first/last samples are close to the boundaries of the range, // extrapolate the result. This is as we expect that another sample // will exist given the spacing between samples we've seen thus far, // with an allowance for noise. extrapolationThreshold := averageDurationBetweenSamples * 1.1 extrapolateToInterval := sampledInterval if durationToStart &lt; extrapolationThreshold &#123; extrapolateToInterval += durationToStart &#125; else &#123; extrapolateToInterval += averageDurationBetweenSamples / 2 &#125; if durationToEnd &lt; extrapolationThreshold &#123; extrapolateToInterval += durationToEnd &#125; else &#123; extrapolateToInterval += averageDurationBetweenSamples / 2 &#125; resultValue = resultValue * (extrapolateToInterval / sampledInterval) // increase 函数不用除以区间时间长度 if isRate &#123; resultValue = resultValue / ms.Range.Seconds() &#125; return append(enh.out, Sample&#123; Point: Point&#123;V: resultValue&#125;, &#125;)&#125;// === rate(node parser.ValueTypeMatrix) Vector ===func funcRate(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector &#123; return extrapolatedRate(vals, args, enh, true, true)&#125;// === increase(node parser.ValueTypeMatrix) Vector ===func funcIncrease(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector &#123; return extrapolatedRate(vals, args, enh, true, false)&#125;// === irate(node parser.ValueTypeMatrix) Vector ===func funcIrate(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector &#123; return instantValue(vals, enh.out, true)&#125;func instantValue(vals []parser.Value, out Vector, isRate bool) Vector &#123; samples := vals[0].(Matrix)[0] // No sense in trying to compute a rate without at least two points. Drop // this Vector element. if len(samples.Points) &lt; 2 &#123; return out &#125; // 使用最近 2 个采样计算 lastSample := samples.Points[len(samples.Points)-1] previousSample := samples.Points[len(samples.Points)-2] var resultValue float64 if isRate &amp;&amp; lastSample.V &lt; previousSample.V &#123; // Counter reset. resultValue = lastSample.V &#125; else &#123; resultValue = lastSample.V - previousSample.V &#125; // 使用最近 2 个采样计算 sampledInterval := lastSample.T - previousSample.T if sampledInterval == 0 &#123; // Avoid dividing by 0. return out &#125; if isRate &#123; // Convert to per-second. resultValue /= float64(sampledInterval) / 1000 &#125; return append(out, Sample&#123; Point: Point&#123;V: resultValue&#125;, &#125;)&#125;histogram 过去 5 分钟的平均响应时间 123 rate(http_request_duration_seconds_sum[5m])/ rate(http_request_duration_seconds_count[5m])apdex 分数，假设 T=300ms12345( sum(rate(http_request_duration_seconds_bucket&#123;le=&quot;0.3&quot;&#125;[5m])) by (job)+ sum(rate(http_request_duration_seconds_bucket&#123;le=&quot;1.2&quot;&#125;[5m])) by (job)) / 2 / sum(rate(http_request_duration_seconds_count[5m])) by (job)histogram 可以聚合计算。1histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) 参考QUERYING PROMETHEUSPromQL 内置函数prometheus 的 summary 和 histogram 指标的简单理解]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome 80 SameSite 问题]]></title>
    <url>%2Fp%2Fchrome-samesite%2F</url>
    <content type="text"><![CDATA[背景 浏览器升级到 chrome 80，结果 CAS 登录无限跳转。经同事指出，是因为 chrome 80 以后默认 SameSite 属性问题导致。cookies 的 SameSite 属性 http 协议是无状态的，于是有了通过附加 cookies 来存储状态信息。服务器响应增加Set-Cookie 头部告诉浏览器在 cookies 保存信息。浏览器再次发送请求，则附加 cookies 字段。1Set-Cookie: is_login=1但是，如果每个请求都附加 cookies，又有新的问题：cookies 消耗上传带宽。上行带宽本来就比较小，cookies 最大可以去到 4KB，每个请求都带上的话带宽非常可观。CSRF 攻击可以获取 cookies，导致安全隐患。为了从源头减少 CSRF 攻击的危害，cookies 增加了 SameSite 属性，控制向哪些站点发送 cookies。None关闭 SameSite 属性。则允许发送 cookies。前提是必须同时设置 Secure 属性（Cookie 只能通过 HTTPS 协议发送），否则无效。1Set-Cookie: is_login=1; SameSite=None; SecureLax对于某些“安全”的方法，允许发送第三方 cookies。比如：get 方法。因为 get 的语义是“只读”链接 预加载，&lt;link rel=&quot;prerender&quot; href=&quot;...&quot;/&gt;如果向其他站点发送 ajax 请求、post 请求，则不会附加 cookies。1Set-Cookie: is_login=1; SameSite=Lax;重点 chrome 80 以后 Lax 是默认值。Strict跨站点无论如何都不发送第三方 cookies。但是可能导致某些站点访问不正常。比如 a.taobao.com 和s.taobao.com被认为 2 个站点的话就不会正常识别登录状态（假设而已）1Set-Cookie: is_login=1; SameSite=Strict;如何判断 same-site 还是 cross-siteSameSite 属性引发了一个问题，如何判断是同站点、跨站点？通常根据二级域名区分。比如 ycwu314.top 和www.ycwu314.top是同站点。但是，gitpage 托管站点，例如 a.github.io 和b.github.io明显是属于不同的站点。实际上，浏览器通常根据 public suffix list 区分站点。修改 chrome SameSite 配置 在 chrome 中打开地址：1chrome://flags/#same-site-by-default-cookies参考 draft-ietf-httpbis-cookie-same-site-00SameSite Cookie 变更：您需要了解的都在这里 当浏览器全面禁用三方 Cookie]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>浏览器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot acturator 2 升级体验]]></title>
    <url>%2Fp%2Fspringboot-acturator-2-intro%2F</url>
    <content type="text"><![CDATA[背景 springboot 从 1.5.x 升级到 2.2.4，发现 acturator 有变化。特此记录。health 详细信息 升级了 acturator 2.x 之后，health 接口的详细信息没了，只有基本信息：123&#123; "status": "UP"&#125;2.x 默认配置不现实详细信息 1management.endpoint.health.show-details=never 修改为 123# when_authorized 或者 alwaysmanagement.endpoint.health.show-details=when_authorized endpoints.health.sensitive=false 这时候可以正常显示详细信息。1234567891011// just for demo@Componentpublic class LuckyHealthIndicator implements HealthIndicator &#123; Random random = new Random(); @Override public Health health() &#123; return Health.up().withDetail("lucky", random.nextBoolean()).build(); &#125;&#125;123456"lucky": &#123; "status": "UP", "details": &#123; "lucky": false &#125;&#125;默认端点 2.x 默认只放开/health 和/info端点。如果要放开所有端点，可以修改 1management.endpoints.web.exposure.include=*metrics 端点1.x 的 metrics 端点会显示各个 metrics 的详细信息。2.x 的 metrics 端点只会返回所有 metrics 名字。如果要获取具体的 metrics 详情，使用/metrics/&lt; 指标名 &gt;。12345678910111213141516171819202122232425262728293031323334&#123; "names": [ "jvm.threads.states", "jvm.memory.used", "jvm.memory.committed", "tomcat.sessions.rejected", "process.cpu.usage", "jvm.memory.max", "jvm.gc.pause", "jvm.classes.loaded", "jvm.classes.unloaded", "tomcat.sessions.active.current", "tomcat.sessions.alive.max", "jvm.gc.live.data.size", "jvm.buffer.count", "jvm.buffer.total.capacity", "tomcat.sessions.active.max", "process.start.time", "http.server.requests", "jvm.gc.memory.promoted", "logback.events", "jvm.gc.max.data.size", "system.cpu.count", "jvm.buffer.memory.used", "tomcat.sessions.created", "jvm.threads.daemon", "system.cpu.usage", "jvm.gc.memory.allocated", "tomcat.sessions.expired", "jvm.threads.live", "jvm.threads.peak", "process.uptime" ]&#125; 获取具体指标值： http://localhost:8080/actuator/metrics/jvm.threads.states123456789101112131415161718192021222324252627282930&#123; "name": "jvm.threads.states", "description": "The current number of threads having WAITING state", "baseUnit": "threads", "measurements": [ &#123; "statistic": "VALUE", "value": 33 &#125; ], "availableTags": [ &#123; "tag": "application", "values": [ "springboot_prometheus" ] &#125;, &#123; "tag": "state", "values": [ "timed-waiting", "new", "runnable", "waiting", "blocked", "terminated" ] &#125; ]&#125;集成 micrometer 框架 2.x 集成 micrometer 框架。自定义 metrics 直接使用 micrometer 的工具类。springboot 也提供了 MeterRegistry 注入。123456789101112@Componentpublic class FreeMemoryGauge &#123; @Autowired private MeterRegistry meterRegistry; @PostConstruct public void init() &#123; Gauge.builder("free.memory.gauge", Runtime.getRuntime()::freeMemory).description("free memory").baseUnit("bytes").register(meterRegistry); &#125;&#125;http://localhost:8080/actuator/metrics/free.memory.gauge123456789101112131415161718&#123; "name": "free.memory.gauge", "description": "free memory", "baseUnit": "bytes", "measurements": [ &#123; "statistic": "VALUE", "value": 274451032 &#125; ], "availableTags": [ &#123; "tag": "application", "values": [ "springboot_prometheus" ] &#125; ] 读写模型 在 1.x 中，Actuator 遵循 R/W 模型，这意味着我们可以从中读取或写入它。2.x 版本现在支持 CRUD 模型，并且映射到 http 方法：@ReadOperation - 它将映射到 HTTP GET@WriteOperation - 它将映射到 HTTP POST@DeleteOperation - 它将映射到 HTTP DELETE]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql ONLY_FULL_GROUP_BY 问题]]></title>
    <url>%2Fp%2Fmysql-ONLY-FULL-GROUP-BY%2F</url>
    <content type="text"><![CDATA[背景 mysql 开发环境执行报错： 这份代码原来能正常执行，切换环境不正常。后来发现是两个环境的 mysql 配置不一样，其中一个增加了 sql_mode 配置。MySQL 5.7.5 和以后版本默认 ONLY_FULL_GROUP_BY 为开启，增加 group by 字段检查。ONLY_FULL_GROUP_BYSQL-92 和以前的标准，如果包含 group by 条件，则 select 列表、having 条件、order by 条件出现的非聚合字段必须在 group by 中。1234SELECT o.custid, c.name, MAX(o.payment) FROM orders AS o, customers AS c WHERE o.custid = c.custid GROUP BY o.custid;payment 字段不出现在 group by 条件，但是执行 max 函数后得到唯一的值。因为 name 不出现在 group by 条件中，会报错：该字段可能有多个值，数据库不知道使用哪个值作为返回。SQL-99 标准放宽了要求。如果能够确认非聚合字段和 group by 条件的关系，则允许执行该 sql。SQL:1999 and later permits such nonaggregates per optional feature T301 if they are functionally dependent on GROUP BY columns: If such a relationship exists between name and custid, the query is legal. This would be the case, for example, were custid a primary key of customers.还是上面的例子，如果 custid 是主键，那么 group by custid 后只有一条记录，那么该行的 name 是唯一确定的。对应于 mysql sql_mode 的 ONLY_FULL_GROUP_BY 配置：关闭，则允许出现非聚合字段，即使跟 group by 条件没有依赖（ functionally dependent）开启，则按照 sql-99 标准检查非聚合字段 如果不想修改全局 sql_mode 配置、只要单独放行个别包含非聚合字段的 group by sql 语句，可以使用 ANY_VALUE() 函数。ANY_VALUE()函数告知 mysql 不要对该字段进行 group by 依赖检查。参考MySQL Handling of GROUP BY]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 8： PauseDetector 以及 LatencyUtils]]></title>
    <url>%2Fp%2Fmicrometer-p8-pausedetector%2F</url>
    <content type="text"><![CDATA[背景 之前在 AbstractTimer 源码第一次接触到 PauseDetector，下面是调研笔记。why latency ? 复制粘贴自 latencyutils 官网，解释为什么需要暂停统计追踪：When a pause occurs during a tracked operation, a single long recorded latency will appear in the recorded values, with no long latencies associated with any pending requests that may be stalled by the pause.When a pause occurs outside of the tracked operation (and outside of the tracked time window) no long latency value would be recorded, even though any requested operation would be stalled by the pause.翻译过来，大概是：当在跟踪操作期间发生暂停时，记录的值中将出现一个长记录的延迟，那么后续挂起的请求不会被关联到长延迟。当在跟踪的操作之外（以及在跟踪的时间窗口之外）发生暂停时，也不会记录长时间的延迟值。有了暂停跟踪，能够对被测量操作获取更加精确的耗时统计。PauseDetector 体系 在 micrometer 中，使用 PauseDetector 包装延迟统计的实现。因为暂停统计会带来性能上的 overhead，所以新版 micrometer 默认实现是 NoPauseDetector。ClockDriftPauseDetector 是另一个实现，考虑到时钟漂移：123public class ClockDriftPauseDetector implements PauseDetector &#123; private final Duration sleepInterval; private final Duration pauseThreshold;嗯，然而并没看出什么神奇地方，只是携带 sleepInterval 和 pauseThreshold 两个参数而已。真正应用 PauseDetector 是在 AbstractTimer 的实现中：12345678910111213141516171819202122232425262728293031 private static Map&lt;PauseDetector, org.LatencyUtils.PauseDetector&gt; pauseDetectorCache = new ConcurrentHashMap&lt;&gt;(); private void initPauseDetector(PauseDetector pauseDetectorType) &#123; pauseDetector = pauseDetectorCache.computeIfAbsent(pauseDetectorType, detector -&gt; &#123; if (detector instanceof ClockDriftPauseDetector) &#123; ClockDriftPauseDetector clockDriftPauseDetector = (ClockDriftPauseDetector) detector; // 底层实现是 SimplePauseDetector， 来自 LatencyUtils 工具包 return new SimplePauseDetector(clockDriftPauseDetector.getSleepInterval().toNanos(), clockDriftPauseDetector.getPauseThreshold().toNanos(), 1, false); &#125; return null; &#125;); if (pauseDetector instanceof SimplePauseDetector) &#123; this.intervalEstimator = new TimeCappedMovingAverageIntervalEstimator(128, 10000000000L, pauseDetector); pauseDetector.addListener((pauseLength, pauseEndTime) -&gt; &#123;// System.out.println("Pause of length" + (pauseLength / 1e6) + "ms, end time" + pauseEndTime); if (intervalEstimator != null) &#123; // 间隔区间 long estimatedInterval = intervalEstimator.getEstimatedInterval(pauseEndTime); long observedLatencyMinbar = pauseLength - estimatedInterval; if (observedLatencyMinbar &gt;= estimatedInterval) &#123; recordValueWithExpectedInterval(observedLatencyMinbar, estimatedInterval); &#125; &#125; &#125;); &#125; &#125;SimplePauseDetectorLatencyUtils 是一个暂停统计追踪开发包，提供了 SimplePauseDetector 等工具实现。SimplePauseDetector 通过一组线程检测是否发生了暂停，以及监听器机制（PauseDetectorListener）通知调用方发生了 pause。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class SimplePauseDetector extends PauseDetector &#123; // consensus time： 共识时间 // 结合 CAS 操作解决多个检测线程的并发安全 final AtomicLong consensusLatestTime = new AtomicLong(); // 检查线程，最多 64 个 private final SimplePauseDetectorThread detectors[]; // 挂起 detector 线程的标记位 private volatile long stallThreadMask = 0; private volatile long stopThreadMask = 0; private class SimplePauseDetectorThread extends Thread &#123; volatile long observedLasUpdateTime; final int threadNumber; final long threadMask; // 省略构造函数 public void run() &#123; long shortestObservedTimeAroundLoop = Long.MAX_VALUE; observedLasUpdateTime = consensusLatestTime.get(); long now = TimeServices.nanoTime(); long prevNow = now; // 初始化共识时间 consensusLatestTime.compareAndSet(observedLasUpdateTime, now); while ((stopThreadMask &amp; threadMask) == 0) &#123; if (sleepInterval != 0) &#123; TimeServices.sleepNanos(sleepInterval); &#125; // 外部调用明确要挂起该检测线程，则自旋等待 // This is ***TEST FUNCTIONALITY***: Spin as long as we are externally asked to stall: while ((stallThreadMask &amp; threadMask) != 0); observedLasUpdateTime = consensusLatestTime.get(); // Volatile store above makes sure new "now" is measured after observedLasUpdateTime sample now = TimeServices.nanoTime(); // Track shortest time around loop: shortestObservedTimeAroundLoop = Math.min(now - prevNow, shortestObservedTimeAroundLoop); // Update consensus time as long as it is is the past: // 这里考虑多线程环境 / 上下文切换等场景导致 observedLasUpdateTime 可能大于 now while (now &gt; observedLasUpdateTime) &#123; if (consensusLatestTime.compareAndSet(observedLasUpdateTime, now)) &#123; // Successfully and atomically moved consensus time forward. Act on the known delta: final long deltaTimeNs = now - observedLasUpdateTime; // Calculate hiccup time (accounting for known time around loop): long hiccupTime = Math.max(deltaTimeNs - shortestObservedTimeAroundLoop, 0); // 超过暂停阈值才通知监听器 if (hiccupTime &gt; pauseNotificationThreshold) &#123; if (verbose) &#123; System.out.println("SimplePauseDetector thread" + threadNumber + ": sending pause notification message: pause of" + hiccupTime + "nsec detected at nanoTime:" + now); &#125; notifyListeners(hiccupTime, now); &#125; &#125; else &#123; // Failed to atomically move consensus time forward. Try again with current value: observedLasUpdateTime = consensusLatestTime.get(); &#125; &#125; prevNow = now; &#125; if (verbose) &#123; System.out.println("SimplePauseDetector thread" + threadNumber + "terminating..."); &#125; &#125; &#125;IntervalEstimatorAbstractTimer 在配置 PauseDetector 时候，还要指定间隔估算器。TODO：MovingAverageIntervalEstimatorTimeCappedMovingAverageIntervalEstimator以后继续深入。参考LatencyUtils: A latency stats tracking package.Your Load Generator Is Probably Lying To You - Take The Red Pill And Find Out Why]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas login 流程笔记]]></title>
    <url>%2Fp%2Fcas-login%2F</url>
    <content type="text"><![CDATA[基于 cas server 4.x 代码以及 CAS 1.0 协议。背景 单点登录（Single Sign On）在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。CAS（Central Authentication Service），中央认证服务，一个开源的 SSO 框架。CAS ticket 相关概念 TGT （Ticket Grangting Ticket）TGT 是 CAS 为用户签发的登录票据，拥有了 TGT，用户就可以证明自己在 CAS 成功登录过。 用户在 CAS 认证成功后，生成一个 TGT 对象，放入自己的缓存，同时生成 TGC cookie，写入浏览器。TGT 对象的 ID 就是 cookie 的值。TGC （Ticket Granting Cookies）存放用户身份认证凭证的 cookie，在浏览器和 CAS Server 间通讯时使用。当 HTTP 再次请求，如果传过来 CAS 生成的 cookie，则 CAS 以此 cookie 值为 key 查询缓存中有无 TGT，如果有的话，则说明用户之前登录过，如果没有，则用户需要重新登录。ST（Service Ticket）ST 是 CAS 为用户签发的访问某一 service 的票据。用户访问 service 时，service 发现用户没有 ST，则要求用户去 CAS 获取 ST。用户向 CAS 发出获取 ST 的请求，如果用户的请求中包含 cookie，则 CAS 会以此 cookie 值为 key 查询缓存中有无 TGT，如果存在 TGT，则用此 TGT 签发一个 ST，返回给用户。用户凭借 ST 去访问 service，service 拿 ST 去 CAS 验证，验证通过后，允许用户访问资源。小结 TGT：在 CAS 登录后，server 端保留的凭证。TGC：登录后的 cas server cookies。根据 TGC 反查 TGT，判断用户是否登录。ST：一次性使用，service 拿 ST 去 CAS 验证。CAS 登录流程 图片来源自官网。TLDR：app 先检查 JSESSIONID 有效性。如果 session 无效，则检查有无 ST。有 ST 则去 CAS server 校验 ST 有效性；没有 ST 则告诉客户端去 CAS server 登录。留意登录流程的 302 响应和 cookies 变化。未登录的用户访问受保护的资源。第一个 302： app 发现用户未验证。app 通过返回 302 响应，把 cas 登录地址、访问 servcie（作为 url 参数）告诉浏览器。浏览器处理 302 响应，重定向到 cas 登录页。用户登录。第二个 302： CAS server 验证用户登录成功，生成了 TGT，并且向客户端写入 TGC cookies。同时返回 302， 告诉浏览器要跳转的（service）地址，并且携带 service ticket。浏览器处理 302 响应，访问目标 app（第 1 步访问的资源）。app 向 CAS server 验证 ST 的有效性。第三个 302： ST 验证成功后，app 为用户创建 session。JSESSIONID 写入 cookies，返回 302 告诉浏览器要跳转的地址。浏览器处理 302 响应，携带 JSESSIONID 访问 app 目标路径。app 检验 session 有效性，返回响应。当浏览器登录 cas 后，第一次访问另一个受保护的 app2：浏览器访问 app2，这时候没有 app2 的 JSESSION。第一个 302： app2 告诉浏览器重定向到 CAS server 登录，以及要访问的 service。因为之前在 CAS server 登录过（假设登录未过期），再次访问 CAS server，携带了 TGC cookies。第二个 302： CAS server 根据 TGC，验证了 TGT 有效，告诉浏览器重定向到目标 service，并且为要访问的 service 颁发 ST。接下来 app2 验证 ST 有效，创建 session，重定向，blablabla。。。留意 cookies 的作用域：TGC cookies 默认有效域是 /cas。app 有效域是/app。app2 有效域是/app2。 不同 app 根据各自的 JESSIONID 判定登录状态。 如果多个 app 的 context-root 相同，则 JSESSIONID 会被相互覆盖！留意访问 cas server 登录的 url，service 写在 url 参数上。一个有效的 TGC cookies，可以在多个 app 创建 session。ajax 和 302 问题 请求分为普通请求（HttpRequest）和 Ajax 请求（XMLHttpRequest）。ajax 请求跨域的时候，默认不会携带 cookie。在做前端后端接入 cas，前端使用 ajax 发送请求，遇到问题：ajax 请求处理不了 302 响应，导致 cas server 返回的重定向跳转无法被正确处理。具体方案以后文章再展开。]]></content>
      <categories>
        <category>cas</category>
      </categories>
      <tags>
        <tag>cas</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas logout 分析]]></title>
    <url>%2Fp%2Fcas-logout%2F</url>
    <content type="text"><![CDATA[基于 cas server 4.x 代码。CAS logout 类型 cas 有 2 种类型的登出： 应用登出。结束单个应用的 session。不会影响其他应用的 session 状态。CAS 登出。结束 cas sso session。缩写为 SLO（Single Logout）。影响所有应用。应用 session 和 cas sso session 的关系如下图（来源自 cas 官网）：SLO 请求类型 SLO 请求分为 2 种，BACK_CHANNEL 和 FRONT_CHANNEL 。定义在LogoutType:12345678910111213141516package org.jasig.cas.services;public enum LogoutType &#123; /** * For no SLO. */ NONE, /** * For back channel SLO. */ BACK_CHANNEL, /** * For front channel SLO. */ FRONT_CHANNEL&#125;CAS SLO 默认是 BACK_CHANNEL 模式。 所谓的 back、 front，是指在 cas server 端 /web 端向各个应用发送 logout 通知。SLO back channelCAS server 向各个接入的 service 发送 post 消息。这个操作是 best-effort but no promise，但在通常情况下表现还好。SLO front channel在 CAS 4.x，SLO front channel 是实验特性，借鉴了 SAML SLO。CAS server 返回 RelayState 和重定向，再由客户端向各个 service 发起 logout 通知。这个 proposal 描述了 front channel 的流程：Proposal: Front-Channel Single Sign-Out，引用里面的时序图描述了 SLO front channel 的流程，一目了然：back channel vs front channelback channel 模式，由 cas server 承载通信压力，通知各个 service logout。与之相反，front channel 模式，需要前端向各个 service 发送 logout 通知。站在前端角度看，back channel 发送更少的网络请求，体验更好。但是 back channel 可能不能正确处理接入负载均衡的 service logout。如果 load balance 跟 client 绑定，那么由 cas server 代替 client 发送向 service 发送 logout 请求，可能不会落到对应 service 所在机器上。如果该 service 的 session 不是存储在共享存储（比如记录在 redis、mysql 等），而只是保存在单实例内存，那么这个 logout 请求不能正确清除对应账号的 session 状态。为了处理这个场景，cas 引入了 front channel 模式，由 client 向各个 service 发送 logout 请求。service 配置 12345678910&#123; "@class" : "org.jasig.cas.services.RegexRegisteredService", "serviceId" : "testId", "name" : "testId", "id" : 1, // SLO 类型， 默认 BACK_CHANNEL "logoutType" : "BACK_CHANNEL", // service 回调接口 "logoutUrl" : "https://web.application.net/logout",&#125;CAS logout 流程分析cas 4.x 使用 spring webflow 配置流程。对应 logout 流程配置在logout-webflow.xml。 入口是 terminateSession。1234&lt;action-state id="terminateSession"&gt; &lt;evaluate expression="terminateSessionAction.terminate(flowRequestContext)" /&gt; &lt;transition to="doLogout" /&gt;&lt;/action-state&gt; 终结 SSO session 要做的事情：清理 TGT清理 sso session 管理的 cookiesslo back channel实现入口在 TerminateSessionAction:123456789101112131415161718192021222324252627public class TerminateSessionAction &#123; /** * Terminates the CAS SSO session by destroying the TGT (if any) and removing cookies related to the SSO session. * * @param context Request context. * * @return "success" */ public Event terminate(final RequestContext context) &#123; // in login's webflow : we can get the value from context as it has already been stored String tgtId = WebUtils.getTicketGrantingTicketId(context); // for logout, we need to get the cookie's value if (tgtId == null) &#123; final HttpServletRequest request = WebUtils.getHttpServletRequest(context); tgtId = this.ticketGrantingTicketCookieGenerator.retrieveCookieValue(request); &#125; // 清理 TGT if (tgtId != null) &#123; WebUtils.putLogoutRequests(context, this.centralAuthenticationService.destroyTicketGrantingTicket(tgtId)); &#125; final HttpServletResponse response = WebUtils.getHttpServletResponse(context); // 清理对应 cookies this.ticketGrantingTicketCookieGenerator.removeCookie(response); this.warnCookieGenerator.removeCookie(response); return this.eventFactorySupport.success(this); &#125;&#125;CentralAuthenticationService 负责清理 TGT。默认实现是CentralAuthenticationServiceImpl:12345678910111213141516// 处理 slo back channelpublic List&lt;LogoutRequest&gt; destroyTicketGrantingTicket(@NotNull final String ticketGrantingTicketId) &#123; try &#123; logger.debug("Removing ticket [&#123;&#125;] from registry...", ticketGrantingTicketId); final TicketGrantingTicket ticket = getTicket(ticketGrantingTicketId, TicketGrantingTicket.class); logger.debug("Ticket found. Processing logout requests and then deleting the ticket..."); // 由 LogoutManager 处理 SLO final List&lt;LogoutRequest&gt; logoutRequests = logoutManager.performLogout(ticket); // 清理 TGT this.ticketRegistry.deleteTicket(ticketGrantingTicketId); return logoutRequests; &#125; catch (final InvalidTicketException e) &#123; logger.debug("TicketGrantingTicket [&#123;&#125;] cannot be found in the ticket registry.", ticketGrantingTicketId); &#125; return Collections.emptyList();&#125;LogoutManager 根据 service 配置，处理 back channel logout：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * Perform a back channel logout for a given ticket granting ticket and returns all the logout requests. * * @param ticket a given ticket granting ticket. * @return all logout requests. */@Overridepublic List&lt;LogoutRequest&gt; performLogout(final TicketGrantingTicket ticket) &#123; final Map&lt;String, Service&gt; services = ticket.getServices(); final List&lt;LogoutRequest&gt; logoutRequests = new ArrayList&lt;&gt;(); // if SLO is not disabled if (!this.singleLogoutCallbacksDisabled) &#123; // through all services for (final Map.Entry&lt;String, Service&gt; entry : services.entrySet()) &#123; // it's a SingleLogoutService, else ignore final Service service = entry.getValue(); if (service instanceof SingleLogoutService) &#123; final LogoutRequest logoutRequest = handleLogoutForSloService((SingleLogoutService) service, entry.getKey()); if (logoutRequest != null) &#123; LOGGER.debug("Captured logout request [&#123;&#125;]", logoutRequest); logoutRequests.add(logoutRequest); &#125; &#125; &#125; &#125; return logoutRequests;&#125;// 获取 service 配置，构建 DefaultLogoutRequestprivate LogoutRequest handleLogoutForSloService(final SingleLogoutService singleLogoutService, final String ticketId) &#123; if (!singleLogoutService.isLoggedOutAlready()) &#123; final RegisteredService registeredService = servicesManager.findServiceBy(singleLogoutService); if (serviceSupportsSingleLogout(registeredService)) &#123; final URL logoutUrl = determineLogoutUrl(registeredService, singleLogoutService); final DefaultLogoutRequest logoutRequest = new DefaultLogoutRequest(ticketId, singleLogoutService, logoutUrl); final LogoutType type = registeredService.getLogoutType() == null ? LogoutType.BACK_CHANNEL : registeredService.getLogoutType(); switch (type) &#123; case BACK_CHANNEL: if (performBackChannelLogout(logoutRequest)) &#123; logoutRequest.setStatus(LogoutRequestStatus.SUCCESS); &#125; else &#123; logoutRequest.setStatus(LogoutRequestStatus.FAILURE); LOGGER.warn("Logout message not sent to [&#123;&#125;]; Continuing processing...", singleLogoutService.getId()); &#125; break; default: logoutRequest.setStatus(LogoutRequestStatus.NOT_ATTEMPTED); break; &#125; return logoutRequest; &#125; &#125; return null;&#125; // 向 service 发送 logout 通知private boolean performBackChannelLogout(final LogoutRequest request) &#123; try &#123; final String logoutRequest = this.logoutMessageBuilder.create(request); final SingleLogoutService logoutService = request.getService(); logoutService.setLoggedOutAlready(true); LOGGER.debug("Sending logout request for: [&#123;&#125;]", logoutService.getId()); final LogoutHttpMessage msg = new LogoutHttpMessage(request.getLogoutUrl(), logoutRequest); LOGGER.debug("Prepared logout message to send is [&#123;&#125;]", msg); return this.httpClient.sendMessageToEndPoint(msg); &#125; catch (final Exception e) &#123; LOGGER.error(e.getMessage(), e); &#125; return false;&#125;slo front channel 还是 logout-webflow.xml12345&lt;action-state id="frontLogout"&gt; &lt;evaluate expression="frontChannelLogoutAction" /&gt; &lt;transition on="finish" to="finishLogout" /&gt; &lt;transition on="redirectApp" to="redirectToFrontApp" /&gt;&lt;/action-state&gt;FrontChannelLogoutAction 是入口：123456789101112131415161718192021222324252627282930313233343536373839public final class FrontChannelLogoutAction extends AbstractLogoutAction &#123; @Override protected Event doInternalExecute(final HttpServletRequest request, final HttpServletResponse response, final RequestContext context) throws Exception &#123; final List&lt;LogoutRequest&gt; logoutRequests = WebUtils.getLogoutRequests(context); final Integer startIndex = getLogoutIndex(context); if (logoutRequests != null) &#123; for (int i = startIndex; i &lt; logoutRequests.size(); i++) &#123; final LogoutRequest logoutRequest = logoutRequests.get(i); if (logoutRequest.getStatus() == LogoutRequestStatus.NOT_ATTEMPTED) &#123; // assume it has been successful logoutRequest.setStatus(LogoutRequestStatus.SUCCESS); // save updated index putLogoutIndex(context, i + 1); final String logoutUrl = logoutRequest.getLogoutUrl().toExternalForm(); LOGGER.debug("Using logout url [&#123;&#125;] for front-channel logout requests", logoutUrl); final String logoutMessage = logoutManager.createFrontChannelLogoutMessage(logoutRequest); LOGGER.debug("Front-channel logout message to send under [&#123;&#125;] is [&#123;&#125;]", this.logoutRequestParameter, logoutMessage); // redirect to application with SAML logout message final UriComponentsBuilder builder = UriComponentsBuilder.fromHttpUrl(logoutUrl); builder.queryParam(this.logoutRequestParameter, URLEncoder.encode(logoutMessage, "UTF-8")); return result(REDIRECT_APP_EVENT, DEFAULT_FLOW_ATTRIBUTE_LOGOUT_URL, builder.build().toUriString()); &#125; &#125; &#125; // no new service with front-channel logout -&gt; finish logout return new Event(this, FINISH_EVENT); &#125;&#125; 实现要点：获取所有的 logout request分别向各个 service 创建 SAML logout message（其实是处理一个就 return 了）。注意使用了 redirect 并且 return。因此发送单个 logout 请求后，更新了 spring webflow 的 state，再由前端触发 front channel action 逻辑，再次进入。saml 格式的 logout message，参照上面的 front channel 时序图。小结 back channel 入口在 TerminateSessionActionfront channel 入口在 FrontChannelLogoutActionLogoutManager 提供了 back channel 和 front channel 的实现case: CAS client 无法全部 logout 上面已经分析过，service 的负载均衡方式和 session 存储的实现方式，可能导致 cas server 向 service 发送的 logout 请求不能被正确处理。解决方式：由 client 向各个 service 发送 logout 请求，即 SLO front channel。service 的 session，使用统一存储，比如 redis、mysql，而非应用 in-memory 方式。参考Logout and Single Logout (SLO)]]></content>
      <categories>
        <category>cas</category>
      </categories>
      <tags>
        <tag>cas</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 7：jvm metrics]]></title>
    <url>%2Fp%2Fmicrometer-p7-jvm-metrics%2F</url>
    <content type="text"><![CDATA[MeterBinderMeterBinder 把 metrics 注册到 MeterRegistry。12345678910/** * Binders register one or more metrics to provide information about the state * of some aspect of the application or its container. * &lt;p&gt; * Binders are enabled by default if they source data for an alert * that is recommended for a production ready app. */public interface MeterBinder &#123; void bindTo(@NonNull MeterRegistry registry);&#125;io.micrometer.core.instrument.binder.jvm提供了 jvm 相关的指标：ClassLoaderMetricsDiskSpaceMetricsExecutorServiceMetricsJvmGcMetricsJvmMemoryMetricsJvmThreadMetricsJvmMetricsAutoConfiguration是 springboot 自动装配类，注册了 gc、memory、thread、classloader 的 metrics。MeterBinder 实现 ClassLoaderMetrics 对于 jvm metrics，实现上依赖 JMX bean。123456789101112131415161718192021222324252627282930313233@NonNullApi@NonNullFieldspublic class ClassLoaderMetrics implements MeterBinder &#123; private final Iterable&lt;Tag&gt; tags; public ClassLoaderMetrics() &#123; this(emptyList()); &#125; public ClassLoaderMetrics(Iterable&lt;Tag&gt; tags) &#123; this.tags = tags; &#125; @Override public void bindTo(MeterRegistry registry) &#123; // 通过 jmx bean，获取对应的数值 ClassLoadingMXBean classLoadingBean = ManagementFactory.getClassLoadingMXBean(); // parameter: (String name, @Nullable T obj, ToDoubleFunction&lt;T&gt; f) Gauge.builder("jvm.classes.loaded", classLoadingBean, ClassLoadingMXBean::getLoadedClassCount) .tags(tags) .description("The number of classes that are currently loaded in the Java virtual machine") .baseUnit("classes") .register(registry); // 单调递增函数计数器 FunctionCounter.builder("jvm.classes.unloaded", classLoadingBean, ClassLoadingMXBean::getUnloadedClassCount) .tags(tags) .description("The total number of classes unloaded since the Java virtual machine has started execution") .baseUnit("classes") .register(registry); &#125;&#125;ExecutorServiceMetricsmicrometer 也提供了线程池的 metrics，需要手动注册。12345678910111213141516public void bindTo(MeterRegistry registry) &#123; if (executorService == null) &#123; return; &#125; String className = executorService.getClass().getName(); if (executorService instanceof ThreadPoolExecutor) &#123; monitor(registry, (ThreadPoolExecutor) executorService); &#125; else if (className.equals("java.util.concurrent.Executors$DelegatedScheduledExecutorService")) &#123; monitor(registry, unwrapThreadPoolExecutor(executorService, executorService.getClass())); &#125; else if (className.equals("java.util.concurrent.Executors$FinalizableDelegatedExecutorService")) &#123; monitor(registry, unwrapThreadPoolExecutor(executorService, executorService.getClass().getSuperclass())); &#125; else if (executorService instanceof ForkJoinPool) &#123; monitor(registry, (ForkJoinPool) executorService); &#125;&#125; 对于代理的线程池，先进行 unwrap 操作，获取底层的线程池。123456789101112131415/** * Every ScheduledThreadPoolExecutor created by &#123;@link Executors&#125; is wrapped. Also, * &#123;@link Executors#newSingleThreadExecutor()&#125; wrap a regular &#123;@link ThreadPoolExecutor&#125;. */@Nullableprivate ThreadPoolExecutor unwrapThreadPoolExecutor(ExecutorService executor, Class&lt;?&gt; wrapper) &#123; try &#123; Field e = wrapper.getDeclaredField("e"); e.setAccessible(true); return (ThreadPoolExecutor) e.get(executor); &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123; // Do nothing. We simply can't get to the underlying ThreadPoolExecutor. &#125; return null;&#125;核心实现是 montior 方法，针对 ThreadPoolExecutor 和 ForkJoinPool 提供不同实现。123456789101112private void monitor(MeterRegistry registry, @Nullable ThreadPoolExecutor tp) &#123; if (tp == null) &#123; return; &#125; FunctionCounter.builder("executor.completed", tp, ThreadPoolExecutor::getCompletedTaskCount) .tags(tags) .description("The approximate total number of tasks that have completed execution") .baseUnit(BaseUnits.TASKS) .register(registry); // more codes&#125;参考JVM and System Metrics]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 资源释放 case]]></title>
    <url>%2Fp%2Fjava-cleanup-method%2F</url>
    <content type="text"><![CDATA[stream 的 close 方法 配置了 sonarcube 规则扫描代码，其中一个资源关闭问题报错： 因为传入的 FileOutputStream 资源是匿名，没有手动释放；这里只释放了 BufferedOutputStream 的资源，导致 sonar 检查报错。不过看了下 java 8 的源码，BufferedOutputStream 的 close 方法来自 FilterOutputStream：12345678910111213141516171819202122232425262728293031323334353637383940public class FilterOutputStream extends OutputStream &#123; /** * The underlying output stream to be filtered. */ protected OutputStream out; /** * Flushes this output stream and forces any buffered output bytes * to be written out to the stream. * &lt;p&gt; * The &lt;code&gt;flush&lt;/code&gt; method of &lt;code&gt;FilterOutputStream&lt;/code&gt; * calls the &lt;code&gt;flush&lt;/code&gt; method of its underlying output stream. * * @exception IOException if an I/O error occurs. * @see java.io.FilterOutputStream#out */ public void flush() throws IOException &#123; out.flush(); &#125; /** * Closes this output stream and releases any system resources * associated with the stream. * &lt;p&gt; * The &lt;code&gt;close&lt;/code&gt; method of &lt;code&gt;FilterOutputStream&lt;/code&gt; * calls its &lt;code&gt;flush&lt;/code&gt; method, and then calls the * &lt;code&gt;close&lt;/code&gt; method of its underlying output stream. * * @exception IOException if an I/O error occurs. * @see java.io.FilterOutputStream#flush() * @see java.io.FilterOutputStream#out */ @SuppressWarnings("try") public void close() throws IOException &#123; // java 7 try with resource 的写法 // 这里用了一个临时变量指向底层的流，保证底层资源被释放 try (OutputStream ostream = out) &#123; flush(); &#125; &#125;close() 会强制刷新流。在写法上，使用了 java7 的 try-with-resource 特性，因此会释放底层的流。但是目前的 sonar 规则没有识别出来。so，还是手动改下应用代码写法。lombok 的 cleanup 注解 经同事提醒，lombok 提供了 @Cleanup，自动关闭资源，简化代码。 来自官网的例子：123456789101112public class CleanupExample &#123; public static void main(String[] args) throws IOException &#123; @Cleanup InputStream in = new FileInputStream(args[0]); @Cleanup OutputStream out = new FileOutputStream(args[1]); byte[] b = new byte[10000]; while (true) &#123; int r = in.read(b); if (r == -1) break; out.write(b, 0, r); &#125; &#125;&#125;对比以前的写法，简洁多了：123456789101112131415161718192021222324public class CleanupExample &#123; public static void main(String[] args) throws IOException &#123; InputStream in = new FileInputStream(args[0]); try &#123; OutputStream out = new FileOutputStream(args[1]); try &#123; byte[] b = new byte[10000]; while (true) &#123; int r = in.read(b); if (r == -1) break; out.write(b, 0, r); &#125; &#125; finally &#123; if (out != null) &#123; out.close(); &#125; &#125; &#125; finally &#123; if (in != null) &#123; in.close(); &#125; &#125; &#125;&#125;@Cleanup会自动调用对象的 close 方法。可以指定其他资源释放的方法：1@Cleanup("dispose") org.eclipse.swt.widgets.CoolBar bar = new CoolBar(parent, 0);]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 客户端比较]]></title>
    <url>%2Fp%2Felasticsearch-kinds-of-client%2F</url>
    <content type="text"><![CDATA[背景 项目上考虑兼容 es 2.x 和 es 7.x，于是调研了不同 elasticsearch client，做下笔记。transport vs rest ?elasticsearch client 主要分为 transport 和 rest 两大类。So, what’s the difference between these two APIs? When a user sends a REST request to an Elasticsearch node, the coordinating node parses the JSON body and transforms it into its corresponding Java object. From then on, the request is sent to other nodes in the cluster in a binary format – the Java API – using the transport networking layer. A Java user uses the Transport Client to build these Java objects directly in their application, then makes requests using the same binary format passed across the transport layer, skipping the need for the parsing step needed by REST.transport 相比 rest 客户端，直接发送二进制序列化数据，省去 json 反序列化，性能好一点。transport 原生支持集群。但是要连接多个不同版本的 es node，transport 容易出问题。transport client优点：能够使用 ES 集群中的一些特性 少了 json 到 java object 的反序列化过程，性能好一点点 缺点：JAR 包版本需与 ES 集群版本一致，ES 集群升级，客户端也跟着升级到相同版本 因为 transport client 的兼容性问题，官方逐渐淘汰并且建议迁移到 rest client。es 5.x 开始提供原生的 rest client （high / low）。low level rest client12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.2.1&lt;/version&gt;&lt;/dependency&gt;The low-level client’s features include:minimal dependenciesload balancing across all available nodesfailover in case of node failures and upon specific response codesfailed connection penalization (whether a failed node is retried depends on how many consecutive times it failed; the more failed attempts the longer the client will wait before trying that same node again)persistent connectionstrace logging of requests and responsesoptional automatic discovery of cluster nodeslow level rest client 支持不同的 es 版本。但是不提供自动的数据组装，使用相对 high level client 繁琐一点。high level rest client12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.2.1&lt;/version&gt;&lt;/dependency&gt;high level rest client 基于 low level 封装，支持自动数据组装 (marshalling 和 un-marshalling)。high level 提供 major 版本的兼容性。 兼容性：The High Level Client is guaranteed to be able to communicate with any Elasticsearch node running on the same major version and greater or equal minor version. It doesn’t need to be in the same minor version as the Elasticsearch nodes it communicates with, as it is forward compatible meaning that it supports communicating with later versions of Elasticsearch than the one it was developed for.实际上高版本的 high level client 是可以连接到低版本的 server，但是一些高级查询特性就不支持了。实测使用 7.2.1 的 high level client 可以查询 es 2 server。jestjest 是一个第三方的 rest 客户端，同样可以屏蔽 es 版本差异。在官方 rest client 出来之前是一个不错的选择。12345&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;6.3.1&lt;/version&gt;&lt;/dependency&gt;如果要使用 QueryBuilder、Settings 等类，需要引用 ElasticSearch 依赖，可能导致了跟版本依赖。但是不引用的话，手动拼接复杂 query 语句又很麻烦。12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt;&lt;/dependency&gt;spring-data-elasticsearchspring-data-elasticsearch 也是 rest client，尝试屏蔽版本差异。底层基于 high level client 二次封装。但是对新版本 es 的支持落后很多。小结 使用 high level client 去适配 es2 和 es7，并且尽快推动 es2 升级 es7。参考Java High Level REST Client Compatibilityelasticsearch 6.x 升级调研报告]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 6： DistributionSummary]]></title>
    <url>%2Fp%2Fmicrometer-p6-distributionsummary-histogram%2F</url>
    <content type="text"><![CDATA[背景 分布概要（Distribution summary）用来记录事件的分布情况。分布概要根据每个事件所对应的值，把事件分配到对应的桶（bucket）中。与分布概要密切相关的是直方图和百分比（percentile）。大多数时候，我们并不关注具体的数值，而是数值的分布区间。DistributionSummaryDistributionSummary 细分为间隔型（StepDistributionSummary）和累积型（CumulativeDistributionSummary）。实现套路和 Timer 类似，具体分析参见：micrometer 系列 3：timerAbstractDistributionSummary 是基本实现，其构造函数主要是配置 histogram 实例。12345678910111213141516171819202122public abstract class AbstractDistributionSummary extends AbstractMeter implements DistributionSummary &#123; protected final Histogram histogram; private final double scale; protected AbstractDistributionSummary(Id id, Clock clock, DistributionStatisticConfig distributionStatisticConfig, double scale, boolean supportsAggregablePercentiles) &#123; super(id); this.scale = scale; if (distributionStatisticConfig.isPublishingPercentiles()) &#123; // hdr-based histogram this.histogram = new TimeWindowPercentileHistogram(clock, distributionStatisticConfig, supportsAggregablePercentiles); &#125; else if (distributionStatisticConfig.isPublishingHistogram()) &#123; // fixed boundary histograms, which have a slightly better memory footprint // when we don't need Micrometer-computed percentiles this.histogram = new TimeWindowFixedBoundaryHistogram(clock, distributionStatisticConfig, supportsAggregablePercentiles); &#125; else &#123; // noop histogram this.histogram = NoopHistogram.INSTANCE; &#125; &#125;&#125;Histogrammicrometer 提供 2 种类型的 histogram：TimeWindowPercentileHistogramTimeWindowFixedBoundaryHistogramTimeWindowFixedBoundaryHistogram不支持预先计算百分比（client 端），但是可以计算聚合百分比（在监控系统 server 端计算）。不支持 hdr。12345/* * A histogram implementation that does not support precomputed percentiles but supports * aggregable percentile histograms and SLA boundaries. There is no need for a high dynamic range * histogram and its more expensive memory footprint if all we are interested in is fixed histogram counts. */核心是内部类 FixedBoundaryHistogram，记录了每个 bucket 数值。为了节省内存，使用 AtomicLongArray 存储，而非 AtomicLong[]。 对于累积计数，在调用的时候再实时计算。12345678910111213141516171819202122232425262728293031public class TimeWindowFixedBoundaryHistogram extends AbstractTimeWindowHistogram&lt;TimeWindowFixedBoundaryHistogram.FixedBoundaryHistogram, Void&gt; &#123; private final long[] buckets; @Override FixedBoundaryHistogram newBucket() &#123; return new FixedBoundaryHistogram(); &#125; class FixedBoundaryHistogram &#123; /** * For recording efficiency, this is a normal histogram. We turn these values into * cumulative counts only on calls to &#123;@link #countAtValue(long)&#125;. */ final AtomicLongArray values; FixedBoundaryHistogram() &#123; this.values = new AtomicLongArray(buckets.length); &#125; long countAtValue(long value) &#123; int index = Arrays.binarySearch(buckets, value); if (index &lt; 0) return 0; long count = 0; for (int i = 0; i &lt;= index; i++) count += values.get(i); return count; &#125; &#125; &#125;TimeWindowPercentileHistogram 支持由 micrometer 预先计算百分比、再发送到监控系统。12345public class TimeWindowPercentileHistogram extends AbstractTimeWindowHistogram&lt;DoubleRecorder, DoubleHistogram&gt; &#123; private final DoubleHistogram intervalHistogram;&#125;核心功能依赖 HdrHistogram 包的 DoubleHistogram 实现，这个以后再研究。AbstractTimeWindowHistogram不管是 percentile 还是 fixed boundary 类型的直方图，都继承自 AbstractTimeWindowHistogram。123456789101112131415161718192021222324252627282930313233/** * An abstract base class for histogram implementations who maintain samples in a ring buffer * to decay older samples and give greater weight to recent samples. * * @param &lt;T&gt; the type of the buckets in a ring buffer * @param &lt;U&gt; the type of accumulated histogram * @author Jon Schneider * @author Trustin Heuiseung Lee */@SuppressWarnings("ConstantConditions")abstract class AbstractTimeWindowHistogram&lt;T, U&gt; implements Histogram &#123; @SuppressWarnings("rawtypes") private static final AtomicIntegerFieldUpdater&lt;AbstractTimeWindowHistogram&gt; rotatingUpdater = AtomicIntegerFieldUpdater.newUpdater(AbstractTimeWindowHistogram.class, "rotating"); final DistributionStatisticConfig distributionStatisticConfig; private final Clock clock; private final boolean supportsAggregablePercentiles; private final T[] ringBuffer; private short currentBucket; private final long durationBetweenRotatesMillis; private volatile boolean accumulatedHistogramStale; private volatile long lastRotateTimestampMillis; @SuppressWarnings(&#123;"unused", "FieldCanBeLocal"&#125;) private volatile int rotating; // 0 - not rotating, 1 - rotating @Nullable private U accumulatedHistogram;设计套路和 TimeWindowMax 很相似。具体见：micrometer 系列 4：rate aggregated使用 ringbuffer 存储数据，并且设定重置的时间间隔 (rotate 方法)。 基于 cas 乐观锁（rotating 字段）进行并发控制。一个常见的操作是获取直方图快照。12345678910111213public final HistogramSnapshot takeSnapshot(long count, double total, double max) &#123; rotate(); final ValueAtPercentile[] values; final CountAtBucket[] counts; synchronized (this) &#123; accumulateIfStale(); values = takeValueSnapshot(); counts = takeCountSnapshot(); &#125; return new HistogramSnapshot(count, total, max, values, counts, this::outputSummary);&#125;PercentileHistogramBuckets负责 histogram 的分桶方式，适用于可聚合百分数的监控系统（例如 Prometheus）。12345678910111213141516171819202122232425262728293031323334353637383940414243public class PercentileHistogramBuckets &#123; // Number of positions of base-2 digits to shift when iterating over the long space. private static final int DIGITS = 2; // Bucket values to use, see static block for initialization. private static final NavigableSet&lt;Long&gt; PERCENTILE_BUCKETS; // The set of buckets is generated by using powers of 4 and incrementing by one-third of the // previous power of 4 in between as long as the value is less than the next power of 4 minus // the delta. // // &lt;pre&gt; // Base: 1, 2, 3 // // 4 (4^1), delta = 1 // 5, 6, 7, ..., 14, // // 16 (4^2), delta = 5 // 21, 26, 31, ..., 56, // // 64 (4^3), delta = 21 // ... // &lt;/pre&gt; static &#123; PERCENTILE_BUCKETS = new TreeSet&lt;&gt;(); PERCENTILE_BUCKETS.add(1L); PERCENTILE_BUCKETS.add(2L); PERCENTILE_BUCKETS.add(3L); int exp = DIGITS; while (exp &lt; 64) &#123; long current = 1L &lt;&lt; exp; long delta = current / 3; long next = (current &lt;&lt; DIGITS) - delta; while (current &lt; next) &#123; PERCENTILE_BUCKETS.add(current); current += delta; &#125; exp += DIGITS; &#125; PERCENTILE_BUCKETS.add(Long.MAX_VALUE); &#125;参考 使用 Micrometer 记录 Java 应用性能指标]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 5： gauge]]></title>
    <url>%2Fp%2Fmicrometer-p5-gauge%2F</url>
    <content type="text"><![CDATA[gaugegauge 是测量值，数值可以上下变化。（ps. 对比 counter，通常用于自增场景）123456789101112131415161718192021222324252627282930313233343536373839/** * A gauge tracks a value that may go up or down. The value that is published for gauges is * an instantaneous sample of the gauge at publishing time. * */public interface Gauge extends Meter &#123; class Builder&lt;T&gt; &#123; private final String name; private final ToDoubleFunction&lt;T&gt; f; private Tags tags = Tags.empty(); // 默认是弱引用方式，可能导致 value()返回 Double.NAN private boolean strongReference = false; @Nullable private Meter.Id syntheticAssociation = null; // 被关联的对象 @Nullable private final T obj; @Nullable private String description; @Nullable private String baseUnit; public Gauge register(MeterRegistry registry) &#123; return registry.gauge(new Meter.Id(name, tags, baseUnit, description, Type.GAUGE, syntheticAssociation), obj, strongReference ? new StrongReferenceGaugeFunction&lt;&gt;(obj, f) : f); &#125; &#125;&#125;@FunctionalInterfacepublic interface ToDoubleFunction&lt;T&gt; &#123; double applyAsDouble(T value);&#125;gauge 会关联真实对象，并且把从真实对象获取计数值，抽象为ToDoubleFunction。 DefaultGauge 是基本实现，用 WeakReference 关联真实的对象。 使用例子：12345// 参数： // - metric name// - 要关联的真实对象 // - ToDoubleFunction 的实现Queue&lt;Message&gt; receivedMessages = registry.gauge("unprocessed.messages", new ConcurrentLinkedQueue&lt;&gt;(), ConcurrentLinkedQueue::size); 注册 gauge 直接返回了底层关联的真实对象，这和其他的 metrics 类型不一样。123456// maintain a reference to myGaugeAtomicInteger myGauge = registry.gauge("numberGauge", new AtomicInteger(0));// ... elsewhere you can update the value it holds using the object referencemyGauge.set(27);myGauge.set(11);反复手动更新 gauge 的值是无意义的。只有在发布时刻（publish time）的实例值才有意义。场景问题： 返回 NANgauge 使用过程中，常见问题是 gauge 测量值返回 NAN。DefaultGauge 使用 WeakReference 关联 gauge 和真实对象。如果对象被回收，那么就回返回 NAN。之所以使用 WeakReference，是为了避免影响 gc。123456789101112131415161718192021222324public class DefaultGauge&lt;T&gt; extends AbstractMeter implements Gauge &#123; private final WeakReference&lt;T&gt; ref; private final ToDoubleFunction&lt;T&gt; value; public DefaultGauge(Meter.Id id, @Nullable T obj, ToDoubleFunction&lt;T&gt; value) &#123; super(id); this.ref = new WeakReference&lt;&gt;(obj); this.value = value; &#125; public double value() &#123; T obj = ref.get(); if (obj != null) &#123; try &#123; return value.applyAsDouble(ref.get()); &#125; catch (Throwable ex) &#123; logger.log("Failed to apply the value function for the gauge'" + getId().getName() + "'.", ex); &#125; &#125; return Double.NaN; &#125; &#125;micormeter 也提供了强引用的方式关联 gauge 和真实对象的方式。123456789101112131415161718192021222324class StrongReferenceGaugeFunction&lt;T&gt; implements ToDoubleFunction&lt;T&gt; &#123; /** * Holding a reference to obj inside of this function effectively prevents it from being * garbage collected. Implementors of &#123;@link Gauge&#125; can then assume that they should hold * &#123;@code obj&#125; as a weak reference. * &lt;p&gt; * If obj is &#123;@code null&#125; initially then this gauge will not be reported. */ @Nullable @SuppressWarnings("FieldCanBeLocal") private final T obj; private final ToDoubleFunction&lt;T&gt; f; StrongReferenceGaugeFunction(@Nullable T obj, ToDoubleFunction&lt;T&gt; f) &#123; this.obj = obj; this.f = f; &#125; @Override public double applyAsDouble(T value) &#123; return f.applyAsDouble(value); &#125;&#125; 参考Why is my Gauge reporting NaN or disappearing?]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 4：rate aggregated]]></title>
    <url>%2Fp%2Fmicrometer-p4-rate-aggregation%2F</url>
    <content type="text"><![CDATA[rate aggregated学习监控系统和 micrometer 的时候，接触了 rate aggregated 概念。对于某些系统的监控，数据聚合的速率比绝对值更有意义。这类监控系统的数学计算更少。引用 micrometer 官网的一张图：(来源：https://micrometer.io/docs/concepts#_client_side)为了支持对数据进行 rate aggregate，micrometer 对当前发布间隔，通过记录 step value 来累计数值。Micrometer efficiently maintains rate data by means of a step value that accumulates data for the current publishing interval.发布间隔分为 current 和 previous。每次轮询返回 previous 的值。一旦当前发布间隔结束，把 current 的值更新到 previous。TimeWindowMax最大值衰减，对最近接收的采样值给予更高的权重。TimeWindowMax 使用 ringbuffer 方式 (一个 AtomicLong 数组) 实现最大值衰减。12345678910111213141516171819202122232425262728293031323334/** * An implementation of a decaying maximum for a distribution based on a configurable ring buffer. * * @author Jon Schneider */public class TimeWindowMax &#123; @SuppressWarnings("rawtypes") private static final AtomicIntegerFieldUpdater&lt;TimeWindowMax&gt; rotatingUpdater = AtomicIntegerFieldUpdater.newUpdater(TimeWindowMax.class, "rotating"); private final Clock clock; // ringbuffer 反转间隔 private final long durationBetweenRotatesMillis; // 留意 ringbuffer 是一个 AtomicLong 数组 // 采用值是 double，保存数值的 long bit private AtomicLong[] ringBuffer; private int currentBucket; private volatile long lastRotateTimestampMillis; @SuppressWarnings(&#123;"unused", "FieldCanBeLocal"&#125;) private volatile int rotating = 0; // 0 - not rotating, 1 - rotating public TimeWindowMax(Clock clock, long rotateFrequencyMillis, int bufferLength) &#123; this.clock = clock; this.durationBetweenRotatesMillis = rotateFrequencyMillis; this.lastRotateTimestampMillis = clock.wallTime(); this.currentBucket = 0; this.ringBuffer = new AtomicLong[bufferLength]; for (int i = 0; i &lt; bufferLength; i++) &#123; this.ringBuffer[i] = new AtomicLong(); &#125; &#125;&#125; 基础操作是 rotate，负责按照设置的滑动时间窗口重置 ringbuffer，维护当前使用的槽位。123456789101112131415161718192021222324252627282930private void rotate() &#123; long timeSinceLastRotateMillis = clock.wallTime() - lastRotateTimestampMillis; // 到达滚动 ringbuffer 的时间点 if (timeSinceLastRotateMillis &lt; durationBetweenRotatesMillis) &#123; // Need to wait more for next rotation. return; &#125; // cas 并发检查 if (!rotatingUpdater.compareAndSet(this, 0, 1)) &#123; // Being rotated by other thread already. return; &#125; try &#123; int iterations = 0; synchronized (this) &#123; do &#123; // 重置槽位数值 ringBuffer[currentBucket].set(0); if (++currentBucket &gt;= ringBuffer.length) &#123; currentBucket = 0; &#125; timeSinceLastRotateMillis -= durationBetweenRotatesMillis; lastRotateTimestampMillis += durationBetweenRotatesMillis; &#125; while (timeSinceLastRotateMillis &gt;= durationBetweenRotatesMillis &amp;&amp; ++iterations &lt; ringBuffer.length); &#125; &#125; finally &#123; rotating = 0; &#125;&#125; 因为滑动窗口，因此在记录采样器数值时，要尝试更新所有的 ringbuffer 槽位。123456789101112131415161718192021/** * For use by distribution summary implementations. * * @param sample The value to record. */public void record(double sample) &#123; rotate(); // 把底层的二进制位转换为 long 类型。 long sampleLong = Double.doubleToLongBits(sample); for (AtomicLong max : ringBuffer) &#123; updateMax(max, sampleLong); &#125;&#125;private void updateMax(AtomicLong max, long sample) &#123; for (; ;) &#123; long curMax = max.get(); if (curMax &gt;= sample || max.compareAndSet(curMax, sample)) break; &#125;&#125;新技能 get：JUC 没有 AtomicDouble，只有 AtomicLong。为了使用 CAS 乐观方式高效更新计数器的值 (double 类型)，直接把 sample 底层的二进制位转换为 long 类型更新到 ringbuffer 槽位！ 读取数值，直接把当前槽位的 long bit 转为 double 类型。123456789/** * @return An unscaled max. For use by distribution summary implementations. */public double poll() &#123; rotate(); synchronized (this) &#123; return Double.longBitsToDouble(ringBuffer[currentBucket].get()); &#125;&#125;DistributionStatisticConfigDistributionStatisticConfig 是 Timer 和 DistributionSummary 的配置类。slahistogram 会划分多个 bucket。sla 用于指定发布哪些 bucket。expiry 和 bufferLength123456/** * Statistics like max, percentiles, and histogram counts decay over time to give greater weight to recent * samples (exception: histogram counts are cumulative for those systems that expect cumulative * histogram buckets). Samples are accumulated to such statistics in ring buffers which rotate after * &#123;@link #expiry&#125;, with this buffer length. */max, percentiles, and histogram 给最近的采样值更高的权重。采样值记录在 ringbuffer 环形数组。经过 expiry 时间之后，ringbuffer 数组会被重置。bufferLength: ringbuffer 数组长度。expiry: 采样值重置的时间间隔（The amount of time samples are accumulated to a histogram before it is reset and rotated.）。percentiles本地计算的百分比时序，因此不能跨 dimension 合并。PrometheusTimer有了上面的基础，来看看 PrometheusTimer 的相关实现。123456789101112131415161718192021public class PrometheusTimer extends AbstractTimer &#123; private static final CountAtBucket[] EMPTY_HISTOGRAM = new CountAtBucket[0]; private final LongAdder count = new LongAdder(); private final LongAdder totalTime = new LongAdder(); private final TimeWindowMax max; @Nullable private final Histogram histogram; protected void recordNonNegative(long amount, TimeUnit unit) &#123; count.increment(); long nanoAmount = TimeUnit.NANOSECONDS.convert(amount, unit); totalTime.add(nanoAmount); max.record(nanoAmount, TimeUnit.NANOSECONDS); if (histogram != null) histogram.recordLong(TimeUnit.NANOSECONDS.convert(amount, unit)); &#125;&#125;使用 LongAdder 类型记录次数和总时间，这里考虑了相当高并发情况。TimeWindowMax 支持区间最值衰减。PrometheusTimer 另一个功能是返回累积直方图，这个放在以后再说。参考Move Timer max to a ring buffer implementation #317]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 3：timer]]></title>
    <url>%2Fp%2Fmicrometer-p3-timer%2F</url>
    <content type="text"><![CDATA[TimerTimer 适用于测量短时间执行的事件。12345/* Timer intended to track of a large number of short running events. Example would be something like * an HTTP request. Though "short running" is a bit subjective the assumption is that it should be * under a minute.*/public interface Timer extends Meter, HistogramSupport &#123;&#125;Timer 继承了 HistogramSupport。在 micrometer 中，使用 Timer 映射 Prometheus 的 histogram 类型。计时器会记录两类数据：事件的数量和总的持续时间。Sample计时功能依赖内部类 Sample 实现。123456789101112131415class Sample &#123; private final long startTime; private final Clock clock; Sample(Clock clock) &#123; this.clock = clock; this.startTime = clock.monotonicTime(); &#125; public long stop(Timer timer) &#123; long durationNs = clock.monotonicTime() - startTime; timer.record(durationNs, TimeUnit.NANOSECONDS); return durationNs; &#125;&#125;Sample 初始化的时候记录当前的 monotonicTime。当结束采样，通过stop() 返回期间流逝的时间。关于 wallTime 和monotonicTime，参见：wall time 和 monotonic time 简介 Timer 只有在任务完成之后才会记录时间。具体实现见1234567891011121314151617public abstract class AbstractTimer extends AbstractMeter implements Timer &#123;/** * Executes the runnable &#123;@code f&#125; and records the time taken. * * @param f Function to execute and measure the execution time. */ @Override public void record(Runnable f) &#123; final long s = clock.monotonicTime(); try &#123; f.run(); &#125; finally &#123; final long e = clock.monotonicTime(); record(e - s, TimeUnit.NANOSECONDS); &#125; &#125;&#125; 长时间执行的任务，应该使用 LongTaskTimer。AbstractTimerAbstractTimer 是 Timer 的基本实现。Timer 实现的时候考虑了暂停 pause 的问题，引入了 PauseDetector。通过注册 PauseDetectorListener，Timer 能够收到暂停事件。（MeterRegistry 默认使用 NoPauseDetector。）123456789101112131415public abstract class AbstractTimer extends AbstractMeter implements Timer &#123; private static Map&lt;PauseDetector, org.LatencyUtils.PauseDetector&gt; pauseDetectorCache = new ConcurrentHashMap&lt;&gt;(); protected final Clock clock; protected final Histogram histogram; private final TimeUnit baseTimeUnit; // Only used when pause detection is enabled @Nullable private IntervalEstimator intervalEstimator = null; @Nullable private org.LatencyUtils.PauseDetector pauseDetector;&#125;AbstractTimer 包含 Histogram，处理直方图问题。以后再探讨。CumulativeTimerCumulativeTimer 是累积型的 timer。计数和计时都使用 AtomicLong 类型保存。比较有意思的是 TimeWindowMax 成员，会在未来讲解。1234567891011public class CumulativeTimer extends AbstractTimer &#123; // 次数 private final AtomicLong count; // 时间 private final AtomicLong total; private final TimeWindowMax max; @Override public double max(TimeUnit unit) &#123; return max.poll(unit); &#125;StepTimerStepTimer 是区间间隔的 timer。计数和计时都是用 StepLong。1234public class StepTimer extends AbstractTimer &#123; private final StepLong count; private final StepLong total; private final TimeWindowMax max;StepLong 和 StepDouble 类似，底层使用 Striped64 的子类作为存储，解决高并发的性能问题。 具体参见 StepDouble 部分：micrometer 系列 2：counter123456789public class StepLong &#123; private final Clock clock; // 区间间隔 private final long stepMillis; // Striped64 子类 private final LongAdder current = new LongAdder(); private final AtomicLong lastInitPos; // 上一个区间结束的计数 private volatile double previous = 0.0;PrometheusTimer 和 TimeWindowMax、 DistributionStatisticConfig 一起再看。LongTaskTimerA long task timer is used to track the total duration of all in-flight long-running tasks and the number of such tasks.1public interface LongTaskTimer extends Meter&#123;&#125;和 Timer 类似，LongTaskTimer 包含内部类 Sample，用于统计耗时；内部会保存任务 id。1234567891011class Sample &#123; private final LongTaskTimer timer; // task 是任务 id private final long task; public Sample(LongTaskTimer timer, long task) &#123; this.timer = timer; this.task = task; &#125; // more code&#125;DefaultLongTaskTimer 使用 ConcurrentHashMap 保存多个任务的计时。12345678910111213141516171819202122232425public class DefaultLongTaskTimer extends AbstractMeter implements LongTaskTimer &#123; // key: taskId // value: 开始时间，monotonic time 单调时间 private final ConcurrentMap&lt;Long, Long&gt; tasks = new ConcurrentHashMap&lt;&gt;(); private final AtomicLong nextTask = new AtomicLong(0L); private final Clock clock; @Override public Sample start() &#123; long task = nextTask.getAndIncrement(); tasks.put(task, clock.monotonicTime()); return new Sample(this, task); &#125; @Override public long stop(long task) &#123; Long startTime = tasks.get(task); if (startTime != null) &#123; tasks.remove(task); return clock.monotonicTime() - startTime; &#125; else &#123; return -1L; &#125; &#125;&#125;]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 2：counter]]></title>
    <url>%2Fp%2Fmicrometer-p2-counter%2F</url>
    <content type="text"><![CDATA[CounterCounter 是计数器。核心方法就 increment 和 count。同样提供 Builder 模式构建实例。Counter 包含 StepCounter 和 CumulativeCounter 两大类 1234567891011121314151617181920212223public interface Counter extends Meter &#123; static Builder builder(String name) &#123; return new Builder(name); &#125; /** * Update the counter by one. */ default void increment() &#123; increment(1.0); &#125; /** * Update the counter by &#123;@code amount&#125;. * * @param amount Amount to add to the counter. */ void increment(double amount); /** * @return The cumulative count since this counter was created. */ double count();Counter 有 2 个类型：CumulativeCounter。 一直累加的计数器。StepCounter。 时间区间累加的计数器。CumulativeCounter1234567891011121314public class CumulativeCounter extends AbstractMeter implements Counter &#123; private final DoubleAdder value; @Override public void increment(double amount) &#123; // 注意 amount 可以为负数 value.add(amount); &#125; @Override public double count() &#123; return value.sum(); &#125;&#125;CumulativeCounter 底层使用 java8 新增的 DoubleAdder 存储数据。在高并发的情况下，DoubleAdder 提供更好的性能。DoubleAdder 基于 Striped64 的设计，因此访问当前值，转换为对所有槽位求和。StepCounter12345678910111213141516171819202122/** * Counter that reports a rate per second to a monitoring system. Note that &#123;@link #count()&#125; * will report the number events in the last complete interval rather than the total for * the life of the process. */public class StepCounter extends AbstractMeter implements Counter &#123; private final StepDouble value; public StepCounter(Id id, Clock clock, long stepMillis) &#123; super(id); this.value = new StepDouble(clock, stepMillis); &#125; @Override public void increment(double amount) &#123; value.getCurrent().add(amount); &#125; @Override public double count() &#123; return value.poll(); &#125;StepCounter 底层使用 StepDouble 存储数据。12345678910111213141516171819202122232425262728293031323334353637383940public class StepDouble &#123; private final Clock clock; private final long stepMillis; private final DoubleAdder current = new DoubleAdder(); // 上一个 interval private final AtomicLong lastInitPos; // 上一个完成 interval 的值 private volatile double previous = 0.0; public StepDouble(Clock clock, long stepMillis) &#123; this.clock = clock; this.stepMillis = stepMillis; // 计算当前所在的 interval lastInitPos = new AtomicLong(clock.wallTime() / stepMillis); &#125; private void rollCount(long now) &#123; final long stepTime = now / stepMillis; final long lastInit = lastInitPos.get(); if (lastInit &lt; stepTime &amp;&amp; lastInitPos.compareAndSet(lastInit, stepTime)) &#123; final double v = current.sumThenReset(); // Need to check if there was any activity during the previous step interval. If there was // then the init position will move forward by 1, otherwise it will be older. No activity // means the previous interval should be set to the `init` value. previous = (lastInit == stepTime - 1) ? v : 0.0; &#125; &#125; public DoubleAdder getCurrent() &#123; return current; &#125; /** * @return The value for the last completed interval. */ public double poll() &#123; rollCount(clock.wallTime()); return previous; &#125;&#125;StepDouble 初始化的时候，先计算当前的 interval。 读取上一个完成的 interval 的计数器值，先计算当前的间隔 stepTime。如果 CAS 更新 lastInitPos 成功，则判断是否经历只一个 interval。如果是，则返回当前计数值；否则，计数器的值已经过期，重置为 0。PrometheusCounterPrometheusCounter 和 CumulativeCounter 类似，但是要求每次增加的值为正数。12345678910111213public class PrometheusCounter extends AbstractMeter implements Counter &#123; private DoubleAdder count = new DoubleAdder(); PrometheusCounter(Meter.Id id) &#123; super(id); &#125; @Override public void increment(double amount) &#123; if (amount &gt; 0) count.add(amount); &#125;&#125;]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[micrometer 系列 1：Meter 和 MeterRegistry]]></title>
    <url>%2Fp%2Fmicrometer-p1-meter%2F</url>
    <content type="text"><![CDATA[micrometer 源码分析之 Meter 和 MeterRegistry。MeterMeter 是整个 micrometer 指标的基础接口。一个 Meter 包含的核心成员有 Id、MeasurementIdId 包含名字、标签列表、Meter 类型、基础单位等属性。留意这里有个 syntheticAssociation 属性，用于记录当前 Meter 从哪个 Meter 衍生出来。在 micrometer 中，tag 和其他监控系统中的 dimension 是相同的概念。支持 tag 的好处就是可以进行多维度的统计和查询。例如把服务作为一个 tag，主机作为另一个 tag，就可以观察到一个服务在不同主机的运行情况。123456789101112131415161718192021class Id &#123; private final String name; private final Tags tags; private final Type type; @Nullable /** * For internal use. Indicates that this Id is tied to a meter that is a derivative of another metric. * For example, percentiles and histogram gauges generated by &#123;@link HistogramGauges&#125; are derivatives * of a &#123;@link Timer&#125; or &#123;@link DistributionSummary&#125;. * &lt;p&gt; * This method may be removed in future minor or major releases if we find a way to mark derivatives in a * private way that does not have other API compatibility consequences. * * @return The meter id of a meter for which this metric is a synthetic derivative. */ private final Meter.Id syntheticAssociation; @Nullable private final String description; @Nullable private final String baseUnit;&#125;MeasurementMeasurement 包含了测量 Meter 的方式。其中测量值抽象为 Supplier&lt;Double&gt; valueFunction，方便提供不同的实现。12345678910public class Measurement &#123; private final Supplier&lt;Double&gt; f; private final Statistic statistic; public Measurement(Supplier&lt;Double&gt; valueFunction, Statistic statistic) &#123; this.f = valueFunction; this.statistic = statistic; &#125; // more code&#125;Statistic 是对 value 类型描述的枚举类型。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public enum Statistic &#123; /** * The sum of the amounts recorded. */ TOTAL("total"), /** * The sum of the times recorded. Reported in the monitoring system's base unit of time */ TOTAL_TIME("total"), /** * Rate per second for calls. */ COUNT("count"), /** * The maximum amount recorded. When this represents a time, it is reported in the monitoring system's base unit of time. */ MAX("max"), /** * Instantaneous value, such as those reported by gauges. */ VALUE("value"), /** * Undetermined. */ UNKNOWN("unknown"), /** * Number of currently active tasks for a long task timer. */ ACTIVE_TASKS("active"), /** * Duration of a running task in a long task timer. Always reported in the monitoring system's base unit of time. */ DURATION("duration"); private final String tagValueRepresentation; Statistic(String tagValueRepresentation) &#123; this.tagValueRepresentation = tagValueRepresentation; &#125; public String getTagValueRepresentation() &#123; return tagValueRepresentation; &#125;&#125;Builder 因为可配置属性多，Meter 使用了 builder 设计模式，提供 Builder 内部类，方便自定义构建 Meter。TypeType 枚举包含具体的 metris 类型，以后会分别探讨。12345678enum Type &#123; COUNTER, GAUGE, LONG_TASK_TIMER, TIMER, DISTRIBUTION_SUMMARY, OTHER;&#125;MeterRegistryMeterRegistry 是注册 Meter 的基类实现。MeterRegistry 包含了 Clock。用于测量持续时间。MeterFilter。过滤 Meter 的条件，还提供重命名、tag 改名、修改 tag value 等功能。Meter 监听机制（meterAddedListeners、meterRemovedListeners）。 配置工具 Config。More 类，提供不常用 metrics 的工具方法，例如 FunctionCounter、LongTaskTimer 等。PauseDetector，检测暂停，以后再讨论。syntheticAssociations 保存了 meter 之间的衍生关系。NamingConvention，用于转换不同底层监控系统的命名规则。1234567891011121314151617181920212223242526272829303132public abstract class MeterRegistry &#123; protected final Clock clock; private final Object meterMapLock = new Object(); private volatile MeterFilter[] filters = new MeterFilter[0]; private final List&lt;Consumer&lt;Meter&gt;&gt; meterAddedListeners = new CopyOnWriteArrayList&lt;&gt;(); private final List&lt;Consumer&lt;Meter&gt;&gt; meterRemovedListeners = new CopyOnWriteArrayList&lt;&gt;(); private final Config config = new Config(); private final More more = new More(); private volatile PMap&lt;Id, Meter&gt; meterMap = HashTreePMap.empty(); /** * Map of meter id whose associated meter contains synthetic counterparts to those synthetic ids. * We maintain these associations so that when we remove a meter with synthetics, they can removed * as well. */ private volatile PMap&lt;Id, PSet&lt;Id&gt;&gt; syntheticAssociations = HashTreePMap.empty(); private final AtomicBoolean closed = new AtomicBoolean(false); private PauseDetector pauseDetector = new NoPauseDetector(); /** * We'll use snake case as a general-purpose default for registries because it is the most * likely to result in a portable name. Camel casing is also perfectly acceptable. '-' and '.' * separators can pose problems for some monitoring systems. '-' is interpreted as metric * subtraction in some (including Prometheus), and '.' is used to flatten tags into hierarchical * names when shipping metrics to hierarchical backends such as Graphite. */ private NamingConvention namingConvention = NamingConvention.snakeCase; // more code&#125;不同的监控系统，通过实现各自的 MeterRegistry，对接到 micrometer。micrometer 也提供了 2 个基础 MeterRegistry 的实现。SimpleMeterRegistrySimpleMeterRegistry 是一个 in-memory 的实现。它不会对外暴露 metrics。适合于简单测试。CompositeMeterRegistry用于组合多个不同的监控系统。这样一个 meter 采集的数据可以发送到不同的监控系统。把 meter 添加到各个 registry，是一个可能发生并发但是低频的操作。在实现上，CompositeMeterRegistry 使用 CAS 乐观锁提高性能。12345678910111213141516171819202122232425262728293031323334353637public class CompositeMeterRegistry extends MeterRegistry &#123; private final AtomicBoolean registriesLock = new AtomicBoolean(false); private final Set&lt;MeterRegistry&gt; registries = Collections.newSetFromMap(new IdentityHashMap&lt;&gt;()); private void lock(AtomicBoolean lock, Runnable r) &#123; for (; ;) &#123; // CAS 操作 if (lock.compareAndSet(false, true)) &#123; try &#123; r.run(); break; &#125; finally &#123; lock.set(false); &#125; &#125; &#125; &#125; public CompositeMeterRegistry(Clock clock, Iterable&lt;MeterRegistry&gt; registries) &#123; super(clock); config() .namingConvention(NamingConvention.identity) .onMeterAdded(m -&gt; &#123; if (m instanceof CompositeMeter) &#123; // should always be lock(registriesLock, () -&gt; nonCompositeDescendants.forEach(((CompositeMeter) m)::add)); &#125; &#125;) .onMeterRemoved(m -&gt; &#123; if (m instanceof CompositeMeter) &#123; // should always be lock(registriesLock, () -&gt; nonCompositeDescendants.forEach(r -&gt; r.remove(m))); &#125; &#125;); registries.forEach(this::add); &#125;&#125;CompositeMeterRegistry 注册的 meter 都是 AbstractCompositeMeter 的子类。12345678910111213141516171819abstract class AbstractCompositeMeter&lt;T extends Meter&gt; extends AbstractMeter implements CompositeMeter &#123; private AtomicBoolean childrenGuard = new AtomicBoolean(); private Map&lt;MeterRegistry, T&gt; children = Collections.emptyMap(); // There are no child meters at the moment. Return a lazily instantiated no-op meter. @Nullable private volatile T noopMeter; AbstractCompositeMeter(Id id) &#123; super(id); &#125; abstract T newNoopMeter(); @Nullable abstract T registerNewMeter(MeterRegistry registry); // more code&#125; 子类需要实现 newNoopMeter 和 registerNewMeter 方法。和 CompositeMeterRegistry 类似，AbstractCompositeMeter 也是使用 CAS 方式解决并发安全问题。PrometheusMeterRegistryPrometheusMeterRegistry的 scrape()对外暴露 Prometheus 指标。1234567891011121314public String scrape() &#123; Writer writer = new StringWriter(); try &#123; scrape(writer); &#125; catch (IOException e) &#123; // This actually never happens since StringWriter::write() doesn't throw any IOException throw new RuntimeException(e); &#125; return writer.toString();&#125;public void scrape(Writer writer) throws IOException &#123; TextFormat.write004(writer, registry.metricFamilySamples());&#125;io.prometheus.client.exporter.common.TextFormat根据 version 0.0.4 暴露 Prometheus 指标。prometheus 指标协议具体见 EXPOSITION FORMATS。123456789# HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile="0"&#125; 0go_gc_duration_seconds&#123;quantile="0.25"&#125; 0go_gc_duration_seconds&#123;quantile="0.5"&#125; 0go_gc_duration_seconds&#123;quantile="0.75"&#125; 0go_gc_duration_seconds&#123;quantile="1"&#125; 0.001996go_gc_duration_seconds_sum 0.0039907go_gc_duration_seconds_count 48MetricsMetrics 是一个工具类，提供了全局静态 meter 注册器（是一个 CompositeMeterRegistry）。 另外包含一个内部类 More，提供不常用的 meter 类型，例如 LongTaskTimer、FunctionCounter 等。12345public class Metrics &#123; public static final CompositeMeterRegistry globalRegistry = new CompositeMeterRegistry(); private static final More more = new More(); // more&#125;]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>micrometer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[监控基础理论： logging、metrics 和 tracing]]></title>
    <url>%2Fp%2Fmonitoring-theory-log-metrics-tracing-notation%2F</url>
    <content type="text"><![CDATA[前言 监控体系里面有 3 个容易混淆的概念：logging、metrics 和 tracing。整理做下笔记。logging, metrics and tracing关注点 logging： 记录离散事件（discrete events）。可以是纯文本、结构化、甚至是二进制的日志，用于记录某个时间点发生的事情。metrics： 对于一段时间内事件的观察指标（measurement over time）。通常是可以聚合（aggregatable）的指标，但也有例外，比如 percentile、average。tracing： 展现系统中关联但是又离散的（related and discrete）事件流，因而是有序的（ordered）。通常用于分布式系统。 用途 logging： 日志包含某个 event 的详细情况。例如一个 error 级别的日志详情。metrics： 可以发现观察指标的变化趋势，比如过去 1 min http 请求的失败率、平均响应时间。常见的指标有 counter、gauge、histogram。tracing：用于发现跨服务调用的影响（identify cause across services）。比如一个 request 涉及多个分布式服务，如果 request 响应慢了，可以从 tracing 发现哪个调用环节慢了，成为瓶颈。 资源消耗 logging： 跟记录流量和记录粒度成正比metrics： 固定的写流量（因为 metrics 只保留指标）tracing： 跟流量成正比（logging and tracing generally increase volume with traffic） 为了减少资源消耗 logging： 过滤掉不必要的 eventmetrics： 粗粒度（coarser grain）指标tracing： 采样（sampling），而非记录所有跟踪事件ps.adrian cole 的 ppt 说 metrics 还可以通过 read-your-writes 来减少记录资源消耗，不是很理解。 关于 logging日志可以细分为多种类型：Transaction logs关键事务日志，比如跟钱相关的。Request logshttp 请求、数据库请求等。通常尽可能要记录，但丢失一小部分也不是大不了的事情。Application logs应用进程本身的日志，比如启动信息、后台任务等。Debug logs针对某些场景非常详细的 debug 日志。小结 logging: 记录详细的事件，例如异常信息metrics: 发现趋势，发送告警tracing: 分布式服务调用中分析慢请求 参考Observability 3 ways: logging metrics and tracingMetrics, tracing, and loggingAwesome Observability]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wall time 和 monotonic time 简介]]></title>
    <url>%2Fp%2Flinux-wall-time-monotonic-time%2F</url>
    <content type="text"><![CDATA[背景 使用 micrometer 集成 Prometheus 的时候，发现对于时钟 micrometer 抽象了 wallTime 和monotonicTime，做下笔记。micrometer 的 Clock 接口如下。123456789101112131415161718192021222324252627282930313233343536373839/** * Used to measure absolute and relative time. * * @see MockClock for a clock that can be manually advanced for use in tests. * @author Jon Schneider */public interface Clock &#123; Clock SYSTEM = new Clock() &#123; @Override public long wallTime() &#123; return System.currentTimeMillis(); &#125; @Override public long monotonicTime() &#123; return System.nanoTime(); &#125; &#125;; /** * Current wall time in milliseconds since the epoch. Typically equivalent to * System.currentTimeMillis. Should not be used to determine durations. Used * for timestamping metrics being pushed to a monitoring system or for determination * of step boundaries (e.g. &#123;@link StepLong&#125;. * * @return Wall time in milliseconds */ long wallTime(); /** * Current time from a monotonic clock source. The value is only meaningful when compared with * another snapshot to determine the elapsed time for an operation. The difference between two * samples will have a unit of nanoseconds. The returned value is typically equivalent to * System.nanoTime. * * @return Monotonic time in nanoseconds */ long monotonicTime();&#125;wall time字面意义是墙上的时间，是真实时间。又叫 real time。对应 java api：1System.currentTimeMillis()在 linux 内核中，使用 xtime 内存变量维护 wall time。wall time 会收到系统时间调整的影响，例如 ntp。monotonic timemonotonic，单调的。是系统启动以后流逝的时间（相对时间）。对应 Java api：1System.nanoTime()javadoc 上的说明 此方法只能用于测量已过的时间，与系统或钟表时间的其他任何时间概念无关。返回值表示从某一固定但任意的时间算起的毫微秒数（或许从以后算起，所以该值可能为负）。此方法提供毫微秒的精度，但不是必要的毫微秒的准确度。它对于值的更改频率没有作出保证。在取值范围大于约 292 年（263 毫微秒）的连续调用的不同点在于：由于数字溢出，将无法准确计算已过的时间。public static native long nanoTime();由于可能存在数字溢出，比较两个 nanoTime 数值，应该使用相减的方式：1234long t0 = System.nanoTime();long t1 = System.nanoTime();// one should use &#123;@code t1 - t0 &lt; 0&#125;, not &#123;@code t1 &lt; t0&#125;,// because of the possibility of numerical overflow.linux 内核中，使用 jiffies 变量存储 monotonic time。每次 timer 中断，jiffies 增加一次。The original kernel timer system (called the “timer wheel) was based on incrementing a kernel-internal value (jiffies) every timer interrupt.The timer interrupt becomes the default scheduling quantum, and all other timers are based on jiffies.The timer interrupt rate (and jiffy increment rate) is defined by a compile-time constant called HZ.ps.123jiffy英 [&apos;dʒɪfi] 美 [&apos;dʒɪfi] n. 瞬间；一会儿 系统休眠时，monotonic time 不会递增。在 linux 中，还有 raw monotonic time，不受 ntp 调节影响。另外还有 boot time，会累加上系统休眠的时间，它代表着系统上电后的总时间。参考Kernel Timer SystemsLinux 时间子系统之三：时间的维护者：timekeeper]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus 基本概念]]></title>
    <url>%2Fp%2Fprometheus-basic-concepts%2F</url>
    <content type="text"><![CDATA[前言 prometheus 采集的数据都以“时序数据”形式存储。 采样数据（sample）包含：一个 float64 精度的数值 一个毫秒精度的时间戳 接下来了解 Prometheus 的数据模型。metrics &amp; labelmetric 是一个时间序列的唯一标识。一个 metric 可以有多个可选的 label 属性。1&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;命名满足正则模式：[a-zA-Z_:][a-zA-Z0-9_:]*。metric 代表一个通用的统计概念，例如 http_requests_total。 label 是同一个 metric 下面不同维度（dimension）的实例。例入 http 请求统计里面的get。 一些命名规范：12345678910# 使用有意义的前缀代表一系列相关概念的 metric，例如 http_ http_requests_total (for a unit-less accumulating count)# 单位使用复数，使用基础计量单位 http_request_duration_seconds (for all HTTP requests)# 汇总统计使用 _total 后缀process_cpu_seconds_total (exported by many client libraries)# label 定义api_http_requests_total - differentiate request types: operation=&quot;create|update|delete&quot; 下面是一些 springboot acturator 集成 Prometheus 暴露的 metrics：12345678910111213# HELP jvm_threads_states_threads The current number of threads having NEW state# TYPE jvm_threads_states_threads gaugejvm_threads_states_threads&#123;application="springboot_prometheus",state="blocked",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="waiting",&#125; 12.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="timed-waiting",&#125; 7.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="runnable",&#125; 10.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="new",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="terminated",&#125; 0.0# HELP http_server_requests_seconds # TYPE http_server_requests_seconds summaryhttp_server_requests_seconds_count&#123;application="springboot_prometheus",exception="None",method="GET",outcome="CLIENT_ERROR",status="404",uri="/**",&#125; 3.0http_server_requests_seconds_sum&#123;application="springboot_prometheus",exception="None",method="GET",outcome="CLIENT_ERROR",status="404",uri="/**",&#125;基本的 metricscounter一种累加的 metric，典型的应用如：请求的个数，结束的任务数，出现的错误数等等。只能累加，不能减少。可以在重启后 reset。123# HELP jvm_classes_unloaded_classes_total The total number of classes unloaded since the Java virtual machine has started execution# TYPE jvm_classes_unloaded_classes_total counterjvm_classes_unloaded_classes_total&#123;application="springboot_prometheus",&#125; 1.0gauge一种常规的 metric，典型的应用如：温度，运行的 jvm 线程的个数。可以任意加减。12345678# HELP jvm_threads_states_threads The current number of threads having NEW state# TYPE jvm_threads_states_threads gaugejvm_threads_states_threads&#123;application="springboot_prometheus",state="blocked",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="waiting",&#125; 12.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="timed-waiting",&#125; 7.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="runnable",&#125; 10.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="new",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="terminated",&#125; 0.0histogramA histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values.Histogram 会在一段时间范围内对数据进行采样（通常是请求持续时间或响应大小等），并将其计入可配置的存储桶（bucket）中。可以对观察结果采样，分组及统计。Histogram 在 Prometheus 系统中的查询语言中，有三种作用：对每个采样点进行统计，打到各个分类值中 (bucket) 对每个采样点值累计和 (sum) 对采样点的次数累计和 (count) 度量指标名称: [basename]的柱状图, 上面三类的作用度量指标名称：[basename]_bucket{le=&quot; 上边界 &quot;}, 这个值为小于等于上边界的所有采样点数量 [basename]_sum[basename]_count 下面是 Prometheus 自带的监控数据 12345678910111213# HELP prometheus_http_response_size_bytes Histogram of response size for HTTP requests.# TYPE prometheus_http_response_size_bytes histogramprometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="100"&#125; 0prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="1000"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="10000"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="100000"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="1e+06"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="1e+07"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="1e+08"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="1e+09"&#125; 2prometheus_http_response_size_bytes_bucket&#123;handler="/api/v1/query",le="+Inf"&#125; 2prometheus_http_response_size_bytes_sum&#123;handler="/api/v1/query"&#125; 207prometheus_http_response_size_bytes_count&#123;handler="/api/v1/query"&#125; 2 一般的直方图，是在区间中划分多个桶，彼此独立。但是 Prometheus 的直方图是累加的（cumulative），即后面一个 bucket 包含前面所有 bucket 的样本。这样做的好处是，在抓取指标时就可以根据需要丢弃某些 bucket，这样可以在降低 Prometheus 维护成本的同时，还可以粗略计算样本值的分位数。可以使用 relabel 丢弃不需要的 bucket。可以丢弃任意的 bucket，但不能丢弃 le=&quot;+Inf&quot; 的 bucket，因为 histogram_quantile 函数需要使用这个标签。Histogram 的统计精度受限于 bucket 数量。bucket 越多，精度越高，但是数据增长很快。summarySummary 是采样点分位图统计，用于得到 数据的分布情况 。 首先要了解几个统计学的概念。分位数 表示了在这个样本集中从小至大排列之后小于某值的样本子集占总样本集的比例 百分位 (来自百度百科)用 99 个数值或 99 个点，将按大小顺序排列的观测值划分为 100 个等分，则这 99 个数值或 99 个点就称为百分位数，分别以 Pl，P2，…，P99 代表第 1 个，第 2 个，…，第 99 个百分位数。第 j 个百分位数 j=1,2…100。式中 Lj，fj 和 CFj 分别是第 j 个百分位数所在组的下限值、频数和该组以前的累积频数，Σf 是观测值的数目。百分位通常用第几百分位来表示，如第五百分位，它表示在所有测量数据中，测量值的累计频次达 5%。以身高为例，身高分布的第五百分位表示有 5% 的人的身高小于此测量值，95% 的身高大于此测量值。第 25 百分位数又称第一个四分位数（First Quartile），用 Q1 表示；第 50 百分位数又称第二个四分位数（Second Quartile），用 Q2 表示；第 75 百分位数又称第三个四分位数（Third Quartile）, 用 Q3 表示。若求得第 p 百分位数为小数，可完整为整数。分位数是用于衡量数据的位置的量度，但它所衡量的，不一定是中心位置。百分位数提供了有关各数据项如何在最小值与最大值之间分布的信息。第 p 百分位数是这样一个值，它使得至少有 p% 的数据项小于或等于这个值，且至少有 (100-p)% 的数据项大于或等于这个值。 百分位和数据分位。Summary 是 在客户端直接聚合生成的百分位数 。 带有度量指标的 [basename] 的 summary 在抓取时间序列数据展示。观察时间的φ-quantiles (0 ≤ φ ≤ 1), 显示为 [basename]{分位数 =”[φ]”}[basename]_sum， 是指所有观察值的总和[basename]_count, 是指已观察到的事件计数值Prometheus 自带的 summary 监控：123456789# HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile="0"&#125; 0go_gc_duration_seconds&#123;quantile="0.25"&#125; 0go_gc_duration_seconds&#123;quantile="0.5"&#125; 0go_gc_duration_seconds&#123;quantile="0.75"&#125; 0go_gc_duration_seconds&#123;quantile="1"&#125; 0.001996go_gc_duration_seconds_sum 0.0039907go_gc_duration_seconds_count 48 设置 quantile={0.5: 0.05, 0.9: 0.01, 0.99: 0.001}。每个 quantile 后面还有一个数，0.5-quantile 后面是 0.05，0.9-quantile 后面是 0.01，而 0.99 后面是 0.001。这些是我们设置的能容忍的误差。histogram vs summary从官网复制（https://prometheus.io/docs/practices/histograms/）HistogramSummaryRequired configurationPick buckets suitable for the expected range of observed values.Pick desired φ-quantiles and sliding window. Other φ-quantiles and sliding windows cannot be calculated later.Client performanceObservations are very cheap as they only need to increment counters.Observations are expensive due to the streaming quantile calculation.Server performanceThe server has to calculate quantiles. You can use recording rules should the ad-hoc calculation take too long (e.g. in a large dashboard).Low server-side cost.Number of time series (in addition to the _sum and _count series)One time series per configured bucket.One time series per configured quantile.Quantile error (see below for details)Error is limited in the dimension of observed values by the width of the relevant bucket.Error is limited in the dimension of φ by a configurable value.Specification of φ-quantile and sliding time-windowAd-hoc with Prometheus expressions.Preconfigured by the client.AggregationAd-hoc with Prometheus expressions.In general not aggregatable.划重点：histogram 是 server 端计算。summary 是 client 端计算。histogram 可以灵活设置φ （因为是动态计算丫）。summary 一旦设置不可以修改。histogram 可以聚合 (histogram_quantile)。summary 不可以聚合（聚合不同客户端上报的百分数没有意义）histogram 的精度受限于 bucket 数量。bucket 越多，消耗越大存储空间。 如何选择：If you need to aggregate, choose histograms.Otherwise, choose a histogram if you have an idea of the range and distribution of values that will be observed. Choose a summary if you need an accurate quantile, no matter what the range and distribution of the values is.instance 和 jobinstance: 一个单独 scrape 的目标， 一般对应于一个进程。jobs: 一组同种类型的 instances（主要用于保证可扩展性和可靠性）当 scrape 目标时，Prometheus 会自动给这个 scrape 的时间序列附加一些标签以便更好的分别，例如： instance，job。job: The configured job name that the target belongs to.instance: The &lt;host&gt;:&lt;port&gt; part of the target’s URL that was scraped.对于每个 scrape 的 instance，Prometheus 会自动增加采样到以下的时间序列：up{job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;}: 1 if the instance is healthy, i.e. reachable, or 0 if the scrape failed.scrape_duration_seconds{job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;}: duration of the scrape.scrape_samples_post_metric_relabeling{job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;}: the number of samples remaining after metric relabeling was applied.scrape_samples_scraped{job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;}: the number of samples the target exposed.scrape_series_added{job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;}: the approximate number of new series in this scrape. New in v2.10小结 Histogram 也能计算百分位数但精度受分桶影响很大，分桶少的话会使百分位数计算很不准确，而分桶多的话会使数据量成倍增加。Summary 则是依靠原始数据计算出的百分位数，是很准确的值。 但是平时一般不用 Summary，因为它无法聚合。想象一下，prometheus 抓取了一个集群下多台机器的百分位数，我们怎么根据这些数据得到整个集群的百分位数呢？参考 METRIC TYPES 一文搞懂 Prometheus 的直方图 分位数 (quantile)metrics 类型 一篇文章带你理解和使用 Prometheus 的指标Prometheus 入门与实践]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot 集成 prometheus 监控]]></title>
    <url>%2Fp%2Fprometheus-springboot-integration%2F</url>
    <content type="text"><![CDATA[springboot 集成 prometheus 监控 pom.xml 引入 acturator 和 prometheus。这里使用 springboot 2.2.4。12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.3.3&lt;/version&gt;&lt;/dependency&gt;隐式导入 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt; &lt;version&gt;1.3.3&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_common&lt;/artifactId&gt; &lt;version&gt;0.7.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;application.yml 打开 endpoints 和 metrics：123456789management: endpoints: web: exposure: include: "*" metrics: tags: application: $&#123;spring.application.name&#125;java config12345@BeanMeterRegistryCustomizer&lt;MeterRegistry&gt; configurer( @Value("$&#123;spring.application.name&#125;") String applicationName) &#123; return (registry) -&gt; registry.config().commonTags("application", applicationName);&#125;打开 http://localhost:8080/actuator/prometheus123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# HELP jvm_threads_states_threads The current number of threads having NEW state# TYPE jvm_threads_states_threads gaugejvm_threads_states_threads&#123;application="springboot_prometheus",state="blocked",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="waiting",&#125; 12.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="timed-waiting",&#125; 7.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="runnable",&#125; 10.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="new",&#125; 0.0jvm_threads_states_threads&#123;application="springboot_prometheus",state="terminated",&#125; 0.0# HELP system_cpu_usage The "recent cpu usage" for the whole system# TYPE system_cpu_usage gaugesystem_cpu_usage&#123;application="springboot_prometheus",&#125; 0.0# HELP tomcat_sessions_active_current_sessions # TYPE tomcat_sessions_active_current_sessions gaugetomcat_sessions_active_current_sessions&#123;application="springboot_prometheus",&#125; 0.0# HELP process_start_time_seconds Start time of the process since unix epoch.# TYPE process_start_time_seconds gaugeprocess_start_time_seconds&#123;application="springboot_prometheus",&#125; 1.579920932014E9# HELP tomcat_sessions_expired_sessions_total # TYPE tomcat_sessions_expired_sessions_total countertomcat_sessions_expired_sessions_total&#123;application="springboot_prometheus",&#125; 0.0# HELP process_cpu_usage The "recent cpu usage" for the Java Virtual Machine process# TYPE process_cpu_usage gaugeprocess_cpu_usage&#123;application="springboot_prometheus",&#125; 0.10879525186187843# HELP jvm_gc_pause_seconds Time spent in GC pause# TYPE jvm_gc_pause_seconds summaryjvm_gc_pause_seconds_count&#123;action="end of major GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 1.0jvm_gc_pause_seconds_sum&#123;action="end of major GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 0.039jvm_gc_pause_seconds_count&#123;action="end of minor GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 1.0jvm_gc_pause_seconds_sum&#123;action="end of minor GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 0.006jvm_gc_pause_seconds_count&#123;action="end of minor GC",application="springboot_prometheus",cause="Allocation Failure",&#125; 1.0jvm_gc_pause_seconds_sum&#123;action="end of minor GC",application="springboot_prometheus",cause="Allocation Failure",&#125; 0.011# HELP jvm_gc_pause_seconds_max Time spent in GC pause# TYPE jvm_gc_pause_seconds_max gaugejvm_gc_pause_seconds_max&#123;action="end of major GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 0.039jvm_gc_pause_seconds_max&#123;action="end of minor GC",application="springboot_prometheus",cause="Metadata GC Threshold",&#125; 0.006jvm_gc_pause_seconds_max&#123;action="end of minor GC",application="springboot_prometheus",cause="Allocation Failure",&#125; 0.011# HELP jvm_buffer_total_capacity_bytes An estimate of the total capacity of the buffers in this pool# TYPE jvm_buffer_total_capacity_bytes gaugejvm_buffer_total_capacity_bytes&#123;application="springboot_prometheus",id="direct",&#125; 57344.0jvm_buffer_total_capacity_bytes&#123;application="springboot_prometheus",id="mapped",&#125; 0.0# HELP jvm_memory_max_bytes The maximum amount of memory in bytes that can be used for memory management# TYPE jvm_memory_max_bytes gaugejvm_memory_max_bytes&#123;application="springboot_prometheus",area="heap",id="PS Survivor Space",&#125; 1.6777216E7jvm_memory_max_bytes&#123;application="springboot_prometheus",area="heap",id="PS Old Gen",&#125; 2.82591232E9jvm_memory_max_bytes&#123;application="springboot_prometheus",area="nonheap",id="Code Cache",&#125; 2.5165824E8jvm_memory_max_bytes&#123;application="springboot_prometheus",area="heap",id="PS Eden Space",&#125; 1.379926016E9jvm_memory_max_bytes&#123;application="springboot_prometheus",area="nonheap",id="Compressed Class Space",&#125; 1.073741824E9jvm_memory_max_bytes&#123;application="springboot_prometheus",area="nonheap",id="Metaspace",&#125; -1.0# HELP jvm_buffer_count_buffers An estimate of the number of buffers in the pool# TYPE jvm_buffer_count_buffers gaugejvm_buffer_count_buffers&#123;application="springboot_prometheus",id="direct",&#125; 7.0jvm_buffer_count_buffers&#123;application="springboot_prometheus",id="mapped",&#125; 0.0# HELP jvm_threads_peak_threads The peak live thread count since the Java virtual machine started or peak was reset# TYPE jvm_threads_peak_threads gaugejvm_threads_peak_threads&#123;application="springboot_prometheus",&#125; 30.0# HELP jvm_threads_daemon_threads The current number of live daemon threads# TYPE jvm_threads_daemon_threads gaugejvm_threads_daemon_threads&#123;application="springboot_prometheus",&#125; 25.0# HELP tomcat_sessions_rejected_sessions_total # TYPE tomcat_sessions_rejected_sessions_total countertomcat_sessions_rejected_sessions_total&#123;application="springboot_prometheus",&#125; 0.0# HELP jvm_classes_unloaded_classes_total The total number of classes unloaded since the Java virtual machine has started execution# TYPE jvm_classes_unloaded_classes_total counterjvm_classes_unloaded_classes_total&#123;application="springboot_prometheus",&#125; 1.0# HELP jvm_threads_live_threads The current number of live threads including both daemon and non-daemon threads# TYPE jvm_threads_live_threads gaugejvm_threads_live_threads&#123;application="springboot_prometheus",&#125; 29.0# HELP jvm_classes_loaded_classes The number of classes that are currently loaded in the Java virtual machine# TYPE jvm_classes_loaded_classes gaugejvm_classes_loaded_classes&#123;application="springboot_prometheus",&#125; 7733.0# HELP tomcat_sessions_alive_max_seconds # TYPE tomcat_sessions_alive_max_seconds gaugetomcat_sessions_alive_max_seconds&#123;application="springboot_prometheus",&#125; 0.0# HELP jvm_memory_committed_bytes The amount of memory in bytes that is committed for the Java virtual machine to use# TYPE jvm_memory_committed_bytes gaugejvm_memory_committed_bytes&#123;application="springboot_prometheus",area="heap",id="PS Survivor Space",&#125; 1.6777216E7jvm_memory_committed_bytes&#123;application="springboot_prometheus",area="heap",id="PS Old Gen",&#125; 1.09051904E8jvm_memory_committed_bytes&#123;application="springboot_prometheus",area="nonheap",id="Code Cache",&#125; 7340032.0jvm_memory_committed_bytes&#123;application="springboot_prometheus",area="heap",id="PS Eden Space",&#125; 2.31211008E8jvm_memory_committed_bytes&#123;application="springboot_prometheus",area="nonheap",id="Compressed Class Space",&#125; 5767168.0jvm_memory_committed_bytes&#123;application="springboot_prometheus",area="nonheap",id="Metaspace",&#125; 4.0108032E7# HELP jvm_gc_memory_promoted_bytes_total Count of positive increases in the size of the old generation memory pool before GC to after GC# TYPE jvm_gc_memory_promoted_bytes_total counterjvm_gc_memory_promoted_bytes_total&#123;application="springboot_prometheus",&#125; 8487008.0# HELP logback_events_total Number of error level events that made it to the logs# TYPE logback_events_total counterlogback_events_total&#123;application="springboot_prometheus",level="error",&#125; 0.0logback_events_total&#123;application="springboot_prometheus",level="debug",&#125; 0.0logback_events_total&#123;application="springboot_prometheus",level="trace",&#125; 0.0logback_events_total&#123;application="springboot_prometheus",level="warn",&#125; 0.0logback_events_total&#123;application="springboot_prometheus",level="info",&#125; 8.0# HELP system_cpu_count The number of processors available to the Java virtual machine# TYPE system_cpu_count gaugesystem_cpu_count&#123;application="springboot_prometheus",&#125; 8.0# HELP http_server_requests_seconds # TYPE http_server_requests_seconds summaryhttp_server_requests_seconds_count&#123;application="springboot_prometheus",exception="None",method="GET",outcome="CLIENT_ERROR",status="404",uri="/**",&#125; 3.0http_server_requests_seconds_sum&#123;application="springboot_prometheus",exception="None",method="GET",outcome="CLIENT_ERROR",status="404",uri="/**",&#125; 0.0249087# HELP http_server_requests_seconds_max # TYPE http_server_requests_seconds_max gaugehttp_server_requests_seconds_max&#123;application="springboot_prometheus",exception="None",method="GET",outcome="CLIENT_ERROR",status="404",uri="/**",&#125; 0.0173263# HELP jvm_gc_memory_allocated_bytes_total Incremented for an increase in the size of the young generation memory pool after one GC to before the next# TYPE jvm_gc_memory_allocated_bytes_total counterjvm_gc_memory_allocated_bytes_total&#123;application="springboot_prometheus",&#125; 1.43200936E8# HELP jvm_memory_used_bytes The amount of used memory# TYPE jvm_memory_used_bytes gaugejvm_memory_used_bytes&#123;application="springboot_prometheus",area="heap",id="PS Survivor Space",&#125; 0.0jvm_memory_used_bytes&#123;application="springboot_prometheus",area="heap",id="PS Old Gen",&#125; 1.3831144E7jvm_memory_used_bytes&#123;application="springboot_prometheus",area="nonheap",id="Code Cache",&#125; 7314112.0jvm_memory_used_bytes&#123;application="springboot_prometheus",area="heap",id="PS Eden Space",&#125; 6.7909056E7jvm_memory_used_bytes&#123;application="springboot_prometheus",area="nonheap",id="Compressed Class Space",&#125; 5190968.0jvm_memory_used_bytes&#123;application="springboot_prometheus",area="nonheap",id="Metaspace",&#125; 3.7164888E7# HELP jvm_gc_max_data_size_bytes Max size of old generation memory pool# TYPE jvm_gc_max_data_size_bytes gaugejvm_gc_max_data_size_bytes&#123;application="springboot_prometheus",&#125; 2.82591232E9# HELP process_uptime_seconds The uptime of the Java virtual machine# TYPE process_uptime_seconds gaugeprocess_uptime_seconds&#123;application="springboot_prometheus",&#125; 64.781# HELP jvm_gc_live_data_size_bytes Size of old generation memory pool after a full GC# TYPE jvm_gc_live_data_size_bytes gaugejvm_gc_live_data_size_bytes&#123;application="springboot_prometheus",&#125; 1.3831144E7# HELP tomcat_sessions_created_sessions_total # TYPE tomcat_sessions_created_sessions_total countertomcat_sessions_created_sessions_total&#123;application="springboot_prometheus",&#125; 0.0# HELP jvm_buffer_memory_used_bytes An estimate of the memory that the Java virtual machine is using for this buffer pool# TYPE jvm_buffer_memory_used_bytes gaugejvm_buffer_memory_used_bytes&#123;application="springboot_prometheus",id="direct",&#125; 57344.0jvm_buffer_memory_used_bytes&#123;application="springboot_prometheus",id="mapped",&#125; 0.0# HELP tomcat_sessions_active_max_sessions # TYPE tomcat_sessions_active_max_sessions gaugetomcat_sessions_active_max_sessions&#123;application="springboot_prometheus",&#125; 0.0 更新 Prometheus 配置文件，增加 job1234567891011121314151617181920212223242526272829303132333435# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - "first_rules.yml" # - "second_rules.yml"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to'/metrics' # scheme defaults to'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'springboot_prometheus' scrape_interval: 5s metrics_path: '/actuator/prometheus' static_configs: - targets: ['127.0.0.1:8080']打开 localhost:9090，status=》targetPrometheus metrics 自动配置PrometheusMetricsExportAutoConfiguration 自动配置 prometheus metrics。123456789101112package org.springframework.boot.actuate.autoconfigure.metrics.export.prometheus;@Configuration(proxyBeanMethods = false)@AutoConfigureBefore(&#123; CompositeMeterRegistryAutoConfiguration.class, SimpleMetricsExportAutoConfiguration.class &#125;)@AutoConfigureAfter(MetricsAutoConfiguration.class)@ConditionalOnBean(Clock.class)@ConditionalOnClass(PrometheusMeterRegistry.class)@ConditionalOnProperty(prefix = "management.metrics.export.prometheus", name = "enabled", havingValue = "true", matchIfMissing = true)@EnableConfigurationProperties(PrometheusProperties.class)public class PrometheusMetricsExportAutoConfiguration &#123;&#125;很简单，不展开了。micrometer metric typesmicrometer 定义的 metric 类型在 io.micrometer.core.instrument.Meter12345678enum Type &#123; COUNTER, GAUGE, LONG_TASK_TIMER, TIMER, DISTRIBUTION_SUMMARY, OTHER;&#125; 原生 Prometheus client 定义 metric 类型在 io.prometheus.client.Collector1234567public enum Type &#123; COUNTER, GAUGE, SUMMARY, HISTOGRAM, UNTYPED,&#125; 二者的类型不是一一对应。micrometer 的 metric 类型和 Prometheus 在 PrometheusMeterRegistry 中进行映射：1234567891011121314151617181920protected Meter newMeter(Meter.Id id, Meter.Type type, Iterable&lt;Measurement&gt; measurements) &#123; Collector.Type promType = Collector.Type.UNTYPED; switch (type) &#123; case COUNTER: promType = Collector.Type.COUNTER; break; case GAUGE: promType = Collector.Type.GAUGE; break; case DISTRIBUTION_SUMMARY: case TIMER: promType = Collector.Type.SUMMARY; break; &#125; MicrometerCollector collector = collectorByName(id); List&lt;String&gt; tagValues = tagValues(id); // more codes&#125;注意，micrometer 的 timer 对应 Prometheus 的 histogram，但是映射为 Prometheus 的 summary 类型。]]></content>
      <categories>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>监控</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis keyspace 消息通知]]></title>
    <url>%2Fp%2Fredis-keyspace%2F</url>
    <content type="text"><![CDATA[前言 redis 提供了监听 key 变更（新增、修改、删除等）通知的能力。通过订阅（subscribe）键空间主题即可。 对应有 2 个主题：__keyspace@database__:KeyPattern__keyevent@database__:OpsType对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件消息：keyspace 和 keyevent。以 keyspace 为前缀的频道被称为键空间通知（key-space notification）， 而以 keyevent 为前缀的频道则被称为键事件通知（key-event notification）。因为 Redis 目前的订阅与发布功能采取的是发送即忘（fire and forget）策略， 所以如果你的程序需要可靠事件通知（reliable notification of events）， 那么目前的键空间通知可能并不适合你：当订阅事件的客户端断线时， 它会丢失所有在断线期间分发给它的事件。并不能确保消息送达。开启配置 键空间通知会带来额外的性能消耗，因此默认是关闭状态。为了开启键空间通知，需要修改配置文件：1notify-keyspace-events &quot;KEA&quot;具体选项有：选项 详情 KKeyspace events, published with keyspace@prefix.EKeyevent events, published with keyevent@prefix.gGeneric commands (non-type specific) like DEL, EXPIRE, RENAME, …$String commandslList commandssSet commandshHash commandszSorted set commandstStream commandsxExpired events (events generated every time a key expires)eEvicted events (events generated when a key is evicted for maxmemory)AAlias for g$lshztxe, so that the “AKE” string means all the events.K、E 必须至少填写一个，否则不生效。 另外，选项要带有双引号，否则不生效。订阅键空间通知 subscribe 或者 psubscribe (支持模式匹配) 订阅对应话题。123456789101112131415161718192021127.0.0.1:6379&gt; subscribe __keyspace@0__:xxReading messages... (press Ctrl-C to quit)// 执行订阅命令1) &quot;subscribe&quot;2) &quot;__keyspace@0__:xx&quot;3) (integer) 1// 以下操作在另一个窗口进行// set xx 11) &quot;message&quot;2) &quot;__keyspace@0__:xx&quot;3) &quot;set&quot;// expire xx 31) &quot;message&quot;2) &quot;__keyspace@0__:xx&quot;3) &quot;expire&quot;1) &quot;message&quot;2) &quot;__keyspace@0__:xx&quot;3) &quot;expired&quot; 参考Redis Keyspace NotificationsRedis 事件通知（keyspace &amp; keyevent notification）]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis unlink 释放大 key]]></title>
    <url>%2Fp%2Fredis-unlink%2F</url>
    <content type="text"><![CDATA[背景 redis 单线程模型设计简单、高效。但是一个长耗时操作就会拖住 redis 服务器性能，在操作完成之前不能处理其他命令请求。 典型的耗时操作是删除大 key。当被删除的 key 是 list、set、sorted set 或 hash 类型时，时间复杂度为 O(M)，其中 M 是 key 中包含的元素的个数。为了优化 redis 删除大 key 性能，redis 4.0 提供了 unlink 命令。以下代码基于 redis 5.0。delete源码分析 原来的 delete 命令使用 dbSyncDelete。见db.c123456789101112/* Delete a key, value, and associated expiration entry if any, from the DB */int dbSyncDelete(redisDb *db, robj *key) &#123; /* Deleting an entry from the expires dict will not free the sds of * the key, because it is shared with the main dictionary. */ if (dictSize(db-&gt;expires) &gt; 0) dictDelete(db-&gt;expires,key-&gt;ptr); if (dictDelete(db-&gt;dict,key-&gt;ptr) == DICT_OK) &#123; if (server.cluster_enabled) slotToKeyDel(key); return 1; &#125; else &#123; return 0; &#125;&#125; 底层依赖 dict.c 的dictDelete()。当 list、set、zset、hash 类型包含的元素很多，同步的循环删除会耗费大量时间。123456789101112131415161718192021222324252627282930/* Search and remove an element */static int dictDelete(dict *ht, const void *key) &#123; unsigned int h; dictEntry *de, *prevde; if (ht-&gt;size == 0) return DICT_ERR; h = dictHashKey(ht, key) &amp; ht-&gt;sizemask; de = ht-&gt;table[h]; prevde = NULL; while(de) &#123; if (dictCompareHashKeys(ht,key,de-&gt;key)) &#123; /* Unlink the element from the list */ if (prevde) prevde-&gt;next = de-&gt;next; else ht-&gt;table[h] = de-&gt;next; dictFreeEntryKey(ht,de); dictFreeEntryVal(ht,de); free(de); ht-&gt;used--; return DICT_OK; &#125; prevde = de; de = de-&gt;next; &#125; return DICT_ERR; /* not found */&#125;redis 4 之前删除大 key因为一次同步删除大 key 会造成长时间阻塞，所以要分批删除，每次删除一小部分，减少单次阻塞时间。unlinkredis 4.0 unlink 的思路是：仅将 keys 从 keyspace 元数据中删除，真正的删除会在后续异步操作。unlink 的核心操作见：lazyfree.c中的 dbAsyncDelete()。 当 entry 数量很少，异步回收速度不如同步回收快。因此当 entry 数量大于 LAZYFREE_THRESHOLD 才触发异步回收。异步回收的原理是创建后台任务，并且把 val 链表添加进去（bioCreateBackgroundJob）123456789101112131415161718192021222324252627282930313233343536373839#define LAZYFREE_THRESHOLD 64int dbAsyncDelete(redisDb *db, robj *key) &#123; /* Deleting an entry from the expires dict will not free the sds of * the key, because it is shared with the main dictionary. */ if (dictSize(db-&gt;expires) &gt; 0) dictDelete(db-&gt;expires,key-&gt;ptr); /* If the value is composed of a few allocations, to free in a lazy way * is actually just slower... So under a certain limit we just free * the object synchronously. */ dictEntry *de = dictUnlink(db-&gt;dict,key-&gt;ptr); if (de) &#123; robj *val = dictGetVal(de); size_t free_effort = lazyfreeGetFreeEffort(val); /* If releasing the object is too much work, do it in the background * by adding the object to the lazy free list. * Note that if the object is shared, to reclaim it now it is not * possible. This rarely happens, however sometimes the implementation * of parts of the Redis core may call incrRefCount() to protect * objects, and then call dbDelete(). In this case we'll fall * through and reach the dictFreeUnlinkedEntry() call, that will be * equivalent to just calling decrRefCount(). */ if (free_effort &gt; LAZYFREE_THRESHOLD &amp;&amp; val-&gt;refcount == 1) &#123; atomicIncr(lazyfree_objects,1); bioCreateBackgroundJob(BIO_LAZY_FREE,val,NULL,NULL); dictSetVal(db-&gt;dict,de,NULL); &#125; &#125; /* Release the key-val pair, or just the key if we set the val * field to NULL in order to lazy free it later. */ if (de) &#123; dictFreeUnlinkedEntry(db-&gt;dict,de); if (server.cluster_enabled) slotToKeyDel(key); return 1; &#125; else &#123; return 0; &#125;&#125;bio.c 创建后台任务，使用 mutext lock 保证对链表的线程安全操作。12345678910111213void bioCreateBackgroundJob(int type, void *arg1, void *arg2, void *arg3) &#123; struct bio_job *job = zmalloc(sizeof(*job)); job-&gt;time = time(NULL); job-&gt;arg1 = arg1; job-&gt;arg2 = arg2; job-&gt;arg3 = arg3; pthread_mutex_lock(&amp;bio_mutex[type]); listAddNodeTail(bio_jobs[type],job); bio_pending[type]++; pthread_cond_signal(&amp;bio_newjob_cond[type]); pthread_mutex_unlock(&amp;bio_mutex[type]);&#125;dict.c的 dictUnlink() 从 dict 中删除一个 key、但不进行底层 value 的释放。稍后使用 dictFreeUnlinkedEntry() 进行释放。123456789101112131415161718192021222324/* Remove an element from the table, but without actually releasing * the key, value and dictionary entry. The dictionary entry is returned * if the element was found (and unlinked from the table), and the user * should later call `dictFreeUnlinkedEntry()` with it in order to release it. * Otherwise if the key is not found, NULL is returned. * * This function is useful when we want to remove something from the hash * table but want to use its value before actually deleting the entry. * Without this function the pattern would require two lookups: * * entry = dictFind(...); * // Do something with entry * dictDelete(dictionary,entry); * * Thanks to this function it is possible to avoid this, and use * instead: * * entry = dictUnlink(dictionary,entry); * // Do something with entry * dictFreeUnlinkedEntry(entry); // &lt;- This does not need to lookup again. */dictEntry *dictUnlink(dict *ht, const void *key) &#123; return dictGenericDelete(ht,key,1);&#125;小结 key 的自然过期和手动删除，都会阻塞 Redis。 尽量从业务上避免 Redis 大 Key，无论从性能角度（hash 成本）还是过期删除成本角度，都会比较高。redis 4 之前删除大 key，应该使用分批删除方式，减少单次阻塞时间。reids 4 以后尽量使用 unlink 代替 delete 删除大 key。当元素少于 LAZYFREE_THRESHOLD(默认 64)，unlink 直接使用同步删除。 参考UNLINK key [key …]Is the UNLINK command always better than DEL command?]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yaml 笔记]]></title>
    <url>%2Fp%2Fyaml-note%2F</url>
    <content type="text"><![CDATA[最近修改 yaml 比较多，记录使用笔记。用冒号和空格表示键值对 key: valuekubernetes yaml 文件报错：error: yaml: line 2: mapping values are not allowed in this contextkey: value，注意在 value 和“:”之间要有一个空格。编码 springboot yaml 文件报错：1Caused by: org.yaml.snakeyaml.error.YAMLException: java.nio.charset.MalformedInputException: Input length = 1 指定配置文件编码不是 UTF-8 的，转换成 UTF-8 就行了。特殊符号 引号 简单数据（scalars，标量数据）可以不使用引号括起来，包括字符串数据。用单引号或者双引号括起来的被当作字符串数据，在单引号或双引号中使用 C 风格的转义字符 springboot yaml 报错1234567management: endpoints: web: exposure: include: * # 注意这里少了引号``` 报错：expected alphabetic or numeric character, but found(10)in ‘reader’, line 33, column 19:include: * ^```改为 &quot;*&quot; 即可。tabkubernetes yaml 报错：error: yaml: line 3: found character that cannot start any tokenYAML 文件里面不能出现 tab 键。]]></content>
      <categories>
        <category>yaml</category>
      </categories>
      <tags>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 支持 docker 的资源限制]]></title>
    <url>%2Fp%2Fjava-8-support-cgroups-docker-limit%2F</url>
    <content type="text"><![CDATA[背景 前 2 天开发环境挂了，原因是物理机部署了 kafka 进程，没有做资源限制，耗光 cpu 资源：cgroups 是 linux 的资源隔离方式，是 docker 的基础技术，可以限制 cpu、内存资源的使用。后来和运维同事聊了，应该使用 cgroups 限制资源，但是部署的 java 版本太老（java 8 u60），不支持。限制 jvm 使用内存 jvm 中使用-Xmx 限制 jvm 最大内存使用。在 docker 里，使用 -m 参数指定容器的内存限制。1docker run -m 1G -it openjdk:8u60如果使用旧版本 jvm，那么进入容器，发现（在一台 32G 的服务器上）123[root@bdgpu bin]# docker exec -it 8e9d40b2878d bashbash-4.4# java -XX:+PrintFlagsFinal -version | grep MaxHeapSize uintx MaxHeapSize := 32210157568 &#123;product&#125;在 java 8u131 之前，docker 中的 jvm 检测到的是宿主机的内存信息，它无法感知容器的资源上限，这样可能会导致意外的情况。java 8u131+ 可以通过开启实验性参数来实现 cgroups 限制：-XX:+UnlockExperimentalVMOptions-XX:+UseCGroupMemoryLimitForHeap。java11 废弃了 UseCGroupMemoryLimitForHeap，使用UseContainerSupport 代替：Remove deprecated experimental flag UseCGroupMemoryLimitForHeapUseContainerSupportjava 8u191 新增 -XX:+UseContainerSupport 参数，且默认开启：JDK 8u191 Update Release Notes。 该参数只在 linux 系统生效（因为是依赖 cgroups 嘛）。提供了 3 个新的内存参数，微调 docker 中 jvm 内存使用：-XX:InitialRAMPercentage-XX:MaxRAMPercentage-XX:MinRAMPercentage同时干掉 These options replace the deprecated Fraction forms (-XX:InitialRAMFraction, -XX:MaxRAMFraction, and -XX:MinRAMFraction). 具体效果参见：-XX:[+|-]UseContainerSupport，这里复制默认的内存限制。Container memory limitMaximum Java heap sizeContainer memory limitMaximum Java heap sizeLess than 1 GB50%Less than 1 GB50%1 GB - 2 GB- 512 MB1 GB - 2 GB- 512 MBGreater than 2 GB75%Greater than 2 GB75%限制 jvm 使用 cpu 资源 java 8u191 增加了-XX:ActiveProcessorCount=count，可以用来指定 cpu 的个数，覆盖原来 jvm 自动检测 cpu 个数。 这个 bug 记录讨论了相关问题：JDK-8189497 : Improve docker container detection and resource configuration usage。对应一个 CR，其中osContainer_linux.cpp 是重点。去 openjdk 搜索了此时最新源码：http://hg.openjdk.java.net/jdk/jdk/rev/931354c6323d12345678910bool OSContainer::_is_initialized = false;bool OSContainer::_is_containerized = false;CgroupSubsystem* cgroup_subsystem;// more codeint OSContainer::active_processor_count() &#123; assert(cgroup_subsystem != NULL, "cgroup subsystem not available"); return cgroup_subsystem-&gt;active_processor_count();&#125;由 cgroup_subsystem 提供实现。active_processor_count实现见 cgroupSubsystem_linux.cpp。 支持 cpuset 和 cpushare。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* active_processor_count * * Calculate an appropriate number of active processors for the * VM to use based on these three inputs. * * cpu affinity * cgroup cpu quota &amp; cpu period * cgroup cpu shares * * Algorithm: * * Determine the number of available CPUs from sched_getaffinity * * If user specified a quota (quota != -1), calculate the number of * required CPUs by dividing quota by period. * * If shares are in effect (shares != -1), calculate the number * of CPUs required for the shares by dividing the share value * by PER_CPU_SHARES. * * All results of division are rounded up to the next whole number. * * If neither shares or quotas have been specified, return the * number of active processors in the system. * * If both shares and quotas have been specified, the results are * based on the flag PreferContainerQuotaForCPUCount. If true, * return the quota value. If false return the smallest value * between shares or quotas. * * If shares and/or quotas have been specified, the resulting number * returned will never exceed the number of active processors. * * return: * number of CPUs */int CgroupSubsystem::active_processor_count() &#123; int quota_count = 0, share_count = 0; int cpu_count, limit_count; int result; // We use a cache with a timeout to avoid performing expensive // computations in the event this function is called frequently. // [See 8227006]. CachingCgroupController* contrl = cpu_controller(); CachedMetric* cpu_limit = contrl-&gt;metrics_cache(); if (!cpu_limit-&gt;should_check_metric()) &#123; int val = (int)cpu_limit-&gt;value(); log_trace(os, container)("CgroupSubsystem::active_processor_count (cached): %d", val); return val; &#125; cpu_count = limit_count = os::Linux::active_processor_count(); int quota = cpu_quota(); int period = cpu_period(); int share = cpu_shares(); if (quota &gt; -1 &amp;&amp; period &gt; 0) &#123; quota_count = ceilf((float)quota / (float)period); log_trace(os, container)("CPU Quota count based on quota/period: %d", quota_count); &#125; if (share &gt; -1) &#123; share_count = ceilf((float)share / (float)PER_CPU_SHARES); log_trace(os, container)("CPU Share count based on shares: %d", share_count); &#125; // If both shares and quotas are setup results depend // on flag PreferContainerQuotaForCPUCount. // If true, limit CPU count to quota // If false, use minimum of shares and quotas if (quota_count !=0 &amp;&amp; share_count != 0) &#123; if (PreferContainerQuotaForCPUCount) &#123; limit_count = quota_count; &#125; else &#123; limit_count = MIN2(quota_count, share_count); &#125; &#125; else if (quota_count != 0) &#123; limit_count = quota_count; &#125; else if (share_count != 0) &#123; limit_count = share_count; &#125; result = MIN2(cpu_count, limit_count); log_trace(os, container)("OSContainer::active_processor_count: %d", result); // Update cached metric to avoid re-reading container settings too often cpu_limit-&gt;set_value(result, OSCONTAINER_CACHE_TIMEOUT); return result;&#125; 小结docker 里的应用应该使用 java 8u191 以后 jvm 版本，支持容器 memory 和 cpu 限制。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>jvm</tag>
        <tag>docker</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka-consumer-groups.sh 排查问题]]></title>
    <url>%2Fp%2Fkafka-consumer-groups-script%2F</url>
    <content type="text"><![CDATA[背景 下游系统消费 kafka 消息数量和上游投递消息数量不一致。下游系统怀疑是 kafka 丢消息了（嗯，啥都不管，先让中间件背锅）。排查问题 生产者到 kafka，日志未见异常。重点排查 kafka 到下游消费者这条链路。消费者日志未见明显异常，接收到的消息都正常处理了。于是让消费者改了一下，打印消息 id 和 partition 消息，看下哪些消息没有消费到。结果发现有的 partition 一直消费不到。怀疑是这些 partition 被分配到一些消费者去了。用 kafka-consumer-groups.sh 这个官方工具看下消费者的 ip。（截图是后来补充的）最后发现，有其他环境的起了相同的 consumer group 的消费者，因为是相同的 cg，因此有的 partition 被意外的消费者消费了。。。12345678[root@bdgpu bin]# ./kafka-consumer-groups.sh --bootstrap-server xxx.xxx.xxx.xxx:9092 --group yyy --describeNote: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-IDreq_zzzzzzz 1 2 2 0 consumer-5-cc48fa17-21e6-4c3b-9fb6-d209682ef849 /172.25.xxx.xxx consumer-5req_xxxxxxx 1 0 0 0 consumer-5-cc48fa17-21e6-4c3b-9fb6-d209682ef849 /172.25.xxx.xxx consumer-5req_test 1 3 3 0 consumer-5-cc48fa17-21e6-4c3b-9fb6-d209682ef849 /172.25.xxx.xxx consumer-5注意这里有一行提示：This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers)老版本 consumer 的元数据信息是存储在 zookeeper 当中的，需要手动去 zookeeper 当中去修改偏移量。后来 consumer 的元数据信息存储在 consumer-offsets。因此工具多了这行提示。kafka-consumer-groups.sh另一个重要的用途是重置 consumer offset，可以参见这个文章：Kafka consumer group 位移重设 小结：相同 consumer group 下的消费者被指派不同的 partition 来消费]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[activiti 提示缺少 bpmn di 信息]]></title>
    <url>%2Fp%2Factiviti-522-no-bpmn-di%2F</url>
    <content type="text"><![CDATA[问题背景 activiti explorer 5.22 自带的例子，导入到 activiti explorer 发现报错，提示没有 bpmn di 信息： 查资料得知，BPMN DI 信息用于描述流程中每一个组件的 XY 坐标、宽度、高度。activiti 官方的例子只用于代码测试，因此省去了 bpmn di 信息。BpmnAutoLayout 转换 activiti 提供了 activiti-bpmn-layout 模块，可以自动生成 bpmndi 信息。pom 引入12345&lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-layout&lt;/artifactId&gt; &lt;version&gt;5.22.0&lt;/version&gt;&lt;/dependency&gt; 读取 bpmn xml 文件，然后转换为 BpmnModel，再用 BpmnAutoLayout 自动生成 bpmndi 信息。代码如下：1234567891011121314151617181920212223242526272829@Testpublic void testAutoLayout() &#123; InputStream in = TestActiviti.class.getResourceAsStream("/VacationRequest.bpmn20.xml"); BpmnXMLConverter converter = new BpmnXMLConverter(); XMLInputFactory factory = XMLInputFactory.newInstance(); XMLStreamReader reader = null; //createXmlStreamReader try &#123; reader = factory.createXMLStreamReader(in); // 将 xml 文件转换成 BpmnModel BpmnModel bpmnModel = converter.convertToBpmnModel(reader); // 自动生成 bpmndi 信息 new BpmnAutoLayout(bpmnModel).execute(); Deployment deployment = repositoryService.createDeployment() .addBpmnModel("dynamic-model.bpmn", bpmnModel).name("Dynamic process deployment") .deploy(); InputStream in2 = repositoryService.getResourceAsStream(deployment.getId(), "dynamic-model.bpmn"); FileCopyUtils.copy(in2, new FileOutputStream(new File("/VacationRequest2222.bpmn20.xml"))); &#125; catch (XMLStreamException | IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (XMLStreamException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 打开生成后的文件，多了 &lt;bpmndi:BPMNDiagram&gt; 节点：1234567891011121314&lt;bpmndi:BPMNDiagram id="BPMNDiagram_vacationRequest"&gt; &lt;bpmndi:BPMNPlane bpmnElement="vacationRequest" id="BPMNPlane_vacationRequest"&gt; &lt;bpmndi:BPMNShape bpmnElement="request" id="BPMNShape_request"&gt; &lt;omgdc:Bounds height="30.0" width="30.0" x="0.0" y="178.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;!-- more codes --&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow2" id="BPMNEdge_flow2"&gt; &lt;omgdi:waypoint x="180.0" y="180.5"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="192.0" y="180.5"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="192.0" y="134.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="230.0" y="134.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;!-- more codes --&gt;&lt;/bpmndi:BPMNDiagram&gt;这次可以在 activiti explorer 打开了，自动排版效果不咋的。activiti 6手头上有 activiti app（activiti 6 官方自带的 ui 应用）环境，于是最初的无 bpmndi 信息文件导进去试试，发现可以打开，效果和上图 activiti explorer 一样。]]></content>
      <categories>
        <category>activiti</category>
      </categories>
      <tags>
        <tag>activiti</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[activiti 5.22 explorer 配置问题记录]]></title>
    <url>%2Fp%2Factiviti-522-explorer-setup%2F</url>
    <content type="text"><![CDATA[基础配置 执行 datase/create 下面对应的 ddl。解压缩 activiti-explorer.war 到 tomcat 的 webapps 目录。从 mvnrepostiory 下载 mysql-connector-java 驱动包，复制到 activiti-explorer/WEB-INF/lib 进入 activiti-explorer/WEB-INF/classes 目录：db.properties: 修改 mysql 连接配置 activiti-custom-context.xml: 去掉注释 启动 tomcat。访问 /activiti-explorer。admin 账号： kermit / kermitLiquibase 表不能正常建立 点击“管理”，报错，后端日志：1234567Caused by: org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: java.sql.SQLSyntaxErrorException: Table &apos;activiti5.ACT_DMN_DATABASECHANGELOG&apos; doesn&apos;t exist### The error may exist in org/activiti/db/mapping/entity/TableData.xml### The error may involve org.activiti.engine.impl.TablePageMap.selectTableCount-Inline### The error occurred while setting parameters### SQL: select count(*) from ACT_DMN_DATABASECHANGELOG### Cause: java.sql.SQLSyntaxErrorException: Table &apos;activiti5.ACT_DMN_DATABASECHANGELOG&apos; doesn&apos;t exist最后找到 flowable 资料：123456The original database tables (like those for BPMN and IDM) use a Flowable schema and scripts for version management. The newer engines and tables use Liquibase for table versioning. There are two Liquibase tables related to database schema management:- &lt;prefix&gt;_DATABASECHANGELOG: This table is used by Liquibase to track which changesets have been run.- &lt;prefix&gt;_DATABASECHANGELOGLOCK: This table is used by Liquibase to ensure only one instance of Liquibase is running at a time.Note: Liquibase is a Java based framework for tracking database schema changes. 自动建表有点问题，没有生成对应的 Liquibase 表。于是手动从 activit6 数据库扣出 ddl：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338CREATE TABLE `ACT_DE_DATABASECHANGELOG` ( `ID` varchar(255) NOT NULL, `AUTHOR` varchar(255) NOT NULL, `FILENAME` varchar(255) NOT NULL, `DATEEXECUTED` datetime NOT NULL, `ORDEREXECUTED` int(11) NOT NULL, `EXECTYPE` varchar(10) NOT NULL, `MD5SUM` varchar(35) DEFAULT NULL, `DESCRIPTION` varchar(255) DEFAULT NULL, `COMMENTS` varchar(255) DEFAULT NULL, `TAG` varchar(255) DEFAULT NULL, `LIQUIBASE` varchar(20) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DE_DATABASECHANGELOGLOCK` ( `ID` int(11) NOT NULL, `LOCKED` bit(1) NOT NULL, `LOCKGRANTED` datetime DEFAULT NULL, `LOCKEDBY` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DE_MODEL` ( `id` varchar(255) NOT NULL, `name` varchar(400) NOT NULL, `model_key` varchar(400) NOT NULL, `description` varchar(4000) DEFAULT NULL, `model_comment` varchar(4000) DEFAULT NULL, `created` datetime(6) DEFAULT NULL, `created_by` varchar(255) DEFAULT NULL, `last_updated` datetime(6) DEFAULT NULL, `last_updated_by` varchar(255) DEFAULT NULL, `version` int(11) DEFAULT NULL, `model_editor_json` longtext, `thumbnail` longblob, `model_type` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_proc_mod_created` (`created_by`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DE_MODEL_HISTORY` ( `id` varchar(255) NOT NULL, `name` varchar(400) NOT NULL, `model_key` varchar(400) NOT NULL, `description` varchar(4000) DEFAULT NULL, `model_comment` varchar(4000) DEFAULT NULL, `created` datetime(6) DEFAULT NULL, `created_by` varchar(255) DEFAULT NULL, `last_updated` datetime(6) DEFAULT NULL, `last_updated_by` varchar(255) DEFAULT NULL, `removal_date` datetime(6) DEFAULT NULL, `version` int(11) DEFAULT NULL, `model_editor_json` longtext, `model_id` varchar(255) NOT NULL, `model_type` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_proc_mod_history_proc` (`model_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DE_MODEL_RELATION` ( `id` varchar(255) NOT NULL, `parent_model_id` varchar(255) DEFAULT NULL, `model_id` varchar(255) DEFAULT NULL, `relation_type` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `fk_relation_parent` (`parent_model_id`), KEY `fk_relation_child` (`model_id`), CONSTRAINT `fk_relation_child` FOREIGN KEY (`model_id`) REFERENCES `ACT_DE_MODEL` (`id`), CONSTRAINT `fk_relation_parent` FOREIGN KEY (`parent_model_id`) REFERENCES `ACT_DE_MODEL` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DMN_DATABASECHANGELOG` ( `ID` varchar(255) NOT NULL, `AUTHOR` varchar(255) NOT NULL, `FILENAME` varchar(255) NOT NULL, `DATEEXECUTED` datetime NOT NULL, `ORDEREXECUTED` int(11) NOT NULL, `EXECTYPE` varchar(10) NOT NULL, `MD5SUM` varchar(35) DEFAULT NULL, `DESCRIPTION` varchar(255) DEFAULT NULL, `COMMENTS` varchar(255) DEFAULT NULL, `TAG` varchar(255) DEFAULT NULL, `LIQUIBASE` varchar(20) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DMN_DATABASECHANGELOGLOCK` ( `ID` int(11) NOT NULL, `LOCKED` bit(1) NOT NULL, `LOCKGRANTED` datetime DEFAULT NULL, `LOCKEDBY` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DMN_DECISION_TABLE` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `VERSION_` int(11) DEFAULT NULL, `KEY_` varchar(255) DEFAULT NULL, `CATEGORY_` varchar(255) DEFAULT NULL, `DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `PARENT_DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `TENANT_ID_` varchar(255) DEFAULT NULL, `RESOURCE_NAME_` varchar(255) DEFAULT NULL, `DESCRIPTION_` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DMN_DEPLOYMENT` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `CATEGORY_` varchar(255) DEFAULT NULL, `DEPLOY_TIME_` datetime DEFAULT NULL, `TENANT_ID_` varchar(255) DEFAULT NULL, `PARENT_DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_DMN_DEPLOYMENT_RESOURCE` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `RESOURCE_BYTES_` longblob, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_DATABASECHANGELOG` ( `ID` varchar(255) NOT NULL, `AUTHOR` varchar(255) NOT NULL, `FILENAME` varchar(255) NOT NULL, `DATEEXECUTED` datetime NOT NULL, `ORDEREXECUTED` int(11) NOT NULL, `EXECTYPE` varchar(10) NOT NULL, `MD5SUM` varchar(35) DEFAULT NULL, `DESCRIPTION` varchar(255) DEFAULT NULL, `COMMENTS` varchar(255) DEFAULT NULL, `TAG` varchar(255) DEFAULT NULL, `LIQUIBASE` varchar(20) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_DATABASECHANGELOGLOCK` ( `ID` int(11) NOT NULL, `LOCKED` bit(1) NOT NULL, `LOCKGRANTED` datetime DEFAULT NULL, `LOCKEDBY` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_FORM_DEFINITION` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `VERSION_` int(11) DEFAULT NULL, `KEY_` varchar(255) DEFAULT NULL, `CATEGORY_` varchar(255) DEFAULT NULL, `DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `PARENT_DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `TENANT_ID_` varchar(255) DEFAULT NULL, `RESOURCE_NAME_` varchar(255) DEFAULT NULL, `DESCRIPTION_` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_FORM_DEPLOYMENT` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `CATEGORY_` varchar(255) DEFAULT NULL, `DEPLOY_TIME_` datetime DEFAULT NULL, `TENANT_ID_` varchar(255) DEFAULT NULL, `PARENT_DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_FORM_RESOURCE` ( `ID_` varchar(255) NOT NULL, `NAME_` varchar(255) DEFAULT NULL, `DEPLOYMENT_ID_` varchar(255) DEFAULT NULL, `RESOURCE_BYTES_` longblob, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_FO_SUBMITTED_FORM` ( `ID_` varchar(255) NOT NULL, `FORM_ID_` varchar(255) NOT NULL, `TASK_ID_` varchar(255) DEFAULT NULL, `PROC_INST_ID_` varchar(255) DEFAULT NULL, `PROC_DEF_ID_` varchar(255) DEFAULT NULL, `SUBMITTED_DATE_` datetime DEFAULT NULL, `SUBMITTED_BY_` varchar(255) DEFAULT NULL, `FORM_VALUES_ID_` varchar(255) DEFAULT NULL, `TENANT_ID_` varchar(255) DEFAULT NULL, PRIMARY KEY (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_IDM_PERSISTENT_TOKEN` ( `series` varchar(255) NOT NULL, `user_id` varchar(255) DEFAULT NULL, `token_value` varchar(255) DEFAULT NULL, `token_date` datetime(6) DEFAULT NULL, `ip_address` varchar(39) DEFAULT NULL, `user_agent` varchar(255) DEFAULT NULL, PRIMARY KEY (`series`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_RU_DEADLETTER_JOB` ( `ID_` varchar(64) COLLATE utf8_bin NOT NULL, `REV_` int(11) DEFAULT NULL, `TYPE_` varchar(255) COLLATE utf8_bin NOT NULL, `EXCLUSIVE_` tinyint(1) DEFAULT NULL, `EXECUTION_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROCESS_INSTANCE_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROC_DEF_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `EXCEPTION_STACK_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `EXCEPTION_MSG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `DUEDATE_` timestamp(3) NULL DEFAULT NULL, `REPEAT_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_TYPE_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_CFG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `TENANT_ID_` varchar(255) COLLATE utf8_bin DEFAULT '', PRIMARY KEY (`ID_`), KEY `ACT_FK_DEADLETTER_JOB_EXECUTION` (`EXECUTION_ID_`), KEY `ACT_FK_DEADLETTER_JOB_PROCESS_INSTANCE` (`PROCESS_INSTANCE_ID_`), KEY `ACT_FK_DEADLETTER_JOB_PROC_DEF` (`PROC_DEF_ID_`), KEY `ACT_FK_DEADLETTER_JOB_EXCEPTION` (`EXCEPTION_STACK_ID_`), CONSTRAINT `ACT_FK_DEADLETTER_JOB_EXCEPTION` FOREIGN KEY (`EXCEPTION_STACK_ID_`) REFERENCES `ACT_GE_BYTEARRAY` (`ID_`), CONSTRAINT `ACT_FK_DEADLETTER_JOB_EXECUTION` FOREIGN KEY (`EXECUTION_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_DEADLETTER_JOB_PROCESS_INSTANCE` FOREIGN KEY (`PROCESS_INSTANCE_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_DEADLETTER_JOB_PROC_DEF` FOREIGN KEY (`PROC_DEF_ID_`) REFERENCES `ACT_RE_PROCDEF` (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin;CREATE TABLE `ACT_RU_SUSPENDED_JOB` ( `ID_` varchar(64) COLLATE utf8_bin NOT NULL, `REV_` int(11) DEFAULT NULL, `TYPE_` varchar(255) COLLATE utf8_bin NOT NULL, `EXCLUSIVE_` tinyint(1) DEFAULT NULL, `EXECUTION_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROCESS_INSTANCE_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROC_DEF_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `RETRIES_` int(11) DEFAULT NULL, `EXCEPTION_STACK_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `EXCEPTION_MSG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `DUEDATE_` timestamp(3) NULL DEFAULT NULL, `REPEAT_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_TYPE_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_CFG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `TENANT_ID_` varchar(255) COLLATE utf8_bin DEFAULT '', PRIMARY KEY (`ID_`), KEY `ACT_FK_SUSPENDED_JOB_EXECUTION` (`EXECUTION_ID_`), KEY `ACT_FK_SUSPENDED_JOB_PROCESS_INSTANCE` (`PROCESS_INSTANCE_ID_`), KEY `ACT_FK_SUSPENDED_JOB_PROC_DEF` (`PROC_DEF_ID_`), KEY `ACT_FK_SUSPENDED_JOB_EXCEPTION` (`EXCEPTION_STACK_ID_`), CONSTRAINT `ACT_FK_SUSPENDED_JOB_EXCEPTION` FOREIGN KEY (`EXCEPTION_STACK_ID_`) REFERENCES `ACT_GE_BYTEARRAY` (`ID_`), CONSTRAINT `ACT_FK_SUSPENDED_JOB_EXECUTION` FOREIGN KEY (`EXECUTION_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_SUSPENDED_JOB_PROCESS_INSTANCE` FOREIGN KEY (`PROCESS_INSTANCE_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_SUSPENDED_JOB_PROC_DEF` FOREIGN KEY (`PROC_DEF_ID_`) REFERENCES `ACT_RE_PROCDEF` (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin;CREATE TABLE `ACT_RU_TIMER_JOB` ( `ID_` varchar(64) COLLATE utf8_bin NOT NULL, `REV_` int(11) DEFAULT NULL, `TYPE_` varchar(255) COLLATE utf8_bin NOT NULL, `LOCK_EXP_TIME_` timestamp(3) NULL DEFAULT NULL, `LOCK_OWNER_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `EXCLUSIVE_` tinyint(1) DEFAULT NULL, `EXECUTION_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROCESS_INSTANCE_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `PROC_DEF_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `RETRIES_` int(11) DEFAULT NULL, `EXCEPTION_STACK_ID_` varchar(64) COLLATE utf8_bin DEFAULT NULL, `EXCEPTION_MSG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `DUEDATE_` timestamp(3) NULL DEFAULT NULL, `REPEAT_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_TYPE_` varchar(255) COLLATE utf8_bin DEFAULT NULL, `HANDLER_CFG_` varchar(4000) COLLATE utf8_bin DEFAULT NULL, `TENANT_ID_` varchar(255) COLLATE utf8_bin DEFAULT '', PRIMARY KEY (`ID_`), KEY `ACT_FK_TIMER_JOB_EXECUTION` (`EXECUTION_ID_`), KEY `ACT_FK_TIMER_JOB_PROCESS_INSTANCE` (`PROCESS_INSTANCE_ID_`), KEY `ACT_FK_TIMER_JOB_PROC_DEF` (`PROC_DEF_ID_`), KEY `ACT_FK_TIMER_JOB_EXCEPTION` (`EXCEPTION_STACK_ID_`), CONSTRAINT `ACT_FK_TIMER_JOB_EXCEPTION` FOREIGN KEY (`EXCEPTION_STACK_ID_`) REFERENCES `ACT_GE_BYTEARRAY` (`ID_`), CONSTRAINT `ACT_FK_TIMER_JOB_EXECUTION` FOREIGN KEY (`EXECUTION_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_TIMER_JOB_PROCESS_INSTANCE` FOREIGN KEY (`PROCESS_INSTANCE_ID_`) REFERENCES `ACT_RU_EXECUTION` (`ID_`), CONSTRAINT `ACT_FK_TIMER_JOB_PROC_DEF` FOREIGN KEY (`PROC_DEF_ID_`) REFERENCES `ACT_RE_PROCDEF` (`ID_`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin;CREATE TABLE `ACT_WO_COMMENTS` ( `id` bigint(20) NOT NULL, `message` varchar(4000) NOT NULL, `created` timestamp(6) NULL DEFAULT NULL, `created_by` varchar(255) DEFAULT NULL, `task_id` varchar(255) DEFAULT NULL, `proc_inst_id` varchar(255) DEFAULT NULL, `comment_definition` longtext, PRIMARY KEY (`id`), KEY `comment_task_created` (`task_id`,`created`), KEY `comment_proc_created` (`proc_inst_id`,`created`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `ACT_WO_RELATED_CONTENT` ( `id` bigint(20) NOT NULL, `name` varchar(255) NOT NULL, `created` timestamp(6) NULL DEFAULT NULL, `created_by` varchar(255) DEFAULT NULL, `task_id` varchar(255) DEFAULT NULL, `process_id` varchar(255) DEFAULT NULL, `content_source` varchar(255) DEFAULT NULL, `source_id` varchar(4000) DEFAULT NULL, `store_id` varchar(255) DEFAULT NULL, `mime_type` varchar(255) DEFAULT NULL, `field` varchar(400) DEFAULT NULL, `related_content` bit(1) NOT NULL, `link` bit(1) NOT NULL, `link_url` varchar(4000) DEFAULT NULL, `content_available` bit(1) DEFAULT b'0', `locked` bit(1) DEFAULT b'0', `lock_date` timestamp(6) NULL DEFAULT NULL, `lock_exp_date` timestamp(6) NULL DEFAULT NULL, `lock_owner` varchar(255) DEFAULT NULL, `checked_out` bit(1) DEFAULT b'0', `checkout_date` timestamp(6) NULL DEFAULT NULL, `checkout_owner` varchar(255) DEFAULT NULL, `last_modified` timestamp(6) NULL DEFAULT NULL, `last_modified_by` varchar(255) DEFAULT NULL, `checked_out_to_local` bit(1) DEFAULT b'0', `content_size` bigint(20) DEFAULT '0', PRIMARY KEY (`id`), KEY `idx_relcont_createdby` (`created_by`), KEY `idx_relcont_taskid` (`task_id`), KEY `idx_relcont_procid` (`process_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;]]></content>
      <categories>
        <category>activiti</category>
      </categories>
      <tags>
        <tag>activiti</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes externalTrafficPolicy 笔记]]></title>
    <url>%2Fp%2Fkubernetes-external-traffic-policy-note%2F</url>
    <content type="text"><![CDATA[在折腾 nacos k8s 部署的时候遇到 externalTrafficPolicy ，记录学习笔记。Headless ServiceHeadless Service 的对应的每一个 Endpoints，即每一个 Pod，都会有对应的 DNS 域名；这样 Pod 之间就可以互相访问。对于一些集群类型的应用就可以解决互相之间身份识别的问题了。（与本文无关）externalTrafficPolicyservice.spec.externalTrafficPolicy: 表示此服务是否希望将外部流量路由到节点本地或集群范围的端点。有两个可用选项：Cluster（默认）和 Local。Cluster 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。Local 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，但存在潜在的不均衡流量传播风险。使用保留源 IP 的警告和限制:新功能中，外部的流量不会按照 pod 平均分配，而是在节点（node）层面平均分配（因为 GCE/AWS 和其他外部负载均衡实现没有能力做节点权重， 而是平均地分配给所有目标节点，忽略每个节点上所拥有的 pod 数量）。然而，在 pod 数量（NumServicePods） « 节点数（NumNodes）或者 pod 数量（NumServicePods） » 节点数（NumNodes）的情况下，即使没有权重策略，我们也可以看到非常接近公平分发的场景。podAntiAffinity外部的流量不会按照 pod 平均分配，而是在节点（node）层面平均分配, 那么我们能做的只有保证同一业务的 pod 调度到不同的 node 节点上。podAntiAffinity 使用场景：将一个服务的 POD 分散在不同的主机或者拓扑域中，提高服务本身的稳定性。给 POD 对于一个节点的独占访问权限来保证资源隔离，保证不会有其它 pod 来分享节点资源。把可能会相互影响的服务的 POD 分散在不同的主机上 对于亲和性和反亲和性，每种都有三种规则可以设置：RequiredDuringSchedulingRequiredDuringExecution ：在调度期间要求满足亲和性或者反亲和性规则，如果不能满足规则，则 POD 不能被调度到对应的主机上。在之后的运行过程中，如果因为某些原因（比如修改 label）导致规则不能满足，系统会尝试把 POD 从主机上删除（现在版本还不支持）。RequiredDuringSchedulingIgnoredDuringExecution ：在调度期间要求满足亲和性或者反亲和性规则，如果不能满足规则，则 POD 不能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。PreferredDuringSchedulingIgnoredDuringExecution ：在调度期间尽量满足亲和性或者反亲和性规则，如果不能满足规则，POD 也有可能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。参考 从 service 的 externalTrafficPolicy 到 podAntiAffinity创建一个外部负载均衡器]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[activiti 5.22 springboot2 集成]]></title>
    <url>%2Fp%2Factiviti-522-springboot2-integration%2F</url>
    <content type="text"><![CDATA[调研工作流引擎。先做 acitviti 5.22 的实验，记录集成 springboot2 的问题。application.yml先贴上最终的 yaml，后面是问题记录。1234567891011121314151617181920spring: activiti: check-process-definitions: false # 自动检查、部署流程定义文件 database-schema-update: true # 自动更新数据库结构 # 流程定义文件存放目录 process-definition-location-prefix: classpath:/processes/ #process-definition-location-suffixes: #流程文件格式 datasource: url: jdbc:mysql://xxx.xxx.xxx.xxx:3306/activiti5?characterEncoding=UTF-8&amp;useSSL=false username: zzz password: zzz driver-class-name: com.mysql.cj.jdbc.Driver jpa: database-platform: org.hibernate.dialect.MySQL5Dialect show-sql: true hibernate: ddl-auto: create generate-ddl: truehibernate dialect 问题12Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when &apos;hibernate.dialect&apos; not set at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.determineDialect(DialectFactoryImpl.java:100) ~[hibernate-core-5.4.9.Final.jar:5.4.9.Final] 设置 spring.jpa.database-platform 解决。hibernate 不能正常访问数据库 1234567Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;processEngine&apos;: FactoryBean threw exception on object creation; nested exception is org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: java.sql.SQLException: Access denied for user &apos;&apos;@&apos;172.23.121.138&apos; (using password: NO)### The error may exist in org/activiti/db/mapping/entity/Property.xml### The error may involve org.activiti.engine.impl.persistence.entity.PropertyEntity.selectDbSchemaVersion### The error occurred while executing a query### SQL: select VALUE_ from ACT_GE_PROPERTY where NAME_ = &apos;schema.version&apos;### Cause: java.sql.SQLException: Access denied for user &apos;&apos;@&apos;172.23.121.138&apos; (using password: NO)spring.datasource 有 2 个账号设置：data-username 和 data-password ： Username of the database to execute DML scripts (if different). 专门用来执行 dml 语句。username 和 password： Login username of the database. 一般查询使用的账号。最初配置了 data-username，访问失败，改为username 即可。不能正常自动创建表 最初偷懒没有建立数据库表。设想开启了自动 ddl，让 jpa 自动建表 123456jpa: database-platform: org.hibernate.dialect.MySQL5Dialect show-sql: true hibernate: ddl-auto: create generate-ddl: true 网上也有同样情况，springboot2 上 ddl 不正常。于是手动建表好了，不纠结。mysql 和 mysql55自己建表，发现有个问题，activiti 提供了 mysql 和 mysql55 两份 sql。查阅资料，对于 mysql 来说，版本低于 5.6.4 的 mysql 不支持 带有毫秒精确度的 timestamp 和 date 类型。低于 5.6.4 版本的 mysql 需要执行如下脚本文件：123activiti.mysql55.create.engine.sqlactiviti.mysql.create.identity.sqlactiviti.mysql55.create.history.sql版本号在 5.6.4 以及以上的 mysql 需要执行如下脚本 123activiti.mysql.create.engine.sqlactiviti.mysql.create.identity.sqlactiviti.mysql.create.history.sql 屏蔽 SecurityAutoConfiguration默认会启动 SecurityAutoConfiguration，因为缺少 security 相关配置报错。因为目前没有使用到，因此直接屏蔽。1@SpringBootApplication(exclude = SecurityAutoConfiguration.class)]]></content>
      <categories>
        <category>activiti</category>
      </categories>
      <tags>
        <tag>activiti</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 资源限制]]></title>
    <url>%2Fp%2Fkubernetes-resource-limit%2F</url>
    <content type="text"><![CDATA[资源单位 cpu 单位1 个 k8s cpu 对应1 AWS vCPU1 GCP Core1 Azure vCore1 Hyperthread on a bare-metal Intel processor with Hyperthreadingcpu 也可以是小数。1000m=1。500m = 0.5 cpu。 存储单位 以 byte 作为单位。直接的数字代表 Byte。当然有更大的单位：E，P，T，G，M，KEi，Pi，Ti ，Gi，Mi，Kii后缀表示 1024，非 i 后缀表示 1000。例如：1K = 1000 Byte1Ki = 1024 Byte资源限制 资源限制可以在 pod 级别，也可以在 namespace 级别。pod 级别spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.requests.cpuspec.containers[].resources.requests.memorynamespace 级别spec.limits[].default.cpu，default limitspec.limits[].default.memory，同上spec.limits[].defaultRequest.cpu，default requestspec.limits[].defaultRequest.memory，同上pod 的设置优先级高于 namespace。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis-pagehelper-with-multiple-datasource]]></title>
    <url>%2Fp%2Fmybatis-pagehelper-with-multiple-datasource%2F</url>
    <content type="text"><![CDATA[背景 系统中接入了 mysql 和 postgres，使用 mybatis 作为 ORM 框架。需要支持分页，使用 pagehelper 插件。区分不同的数据源配置 不同数据库分页格式不一样。因此要针对不同的数据库配置 pagehelper。因为有多个数据源，SqlSessionFactory、 DataSourceTransactionManager、 SqlSessionTemplate、 JdbcTemplate 等类要使用 @Qualifier 分别注入。12345678910111213141516171819202122232425262728@Bean(name = "pgSqlSessionFactory")public SqlSessionFactory pgSqlSessionFactory(@Qualifier("pgDataSource") DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); org.apache.ibatis.session.Configuration Configuration = new org.apache.ibatis.session.Configuration(); boolean mapUnderscoreToCamelCase = false; if ("TRUE".equals(mapUnderscoreToCamelCase.toUpperCase())) &#123; mapUnderscoreToCamelCase = true; &#125; // 注意这里是 PageInterceptor 的拦截器，此处有伏笔 Interceptor interceptor = new PageInterceptor(); Properties properties = new Properties(); // 数据库, 分页配置 properties.setProperty("helperDialect", "postgresql"); // 是否分页合理化 properties.setProperty("reasonable", "false"); interceptor.setProperties(properties); bean.setPlugins(new Interceptor[] &#123;interceptor&#125;); Configuration.setMapUnderscoreToCamelCase(mapUnderscoreToCamelCase); bean.setConfiguration(Configuration); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(mapperLocation)); return bean.getObject();&#125;@Bean(name = "pgJdbcTemplate")public JdbcTemplate pgJdbcTemplate(@Qualifier("pgDataSource") DataSource dataSource) throws Exception &#123; return new JdbcTemplate(dataSource);&#125; 方言列表见 PageAutoDialect。解决：”在系统中发现了多个分页插件”直接启动没问题，但是真正查询的时候报错：12org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: java.lang.RuntimeException: 在系统中发现了多个分页插件，请检查系统配置!pagehelper 插件通过 pagehelper-spring-boot-starter 依赖引入。会自动引入 PageHelperAutoConfiguration 自动配置类：12345678910111213141516171819@Bean@ConfigurationProperties(prefix = PageHelperProperties.PAGEHELPER_PREFIX)public Properties pageHelperProperties() &#123; return new Properties();&#125;@PostConstructpublic void addPageInterceptor() &#123; PageInterceptor interceptor = new PageInterceptor(); Properties properties = new Properties(); // 先把一般方式配置的属性放进去 properties.putAll(pageHelperProperties()); // 在把特殊配置放进去，由于 close-conn 利用上面方式时，属性名就是 close-conn 而不是 closeConn，所以需要额外的一步 properties.putAll(this.properties.getProperties()); interceptor.setProperties(properties); for (SqlSessionFactory sqlSessionFactory : sqlSessionFactoryList) &#123; sqlSessionFactory.getConfiguration().addInterceptor(interceptor); &#125;&#125; 因为原来配置文件已经有 pagehelper：12345pagehelper: helperDialect: mysql reasonable: true supportMethodsArguments: true params: count=countSql导致加入了 2 个 PageInterceptor。解决方法：屏蔽掉 PageHelperAutoConfiguration，同时删除多余配置1@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, PageHelperAutoConfiguration.class&#125;)]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nacos k8s mysql standalone 部署]]></title>
    <url>%2Fp%2Fnacos-k8s-with-mysql-standalone%2F</url>
    <content type="text"><![CDATA[需要在开发环境部署 nacos k8s。基础 yaml 文件：nacos-quick-start。官方这个例子的效果是：仅在集群内部访问 nacos3 副本的 nacos2 节点的 mysql目前条件是：需要在 k8s 外部能够访问 nacos 控制台。且没有 ingress 服务。（使用 NodePort 方式暴露 service）只要单节点的 nacos 即可。（修改 NACOS_REPLICAS 即可）现有 k8s 测试集群已经有单节点的 mysql，能够满足开发需求，无需再部署。（需要修改配置）针对 3)，官方 nacos-quick-start 并不满足。要从镜像入手分析，对应镜像的 ENV 参数见 nacos-docker。 最后通过mysql.database.num 实现只访问单节点的 mysql。完整的 yaml 文件如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111---apiVersion: v1kind: Servicemetadata: name: nacos-headless labels: app: nacos-headless namespace: v-basespec: ports: - port: 8848 name: server targetPort: 8848 nodePort: 30848 type: NodePort externalTrafficPolicy: Cluster selector: app: nacos---apiVersion: v1kind: ConfigMapmetadata: name: nacos-cm namespace: v-basedata: mysql.master.db.name: "nacos" mysql.master.port: "3306" mysql.master.user: "nacos" mysql.master.password: "nacos" mysql.database.num: "1" mysql.master.service.host: "xxx.xxx.xxx.xxx"---apiVersion: apps/v1kind: StatefulSetmetadata: name: nacos namespace: v-basespec: serviceName: nacos-headless replicas: 1 template: metadata: labels: app: nacos annotations: pod.alpha.kubernetes.io/initialized: "true" spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: "app" operator: In values: - nacos-headless topologyKey: "kubernetes.io/hostname" containers: - name: k8snacos imagePullPolicy: Always image: nacos/nacos-server:1.1.4 resources: requests: memory: "2Gi" cpu: "1000m" ports: - containerPort: 8848 name: client env: - name: NACOS_REPLICAS value: "1" - name: MYSQL_MASTER_SERVICE_DB_NAME valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.db.name - name: MYSQL_MASTER_SERVICE_PORT valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.port - name: MYSQL_MASTER_SERVICE_USER valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.user - name: MYSQL_MASTER_SERVICE_PASSWORD valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.password - name: MYSQL_MASTER_SERVICE_HOST valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.service.host - name: MYSQL_DATABASE_NUM valueFrom: configMapKeyRef: name: nacos-cm key: mysql.database.num - name: NACOS_SERVER_PORT value: "8848" - name: PREFER_HOST_MODE value: "hostname" - name: NACOS_SERVERS # value: "nacos-0.nacos-headless.default.svc.cluster.local:8848 nacos-1.nacos-headless.default.svc.cluster.local:8848 nacos-2.nacos-headless.default.svc.cluster.local:8848" value: "nacos-0.nacos-headless.default.svc.cluster.local:8848" selector: matchLabels: app: nacos]]></content>
      <categories>
        <category>nacos</category>
      </categories>
      <tags>
        <tag>nacos</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java FileLock 简介]]></title>
    <url>%2Fp%2Fjava-filelock-intro%2F</url>
    <content type="text"><![CDATA[背景 在看 nacos 源码，发现读取文件的工具类 ConcurrentDiskUtil.getFileContent() 涉及了 FileLock，做下笔记。FileLock 简介 FileLock 是一种标记，代表文件中一部分区域的锁定。A token representing a lock on a region of a file.FileLock 底层实现依赖操作系统的文件锁语义。理论上，FileLock 的语义是跨越不同进程、不同编程语言(但不是强制保证)。Java FileLock uses advisory (not mandatory) locks on many platforms.FileLock 和很多其他锁类似，可以是独占或者共享。 一旦获取了 FileLock，内部状态为有效（valid = true），需要手动释放。FileLock 的成员比较简单。123456789public abstract class FileLock implements AutoCloseable &#123; private final Channel channel; private final long position; private final long size; private final boolean shared;// more code&#125;由于是 abstract，真正使用的是 FileLock 的子类，sun 提供了私有实现：1234package sun.nio.ch;public class FileLockImpl extends FileLock &#123; private volatile boolean valid = true;真正使用，是通过 FileChannel 或者 AsynchronousFileChannel 获取 FileLock。12345public abstract FileLock lock(long position, long size, boolean shared) throws IOException;public abstract FileLock tryLock(long position, long size, boolean shared) throws IOException;FileLock 使用例子 ConcurrentDiskUtil.getFileContent()12345678910111213141516171819202122232425262728293031323334353637383940public static String getFileContent(File file, String charsetName) throws IOException &#123; RandomAccessFile fis = null; FileLock rlock = null; try &#123; fis = new RandomAccessFile(file, "r"); FileChannel fcin = fis.getChannel(); int i = 0; do &#123; try &#123; rlock = fcin.tryLock(0L, Long.MAX_VALUE, true); &#125; catch (Exception e) &#123; ++i; if (i &gt; RETRY_COUNT) &#123; LOGGER.error("read &#123;&#125; fail;retryed time:&#123;&#125;", file.getName(), i); throw new IOException("read" + file.getAbsolutePath() + "conflict"); &#125; sleep(SLEEP_BASETIME * i); LOGGER.warn("read &#123;&#125; conflict;retry time:&#123;&#125;", file.getName(), i); &#125; &#125; while (null == rlock); int fileSize = (int)fcin.size(); ByteBuffer byteBuffer = ByteBuffer.allocate(fileSize); fcin.read(byteBuffer); byteBuffer.flip(); return byteBufferToString(byteBuffer, charsetName); &#125; finally &#123; if (rlock != null) &#123; rlock.release(); rlock = null; &#125; if (fis != null) &#123; fis.close(); fis = null; &#125; &#125;&#125; 和其他锁类似，应该在 finally 处保证释放锁。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nacos 单机单数据库部署]]></title>
    <url>%2Fp%2Fnacos-standalone-with-one-mysql%2F</url>
    <content type="text"><![CDATA[目前需要 nacos 单机模式 + 单个 mysql 的部署方案。docker compose naocsnacos 官网（Nacos Docker 快速开始 ）提供了 docker-compose 部署例子。12345# 单机模式 Mysqlgit clone https://github.com/nacos-group/nacos-docker.gitcd nacos-dockerdocker-compose -f example/standalone-mysql.yaml up 但是发现 standalone-mysql.yaml 需要 mysql 部署 master-slave，要提供 2 个 mysql 地址，具体见 standalone-mysql.yaml，只填 master 会报错：1Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder &apos;MYSQL_SLAVE_SERVICE_HOST&apos; in value &quot;jdbc:mysql://$&#123;MYSQL_SLAVE_SERVICE_HOST&#125;:$&#123;MYSQL_SLAVE_SERVICE_PORT:3306&#125;/$&#123;MYSQL_MASTER_SERVICE_DB_NAME&#125;?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&quot; 其实官网提供了一个控制参数 1MYSQL_DATABASE_NUM 数据库数量 default :2 设置为 1，则不需要提供 slave 的地址。提供一份修改后的 docker-compose 文件作为参考，使用外部的 mysql。123456789101112131415161718192021version: "2"services: nacos: image: nacos/nacos-server:latest container_name: nacos-standalone-mysql environment: - PREFER_HOST_MODE=hostname - MODE=standalone - SPRING_DATASOURCE_PLATFORM=mysql - MYSQL_MASTER_SERVICE_HOST=xxx.xxx.xxx.xxx - MYSQL_MASTER_SERVICE_DB_NAME=nacos - MYSQL_MASTER_SERVICE_PORT=3306 - MYSQL_MASTER_SERVICE_USER=nacos - MYSQL_MASTER_SERVICE_PASSWORD=nacos - MYSQL_DATABASE_NUM=1 volumes: - /home/nacos/logs:/home/nacos/logs - /home/nacos/init.d/custom.properties:/home/nacos/init.d/custom.properties ports: - "8848:8848"# restart: on-failure其中：SPRING_DATASOURCE_PLATFORM: 指定数据源。默认情况下，standalone 模式使用内嵌数据库。如果要在 standalone 模式下使用 mysql，需要指定。注意：提前创建 nacos 数据库和建表语句 compose 文件的 volumes： &lt; 宿主机 &gt;:&lt; 容器 &gt;。在宿主机建立对应文件夹，拷贝custom.properties 文件。k8s nacos更新于 2020.5.8：在开发环境使用 k8s 部署单节点的 nacos server，当时忘记指定 standalone 模式，默认是 cluster，导致服务发现功能不正常：容器内的启动脚本 docker-startup.sh：1234567891011121314151617#===========================================================================================# JVM Configuration#===========================================================================================if [["$&#123;MODE&#125;" == "standalone" ]]; then JAVA_OPT="$&#123;JAVA_OPT&#125; -Xms512m -Xmx512m -Xmn256m" JAVA_OPT="$&#123;JAVA_OPT&#125; -Dnacos.standalone=true"else JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms$&#123;JVM_XMS&#125; -Xmx$&#123;JVM_XMX&#125; -Xmn$&#123;JVM_XMN&#125; -XX:MetaspaceSize=$&#123;JVM_MS&#125; -XX:MaxMetaspaceSize=$&#123;JVM_MMS&#125;" if [["$&#123;NACOS_DEBUG&#125;" == "y" ]]; then JAVA_OPT="$&#123;JAVA_OPT&#125; -Xdebug -Xrunjdwp:transport=dt_socket,address=9555,server=y,suspend=n" fi JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;BASE_DIR&#125;/logs/java_heapdump.hprof" JAVA_OPT="$&#123;JAVA_OPT&#125; -XX:-UseLargePages" print_serversfi 怀疑是 cluster 模式下 NACOS_SERVERS 配置有问题：1NACOS_SERVERS: nacos-0.nacos-center.v-base.svc.cluster.local.:30848先改为单机模式，后续再研究。以下是修改后的 k8s yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121---apiVersion: v1kind: Servicemetadata: name: nacos-center labels: app: nacos-center namespace: v-basespec: ports: - port: 30848 name: server targetPort: 30848 nodePort: 30848 selector: app: nacos type: NodePort---apiVersion: v1kind: ConfigMapmetadata: name: nacos-cm namespace: v-basedata: mysql.master.db.name: "nacos" mysql.master.port: "3306" mysql.master.user: "xxx" mysql.master.password: "xxx" mysql.database.num: "1" mysql.master.service.host: "mysql.v-base" mode: "standalone"---apiVersion: apps/v1kind: StatefulSetmetadata: name: nacos namespace: v-basespec: serviceName: nacos-center replicas: 1 template: metadata: labels: app: nacos annotations: pod.alpha.kubernetes.io/initialized: "true" spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: "app" operator: In values: - nacos-center topologyKey: "kubernetes.io/hostname" containers: - name: k8snacos imagePullPolicy: IfNotPresent image: xxx.com/base/nacos-server:1.1.4 resources: limits: memory: "8Gi" cpu: "4000m" requests: memory: "1Gi" cpu: "1000m" ports: - containerPort: 30848 name: client env: - name: NACOS_REPLICAS value: "1" - name: SPRING_DATASOURCE_PLATFORM value: "mysql" - name: MODE valueFrom: configMapKeyRef: name: nacos-cm key: mode - name: MYSQL_MASTER_SERVICE_DB_NAME valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.db.name - name: MYSQL_MASTER_SERVICE_PORT valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.port - name: MYSQL_MASTER_SERVICE_USER valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.user - name: MYSQL_MASTER_SERVICE_PASSWORD valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.password - name: MYSQL_MASTER_SERVICE_HOST valueFrom: configMapKeyRef: name: nacos-cm key: mysql.master.service.host - name: MYSQL_DATABASE_NUM valueFrom: configMapKeyRef: name: nacos-cm key: mysql.database.num - name: NACOS_SERVER_PORT value: "30848" - name: PREFER_HOST_MODE value: "hostname" - name: NACOS_SERVERS #value: "nacos-0.nacos-center.v-base.svc.cluster.local:8848 nacos-1.nacos-center.v-base.svc.cluster.local:8848 nacos-2.nacos-center.v-base.svc.cluster.local:8848" value: "nacos-0.nacos-center.v-base.svc.cluster.local.:30848" selector: matchLabels: app: nacos]]></content>
      <categories>
        <category>nacos</category>
      </categories>
      <tags>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 安装 docker]]></title>
    <url>%2Fp%2Fcentos7-install-docker%2F</url>
    <content type="text"><![CDATA[在 centos7 上安装 docker，中间遇到点问题，记录最后解决方式。安装 docker12345678910111213141516# 添加阿里云 repowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 清理 yum 缓存 yum clean all# 重建 yum 本地缓存yum makecache fast# 更新依赖yum update --skip-broken# 安装前置依赖yum install -y yum-utils device-mapper-persistent-data lvm2yum install docker-ce -y 之前切换 yum 源之后没有 update，导致出错：1234567891011[root@host143 ~]# sudo yum install containerd.io已加载插件：fastestmirror, langpacksLoading mirror speeds from cached hostfile正在解决依赖关系 --&gt; 正在检查事务---&gt; 软件包 containerd.io.x86_64.0.1.2.10-3.2.el7 将被 安装--&gt; 正在处理依赖关系 container-selinux &gt;= 2:2.74，它被软件包 containerd.io-1.2.10-3.2.el7.x86_64 需要--&gt; 解决依赖关系完成 错误：软件包：containerd.io-1.2.10-3.2.el7.x86_64 (docker-ce-stable) 需要：container-selinux &gt;= 2:2.74 您可以尝试添加 --skip-broken 选项来解决该问题 安装 docker-compose安装 python312345678910yum -y install zlib-devel bzip2-devel openssl-devel openssl-static ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel lzma gcccd /usr/local/srcwget https://www.python.org/ftp/python/3.7.2/Python-3.7.2.tgztar xvf Python-3.7.2.tar.xz cd Python-3.7.2/./configure --prefix=/usr/local/sbin/python-3.7make &amp;&amp; make installln -sv /usr/local/sbin/python-3.7/bin/python3 /usr/bin/python3先安装 pip。12yum install -y python34-pippip3 install --upgrade pip3为了加速，设置全局的国内 pip 源。在用户根目录新建 ~/.pip/pip.conf123[global]index-url=https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 安装 docker-compose1pip3 install -y docker-compose]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>devops</tag>
        <tag>docker</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HttpURLConnection disconnect]]></title>
    <url>%2Fp%2Fjava-HttpURLConnection-disconnect%2F</url>
    <content type="text"><![CDATA[背景 在看 nacos client 的代码，发现网络相关 http 实现是包装 HttpURLConnection，其中关闭连接是：12345finally &#123; if (null != conn) &#123; conn.disconnect(); &#125;&#125;很少直接用 HttpURLConnection 写代码了，但是记得之前写代码，通常只关闭流 12345finally &#123; if (null != conn &amp;&amp; conn.getInputSteam()!= null ) &#123; conn.getInputSteam().close(); &#125;&#125; 于是复习下。HttpURLConnection.disconnect()HttpURLConnection 对 disconnect 描述；1234567/** * Indicates that other requests to the server * are unlikely in the near future. Calling disconnect() * should not imply that this HttpURLConnection * instance can be reused for other requests. */public abstract void disconnect();具体实现类：sun.net.www.protocol.http.HttpURLConnection1234567891011121314151617181920212223242526272829303132333435public void disconnect() &#123; this.responseCode = -1; if (this.pi != null) &#123; this.pi.finishTracking(); this.pi = null; &#125; if (this.http != null) &#123; if (this.inputStream != null) &#123; HttpClient var1 = this.http; boolean var2 = var1.isKeepingAlive(); try &#123; this.inputStream.close(); &#125; catch (IOException var4) &#123; &#125; if (var2) &#123; var1.closeIdleConnection(); &#125; &#125; else &#123; this.http.setDoNotRetry(true); this.http.closeServer(); &#125; this.http = null; this.connected = false; &#125; this.cachedInputStream = null; if (this.cachedHeaders != null) &#123; this.cachedHeaders.reset(); &#125;&#125;sun.net.www.http.HttpClient12345678910111213141516171819protected static KeepAliveCache kac = new KeepAliveCache();public void closeIdleConnection() &#123; HttpClient var1 = kac.get(this.url, (Object)null); if (var1 != null) &#123; var1.closeServer(); &#125;&#125;public void closeServer() &#123; try &#123; this.keepingAlive = false; this.serverSocket.close(); &#125; catch (Exception var2) &#123; &#125;&#125;小结：disconnect()清除 keepingAlive，关闭 serverSocket。HttpURLConnection.getInputSteam().close()HttpURLConnection 的 javadocCalling the close() methodson the InputStream or OutputStream of an HttpURLConnectionafter a request may free network resources associated with thisinstance but has no effect on any shared persistent connection.Calling the disconnect() method may close the underlying socketif a persistent connection is otherwise idle at that time.小结 在开启 keep-alive 的情况下：HttpURLConnection.disconnect()会关闭底层 socket。HttpURLConnection.getInputSteam().close()只关闭输入流，可以重用底层 socket。回到 nacos client，定时向 nacos server 轮询配置更新，查询结束主动关闭连接， 不复用底层 socket，见 ClientWorker 的构造函数 12345678910executor.scheduleWithFixedDelay(new Runnable() &#123; @Override public void run() &#123; try &#123; checkConfigInfo(); &#125; catch (Throwable e) &#123; LOGGER.error("[" + agent.getName() + "] [sub-check] rotate check error", e); &#125; &#125;&#125;, 1L, 10L, TimeUnit.MILLISECONDS);checkConfigInfo() 最终会访问 nacos server。如果任务正常结束，那么 10ms 之后又会发起网络请求。资料Safe use of HttpURLConnectionhttp-keepalive]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>http</tag>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql generated column]]></title>
    <url>%2Fp%2Fmysql-generated-column%2F</url>
    <content type="text"><![CDATA[简介 mysql 5.7 提供了 generated column 生成特殊的列，该列的定义是一个函数。mysql 支持 Virtual Generated Column 与 Stored Generated Column。12column_name data_type [GENERATED ALWAYS] AS (expression) [VIRTUAL | STORED] [UNIQUE [KEY]]GENERATED ALWAYS 表明是 generated column。Virtual Generated Column 保存在数据字典中（表的元数据），并不会将这一列数据持久化到磁盘上。Stored Generated Column 将会持久化到磁盘上，而不是每次读取的时候计算所得。Stored 模式会消耗更多的磁盘空间，默认使用的是 Virtual。例子 1外国人的名字 full_name=first_name + last_name，可以表示为：1234567CREATE TABLE contacts ( id INT AUTO_INCREMENT PRIMARY KEY, first_name VARCHAR(50) NOT NULL, last_name VARCHAR(50) NOT NULL, full_name varchar(101) GENERATED ALWAYS AS (CONCAT(first_name,'',last_name)), email VARCHAR(100) NOT NULL);例子 2mysql 5.7.8 以后支持了 json 类型。为了加速检索，可以为 json 内部的字段创建 virtual generated column，再创建索引：123456789CREATE TABLE `t_person` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, `extra` json DEFAULT NULL, `ft` varchar(255) GENERATED ALWAYS AS (json_extract(`extra`,'$.name')) VIRTUAL, PRIMARY KEY (`id`), KEY `idx_ft` (`ft`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;然后就可以在 ft 字段上创建索引了。效果类似 oracle 的函数索引了。但是目前 generated column 不支持全文索引。对于 virtual 类型的衍生列，创建索引时，会将衍生列值物化到索引键里，即把衍生列的值计算出来，然后存放在索引里。针对 virtual 类型的衍生列索引，在 insert 和 update 操作时会消耗额外的写负载。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql json 系列 3：底层实现简介]]></title>
    <url>%2Fp%2Fmysql-json-type-part-3-inner-implementation%2F</url>
    <content type="text"><![CDATA[简介 MySQL 5.7 支持 JSON 的做法是，在 server 层提供了一堆便于操作 JSON 的函数，至于存储，就是简单地将 JSON 编码成 BLOB，然后交由存储引擎层进行处理，也就是说，MySQL 5.7 的 JSON 支持与存储引擎没有关系。mysql json 代码，主要在sql 目录。5.7.27 与 json 相关：12345678910item_json_func.ccitem_json_func.hjson_binary.ccjson_binary.hjson_dom.ccjson_dom.hjson_path.ccjson_path.hopt_explain_json.ccopt_explain_json.h8.0 又新增了：123456json_diff.ccjson_diff.hjson_schema.ccjson_schema.hjson_syntax_check.ccjson_syntax_check.hjson_binary.hjson_binary.h包含了 json 类型的存储结构，比较简单：123456789101112131415161718192021222324252627282930313233The actual keys and values will come after the header, in the sameorder as in the header.object ::= element-count size key-entry* value-entry* key* value*array ::= element-count size value-entry* value*// number of members in object or number of elements in arrayelement-count ::= uint16 | // if used in small JSON object/array uint32 // if used in large JSON object/array// number of bytes in the binary representation of the object or arraysize ::= uint16 | // if used in small JSON object/array uint32 // if used in large JSON object/arraykey-entry ::= key-offset key-lengthkey-offset ::= uint16 | // if used in small JSON object uint32 // if used in large JSON objectkey-length ::= uint16 // key length must be less than 64KBvalue-entry ::= type offset-or-inlined-value// This field holds either the offset to where the value is stored,// or the value itself if it is small enough to be inlined (that is,// if it is a JSON literal or a small enough [u]int).offset-or-inlined-value ::= uint16 | // if used in small JSON object/array uint32 // if used in large JSON object/array注意，key 的长度要小于 64KB。再看一张网上的图 partial update5.7.x 的 json update，即便指定更新某个 path，底层实现是读取整个 json 字段，修改完后再回写整个 json 字段。 对于大型的 json，效率显然不高。8.0 版本引入 partial update，能够只更新对应字段。从 binlog 来观察对比 json update 的实现，就很直观，具体见 Partial (Optimised) JSON updates and Replication in MySQL 8.0。partial update 的好处： 减少 IO，提供 update tps减少 binlog 文件大小 支持 partial update 的函数 123JSON_SET() JSON_REPLACE()JSON_REMOVE() 相关源码：json_diff.h1234The Json_diff class is used to represent a logical change in a JSON column,so that a replication master can send only what has changed, instead ofsending the whole new value to the replication slave when a JSON column isupdated.json 的变更，抽象到一个向量。最终的二进制结构：123456789101112/** Vector of logical diffs describing changes to a JSON column.*/class Json_diff_vector/** The binary format has this form: +--------+--------+--------+ +--------+ | length | diff_1 | diff_2 | ... | diff_N | +--------+--------+--------+ +--------+**/开启 partial update，修改 my.cnf123binlog_format=ROWbinlog_row_image=MINIMALbinlog_row_value_options=partial_jsonbinlog_row_value_options=PARTIAL_JSON takes effect only when binary logging is enabled and binlog_format is set to ROW or MIXED. 参考MySQL5.7 的 JSON 实现]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql json 系列 2：整合 mybatis]]></title>
    <url>%2Fp%2Fmysql-json-type-part-2-integrate-with-mybatis%2F</url>
    <content type="text"><![CDATA[整体方案 数据库的 extra 字段是 json 类型，相应的 jdbc 类型是 JdbcType.VARCHAR 。 对应 java 实体类的 extra 字段，类型是 Map&lt;String, Object&gt; 。通过 Mybatis TypeHandler 机制，实现 java 类型和 jdbc 类型转换。通过自定义 mapper 函数方式，实现 json 字段的增量更新。前置要求 mysql server verson1select version()&gt;= 5.7.8jdbc driver version12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.48&lt;/version&gt;&lt;/dependency&gt;&gt;= 5.1.48mybatis 自定义 Mybatis TypeHandlerTypeHandler 解决 Java 数据类型和 DB 数据类型的转换。mybatis 提供一堆默认实现，具体见 org.apache.ibatis.type。 这里只需要使用 json 字段存储 object，不考虑 json array（当然实现起来也简单）。@MappedTypes是 java 类型。@MappedJdbcTypes是 jdbc 类型。这里使用 fastjson 进行 json 序列化 / 反序列化。123456789101112131415161718192021222324252627282930313233343536@MappedTypes(Map.class)@MappedJdbcTypes(JdbcType.VARCHAR)public class ObjectJsonTypeHandler extends BaseTypeHandler&lt;Map&gt; &#123; @Override public void setNonNullParameter(PreparedStatement ps, int i, Map parameter, JdbcType jdbcType) throws SQLException &#123; ps.setString(i, JSON.toJSONString(parameter)); &#125; @Override public Map getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; String sqlJson = rs.getString(columnName); if (null != sqlJson)&#123; return JSON.parseObject(sqlJson); &#125; return null; &#125; @Override public Map getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; String sqlJson = rs.getString(columnIndex); if (null != sqlJson)&#123; return JSON.parseObject(sqlJson); &#125; return null; &#125; @Override public Map getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; String sqlJson = cs.getString(columnIndex); if (null != sqlJson)&#123; return JSON.parseObject(sqlJson); &#125; return null; &#125;&#125;在 mapper xml 增加映射 在 mapper xml 文件的 /mapper/resultMap 节点增加动态字段类型映射 1&lt;result column="extra" typeHandler="com.xxx.ObjectJsonTypeHandler" property="extra" /&gt;application.yml 增加扫描 typehandler123mybatis: // typehandler 所在的 package 路径 type-handlers-package: com.xxxCRUD 操作select by id 无需特殊处理。从 mysql 读取的 json 字符串，被反序列化为 Map 类型。insert无需特殊处理。整个 Map 被序列化为 json 字符串，以 varchar 类型写入 mysql。update by id更新 json 字段的“全部数据”，无需特殊处理。整个 Map 被序列化为 json 字符串，以 varchar 类型写入 mysql。更新 json 字段的“部分字段”，需要增加自定义 mapper 方法，底层使用 mysql 的 json_set 函数更新，例子如下：1234567891011&lt;update id="updateByPrimaryKeyPartial" parameterType="com.xxx.Person"&gt; update t_person set name = #&#123;name,jdbcType=VARCHAR&#125;, age = #&#123;age,jdbcType=INTEGER&#125;, extra = JSON_SET(extra, &lt;foreach collection="extra" index="key" separator="," item="value"&gt; '$.$&#123;key&#125;', #&#123;value&#125; &lt;/foreach&gt; ) where id = #&#123;id,jdbcType=INTEGER&#125;&lt;/update&gt;这里有 2 个细节：第一，JSON_SET()函数:1JSON_SET(json_doc, path, val[, path, val] ...)设置指定路径的数据（不管是否存在）。如果有参数为 NULL，则返回 NULL。第二，mybatis xml 遍历 Map 类型。使用 foreach 标签。当 collection 指向 Map 类型，index对应 key，item对应 value。由于 key 是字符串，并且是列字段名，这里使用 ${key} 读取字面值。如果使用了 #{key} 则生成的 sql 多添加了 &quot;&quot;，导致语法报错。delete by id 无需特殊处理。json 字段上 where 条件 mysql json type 的字段，需要使用特殊语法访问(&#39;$.&lt;json 内的字段名 &gt;&#39;):1select * from t_person where extra-&gt;'$.type' = 22 为了简化 java 层面使用，基于 mybatis generator example，封装了 GenericExample：12345GenericExample example2 = new GenericExample();GenericExample.Criteria criteria2 = example2.createCriteria();criteria2.andEqualTo("extra-&gt;'$.name'", "aaa", "extra.name");criteria2.andGreaterThan("extra-&gt;'$.type'", 222, "extra.type");List&lt;Person2&gt; result2 = personMapper2.selectByExample(example2);全文检索问题 mysql json type 不支持全文索引。 即使对 json 字段建立伪列 (generated column)，也不支持全文检索。 目前只能使用 like 模糊检索：1select * from t_person where extra like '%aaa%'实践 tips虽然 json 内部可以存储不同类型，但是为了序列化和反序列化方便，我在应用层进行了使用约束：整数 浮点 字符串 布尔 日期类型转换为 unix 时间戳。业务上使用额外的表，存储 json 内部字段的期望类型，在 dto 层进行转换。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql json 系列 1：简介]]></title>
    <url>%2Fp%2Fmysql-json-type-part-1-intro%2F</url>
    <content type="text"><![CDATA[简介 在没有 myql json 类型之前，我们在应用层也可以存储 json 内容，手动序列化为 json 字符串，再写入 varchar/clob 或者 blob 类型，读取的时候再手动反序列化。这样缺点明显：需要用户保证 JSON 的正确性，如果用户插入的数据并不是一个有效的 JSON 字符串，MySQL 并不会报错；所有对 JSON 的操作，都需要在用户的代码里进行处理，不够友好；即使只是提取 JSON 中某一个字段，也需要读出整个 BLOB，效率不高；无法在 JSON 字段上建索引。Mysql 5.7.8 以后版本增加了 json 类型，支持 json object 和 json array。由于是原生支持，提供了 json 语法检查、特殊 dml 函数、也支持字段索引。基本操作 mysql 5.7 / 8.0 基本的 json 函数如下： 注意，json_append()、json_merge()函数已经在 5.7.x 废弃了。json object123&#123;"cnt": 111, "some_attr": "hello"&#125;函数 JSON_OBJECT(key1,val1,key2,val2...)json array123["abc", 10, null, true, false]函数 JSON_ARRAY(val1,val2,val3...)convert 函数支持字符串转换为 json 类型：1select convert('&#123;"name":"aaa","type": 22,"lucky": 33&#125;', json)怎么知道 json 字段具体是 object 还是 array？使用 json_type()测试：1234567mysql&gt; select json_type(extra) from t_person where id = 1;+------------------+| json_type(extra) |+------------------+| OBJECT |+------------------+1 row in set (0.03 sec)读取 json 数据 访问 json 里面的字段。-&gt;操作符，返回值带双引号。-&gt;&gt;操作符，返回值去掉双引号，要求 mysql version 5.7.13+。123456789101112131415mysql&gt; select extra-&gt;'$.name' from t_person where id = 1;+------------------+| extra-&gt; '$.name' |+------------------+| "aaa" |+------------------+1 row in set (0.02 sec)mysql&gt; select extra-&gt;&gt; '$.name' from t_person where id = 1;+-------------------+| extra-&gt;&gt; '$.name' |+-------------------+| aaa |+-------------------+1 row in set (0.02 sec)也可以使用 json_extract()提取数据 1234567mysql&gt; select json_extract(extra, '$.name') from t_person where id = 1;+-------------------------------+| json_extract(extra, '$.name') |+-------------------------------+| "aaa" |+-------------------------------+1 row in set (0.02 sec) 默认情况下，json 类型以压缩字符串形式显示，太丑了？使用 json_pretty()美化输出格式 1234567891011mysql&gt; select json_pretty(extra) from t_person where id = 1;+---------------------------------------------+| json_pretty(extra) |+---------------------------------------------+| &#123; "name": "aaa", "type": 22, "lucky": 1&#125; |+---------------------------------------------+1 row in set (0.05 sec) 想知道 json 字段的长度？使用 json_storage_size()1234567mysql&gt; select json_storage_size(extra) from t_person where id = 1;+--------------------------+| json_storage_size(extra) |+--------------------------+| 43 |+--------------------------+1 row in set (0.04 sec)查找 json 数据 1JSON_SEARCH(json_doc, one_or_all, search_str[, escape_char[, path] ...]) 查询包含指定字符串的 paths，并作为一个 json array 返回。如果有参数为 NUL 或 path 不存在，则返回 NULL。one_or_all：”one”表示查询到一个即返回；”all”表示查询所有。search_str：要查询的字符串。 可以用 LIKE 里的’%’或‘_’匹配。修改 json 数据 JSON_INSERT(json_doc, path, val[, path, val] ...) 在指定 path 下插入数据，如果 path 已存在，则忽略此 val（不存在才插入）。JSON_REPLACE(json_doc, path, val[, path, val] ...)替换指定路径的数据，如果某个路径不存在则略过（存在才替换）。如果有参数为 NULL，则返回 NULL。JSON_SET(json_doc, path, val[, path, val] ...)设置指定路径的数据（不管是否存在）。如果有参数为 NULL，则返回 NULL。索引 为了加快数据检索，可以为 json 内部的字段创建 virtual column，再建立索引，从而加快检索。123456789CREATE TABLE `t_person` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `age` int(11) DEFAULT NULL, `extra` json DEFAULT NULL, `ft` varchar(255) GENERATED ALWAYS AS (json_extract(`extra`,'$.name')) VIRTUAL, PRIMARY KEY (`id`), KEY `idx_ft` (`ft`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;但是，virtual column 不支持全文检索！参考The JSON Data TypeJSON FunctionsMySQL JSON 类型Presentation : JSON improvements in MySQL 8.0]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序分享功能简单使用]]></title>
    <url>%2Fp%2Fminiapp-share-function-usage%2F</url>
    <content type="text"><![CDATA[开启分享功能 page 中增加onShareAppMessage() 就可以开启分享功能。12345678910111213/** * 用户点击右上角分享 */onShareAppMessage: function () &#123; var that = this var playIndex = this.data.playIndex return &#123; imageUrlId: SHARE_MSG_IMG_ID, // 通过 MP 系统审核的图片编号 imageUrl: SHARE_MSG_IMG_URL, // 通过 MP 系统审核的图片地址 path: '/pages/index/index?uid=' + that.data.musicList[playIndex]['uid'] &#125;&#125; 注意：imageUrl 尺寸长宽比例是 5:4。path 是要分享的页面，必须是全路径。url 后面可以增加自定义参数。读取分享参数 我的场景很简单，只要读取分享 url 参数即可。onLoad()增加 options 参数就可以了。12345onLoad: function (options) &#123; if (options &amp;&amp; options.uid) &#123; console.log('share uid=', options.uid) this.data.fromUid = options.uid &#125;回调问题 最初为了调试，增加分享成功的回调，发现模拟器和真机都不生效。后来发现微信把分享回调能力关闭了，具体参见这个公告：“分享监听”能力调整。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy 爬虫经历]]></title>
    <url>%2Fp%2Fscrapy-crawler-in-practise%2F</url>
    <content type="text"><![CDATA[前言 因为版权原因，在线听歌越来越麻烦，于是开始自建一个小乐库，纯属自用。记录一下爬取经历。突破首页防卫 用在线工具测试好了首页 xpath 提取表达式，UserAgent 什么的都设置了，但是在 scrapy 执行一直报错。于是 curl 一下首页，不开心了：12345678910111213&lt;html&gt;&lt;head&gt;&lt;script language="javascript"&gt;setTimeout("try&#123;setCookie();&#125;catch(error)&#123;&#125;;location.replace(location.href.split(\"#\")[0])",2000);&lt;/script&gt;&lt;script type="text/javascript" src="http://********:80/usershare/flash.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt;var ret=getIPs(function(ip)&#123;rtcsetcookie(ip)&#125;);checkflash(ret)&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=7,0,0,0" width="0" height="0" id="m" align="center"&gt; &lt;param name="allowScriptAccess" value="always"/&gt;&lt;param name="movie" value="http://********:80/usershare/1.swf"/&gt;&lt;param name="quality" value="high" /&gt; &lt;embed src="http://********:80/usershare/1.swf" quality="high" width="0" height="0" name="m" align="center" allowScriptAccess="always" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer"/&gt;&lt;/object&gt;&lt;/body&gt;&lt;/html&gt;计算 cookie，然后 loadpage 得到加载页面。js 文件没有混淆，可以模拟计算，或者用 JavaScript 插件执行。不过在浏览器看了请求参数，发现 1&apos;Cookie&apos;: &apos;__cfduid=dcbd0294352d18643417ae3aa8bbdd8fe1576674941; blk=0; jploop=false; jpvolume=0.923&apos;__cfduid，那是用了 cloudflare 防护。想起来了，之前偶尔打开也遇到 5 秒钟防护盾的页面。1The _cfduid cookie collects and anonymizes End User IP addresses using a one-way hash of certain values so they cannot be personally identified. The cookie is a session cookie that expires after 30 days. 于是找到 cloudflare-scrape 模块 A simple Python module to bypass Cloudflare’s anti-bot page (also known as “I’m Under Attack Mode”, or IUAM), implemented with Requests. 看能不能绕过去，得到真实的首页:123456789import cfscrape def __init__(self): # returns a CloudflareScraper instance self.scraper = cfscrape.create_scraper(delay=5) def start_requests(self): url = 'xxxx' text = self.scraper.get(url).text可是报错了 123 File &quot;c:\users\xxx\pycharmprojects\scrapy_learn\venv\lib\site-packages\cfscrape\__init__.py&quot;, line 351, in get_tokens &apos;Unable to find Cloudflare cookies. Does the site actually have Cloudflare IUAM (&quot;I\&apos;m Under Attack Mode&quot;) enabled?&apos;ValueError: Unable to find Cloudflare cookies. Does the site actually have Cloudflare IUAM (&quot;I&apos;m Under Attack Mode&quot;) enabled? 试了一下，先手动打开访问一次网页，再用 CloudflareScraper 解析，就可以正常访问首页数据，具体原因还没想明白，先用着 12345def start_requests(self): url = 'xxxx' r = requests.get(url) r.close() text = self.scraper.get(url).text 多级爬取 爬取思路：首页解析音乐类别列表（genre 列表）分页爬取每个音乐类别的歌曲列表（play list 列表）爬取每个歌曲播放页面的元数据（play music 列表）整个爬取流程经历：首页 -&gt; 音乐分类 -&gt; 歌曲列表 -&gt; 歌曲页面 最终要解析元数据的页面是歌曲页面。但是要经历前面 3 个页面，才能遍历全量的歌曲页面地址。scrapy 的 start_requests 方法用于初始化爬取网页 url。以前写简单的页面爬取，直接在 parse 方法做最终页面的解析即可。难道要在 parse 方法塞几个页面的解析逻辑？Off course not! scrapy 的 Request 支持 callback。不同页面的设置不同回调函数解析即可。1yield Request(play_music_page_url, headers=self.headers, callback=self.parse_music, meta=meta)分页爬取上下文 分页爬取，直接读取接口 1http://xxxx/genre/musicList/&#123;genre&#125;?pageIndex=&#123;pageIdx&#125;&amp;pageSize=20&amp;order=hot 有个小问题，pageIdx 要用来计算下一次的页码。可以用正则提取。但是 scrapy Request 提供了 meta 对象，用来保留多次爬取的上下文，非常方便。发送 meta12345yield Request(str.format(self.GENRE_DETAIL_TEMPLATE, genre_idx, page_idx), headers=self.headers, meta=&#123; "genre_idx": genre_idx, "page_idx": page_idx, "genre": tag&#125;)读取 meta12next_page_idx = response.meta['page_idx'] + 1genre_idx = response.meta['genre_idx']xpath 解析 解析音乐文件路径，发现 url 不是写在某个属性，而是在 script 里面拼接。1234567891011121314151617181920212223&lt;script type="text/javascript"&gt; var format = "mp3"; var PageData = &#123;&#125;; PageData.Host="xxxxx"; PageData.DownHost="xxxxx"; PageData.down="down7"; PageData.musicId = "33"; PageData.likeCount = "1221"; PageData.commentCount = 176; PageData.day=20; PageData.code=12; PageData.format="mp3"; var ip="xxxxx"; var fileHost="xxxxx"; var musicHost="xxxxx"; var isCMobile=isMobileIp(ip) &amp;&amp; window.isCMIp; setItem("cm",isCMobile); var mp3="33/mp3/12"; mp3=fileHost+mp3; var bdText = "清晨 (New Morning) - 班得瑞"; var bdText2 = "分享一首好听的轻音乐：" + bdText; var imgUrl="xxxxx";&lt;/script&gt; 首先要定位这个 script，发现 xpath 表达式在页面工具和 scrapy 解析有兼容性问题，单纯计算 script index 定位的结果不一致😂。那就用文本匹配。涉及 2 个 xpath 函数：contains()：匹配一个属性值中包含的字符串 text()：显示文本信息 定位 script 标签，再转为文本做正则匹配即可。12script = str(response.xpath('//script[contains(text(),"var mp3=")]').extract()[0])match = re.search('mp3=".+";', script)同样方式得到 fileHost 等属性。另外，提取标题的时候，发现元数据出现多个空格，正则解决1re.sub('+', ' ', title)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql binlog 初体验]]></title>
    <url>%2Fp%2Fmysql-binlog-tour%2F</url>
    <content type="text"><![CDATA[业务上引入 canal + binlog 解析作为 mysql 数据实时同步解决方案。于是整理下 binlog 相关运维资料。开启 binlog修改 my.cnf。123456[mysqld]server-id=1log_bin=ONlog_bin_basename=/var/lib/mysql/mysql-binlog_bin_index=/var/lib/mysql/mysql-bin.indexbinlog_format=row最小配置只需要 server-id 和log_bin：1234[mysqld]server-id=1log_bin=/var/lib/mysql/mysql-bin.logbinlog_format=row因为使用了 canal 同步，因此 binlog_format=row。 然后重启 mysqld。检查是否开启 binlog：1234567891011mysql&gt; show variables like &apos;log_bin%&apos;;+---------------------------------+--------------------------------+| Variable_name | Value |+---------------------------------+--------------------------------+| log_bin | ON || log_bin_basename | /var/log/mysql/mysql-bin || log_bin_index | /var/log/mysql/mysql-bin.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF |+---------------------------------+--------------------------------+5 rows in setlog_bin是 ON 则开启 binlog 成功。binlog 操作 查看所有 binlog123456789mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 164416086 || mysql-bin.000002 | 9257415 || mysql-bin.000003 | 3240423 |+------------------+-----------+3 rows in set查看正在使用的 binlog1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000003 | 3265489 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in setposition 是当前的记录位置。刷新 binlog 文件 新建一个 binlog 文件。12345678910mysql&gt; flush logs;Query OK, 0 rows affectedmysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000004 | 1633 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set可以用来保留现场。清空 binlog删除所有 binlog 文件，binlog 序号复位，从 0 开始。12345678910mysql&gt; reset master;Query OK, 0 rows affectedmysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 1074 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set高能警告：不要删除正在使用的 binlog 文件，会导致同步工具异常。参见这个文章：canal 使用环境问题记录 查看 binlogmysql client很简单 1mysql&gt; show binlog events [IN &apos;log_name&apos;] [FROM pos] [LIMIT [offset,] row_count];mysqlbinlogmysqlbinlog 使用要小心1234567--start-datetime 开始时间--stop-datetime 结束时间--database=resource 选择数据库--result-file 结果输出到某个文件--base64-output=decode-rows --start-position 开始位置--stop-position 结束位置 完整参数见 --help。12345678910--base64-output=name Determine when the output statements should be base64-encoded BINLOG statements: &apos;never&apos; disables it and works only for binlogs without row-based events; &apos;decode-rows&apos; decodes row events into commented pseudo-SQL statements if the --verbose option is also given; &apos;auto&apos; prints base64 only when necessary (i.e., for row-based events and format description events). If no --base64-output[=name] option is given at all, the default is &apos;auto&apos;. 一个例子：1mysqlbinlog --no-defaults --database=xxx --start-position=377 --stop-position=583 --base64-output=decode-row mysql-bin.000001效果如下：12345678910/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 377#191217 14:59:26 server id 1 end_log_pos 583 CRC32 0x0092f046 Update_rows: table id 155 flags: STMT_END_FSET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;unknown variable ‘default-character-set=utf8’最初使用 mysqlbinlog 读取 binlog，遇到问题：12$ mysqlbinlog mysql-bin.000001mysqlbinlog: unknown variable &apos;default-character-set=utf8&apos;原因是 mysqlbinlog 无法识别 binlog 中的配置中的 default-character-set=utf8 这个配置。解决方式有 2 个：修改 my.cnf，然后重启 mysqld1234[mysql]default-character-set = utf8[mysqld]character_set_server = utf8或者，使用 --no-defaults 选项 1mysqlbinlog --no-defaults mysql-bin.000001 从 binlog 恢复 1mysqlbinlog &lt;binlog file&gt; | mysql -u 用户名 -p 密码 数据库名 先找到要恢复的开始 / 结束时间，或者 position 范围。然后把对应的 binlog 重放一次。参考资料的链接有案例。mysqlbinlog 恢复要避免无穷复制问题：数据库 A 读取自己的 binlog，然后写回数据库 A。因为恢复的时候也会产生 binlog，导致无限循环。解决的办法是恢复阶段禁用 binlog，涉及选项如下：1234567891011121314-D, --disable-log-bin Disable binary log. This is useful, if you enabled --to-last-log and are sending the output to the same MySQL server. This way you could avoid an endless loop. You would also like to use it when restoring after a crash to avoid duplication of the statements you already have. NOTE: you will need a SUPER privilege to use this option.-t, --to-last-log Requires -R. Will not stop at the end of the requested binlog but rather continue printing until the end of the last binlog of the MySQL server. If you send the output to the same MySQL server, that may lead to an endless loop.清理 binlog删除所有 binlog： reset master; 。开启一个新的 binlog，不影响已有文件：flush logs; 。配置自动删除，修改 my.cnf 并且重启：12[mysqld]expire_logs_days = x // 二进制日志自动删除的天数。默认值为 0, 表示“没有自动删除”手动清理日志 123purge master logs to &apos;mysql-bin.000005&apos;; // 指定清理某文件前所有的文件purge master logs before &apos;2019-12-15 00:00:00&apos;; purge master logs before date_sub(now(), interval 7 day); 温馨提示：不要删除正在使用的 binlog 文件，会导致同步工具异常。不要删除正在使用的 binlog 文件，会导致同步工具异常。不要删除正在使用的 binlog 文件，会导致同步工具异常。参考MySQL 的 binlog 日志]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>binlog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 插件之 git-commit-id-plugin]]></title>
    <url>%2Fp%2Fmaven-git-commit-id-plugin%2F</url>
    <content type="text"><![CDATA[因为每次构建不一定会升级版本号，为了把 artifact 和版本对应起来，可以使用 git-commit-id-plugin 插件。引入插件 1234567891011121314151617181920212223&lt;plugin&gt; &lt;groupId&gt;pl.project13.maven&lt;/groupId&gt; &lt;artifactId&gt;git-commit-id-plugin&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;get-the-git-infos&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;revision&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;initialize&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;generateGitPropertiesFile&gt;true&lt;/generateGitPropertiesFile&gt; &lt;generateGitPropertiesFilename&gt;$&#123;project.build.outputDirectory&#125;/git.properties&lt;/generateGitPropertiesFilename&gt; &lt;includeOnlyProperties&gt; &lt;includeOnlyProperty&gt;^git.build.(time|version)$&lt;/includeOnlyProperty&gt; &lt;includeOnlyProperty&gt;^git.commit.id.(abbrev|full)$&lt;/includeOnlyProperty&gt; &lt;/includeOnlyProperties&gt; &lt;commitIdGenerationMode&gt;full&lt;/commitIdGenerationMode&gt; &lt;/configuration&gt;&lt;/plugin&gt;git-commit-id-plugin 能够收集比较多的 git 信息，因此使用includeOnlyProperties 指定需要保留的信息。所有支持的属性参见：GitCommitPropertyConstant.java:12345678910111213141516171819202122232425262728293031public class GitCommitPropertyConstant &#123; // these properties will be exposed to maven public static final String BRANCH = "branch"; public static final String LOCAL_BRANCH_AHEAD = "local.branch.ahead"; public static final String LOCAL_BRANCH_BEHIND = "local.branch.behind"; public static final String DIRTY = "dirty"; // only one of the following two will be exposed, depending on the commitIdGenerationMode public static final String COMMIT_ID_FLAT = "commit.id"; public static final String COMMIT_ID_FULL = "commit.id.full"; public static final String COMMIT_ID_ABBREV = "commit.id.abbrev"; public static final String COMMIT_DESCRIBE = "commit.id.describe"; public static final String COMMIT_SHORT_DESCRIBE = "commit.id.describe-short"; public static final String BUILD_AUTHOR_NAME = "build.user.name"; public static final String BUILD_AUTHOR_EMAIL = "build.user.email"; public static final String BUILD_TIME = "build.time"; public static final String BUILD_VERSION = "build.version"; public static final String BUILD_HOST = "build.host"; public static final String BUILD_NUMBER = "build.number"; public static final String BUILD_NUMBER_UNIQUE = "build.number.unique"; public static final String COMMIT_AUTHOR_NAME = "commit.user.name"; public static final String COMMIT_AUTHOR_EMAIL = "commit.user.email"; public static final String COMMIT_MESSAGE_FULL = "commit.message.full"; public static final String COMMIT_MESSAGE_SHORT = "commit.message.short"; public static final String COMMIT_TIME = "commit.time"; public static final String REMOTE_ORIGIN_URL = "remote.origin.url"; public static final String TAGS = "tags"; public static final String CLOSEST_TAG_NAME = "closest.tag.name"; public static final String CLOSEST_TAG_COMMIT_COUNT = "closest.tag.commit.count"; public static final String TOTAL_COMMIT_COUNT = "total.commit.count";&#125;这些变量可以在 pom 中使用，比如增加到 artifact 的文件名中：123&lt;properties&gt; &lt;version.number&gt;$&#123;git.commit.time&#125;.$&#123;git.commit.id.abbrev&#125;&lt;/version.number&gt;&lt;/properties&gt;完整使用手册参见：using-the-plugin默认会在 target/classess 目录增加 git.properties 文件。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序 canvas 绘制进度条]]></title>
    <url>%2Fp%2Fminiapp-canvas-draw-progress-bar%2F</url>
    <content type="text"><![CDATA[背景 一个音乐小程序，最初使用 progress 组件实现进度条。模拟器上一切正常，但是放到真机上发现进度条不见了！打开真机调试器，使用“节点审查”，发现 progress 组件被 canvas 遮盖了。 canvascanvas 层级比小程序原生组件更高，因此覆盖在 progress 之上。在 css 中，可以使用 z-index 属性控制层级。但是 canvas 无视 z-index 属性。对于这个问题，小程序提供了 cover-view 等标签，可以覆盖在 canvas 之上。但是 cover-view 不支持内嵌 progress 组件。目前只想到使用 canvas 绘制进度条。rpx 适配 rpx 单位是微信小程序中 css 的尺寸单位，rpx 可以根据屏幕宽度进行自适应。规定屏幕宽为 750rpx。如在 iPhone6 上，屏幕宽度为 375px，共有 750 个物理像素，则 750rpx = 375px = 750 物理像素，1rpx = 0.5px = 1 物理像素。 使用 rpx 作为单位，可以方便进行屏幕适配。rpx 换算成 px 的公式是：1rpx = 屏幕宽度 / 750。然而，小程序 canvas 不支持 rpx 作为单位，使用的是 px。为了实现适配，在 onLoad 事件保存屏幕分辨率信息。1234567891011var h, w;wx.getSystemInfo(&#123; success: function (res) &#123; h = res.screenHeight; w = res.screenWidth; &#125;,&#125;)this.setData(&#123; screenHeight: h, screenWidth: w&#125;)然后再进行缩放。绘制进度条 进度条可以看成两个方块的叠加。一个方块是背景，另一个方块是当前进度。123456var ctx = wx.createCanvasContext('progressBar');ctx.setFillStyle('grey')ctx.fillRect(0, 0, width, 12)ctx.setFillStyle('green')ctx.fillRect(0, 0, parseInt(width * progress), 12)ctx.draw()这里要留意 draw 函数 123456CanvasContext.draw(boolean reserve, function callback) 参数 boolean reserve 本次绘制是否接着上一次绘制。即 reserve 参数为 false，则在本次调用绘制之前 native 层会先清空画布再继续绘制；若 reserve 参数为 true，则保留当前画布上的内容，本次调用 drawCanvas 绘制的内容覆盖在上面，默认 false。做了如下修改，未确定是否有效😂1234567891011121314// reserve 参数:// true：保留原来的绘图，越来越慢 // false：不保留，会刷新绘图区域// var cnt = this.data.progressCntif (cnt++ &gt; PROGRESS_BAR_REFERSH_CNT) &#123; cnt = 0 ctx.draw(false)&#125; else &#123; ctx.draw(true)&#125;this.setData(&#123; progressCnt: cnt&#125;)ps: 后来才知道setData 会触发页面刷新。progressCnt 的更新不涉及页面刷新，可以放在另一个变量存储：123456if (this.counter.progressCnt++ &gt; PROGRESS_BAR_REFERSH_CNT) &#123; this.counter.progressCnt = 0 ctx.draw(false)&#125; else &#123; ctx.draw(true)&#125;获取播放时间 backgroundAudioManager.duration 是只读属性，表示音乐文件的长度，单位是秒。经过测试，播放的时候立即读取这个属性，可能是 undefined。加载音频时可能需要缓冲，因此延迟一段时间再读取，发生异常概率小很多：1234567backgroundAudioManager.onCanplay(() =&gt; &#123; console.log(backgroundAudioManager.duration); setTimeout(() =&gt; &#123; // Yes, I can console.log(backgroundAudioManager.duration); &#125;, 500);&#125;);定时器 wx.setInterval()： 返回 number 类型，是定时器的编号。这个值可以传递给 clearTimeout 来取消该定时。考虑到进度条对小程序在前台展示才有用，注册在 onTimeUpdate 事件：12BackgroundAudioManager.onTimeUpdate(function callback)监听背景音频播放进度更新事件，只有小程序在前台时会回调。闪屏问题 偶尔发生闪屏，但是没有复现，有点麻烦。估计是 canvas 的问题，待跟踪 单曲循环问题 这个跟 canvas 无关，但是值得记下。在 BackgroundAudioManager.onEnded 事件中增加重放，直接从 0 开始播放 12backgroundAudioManager.startTime = 0;backgroundAudioManager.play(); 结果时不时不能正常播放。加上 console 打印，发现 backgroundAudioManager.src 为空，查资料发现，BAM 播放结束后 src 会被置空。解决方式很简单，增加一个变量保存当前 src，单曲循环模式下触发 onEnded 事件后把这个 src 填到 BAM 即可。参考资料 小程序坑 -canvas微信小程序 BackgroundAudioManager currentTime、duration 的问题以及如何规避 小程序 BackgroundAudioManager 踩坑之旅]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springmvc war 配置外部化]]></title>
    <url>%2Fp%2Fspringmvc-war-read-external-config%2F</url>
    <content type="text"><![CDATA[又是一个 legacy 项目，springmvc + war 部署，xml beans 配置文件，现在要镜像部署。原来配置文件写在 WEB-INF 目录，需要改成外部化方便修改、不需要重新构建镜像。试了一下，有几种方式实现。少量几个配置项 启动参数增加： -DIP_CONF=111.222.111.222beans 配置文件使用 ${} 注入 123&lt;bean id="aService" class="com.xxx.services.AService"&gt; &lt;property name="ip" value="$&#123;IP_CONF&#125;"&gt;&lt;/property&gt;&lt;/bean&gt; 配置文件 和上面的例子相似。启动参数增加： -Dip.config.file=/app/xxx/ip.propertiesbeans 配置文件使用 ${} 注入 1&lt;util:properties id="ipProperties" location="$&#123;ip.config.file:/WEB-INF/ip.properties&#125;"/&gt; 这里增加了默认值。解析多个 properties 文件 spring 容器中最多只能定义一个context:property-placeholder，否则会报错：1Could not resolve placeholder &apos;moduleb.jdbc.driverClassName&apos; in string value &quot;$&#123;moduleb.jdbc.driverClassName&#125;&quot; 原因：Spring 容器采用反射扫描的发现机制，在探测到 Spring 容器中有一个 org.springframework.beans.factory.config.PropertyPlaceholderConfigurer 的 Bean 就会停止对剩余 PropertyPlaceholderConfigurer 的扫描。util:propertiesutil:properties支持多个配置文件，location字段传入逗号分隔的列表即可。1&lt;util:properties id="appProperties" location="$&#123;cas.properties.filepath:/WEB-INF/cas.properties&#125;,file:$&#123;catalina.base&#125;/conf/jdbc.properties"/&gt;propertyConfigurer12345678910111213141516&lt;!-- 将多个配置文件位置放到列表中 --&gt;&lt;bean id="propertyResources" class="java.util.ArrayList"&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;!-- 这里支持多种寻址方式：classpath 和 file --&gt; &lt;value&gt;$&#123;cas.properties.filepath:/WEB-INF/cas.properties&#125;&lt;/value&gt; &lt;!-- 推荐使用 file 的方式引入，这样可以将配置和代码分离 --&gt; &lt;value&gt;file:$&#123;catalina.base&#125;/conf/app.properties&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 将配置文件读取到容器中，交给 Spring 管理 --&gt;&lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations" ref="propertyResources" /&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springmvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven war overlays 机制]]></title>
    <url>%2Fp%2Fmaven-war-overlays%2F</url>
    <content type="text"><![CDATA[maven war overlay最近学习 CAS 系统，发现 CAS 推荐使用 overlay 方式进行定制。一般的定制化开发，可能要修改某个页面、修改某个功能实现，为此 clone 整个开源项目做二次开发，这样做法比较重型，侵入性太强，不利于以后升级主干版本。其中一种解决思路是，在原来开源项目的基础上，单独修改某些实现，打包的时候再合并替换掉。maven 的 war overlay 机制实现了上面的功能。overlay 可以把多个项目 war 合并成为一个项目，并且如果项目存在同名文件，那么主项目中的文件将覆盖掉其他项目的同名文件。以 CAS 为例子 2 个概念： 主项目：pom 文件开启 overlay 配置的项目，通常是二次开发源码的项目。从项目：基础项目。如果主项目和从项目出现相同路径、相同名字的文件，那么从项目的内容会被主项目替换掉。这个例子里面是 cas-server-webapp 项目。创建主项目 cas-server-overlays-demo 引入从项目 CAS 的定制开发，会使用cas-server-webapp 作为从项目。1234567&lt;dependency&gt; &lt;groupId&gt;org.jasig.cas&lt;/groupId&gt; &lt;artifactId&gt;cas-server-webapp&lt;/artifactId&gt; &lt;version&gt;$&#123;cas.version&#125;&lt;/version&gt; &lt;type&gt;war&lt;/type&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;配置 maven overlay123456789101112131415&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;configuration&gt; &lt;failOnMissingWebXml&gt;false&lt;/failOnMissingWebXml&gt; &lt;warName&gt;cas&lt;/warName&gt; &lt;overlays&gt; &lt;overlay&gt; &lt;groupId&gt;org.jasig.cas&lt;/groupId&gt; &lt;artifactId&gt;cas-server-webapp&lt;/artifactId&gt; &lt;/overlay&gt; &lt;/overlays&gt; &lt;/configuration&gt;&lt;/plugin&gt;maven-war-plugin 默认要求打包的时候要有 web.xml 文件。因此关闭了 failOnMissingWebXml 选项。修改 finalName1&lt;finalName&gt;cas&lt;/finalName&gt;定制化开发 比如定制化登录页，index.jsp。注意定制化文件，路径和文件名必须要和 cas-server-webapp 项目一致，否则不生效。pros and consoverlay 的优点：源代码修改的管理和跟踪非常轻量级 方便升级主干版本 overlay 的缺点： 第一次配置有点繁琐 参考资料WAR Overlay InstallationOverlays]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迁移老 web 项目到 springboot]]></title>
    <url>%2Fp%2Fmigrate-old-web-project-to-springboot%2F</url>
    <content type="text"><![CDATA[背景 原来有个项目，传统的 spring + war 包部署，在开源基础上做了定制开发，现在接收过来了。由于后续有继续开发维护的需要，期望引入 springboot 框架，使用 swagger 做 api 管理。最初想用 springboot java config 完全重写，但是看到一堆 xml bean/servlet 配置，遂放弃，改用兼容已有 web.xml。引入 spring-boot-legacyspring-boot-legacy是一个 spring 官方组件，用于把 servlet 2.5 应用迁移到 springboot。支持解析 web.xml。注意 GitHub 里面的 web.xml 转换工具已经不可以访问了。pom.xml：123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-legacy&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt;启动类：增加引入原来的 spring xml 配置 12345678@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, SecurityAutoConfiguration.class&#125;)@ImportResource(locations = &#123;"classpath:/spring-configuration/all.xml"&#125;)public class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125;web.xml： 增加启动类，且注释掉原来 contextConfigLocation 的配置。1234&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;com.xxx.cas.App&lt;/param-value&gt;&lt;/context-param&gt;增加 SpringBootContextLoaderListener，去掉原来的 ContextLoaderListener。123&lt;listener&gt; &lt;listener-class&gt;org.springframework.boot.legacy.context.web.SpringBootContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt;日志冲突 12java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.CasLoggerFactory loaded from file:/C:/tool/apache-tomcat-8.5.49/webapps/cas/WEB-INF/lib/cas-server-core-4.1.1.jar). If you are using WebLogic you will need to add &apos;org.slf4j&apos; to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.CasLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext at org.springframework.util.Assert.isInstanceOf(Assert.java:346)Add exclusion to both the spring-boot-starter and spring-boot-starter-web to resolve the conflict.123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;GenericApplicationListener1Caused by: java.lang.NoClassDefFoundError: org/springframework/context/event/GenericApplicationListener 官网 表示 spring 4.2 以后才支持。解决：spring 版本更新到 5.1.5.RELEASE。WEB-INF 资源 1class path resource [WEB-INF/spring-configuration/filters.xml] cannot be opened because it does not exist 问题：@ImportResource试了多种方式，不能访问 WEB-INF 目录下的 xml。解决：复制到 resources 目录 spring security 4 csrfCAS 4.x 集成的是 spring security 3。升级 springboot 2.x 后同时也升级到 spring security 4。Spring Security 4 默认启用了 CSRF 保护功能，该功能在 Spring Security 3 时就已经存在，默认是不启用。12org.springframework.beans.factory.support.BeanDefinitionOverrideException: Invalid bean definition with name &apos;requestDataValueProcessor&apos; defined in null: Cannot register bean definition [Root bean: class [org.springframework.security.web.servlet.support.csrf.CsrfRequestDataValueProcessor]; scope=; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null] for bean &apos;requestDataValueProcessor&apos;: There is already [Root bean: class [org.springframework.security.web.servlet.support.csrf.CsrfRequestDataValueProcessor]; scope=; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null] bound. at org.springframework.beans.factory.support.DefaultListableBeanFactory.registerBeanDefinition(DefaultListableBeanFactory.java:897) 解决：手动增加创建 requestDataValueProcessor。1&lt;bean id="requestDataValueProcessor" class="org.springframework.security.web.servlet.support.csrf.CsrfRequestDataValueProcessor"&gt;&lt;/bean&gt;service 文件 获取 CAS service 文件路径 1234@Beanpublic ServiceRegistryDao serviceRegistryDao() throws IOException &#123; return new JsonServiceRegistryDao(new File(AppConfig.class.getResource("/services").getFile()));&#125;allow-bean-definition-overriding1The bean &apos;userRepository&apos;, defined in null, could not be registered. A bean with that name has already been defined in file XXX and overriding is disabled. 允许覆盖 bean 定义。配置文件增加：1spring.main.allow-bean-definition-overriding=true参见 DefaultListableBeanFactory.registerBeanDefinition()1234567BeanDefinition existingDefinition = this.beanDefinitionMap.get(beanName);if (existingDefinition != null) &#123; if (!isAllowBeanDefinitionOverriding()) &#123; throw new BeanDefinitionOverrideException(beanName, beanDefinition, existingDefinition); &#125; // more codes&#125;Cannot subclass final class1Caused by: java.lang.IllegalArgumentException: Cannot subclass final class org.jasig.cas.services.DefaultServicesManagerImplfinal 类不可以被继承，因此不能被代理 proxy。CGLib 也没戏，这是 java 规范定义的。 解决：复制一份类，去掉 final，相应的引用路径修改。或者使用 maven overlay 方式打包。新类的路径不变。tomcat 日志乱码 windows 默认编码是 GBK。tomcat 日志默认编码是 UTF-8。因此乱码。 解决：修改 tomcat 目录的 conf/logging.properties1java.util.logging.ConsoleHandler.encoding = GBKurl mapping 重复定义12Caused by: java.lang.IllegalArgumentException: The FilterChainProxy contains two filter chains using the matcher Ant [pattern=&apos;/status/**&apos;]. If you are using multiple &lt;http&gt; namespace elements, you must use a &apos;pattern&apos; attribute to define the request patterns to which they apply. at org.springframework.security.config.http.DefaultFilterChainValidator.checkForDuplicateMatchers(DefaultFilterChainValidator.java:70)web.xml 和加载的 securityContext.xml 重复定义。注释掉即可。ObjectPostProcessor1Parameter 0 of method setObjectPostProcessor in org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter required a bean of type &apos;org.springframework.security.config.annotation.ObjectPostProcessor&apos; that could not be found. 关闭 spring security 自动配置。123@EnableAutoConfiguration(exclude = &#123; org.springframework.boot.autoconfigure.security.SecurityAutoConfiguration.class&#125;)ServletContextListener 重复加载 1Caused by: java.lang.IllegalArgumentException: Once the first ServletContextListener has been called, no more ServletContextListeners may be added.ServletContextListener 重复加载。不能为属性:[commandName]找到 setter 方法 1Caused by: org.apache.jasper.JasperException: /WEB-INF/view/jsp/default/ui/casLogin.jsp (行.: [75], 列: [20]) 不能为属性:[commandName] 找到 setter 方法.原来项目使用了 spring form 标签，其中 commandName 已经废弃，已经被 modelAttribute 替代了，于是更改 jsp 文件中的标签：1&lt;form:form method=&quot;post&quot; id=&quot;fm1&quot; commandName=&quot;$&#123;commandName&#125;&quot; htmlEscape=&quot;true&quot;&gt;修改为 1&lt;form:form method=&quot;post&quot; id=&quot;fm1&quot; modelAttribute=&quot;$&#123;commandName&#125;&quot; htmlEscape=&quot;true&quot;&gt; 集成 swagger 2pom.xml：12345678910111213141516&lt;!-- Swagger2 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;swagger-bootstrap-ui&lt;/artifactId&gt; &lt;version&gt;1.9.2&lt;/version&gt;&lt;/dependency&gt;java config：123456789101112131415161718192021222324@Configuration@EnableSwagger2@EnableWebMvcpublic class SwaggerConfig &#123; @Bean public Docket api() &#123; return new Docket(DocumentationType.SWAGGER_2) .select() .apis(RequestHandlerSelectors.basePackage("com.xxx.yyy.controller")) .build().apiInfo(apiInfo()); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title("开放接口 API") .description("HTTP 对外开放接口") .version("1.0.0") .termsOfServiceUrl("http://xxx.xxx.com") .license("LICENSE") .licenseUrl("http://xxx.xxx.com") .build(); &#125;&#125;web.xml 放行 swagger 入口，写在 CAS 之前。1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/swagger-ui.html&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;swagger2 和 guava 兼容性 按上面的配置，启动报错：123An attempt was made to call the method com.google.common.collect.FluentIterable.concat(Ljava/lang/Iterable;Ljava/lang/Iterable;)Lcom/google/common/collect/FluentIterable; but it does not exist. Its class, com.google.common.collect.FluentIterable, is available from the following locations: jar:file:/C:/tool/apache-tomcat-8.5.49/webapps/cas/WEB-INF/lib/guava-18.0.jar!/com/google/common/collect/FluentIterable.class更换新的 guava 包即可12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;28.1-jre&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序开发者工具杂锦 case]]></title>
    <url>%2Fp%2Fminiapp-case%2F</url>
    <content type="text"><![CDATA[上次倒腾仿网易云音乐小程序，发现后端接口组装接口略为复杂，算了，换个 demo 继续搞。只是导入开发者工具，一堆黄色提示，烦人，于是整理一下。sitemap根据 sitemap 的规则 [0]，当前页面 [pages/index/index] 将被索引 网站上的 sitemap 是站点地图，方便搜索引擎爬取页面。小程序的 sitemap 作用也是一样的。当开发者允许微信索引时，微信会通过爬虫的形式，为小程序的页面内容建立索引。当用户的搜索词条触发该索引时，小程序的页面将可能展示在搜索结果中。 爬虫访问小程序内页面时，会携带特定的 user-agent：mpcrawler 及 场景值：1129。开发者工具关闭 sitemap 提示的方式：sitemap 的索引提示是默认开启的，如需要关闭 sitemap 的索引提示，可在小程序项目配置文件 project.config.json 的 setting 中配置字段 checkSiteMap 为 falsewx:keyNow you can provide attr wx:key for a wx:for to improve performance.根据 列表渲染 描述：如果列表中项目的位置会动态改变或者有新的项目添加到列表中，并且希望列表中的项目保持自己的特征和状态（如 input 中的输入内容，switch 的选中状态），需要使用 wx:key 来指定列表中项目的唯一的标识符。当数据改变触发渲染层重新渲染的时候，会校正带有 key 的组件，框架会确保他们被重新排序，而不是重新创建，以确保使组件保持自身的状态，并且提高列表渲染时的效率。如不提供 wx:key，会报一个 warning， 如果明确知道该列表是静态，或者不必关注其顺序，可以选择忽略。cover-view 和 canvas1&lt;cover-view/&gt; 内只能嵌套 &lt;cover-view/&gt; &lt;cover-image/&gt; &lt;button/&gt; &lt;navigator/&gt; &lt;ad/&gt;，canvas 标签的子节点树在真机上都会被忽略。cover-view 能覆盖在原生组件 map、video、canvas、camera 之上，且只能嵌套特定标签。通常可以用来做弹窗。导入的 demo 在 cover-view 内嵌套 canvas，导致提示。注释掉即可。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canal 使用环境问题记录]]></title>
    <url>%2Fp%2Fcanal-in-practise-env%2F</url>
    <content type="text"><![CDATA[上个月使用 canal，因为 kafka 配置、k8s mysql 挂载配置问题导致 canal 异常，记录下来。kafka advertised.listeners把 canal 部署到一台新的开发机器，启动报错。1234567892019-11-15 16:12:19.480 [pool-4-thread-1] ERROR com.alibaba.otter.canal.kafka.CanalKafkaProducer - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. at com.alibaba.otter.canal.kafka.CanalKafkaProducer.produce(CanalKafkaProducer.java:215) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.kafka.CanalKafkaProducer.send(CanalKafkaProducer.java:179) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.kafka.CanalKafkaProducer.send(CanalKafkaProducer.java:120) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter.worker(CanalMQStarter.java:183) [canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter.access$500(CanalMQStarter.java:23) [canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter$CanalMQRunnable.run(CanalMQStarter.java:225) [canal.server-1.1.4.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_60]一开始怀疑是 producer client 的 Jar 包版本与 kafka 集群版本不兼容（以前踩过坑，印象深刻）。但是查资料发现：在 Kafka 0.10.2.0 之前，Kafka 服务器端和客户端版本之间的兼容性是“单向”的，即高版本的 broker 可以处理低版本 client 的请求。反过来，低版本的 broker 不能处理高版本 client 的请求。自 0.10.2.0 版本开始，社区对这个问题进行了优化——对于低版本 broker + 高版本 client(0.10.2.0)的环境而言，现在用户可以运行命令先查看当前 broker 支持的协议版本，然后再选择 broker 支持的最高版本封装请求即可。kafka 服务器是 0.10.2.0，canal 的 kafka client 是 1.1.1。因此是兼容的。不过用测试命令发现：12345678[root@k8s-master bin]# ./kafka-broker-api-versions.sh --bootstrap-server localhost:9092Exception in thread &quot;main&quot; java.lang.RuntimeException: Request METADATA failed on brokers List(localhost:9092 (id: -1 rack: null)) at kafka.admin.AdminClient.sendAnyNode(AdminClient.scala:66) at kafka.admin.AdminClient.findAllBrokers(AdminClient.scala:90) at kafka.admin.AdminClient.listAllBrokerVersionInfo(AdminClient.scala:136) at kafka.admin.BrokerApiVersionsCommand$.execute(BrokerApiVersionsCommand.scala:42) at kafka.admin.BrokerApiVersionsCommand$.main(BrokerApiVersionsCommand.scala:36) at kafka.admin.BrokerApiVersionsCommand.main(BrokerApiVersionsCommand.scala)kafka、canal 是部署在同一台开发服务器。怀疑是配置问题：kafka 的 server.properties 配置如下 12listeners=PLAINTEXT://&lt;ip&gt;:9092advertised.listeners=PLAINTEXT://&lt;ip&gt;:9092 其中 &lt;ip&gt; 是开发服务器的 ip 地址。listeners： 真正监听的网络接口 advertised.listeners： 对外暴露的网络接口，会注册到 zookeeper 但是 canal.properties1canal.mq.servers = 127.0.0.1:9092 把 canal 配置改为 advertised.listeners 的地址即可。mysql binlog position 对齐 运行了 2 天，周一回来发现不能正常接收 binlog 了。mysql 部署在 k8s 上，然后 k8s 挂了！重启 k8s mysql，日志报错：123452019-11-19 11:22:58.844 [destination = example , address = /127.0.0.1:31503 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236, sqlstate = HY000 errmsg = Client requested master to start replication from position &gt; file size at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:235) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:265) at java.lang.Thread.run(Thread.java:745)binlog 同步位置不对。打开 k8s configmap，发现 mysql binlog 没有挂载在宿主机目录，重启后同步位置都没了😂。从日志看，canal 本地存储的 position 比 mysql 的 binlog 还大。检查 canal 保存同步 position：12345678910[root@k8s-master conf]# cd example/[root@k8s-master example]# lltotal 84-rw-r--r-- 1 root root 73728 Nov 19 11:24 h2.mv.db-rwxrwxrwx 1 root root 2092 Nov 18 11:05 instance.properties-rwxr-xr-x 1 root root 2036 Nov 15 14:07 instance.properties.bak-rw-r--r-- 1 root root 336 Nov 19 11:21 meta.dat[root@k8s-master example]# cat meta.dat &#123;&quot;clientDatas&quot;:[&#123;&quot;clientIdentity&quot;:&#123;&quot;clientId&quot;:1001,&quot;destination&quot;:&quot;example&quot;,&quot;filter&quot;:&quot;&quot;&#125;,&quot;cursor&quot;:&#123;&quot;identity&quot;:&#123;&quot;slaveId&quot;:-1,&quot;sourceAddress&quot;:&#123;&quot;address&quot;:&quot;localhost&quot;,&quot;port&quot;:31503&#125;&#125;,&quot;postion&quot;:&#123;&quot;gtid&quot;:&quot;&quot;,&quot;included&quot;:false,&quot;journalName&quot;:&quot;mysql-bin.000001&quot;,&quot;position&quot;:225115292,&quot;serverId&quot;:1,&quot;timestamp&quot;:1574093224000&#125;&#125;&#125;],&quot;destination&quot;:&quot;example&quot;&#125;因为是开发环境，直接把 meta.dat 清空。正规来说，要把 mysql binlog position 更新回 meta.dat。重启 canal，这下正常了。mysql reset master 导致同步异常 在写 binlog 相关操作文档，执行 reset master; 以后，binlog 序号复位，从 0 开始。12345678910mysql&gt; reset master;Query OK, 0 rows affectedmysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 1074 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set后来发现 canal 报错了，example.log 如下：123452019-12-17 15:01:42.374 [destination = example , address = /127.0.0.1:31503 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.io.IOException: Received error packet: errno = 1236, sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:235) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:265) at java.lang.Thread.run(Thread.java:745)Could not find first log file name in binary log index file很可能是同步水位问题。检查 conf/example/meta.dat 如下 1234567891011121314151617181920212223242526272829&#123; "clientDatas": [ &#123; "clientIdentity": &#123; "clientId": 1001, "destination": "example", "filter": "" &#125;, "cursor": &#123; "identity": &#123; "slaveId": -1, "sourceAddress": &#123; "address": "localhost", "port": 31503 &#125; &#125;, "postion": &#123; "gtid": "", "included": false, "journalName": "mysql-bin.000004", "position": 15570, "serverId": 1, "timestamp": 1576565961000 &#125; &#125; &#125; ], "destination": "example"&#125;mysql 已经重置了 binlog，序号复位从 0 开始，删除了其他的 binlog 文件。但是 canal 没有同步更新，本地还是等待消费 mysql-bin.000004，因此报错。 解决：删除 meta.dat思考：不要删除正在复制的 binlog 文件。参考资料Kafka 协议兼容性改进]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 仓库嵌套问题]]></title>
    <url>%2Fp%2Fgit-project-nested-case%2F</url>
    <content type="text"><![CDATA[新建了一个项目，把 github 上一个网易云音乐的小程序和后台 clone 过来做定制。但是 git add 后添加不上，文件不能提交上去。12345678910111213C:\workspace\music_miniapp (master -&gt; origin)λ git add .C:\workspace\music_miniapp (master -&gt; origin)λ git commit -m &quot;repush&quot;On branch masterYour branch is up to date with &apos;origin/master&apos;.Changes not staged for commit: modified: NeteaseMusicWxMiniApp (modified content, untracked content) modified: netmusic-node (modified content, untracked content)no changes added to commit因为这 2 个子项目是从 git 上 clone 过来，有 .git 目录。Git 仓库嵌套后，被嵌套的 Git 仓库不能被外层 Git 仓库检测到。于是删除两个子项目的 .git 目录，更新 cached。12345C:\workspace\music_miniapp (master -&gt; origin)λ git rm -rf --cached .rm &apos;NeteaseMusicWxMiniApp&apos;rm &apos;README.md&apos;rm &apos;netmusic-node&apos;再次 git add 和 commit，可以正常识别文件了。删除子项目的 .git 目录，就不能正常 pull 从而更新。如果要保持嵌套项目更新，可以使用git submodule。1git submodule add https://github.com/sqaiyan/NeteaseMusicWxMiniApp]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis generator 生成 selective mapper 方法]]></title>
    <url>%2Fp%2Fmybatis-generator-targetruntime-and-mapper-selective-method%2F</url>
    <content type="text"><![CDATA[背景 项目使用 mybatis generator 生成 entity、mapper 类。拷贝祖传配置，执行后发现没有生成 xxxSelective 方法，很不习惯。一开始以为是 table 没有指定相应 selective 方法，于是打开 mybatis generator 的 dtd 文件，发现并没有关于生成 selective 的选项：12345678910111213141516171819202122&lt;!ATTLIST table catalog CDATA #IMPLIED schema CDATA #IMPLIED tableName CDATA #REQUIRED alias CDATA #IMPLIED domainObjectName CDATA #IMPLIED mapperName CDATA #IMPLIED sqlProviderName CDATA #IMPLIED enableInsert CDATA #IMPLIED enableSelectByPrimaryKey CDATA #IMPLIED enableSelectByExample CDATA #IMPLIED enableUpdateByPrimaryKey CDATA #IMPLIED enableDeleteByPrimaryKey CDATA #IMPLIED enableDeleteByExample CDATA #IMPLIED enableCountByExample CDATA #IMPLIED enableUpdateByExample CDATA #IMPLIED selectByPrimaryKeyQueryId CDATA #IMPLIED selectByExampleQueryId CDATA #IMPLIED modelType CDATA #IMPLIED escapeWildcards CDATA #IMPLIED delimitIdentifiers CDATA #IMPLIED delimitAllColumns CDATA #IMPLIED&gt;后来经同事指出是 targetRuntime 的不同引起的。这次项目的配置是 1&lt;context id="mysql" targetRuntime="MyBatis3Simple" &gt; 以前的配置是 1&lt;context id="mysql" targetRuntime="MyBatis3" &gt;mybatis generator targetRuntimeIntrospectedTable 定义了 2 种基本 targetRuntime12345678public abstract class IntrospectedTable &#123; public enum TargetRuntime &#123; MYBATIS3, MYBATIS3_DSQL &#125; // more code&#125;MYBATIS3 对应 IntrospectedTableMyBatis3Impl、 IntrospectedTableMyBatis3SimpleImpl。MYBATIS3_DSQL 对应 IntrospectedTableMyBatis3DynamicSqlImplV1 和 V2(不是这次的讨论重点)。IntrospectedTableMyBatis3SimpleImpl 继承于 IntrospectedTableMyBatis3Impl。IntrospectedTableMyBatis3Impl 提供了 createJavaClientGenerator() 默认实现，并且可以被子类覆盖，这是选择 java 代码生成器，生成不同的 mapper 方法。AbstractJavaGenerator 定义了 getCompilationUnits()，获取要生成的模板单元（主要是各种 mapper 方法）123public abstract class AbstractJavaGenerator extends AbstractGenerator &#123; public abstract List&lt;CompilationUnit&gt; getCompilationUnits();&#125; 接下来再看看不同 targetRuntime 选择的 code generator。IntrospectedTableMyBatis3Impl123456789101112131415161718192021222324protected AbstractJavaClientGenerator createJavaClientGenerator() &#123; if (context.getJavaClientGeneratorConfiguration() == null) &#123; return null; &#125; String type = context.getJavaClientGeneratorConfiguration() .getConfigurationType(); AbstractJavaClientGenerator javaGenerator; if ("XMLMAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new JavaMapperGenerator(getClientProject()); &#125; else if ("MIXEDMAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new MixedClientGenerator(getClientProject()); &#125; else if ("ANNOTATEDMAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new AnnotatedClientGenerator(getClientProject()); &#125; else if ("MAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new JavaMapperGenerator(getClientProject()); &#125; else &#123; javaGenerator = (AbstractJavaClientGenerator) ObjectFactory .createInternalObject(type); &#125; return javaGenerator;&#125;打开其中一个生成器，比如 JavaMapperGenerator12345678910111213141516171819@Overridepublic List&lt;CompilationUnit&gt; getCompilationUnits() &#123; // addCountByExampleMethod(interfaze); addDeleteByExampleMethod(interfaze); addDeleteByPrimaryKeyMethod(interfaze); addInsertMethod(interfaze); addInsertSelectiveMethod(interfaze); addSelectByExampleWithBLOBsMethod(interfaze); addSelectByExampleWithoutBLOBsMethod(interfaze); addSelectByPrimaryKeyMethod(interfaze); addUpdateByExampleSelectiveMethod(interfaze); addUpdateByExampleWithBLOBsMethod(interfaze); addUpdateByExampleWithoutBLOBsMethod(interfaze); addUpdateByPrimaryKeySelectiveMethod(interfaze); addUpdateByPrimaryKeyWithBLOBsMethod(interfaze); addUpdateByPrimaryKeyWithoutBLOBsMethod(interfaze); //&#125;IntrospectedTableMyBatis3SimpleImpl12345678910111213141516171819202122protected AbstractJavaClientGenerator createJavaClientGenerator() &#123; if (context.getJavaClientGeneratorConfiguration() == null) &#123; return null; &#125; String type = context.getJavaClientGeneratorConfiguration() .getConfigurationType(); AbstractJavaClientGenerator javaGenerator; if ("XMLMAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new SimpleJavaClientGenerator(getClientProject()); &#125; else if ("ANNOTATEDMAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new SimpleAnnotatedClientGenerator(getClientProject()); &#125; else if ("MAPPER".equalsIgnoreCase(type)) &#123; //$NON-NLS-1$ javaGenerator = new SimpleJavaClientGenerator(getClientProject()); &#125; else &#123; javaGenerator = (AbstractJavaClientGenerator) ObjectFactory .createInternalObject(type); &#125; return javaGenerator;&#125;打开其中一个生成器，比如 SimpleJavaClientGenerator12345678910@Overridepublic List&lt;CompilationUnit&gt; getCompilationUnits() &#123; // addDeleteByPrimaryKeyMethod(interfaze); addInsertMethod(interfaze); addSelectByPrimaryKeyMethod(interfaze); addSelectAllMethod(interfaze); addUpdateByPrimaryKeyMethod(interfaze); //&#125;至此，找到 xxxSelective 方法没有生成的原因。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canal mysql 初体验]]></title>
    <url>%2Fp%2Fcanal-mysql-first-exp%2F</url>
    <content type="text"><![CDATA[背景 技术上的需求，要解析 mysql binlog，再同步数据到 elasticsearch。一个简单的 demo，使用 canal 接收 mysql binlog 日志，并且写入到 kafka topic。新版的 canal 已经原生支持写入 kafka、rocketmq，只需要配置即可，无需开发 canal producer。安装 kafka、zookeeper、canal 就不再重复了。配置 mysql在 [mysqld] 开启 binlog，并且设置 binlog_format 为 ROW。12345server-id = 1log_bin = /var/log/mysql/mysql-bin.logexpire_logs_days = 10max_binlog_size = 100Mbinlog_format = ROW如果使用 statement 或者 mixed format，那么 binlog 里面只能看到 sql 语句，没有对应的数据。修改配置后重启 mysql1sudo systemctl start mysql.service为 canal 创建 mysql 访问账号 嗯，这里我挖了坑。create user 之后 grant all privilege，导致报错，详见后面的解析。(这才是正确的)12CREATE USER 'canal'@'%' IDENTIFIED BY 'canal';GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal';配置 canalconf/canal.properties是 canal server 的配置文件。修改几个重要配置即可 123456789101112# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.zkServers = 127.0.0.1# tcp, kafka, RocketMQcanal.serverMode = kafkacanal.mq.servers = 127.0.0.1:9092conf/example 下面的是 canal client 配置。打开 instace.properties12345678910111213# position infocanal.instance.master.address=127.0.0.1:3306# username/passwordcanal.instance.dbUsername=canalcanal.instance.dbPassword=canal# table regex#canal.instance.filter.regex=.*\\..*canal.instance.filter.regex=user\\.t_person# mq configcanal.mq.topic=binlog-test 包括了 mysql 服务器地址、canal 连接 mysql 的账号、要同步的表、kafka topic 等配置。更多的配置见 Canal Kafka RocketMQ QuickStart 启动 canal1bin/startup.shcanal server 日志在 logs/canal/，canal client 日志在logs/example/。 观察发现 client 日志报错：1234562019-11-14 16:29:57.529 [destination = example , address = /127.0.0.1:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /127.0.0.1:3306 has an error, retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: command : &apos;show master status&apos; has an error!Caused by: java.io.IOException: ErrorPacket [errorNumber=1227, fieldCount=-1, message=Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation, sqlState=42000, sqlStateMarker=#] with command: show master status at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) ~[canal.parse.driver-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:106) ~[canal.parse-1.1.4.jar:na] 因为 canal server 伪装为 mysql slave，读取 binlog 二进制流来解析，因此需要 replication 的权限。1GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%' IDENTIFIED BY 'canal';重新 grant 之后，日志如下 1232019-11-14 16:32:48.276 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - ---&gt; begin to find start position, it will be long time for reset or first position2019-11-14 16:32:48.277 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status2019-11-14 16:32:49.299 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - ---&gt; find start position successfully, EntryPosition[included=false,journalName=mysql-bin.000001,position=4,serverId=1,gtid=&lt;null&gt;,timestamp=1573720110000] cost : 1014ms , the next step is binlog dump 插入一条 sql1INSERT INTO `user`.`t_person` (`id`, `name`, `age`) VALUES ('1', '22', '3');因为配置 canal client 写入 kafka topic，直接查看 1234bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic binlog-test --from-beginning&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1573720366000,&quot;id&quot;:1,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &apos;canal&apos;@&apos;%&apos; IDENTIFIED WITH &apos;mysql_native_password&apos; AS &apos;*E3619321C1A937C46A0D8BD1DAC39F93B27D4458&apos;&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;&quot;,&quot;ts&quot;:1573720369373,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:[&#123;&quot;id&quot;:&quot;1&quot;,&quot;name&quot;:&quot;22&quot;,&quot;age&quot;:&quot;3&quot;&#125;],&quot;database&quot;:&quot;user&quot;,&quot;es&quot;:1573721530000,&quot;id&quot;:2,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:&#123;&quot;id&quot;:&quot;int(11)&quot;,&quot;name&quot;:&quot;varchar(255)&quot;,&quot;age&quot;:&quot;int(11)&quot;&#125;,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;sqlType&quot;:&#123;&quot;id&quot;:4,&quot;name&quot;:12,&quot;age&quot;:4&#125;,&quot;table&quot;:&quot;t_person&quot;,&quot;ts&quot;:1573721530702,&quot;type&quot;:&quot;INSERT&quot;&#125; 第一条是 grant 的记录，是 dcl 语句。第二条是刚刚插入的，是 dml 语句。因为使用了 canal.properties 的默认配置，都放行了。后续可以在 # binlog filter config 加上过滤。尝试 delete 效果 1234567# mysqlINSERT INTO `user`.`t_person` (`id`, `name`, `age`) VALUES (&apos;2&apos;, &apos;33&apos;, &apos;44&apos;);DELETE FROM `user`.`t_person` WHERE ID &gt; -1;# kafka topic&#123;&quot;data&quot;:[&#123;&quot;id&quot;:&quot;1&quot;,&quot;name&quot;:&quot;22&quot;,&quot;age&quot;:&quot;3&quot;&#125;,&#123;&quot;id&quot;:&quot;2&quot;,&quot;name&quot;:&quot;33&quot;,&quot;age&quot;:&quot;44&quot;&#125;],&quot;database&quot;:&quot;user&quot;,&quot;es&quot;:1573736190000,&quot;id&quot;:4,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:&#123;&quot;id&quot;:&quot;int(11)&quot;,&quot;name&quot;:&quot;varchar(255)&quot;,&quot;age&quot;:&quot;int(11)&quot;&#125;,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;sqlType&quot;:&#123;&quot;id&quot;:4,&quot;name&quot;:12,&quot;age&quot;:4&#125;,&quot;table&quot;:&quot;t_person&quot;,&quot;ts&quot;:1573736190235,&quot;type&quot;:&quot;DELETE&quot;&#125; 批量查询可以看到每条受影响的记录。这里要小心，如果批量删除的数据很大，会导致 msg body 很大。考虑在应用层加上 limit 多删除几次。dcl，dml，ddlcanal 可以根据 sql 语句类型，过滤 binlog 日志DML（data manipulation language）。操作数据相关，例如 CRUD。DDL（data definition language）。table 相关，例如 create、drop、alter。DCL（Data Control Language）。权限、角色相关，例如 grant。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重启 mysql 服务]]></title>
    <url>%2Fp%2Fmysql-restart%2F</url>
    <content type="text"><![CDATA[重启 mysql因为要做个 canal binlog 解析的 demo，因此创建了 canal 账号。本地测试登录发现失败：123$ mysql -ucanal -pEnter password: ERROR 1045 (28000): Access denied for user &apos;canal&apos;@&apos;localhost&apos; (using password: YES)密码是对的。换成 -h 127.0.0.1 不行。确认 user 表配置 host 是 %。 感觉是 mysql 的问题，以前也遇到过，最后重启解决。 原本打算直接运行 mysqld 重启：1234567891011121314ubuntu@VM-0-2-ubuntu:/etc/mysql/mysql.conf.d$ mysqldmysqld: Can&apos;t change dir to &apos;/var/lib/mysql/&apos; (Errcode: 13 - Permission denied)2019-11-14T08:04:48.676534Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2019-11-14T08:04:48.676649Z 0 [Warning] Can&apos;t create test file /var/lib/mysql/VM-0-2-ubuntu.lower-test2019-11-14T08:04:48.676683Z 0 [Note] mysqld (mysqld 5.7.27-0ubuntu0.18.04.1) starting as process 644 ...2019-11-14T08:04:48.684530Z 0 [Warning] Can&apos;t create test file /var/lib/mysql/VM-0-2-ubuntu.lower-test2019-11-14T08:04:48.684554Z 0 [Warning] Can&apos;t create test file /var/lib/mysql/VM-0-2-ubuntu.lower-test2019-11-14T08:04:48.684906Z 0 [Warning] One can only use the --user switch if running as root2019-11-14T08:04:48.684943Z 0 [ERROR] failed to set datadir to /var/lib/mysql/2019-11-14T08:04:48.684957Z 0 [ERROR] Aborting2019-11-14T08:04:48.684970Z 0 [Note] Binlog end2019-11-14T08:04:48.685016Z 0 [Note] mysqld: Shutdown completeOne can only use the --user switch if running as root，根据提示修改 123ubuntu@VM-0-2-ubuntu:/etc/mysql/mysql.conf.d$ sudo mysqld --user mysqlubuntu@VM-0-2-ubuntu:/etc/mysql/mysql.conf.d$ ps aux| grep mysqldubuntu 951 0.0 0.0 13772 1032 pts/3 S+ 16:06 0:00 grep --color=auto mysqld 还是失败。尝试使用 service12ubuntu@VM-0-2-ubuntu:/etc/mysql/mysql.conf.d$ sudo service mysqld restartFailed to restart mysqld.service: Unit mysqld.service not found.还是不成功。最后尝试 systemctl1sudo systemctl start mysql.service终于成功了。之后 canal 账号也能登录了。小结：留意安装 mysql 后建立的服务。这个 mysql 是使用 apt 安装，建立的服务方式是 mysql.service。skip-grant-tables对于之前的 28000 错误码，网上有的方法是使用 skip-grant-tables 选项。具体是在 my.cnf 或者 /etc/mysql/mysql.conf.d/mysqld.cnf 增加 12[mysqld]skip-grant-tables 但是，skip-grant-tables 是在数据库启动的时候，跳跃权限表的限制，不用验证密码，直接登录。 非常粗暴，非常危险。除非忘记 root 密码，否则不建议使用。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在腾讯云上构建 CI/CD]]></title>
    <url>%2Fp%2Ftencent-cloud-build-own-ci-cd%2F</url>
    <content type="text"><![CDATA[CI/CD 迁移背景 阿里云 ecs 续费有点贵，于是把个人项目迁移到腾讯云。对应的 CI/CD 也要重新搞一套。其中感受到，贵的东西就一个缺点：贵，剩下都是优点。coding devops 现状 腾讯云提供 coding devops 作为构建平台。coding devops 底层使用 Jenkins。通过 Jenkins file，定义构建、测试、部署等步骤。但是，coding devops 只做了 CI，CD 做的比较一般。coding devops 并没有直接和腾讯云的 ecs 机器打通。构建后的 artifact 没有自动复制到关联机器上（根本就没有关联 ecs 机器的配置），比较麻烦，scp、ssh 又涉及密码、密钥问题。对比阿里云云效，流水线可以关联 ecs 机器，构建的产物自动复制到机器，用户只需要关心最终的部署脚本即可。为什么不用 Jenkins目前只有一个项目的构建和部署，使用 Jenkins 有点重型。后续 ecs 上运行的项目多了，会考虑使用 Jenkins。为什么不用镜像 实际上 coding devops 的有镜像的例子，把 artifact 打包为镜像，上传到私有仓库，再在 deploy 步骤使用 docker 命令启动容器。这是可行的一种方案。只是发现的太晚。。。思路 在腾讯云 ecs 上编写脚本，自行获取源码，构建，部署。配置 ssh源码保存在阿里云 code，目前不想搬过来到 coding。需要在阿里云 code 增加 ssh key，给腾讯云机器访问。操作参考 github 文档：生成新 SSH 密钥并添加到 ssh-agent生成构建专用的 ssh key。1ssh-keygen -t rsa -b 2048 -C &quot;devops@example.com&quot; -f /home/ubuntu/.ssh/devops加载 ssh key。先查看已经加载的 ssh key：12ubuntu@VM-0-2-ubuntu:~/.ssh$ ssh-add -lCould not open a connection to your authentication agent.发现 ssh-agent 没有启动。1234ubuntu@VM-0-2-ubuntu:~/.ssh$ eval `ssh-agent -s`Agent pid 5587ubuntu@VM-0-2-ubuntu:~/.ssh$ ssh-add -lThe agent has no identities.后续优化，使用 service 在后台启动 ssh-agent。这时候可以加载 ssh key。12ubuntu@VM-0-2-ubuntu:~/.ssh$ ssh-add /home/ubuntu/.ssh/devopsIdentity added: /home/ubuntu/.ssh/devops (/home/ubuntu/.ssh/devops)去阿里云 code 增加 ssh 公钥 安装 java、maven12sudo apt-get install openjdk-8-jdk -ysudo apt-get install maven -y增加 maven 中央仓库镜像，加速访问。修改 /etc/maven/settings.xml，在mirrors 节点增加 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 构建脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#! /bin/bashif [$# != 2] ; then echo &quot;USAGE: &lt;ENV&gt; &lt;GIT_BRANCH&gt;&quot; echo &quot;Where ENV is in [dev, prod]&quot; exit 1;fipkill ssh-agenteval `ssh-agent -s`ssh-add /home/ubuntu/.ssh/devopsENV=$1GIT_BRANCH=$2PROJECT_NAME=medicalBASE_DIR=/home/ubuntu/devops/buildPROJECT_DIR=$&#123;BASE_DIR&#125;/$&#123;PROJECT_NAME&#125;OUTPUT_DIR=$&#123;PROJECT_DIR&#125;/targetGIT_REPO=&quot;git@code.aliyun.com:Godzilla555/medical.git&quot;FINAL_ARTIFACT=medical.jarrm -rf $&#123;PROJECT_DIR&#125;mkdir -p $&#123;BASE_DIR&#125;cd $&#123;BASE_DIR&#125;git clone -b $&#123;GIT_BRANCH&#125; $&#123;GIT_REPO&#125;cd $&#123;PROJECT_DIR&#125;mvn compile package -Dmaven.test.skip=truecd $&#123;BASE_DIR&#125;cd ..mkdir -p deploy/$&#123;ENV&#125;/$&#123;PROJECT_NAME&#125;cd deploy/$&#123;ENV&#125;/$&#123;PROJECT_NAME&#125;cp $&#123;OUTPUT_DIR&#125;/$&#123;FINAL_ARTIFACT&#125; .OLD_PID=`ps aux | grep java | grep medical | grep $&#123;ENV&#125; | awk &apos;&#123; print $2 &#125;&apos; `if [-n &quot;$&#123;OLD_PID&#125;&quot;]then echo &quot;running $&#123;ENV&#125; app pid = $&#123;OLD_PID&#125;&quot; kill -9 $&#123;OLD_PID&#125; echo &quot;await termination ...&quot; sleep 5finohup java -jar medical.jar --spring.profiles.active=$&#123;ENV&#125; &amp; 搞定，以后要构建就人手触发一下。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 获取泛型类型对象]]></title>
    <url>%2Fp%2Fjava-parameterizedtype-get-actual-type%2F</url>
    <content type="text"><![CDATA[参数化类型，即泛型，在 Java 中对应 ParameterizedType。在对泛型的反射操作中，常见的是获取真实类型。ParameterizedType 有多个方法，其中 getActualTypeArguments()返回真实的类型，返回的是数组，数组元素个数是泛型个数。例如 Map&lt;K,V&gt;，getActualTypeArguments()[0] 对应 K，getActualTypeArguments()[1]对应 V。一个简单的例子：1234567891011121314151617181920212223public class A &#123; List&lt;String&gt; getList() &#123; return null; &#125; public static void main(String[] args) &#123; try &#123; Method m1 = A.class.getDeclaredMethod("getList"); System.out.println("return type:\t\t\t" + m1.getReturnType()); System.out.println("generic return type:\t" + m1.getGenericReturnType()); Type returnType = m1.getGenericReturnType(); if (returnType instanceof ParameterizedType) &#123; Type actualType = ((ParameterizedType) returnType).getActualTypeArguments()[0]; System.out.println("actual type:\t\t\t" + actualType); System.out.println("raw type:\t\t\t\t" + ((ParameterizedType) returnType).getRawType()); System.out.println("owner type:\t\t\t\t" + ((ParameterizedType) returnType).getOwnerType()); &#125; &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出 12345return type: interface java.util.Listgeneric return type: java.util.List&lt;java.lang.String&gt;actual type: class java.lang.Stringraw type: interface java.util.Listowner type: null 得到 actualType 之后，转换为 Class 对象后再实例化。12Class clz = (Class) actualType;Object obj = clz.newInstance();]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java sql 日期类型]]></title>
    <url>%2Fp%2Fjava-jdbc-sql-date-time-timestamp%2F</url>
    <content type="text"><![CDATA[简介 java.sql.Date、java.sql.Time、java.sql.Timestamp 都是 jdbc 的日期时间类型。 规范化（normalized）的 SQL date 类型，只包括年月日，其余的时分秒毫秒字段都被设置为 0。Time 在 Date 的基础上，增加保留时分秒。如果要保留毫秒精度，则使用 Timestamp。jdbc 使用的是 sql 类型，例如 PreparedStatement.setDate()、ResultSet.getDate()。源码分析 java.sql.Date、java.sql.Time、java.sql.Timestamp 都继承于 java.util.Date，包含毫秒精度的日期类型（就是平常使用的）。实际上这 3 个 sql 类型底层精度是毫秒级别。 以 sql.Date 的构造函数来看，把时间丢给父类存储。1234public Date(long date) &#123; // If the millisecond date value contains time info, mask it out. super(date);&#125;在 util.Date，使用 fastTime 保存毫秒精度的时间。cdate 可以知道是否要规范化（isNormalized()），即丢弃时间部分。12345678910111213141516171819202122232425262728public class Date implements java.io.Serializable, Cloneable, Comparable&lt;Date&gt;&#123; private transient long fastTime; /* * If cdate is null, then fastTime indicates the time in millis. * If cdate.isNormalized() is true, then fastTime and cdate are in * synch. Otherwise, fastTime is ignored, and cdate indicates the * time. */ private transient BaseCalendar.Date cdate; public Date(long date) &#123; fastTime = date; &#125; public long getTime() &#123; return getTimeImpl(); &#125; private final long getTimeImpl() &#123; if (cdate != null &amp;&amp; !cdate.isNormalized()) &#123; normalize(); &#125; return fastTime; &#125;&#125;规范化是一个懒操作，在 getTime()的时候按需要触发。类型转换util.Date 转换为 sql.Date1java.sql.Date sqlDate = new java.sql.Date(new java.util.Date().getTime());sql.Date 是 util.Date 的子类，因此可以12java.util.Date ud = sd;// where sd is an instance of sql.Date]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot 集成 Mybatis PageHelper 不生效的 case]]></title>
    <url>%2Fp%2Fspringboot-mybatis-pagehelper-integration%2F</url>
    <content type="text"><![CDATA[项目使用 mybatis spring boot starter 集成 mybatis 插件。增加 PageHelper 插件，拷贝网上集成 pagerhelper 的方法没有生效。 后来发现可以使用 pagehelper springboot starter 继承。记录下来做参考。pom.xml 配置如下：1234567891011&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt;version 修改为对应版本。pagehelper-spring-boot-starter 会自动引入 pagehelper-spring-boot-autoconfigure 和 pagehelper 依赖。PageHelperAutoConfiguration 类的 addPageInterceptor() 实现配置和加载插件。1234567891011121314151617181920212223242526272829303132333435363738@Configuration@ConditionalOnBean(SqlSessionFactory.class)@EnableConfigurationProperties(PageHelperProperties.class)@AutoConfigureAfter(MybatisAutoConfiguration.class)public class PageHelperAutoConfiguration &#123; @Autowired private List&lt;SqlSessionFactory&gt; sqlSessionFactoryList; @Autowired private PageHelperProperties properties; /** * 接受分页插件额外的属性 * * @return */ @Bean @ConfigurationProperties(prefix = PageHelperProperties.PAGEHELPER_PREFIX) public Properties pageHelperProperties() &#123; return new Properties(); &#125; @PostConstruct public void addPageInterceptor() &#123; PageInterceptor interceptor = new PageInterceptor(); Properties properties = new Properties(); // 先把一般方式配置的属性放进去 properties.putAll(pageHelperProperties()); // 在把特殊配置放进去，由于 close-conn 利用上面方式时，属性名就是 close-conn 而不是 closeConn，所以需要额外的一步 properties.putAll(this.properties.getProperties()); interceptor.setProperties(properties); for (SqlSessionFactory sqlSessionFactory : sqlSessionFactoryList) &#123; sqlSessionFactory.getConfiguration().addInterceptor(interceptor); &#125; &#125;&#125;]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis "Can't open PID file" 经历]]></title>
    <url>%2Fp%2Fredis-cant-open-pid-file-by-ipv6%2F</url>
    <content type="text"><![CDATA[在 ubuntu 上使用 apt 安装 redis 报错。123456789101112131415Job for redis-server.service failed because a timeout was exceeded.See &quot;systemctl status redis-server.service&quot; and &quot;journalctl -xe&quot; for details.invoke-rc.d: initscript redis-server, action &quot;start&quot; failed.● redis-server.service - Advanced key-value store Loaded: loaded (/lib/systemd/system/redis-server.service; disabled; vendor preset: enabled) Active: activating (auto-restart) (Result: timeout) since Mon 2019-11-04 15:42:39 CST; 6ms ago Docs: http://redis.io/documentation, man:redis-server(1)Nov 04 15:42:39 VM-0-2-ubuntu systemd[1]: Failed to start Advanced key-value store.dpkg: error processing package redis-server (--configure): installed redis-server package post-installation script subprocess returned error exit status 1Errors were encountered while processing: redis-serverE: Sub-process /usr/bin/dpkg returned an error code (1)于是查看服务状态 1234# systemctl status redis-server.serviceNov 04 15:42:39 VM-0-2-ubuntu systemd[1]: Starting Advanced key-value store...Nov 04 15:42:39 VM-0-2-ubuntu systemd[1]: redis-server.service: Can&apos;t open PID file /var/run/redis 以为是权限问题，使用 sudo 启动，依然如此。嗯，这个报错信息没卵用。网上搜到类似情况，是 redis 默认支持 IPv6 地址，但是主机没有，导致报错。打开 redis.conf，发现 bind 127.0.0.1 ::1::1 是 IPv6 的 loopback 地址，对应 IPv4 的 127.0.0.1。 因为 ecs 没有分配 IPv6 网卡地址，绑定 IPv6 的 loopback 肯定报错。于是去掉::1。再次启动1sudo systemctl start redis-server.service 搞定。思考：虽然 IPv6 是大趋势，但是也不能默认选项就是 listen v6 地址，运行环境不一定有 v6 地址。提示只有”Can’t open PID file”，对发现问题没有帮助，引起误导。]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 的 label 和 annotation]]></title>
    <url>%2Fp%2Fkubernetes-label-and-annotation%2F</url>
    <content type="text"><![CDATA[Label 和 Annotation 都可以把扩展数据附加到 Kubernetes 资源对象，从而方便微服务的管理。Label 主要用于选择对象。Annotation 不能用于选择对象。labelLabel 主要用于选择对象。Label key 的组成：不得超过 63 个字符 可以使用前缀，使用 / 分隔，前缀必须是 DNS 子域，不得超过 253 个字符，系统中的自动化组件创建的 label 必须指定前缀，kubernetes.io/ 由 kubernetes 保留 起始必须是字母（大小写都可以）或数字，中间可以有连字符、下划线和点 Label value 的组成： 不得超过 63 个字符 起始必须是字母（大小写都可以）或数字，中间可以有连字符、下划线和点 Label selector 有两种类型：equality-based ：可以使用=、==、!= 操作符，可以使用逗号分隔多个表达式 set-based ：可以使用in、notin、! 操作符，另外还可以没有操作符，直接写出某个 label 的 key，表示过滤有某个 key 的 object 而不管该 key 的 value 是何值，!表示没有该 label 的 objectannotationAnnotation 不能用于标识及选择对象，annotation 中的元数据可多可少，可以是结构化的或非结构化的，也可以包含 label 中不允许出现的字符。一些可以记录在 annotation ：声明配置层管理的字段 创建信息、版本信息或镜像信息 用户信息，以及工具或系统来源信息 使用 annotation，方便创建用于部署、管理、内部检查的共享工具和客户端库。label 实战 为节点添加 label12# kubectl label node izwz9h8m2chowowqckbcy0z env=devnode/izwz9h8m2chowowqckbcy0z labeled覆盖已有 label，要添加 --overwrite 选项，否则报错 12345# kubectl label node izwz9h8m2chowowqckbcy0z env=deverror: &apos;env&apos; already has a value (dev), and --overwrite is false# kubectl label node izwz9h8m2chowowqckbcy0z env=test --overwritenode/izwz9h8m2chowowqckbcy0z labeled 查看 lable123# kubectl get node --show-labelsNAME STATUS ROLES AGE VERSION LABELSizwz9h8m2chowowqckbcy0z Ready &lt;none&gt; 19d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,env=dev,kubernetes.io/arch=amd64,kubernetes.io/hostname=izwz9h8m2chowowqckbcy0z,kubernetes.io/os=linux,microk8s.io/cluster=true删除 label，在 key 后面增加 - 即可 12# kubectl label node izwz9h8m2chowowqckbcy0z env-node/izwz9h8m2chowowqckbcy0z labeled 使用 label 过滤 123456789# kubectl get node --show-labels -l env=devNo resources found in default namespace.# kubectl label node izwz9h8m2chowowqckbcy0z env=devnode/izwz9h8m2chowowqckbcy0z labeled# kubectl get node --show-labels -l env=devNAME STATUS ROLES AGE VERSION LABELSizwz9h8m2chowowqckbcy0z Ready &lt;none&gt; 19d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,env=dev,kubernetes.io/arch=amd64,kubernetes.io/hostname=izwz9h8m2chowowqckbcy0z,kubernetes.io/os=linux,microk8s.io/cluster=true 上面提到 label 支持集合操作。过滤 env 是 dev 或者 test 的 node1# kubectl get node --show-labels -l &apos;env in (dev,test)&apos;annotation 实战 annotation 的操作和 label 类似。 添加 annotation1# kubectl annotate node izwz9h8m2chowowqckbcy0z admin=ycwu查看 annotation1# kubectl get node izwz9h8m2chowowqckbcy0z -o yaml | less123456789apiVersion: v1kind: Nodemetadata: annotations: admim: ycwu node.alpha.kubernetes.io/ttl: "0" volumes.kubernetes.io/controller-managed-attach-detach: "true" creationTimestamp: "2019-10-07T05:33:04Z"# 以下省略 删除 annotation12# kubectl annotate node izwz9h8m2chowowqckbcy0z admin-node/izwz9h8m2chowowqckbcy0z annotated小结 Kubernetes 对 labels 进行索引和反向索引用来优化查询和 watch。 不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes swap memory]]></title>
    <url>%2Fp%2Fkubernetes-swap-memory-and-systemoom%2F</url>
    <content type="text"><![CDATA[问题背景 之前发生过 staragent 异常，导致云效流水线部署失败：阿里云 staragent 异常导致 ecs 部署失败 回想起来，是在开启 microk8s 之后才发生的。于是怀疑是 microk8s 导致。1kubectl describe node izwz9h8m2chowowqckbcy0z在 Events 看到几个 SystemOOM 警告。123456789Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 5m57s kube-proxy, izwz9h8m2chowowqckbcy0z Starting kube-proxy. Normal Starting 5m46s kubelet, izwz9h8m2chowowqckbcy0z Starting kubelet. Warning InvalidDiskCapacity 5m46s kubelet, izwz9h8m2chowowqckbcy0z invalid capacity 0 on image filesystem Warning SystemOOM 5m46s kubelet, izwz9h8m2chowowqckbcy0z System OOM encountered, victim process: pip3, pid: 23143 Warning SystemOOM 5m46s kubelet, izwz9h8m2chowowqckbcy0z System OOM encountered, victim process: pip3, pid: 24161 Warning SystemOOM 5m46s kubelet, izwz9h8m2chowowqckbcy0z System OOM encountered, victim process: pip3, pid: 25349从 dmesg 看，这个 pip3 进程申请了 2G+ 的 vm（what！！！）。12345# dmesg | grep 23143[1375956.718910] [23143] 0 23143 528095 435853 3997696 39263 0 pip3[1375956.718916] Out of memory: Kill process 23143 (pip3) score 613 or sacrifice child[1375956.719841] Killed process 23143 (pip3) total-vm:2112380kB, anon-rss:1743412kB, file-rss:0kB, shmem-rss:0kB[1375957.190126] oom_reaper: reaped process 23143 (pip3), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB不过很可惜，不是 staragent 相关进程。最初的假设不成立。疑惑的是，swap 内存空闲率很高。直接触发 SystemOOM，太粗暴了。1234# swaponNAME TYPE SIZE USED PRIO/swapfile file 947.2M 18.6M -2/swapfile.new file 2G 48K -3于是查阅一些资料，了解 kubernetes 对 swap 的使用情况。--fail-swap-on默认情况下，系统开启 swap 会导致 k8s 启动失败。除非使用 --fail-swap-on 参数（kubelet）。–fail-swap-onMakes the Kubelet fail to start if swap is enabled on the node. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet’s –config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)如果为 true（默认值）就要求必须要关闭 swap，false 是表示即使宿主开启了 swap，kubelet 也是可以成功启动，但是 pod 是允许使用 swap 了。关于 swap memory 的讨论 2017 年的这个 issue 讨论了 swap 问题：Kubelet/Kubernetes should work with Swap Enabled #53533。总结起来有这几点： 开启 swap，会使得内存限额和 pod 调度变得复杂。怎样衡量 swap 配置？调度器要怎样根据 swap 去调度？having swap available has very strange and bad interactions with memory limits.For example, a container that hits its memory limit would then start spilling over into swapkubernetes 不是为了 swap 而设计。由于 pod 使用内存的复杂性，kubernetes 缺少一个足够聪明的策略去协调不同 pod 对内存 /swap 的使用。k8s 官方对此觉得产出投入比不高，不如把时间花在提高稳定性上。Support for swap is non-trivial. Guaranteed pods should never require swap. Burstable pods should have their requests met without requiring swap. BestEffort pods have no guarantee. The kubelet right now lacks the smarts to provide the right amount of predictable behavior here across pods.We discussed this topic at the resource mgmt face to face earlier this year. We are not super interested in tackling this in the near term relative to the gains it could realize.在实际应用场景，如果不开启 swap，应用就要为峰值内存申请内存资源，可能导致资源浪费。We have a cron job that occasionally runs into high memory usage (&gt;30GB) and we don’t want to permanently allocate 40+GB nodes.v1.8 后默认不用了 swap。官方不推荐使用 swap。开启 swap 后果自负，官方不背锅。折中的做法 像批处理 job 这种容易产生高峰值内存的 app，就要考虑开启 swap。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云 staragent 异常导致 ecs 部署失败]]></title>
    <url>%2Fp%2Faliyun-staragent-deploy-failure%2F</url>
    <content type="text"><![CDATA[后台 app 要发布新版本，但是在流水线执行部署失败：123调用 agent 执行任务失败，失败主机地址：a885d7ea-74e9-484a-8249-e23f6ed59a95。请检查：1. 机器上的 Staragent 客户端是否启动: /home/staragent/bin/staragentctl status。如果未启动，请执行 /home/staragent/bin/staragentctl start；2. 如果已经启动了，请通过 cat /usr/sbin/staragent_sn 命令查看机器上的 SN 是否是 a885d7ea-74e9-484a-8249-e23f6ed59a95。如果不是，则有可能是在该机器上多次运行了 Agent 安装脚本导致的。您可以将该机器从该环境去关联，然后重新添加。123# /home/staragent/bin/staragentctl statusconnect to inner listener failed:-1,err:11,errinfo:Resource temporarily unavailableoperate failed检查失败，于是执行 /home/staragent/bin/staragentctl start，重新启动服务。 等了半分钟，再次用 status 命令检查，发现同样的报错。于是通过 staragent_sn 命令检查机器 sn，跟提示的一致。嗯，阿里云的操作提示并没有提到上面的 staragentctl 启动失败的处理。。。重启是解决很多问题的方式😥，尝试重启 staragent。查找 staragent 相关的进程 12345678root@iZwz9h8m2chowowqckbcy0Z:/home/staragent# ps aux | grep starroot 466 0.0 0.2 170516 4520 ? Ssl Sep03 0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggersroot 4897 0.0 0.0 4628 228 ? S Oct07 0:00 /bin/sh -c /home/staragent/bin/staragentdroot 4898 0.9 0.2 823656 4680 ? Sl Oct07 90:13 staragent-coreroot 9702 0.0 0.0 41816 168 ? Ss Sep04 1:15 /home/staragent/bin/staragentdroot 9704 0.0 0.0 4628 0 ? S Sep04 0:00 /bin/sh -c /home/staragent/bin/staragentdroot 9705 0.2 1.7 670528 34880 ? Sl Sep04 122:01 staragent-ppfroot 20060 0.0 0.0 14428 1024 pts/0 S+ 23:33 0:00 grep --color=auto starstaragentd、 staragent-core、 staragent-ppf 都是相关进程。 一个小问题：正确的启动顺序呢？这里用粗暴的方式：从控制台删除关联机器，再重新绑定机器资源。这几个进程 pid 改变了，表明被重启了。12345678root@iZwz9h8m2chowowqckbcy0Z:/home/staragent# ps aux | grep starroot 466 0.0 0.2 170516 4520 ? Ssl Sep03 0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggersroot 20126 0.0 0.3 41816 7008 ? Ss 23:33 0:00 /home/staragent/bin/staragentdroot 20127 0.0 0.0 4628 772 ? S 23:33 0:00 /bin/sh -c /home/staragent/bin/staragentdroot 20128 0.0 0.0 4628 844 ? S 23:33 0:00 /bin/sh -c /home/staragent/bin/staragentdroot 20129 1.5 0.6 815560 13176 ? Sl 23:33 0:00 staragent-coreroot 20130 0.2 0.5 277156 10476 ? Sl 23:33 0:00 staragent-ppfroot 20262 0.0 0.0 14428 1008 pts/0 S+ 23:33 0:00 grep --color=auto star再次检查 staragent status:1234567891011121314151617root@iZwz9h8m2chowowqckbcy0Z:~# /home/staragent/bin/staragentctl status------agent running ok------ StartTime : 2019-10-13 23:33:38 CST RegisterTime : 2019-10-13 23:33:40 CST ServiceTag : 6a94612e-0be8-47ec-9c17-fdc7e4f48589 ServerConnected : 1 LastHeartBeatTime : 2019-10-13 23:37:29 CST ServerAddr : ***.***.***.***:**** （此处打码） LocalAddr : 172.18.151.35 Max Core count : 0 Total CPU Count : 1 Total CPU Rate : 100.00% Total MEM Rate : 49.66% Process CPU Rate : 1.00% Load Avg (1,5,15) : 30,14,12 Virtual Memory : 820MB Physical Memory : 14MB然后在云效控制台重新部署，执行成功。]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux signal 笔记]]></title>
    <url>%2Fp%2Flinux-signal%2F</url>
    <content type="text"><![CDATA[几个容易混淆的信号，以及 trap 命令 sigkillThe SIGKILL signal is sent to a process to cause it to terminate immediately (kill). In contrast to SIGTERM and SIGINT, this signal cannot be caught or ignored, and the receiving process cannot perform any clean-up upon receiving this signal. 信号不可以被捕捉、不可以被忽略（init 进程除外）。应该把 sigkill 作为杀死进程的最后方式。（kill -9）sigtermThe SIGTERM signal is sent to a process to request its termination. Unlike the SIGKILL signal, it can be caught and interpreted or ignored by the process. This allows the process to perform nice termination releasing resources and saving state if appropriate. SIGINT is nearly identical to SIGTERM.信号可以被进程捕捉、忽略。sigterm 可以实现进程的优雅关闭。（kill -15） 类似 sigint，但是不限于终端程序。sigintThe SIGINT signal is sent to a process by its controlling terminal when a user wishes to interrupt the process. This is typically initiated by pressing Ctrl+C, but on some systems, the “delete” character or “break” key can be used在终端里需要中断程序，使用 ctrl + c 触发。sigquitThe SIGQUIT signal is sent to a process by its controlling terminal when the user requests that the process quit and perform a core dump.在终端触发，会 core dump。sighupThe SIGHUP signal is sent to a process when its controlling terminal is closed. It was originally designed to notify the process of a serial line drop (a hangup). In modern systems, this signal usually means that the controlling pseudo or virtual terminal has been closed.[4] Many daemons will reload their configuration files and reopen their logfiles instead of exiting when receiving this signal.[5] nohup is a command to make a command ignore the signal.用于通知进程，终端已经关闭。通常是在终端的控制进程结束时, 通知同一 session 内的各个作业, 这时它们与控制终端不再关联。nohup 命令可以让进程忽略 sighup 信号。信号 0在 keepalived 健康检查脚本，看到同事使用 killall -0 keepalived 来检测进程。第一次见到信号 0：“signal 0” is kind of like a moral equivalent of “ping”.Using “kill -0 NNN” in a shell script is a good way to tell if PID “NNN” is alive or not:signal 0 is just used to check process is exists or not.trap 命令 trap 命令可以在 shell 脚本中捕捉 signal。test.sh 如下：123456789#! /bin/bashtrap "echo TRAP SIGTERM" TERMfor i in `seq 1 100`do echo $i sleep 1done运行脚本 123456# 窗口 1chmod u+x test.sh./test.sh# 窗口 2ps ax | grep test.sh | head -1 | awk &#123;'print $1'&#125; | xargs kill -15 参考Signal (IPC)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod 优雅关闭]]></title>
    <url>%2Fp%2Fkubernetes-pod-termination%2F</url>
    <content type="text"><![CDATA[优雅关闭 当一个进程关闭，通常要停止接受新的请求、处理完成当前请求、释放资源。这是优雅关闭（gracefully shutdown）的工作。在 k8s 中，pod 是一个运行在集群中的应用进程。k8s 提供 lifecycle hook、 grace period 等机制，实现 pod 的优雅终止。pod 关闭流程 官网文章 Termination of Pods 描述了 pod 关闭流程，摘录如下：User sends command to delete Pod, with default grace period (30s)The Pod in the API server is updated with the time beyond which the Pod is considered “dead” along with the grace period.Pod shows up as “Terminating” when listed in client commands(simultaneous with 3) When the Kubelet sees that a Pod has been marked as terminating because the time in 2 has been set, it begins the Pod shutdown process.If one of the Pod’s containers has defined a preStop hook, it is invoked inside of the container. If the preStop hook is still running after the grace period expires, step 2 is then invoked with a small (2 second) extended grace period.The container is sent the TERM signal. Note that not all containers in the Pod will receive the TERM signal at the same time and may each require a preStop hook if the order in which they shut down matters.(simultaneous with 3) Pod is removed from endpoints list for service, and are no longer considered part of the set of running Pods for replication controllers. Pods that shutdown slowly cannot continue to serve traffic as load balancers (like the service proxy) remove them from their rotations.When the grace period expires, any processes still running in the Pod are killed with SIGKILL.The Kubelet will finish deleting the Pod on the API server by setting grace period 0 (immediate deletion). The Pod disappears from the API and is no longer visible from the client.简单来说，发送 delete pod 命令后，pod 标记为 terminating 状态，从 service endpoint 列表摘除，并且进入 grace period（优雅关闭）阶段。如果配置了 preStop hook，则执行之。如果 grace period 到时间了，preStop hook 还没有退出，则延长额外的 2s，之后向容器的主进程（pid=1 的进程）发送 SIGTERM 信号。当 grace peroid 结束后，k8s 向还没结束的 pod 发送 SIGKILL 信号。关于 pod lifecycle hook，在这篇文章有介绍：kubernetes pod lifecycle关于 sigterm、sigkill 的区别，可以看这篇文章：linux signal 笔记 修改优雅关闭时间 默认的优雅关闭时间是 30s。在 yaml 文件配置 terminationGracePeriodSeconds 字段 123456789101112apiVersion: v1kind: Deploymentmetadata: name: testspec: replicas: 1 template: spec: containers: - name: test image: ... terminationGracePeriodSeconds: 60 手动指定 1kubectl delete pod &lt;pod_name&gt; --grace-period=&lt;seconds&gt; 如果要强制删除 pod1kubectl delete pod &lt;pod_name&gt; --grace-period=0 --force]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod preset]]></title>
    <url>%2Fp%2Fkubernetes-pod-preset%2F</url>
    <content type="text"><![CDATA[pod preset 是什么 在 pod 创建时，用户可以使用 podpreset 对象将 secrets、卷挂载和环境变量等信息注入其中。使用 Pod Preset 使得 pod 模板的作者可以不必为每个 Pod 明确提供所有信息。这样一来，pod 模板的作者就不需要知道关于该服务的所有细节。需要注意的是，Pod Preset 是 namespace 级别的对象，其作用范围只能是同一个命名空间下容器。pod preset 流程 以下来自官网文档 Pod Preset：检索所有可用的 PodPresets。检查 PodPreset 标签选择器上的标签，看看其是否能够匹配正在创建的 Pod 上的标签。尝试将由 PodPreset 定义的各种资源合并到正在创建的 Pod 中。出现错误时，在该 Pod 上引发记录合并错误的事件，PodPreset 不会注入任何资源到创建的 Pod 中。注释刚生成的修改过的 Pod spec，以表明它已被 PodPreset 修改过。注释的格式为 podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name&gt;: &lt;resource version&gt;开启 pod preset截至 k8s v1.16，pod preset 默认是关闭的。如果不确认集群是否已开启 PodPreset 支持，可以通过 kubectl api-versions 命令查看是否存在该类型，或者 kubectl get podpreset 命令查看，如果没开启会提示 error: the server doesn&#39;t have a resource type &quot;podpreset&quot; 错误。开启 PodPreset：1. 开启 API：在 apiserver 配置文件中增加–runtime-config=settings.k8s.io/v1alpha1=true2. 开启准入控制器：在 apiserver 配置文件中增加–enable-admission-control=PodPreset我使用的是 microk8s 环境，根据官网（Configuring MicroK8s services）指引，修改 ${SNAP_DATA}/args/kube-apiserverwhere${SNAP_DATA} points to /var/snap/microk8s/current 题外话：注意是 /var/snap/microk8s/current 目录，一开始跑到 /snap/microk8s/current 报错了😂。123456789101112root@iZwz9h8m2chowowqckbcy0Z:/snap/microk8s/current# mkdir argsmkdir: cannot create directory ‘args’: Read-only file systemroot@iZwz9h8m2chowowqckbcy0Z:/snap/microk8s/current# mount | grep snap/var/lib/snapd/snaps/core_7396.snap on /snap/core/7396 type squashfs (ro,nodev,relatime,x-gdu.hide)/var/lib/snapd/snaps/core_7713.snap on /snap/core/7713 type squashfs (ro,nodev,relatime,x-gdu.hide)tmpfs on /run/snapd/ns type tmpfs (rw,nosuid,noexec,relatime,size=204124k,mode=755)/var/lib/snapd/snaps/minikube_4.snap on /snap/minikube/4 type squashfs (ro,nodev,relatime,x-gdu.hide)/var/lib/snapd/snaps/microk8s_920.snap on /snap/microk8s/920 type squashfs (ro,nodev,relatime,x-gdu.hide)root@iZwz9h8m2chowowqckbcy0Z:/snap/microk8s/current# ll /var/lib/snapd/snaps/microk8s_920.snap -rw------- 2 root root 187269120 Oct 7 13:31 /var/lib/snapd/snaps/microk8s_920.snap更新配置后，重启 apiserver1sudo systemctl restart snap.microk8s.daemon-apiserver实验 通过 pod preset，为每个容器增加 TZ 环境变量。保存为 tz-preset.yml。12345678910apiVersion: settings.k8s.io/v1alpha1kind: PodPresetmetadata: name: tz-presetspec: selector: matchLabels: env: - name: TZ value: Asia/Shanghaispec.selector.matchLabels是必须的。如果为空，则匹配所有 pod。一个测试用的 pod，保存为 tz-pod.yml。12345678apiVersion: v1kind: Podmetadata: name: tz-presetspec: containers: - name: tz-preset image: nginx先创建 preset 对象，再创建 pod12# kubectl create -f tz-preset.yml# kubectl create -f tz-pod.yml查看 pod，pod preset 会修改 annotation。12345678# kubectl describe pod tz-presetName: tz-presetNamespace: defaultPriority: 0Node: izwz9h8m2chowowqckbcy0z/172.18.151.35Start Time: Tue, 08 Oct 2019 12:14:21 +0800Labels: &lt;none&gt;Annotations: podpreset.admission.kubernetes.io/podpreset-tz-preset: 99719进入容器检查12# kubectl exec -it tz-preset env | grep TZTZ=Asia/Shanghai]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes yaml 中 command 重定向的写法]]></title>
    <url>%2Fp%2Fkubernetes-yaml-command-args-redirect%2F</url>
    <content type="text"><![CDATA[在 init container 的实验中，通过 echo 重定向方式创建文件并且写入内容。最初的版本是这样的：123456spec: ... initContainers: - name: install image: busybox command: ["echo", "hello world", "&gt;", "/work-dir/index.html"]init container 执行返回，但是进入容器后，并没有发现指定 index.html 文件。然后换了另一种写法：1234567spec: ... initContainers: - name: install image: busybox command: ["echo"] args: ["hello world", "&gt;", "/work-dir/index.html"]还是不行。换另一个命令，成功创建文件：123456spec: ... initContainers: - name: install image: busybox command: ["touch", "/work-dir/index.html"]于是怀疑是 &gt; 的问题。采用字符串方式传入整个命令。123456spec: ... initContainers: - name: install image: busybox command: ["/bin/sh", "-c", "echo hello world &gt; /work-dir/index.html"]上述命令表示以 sh 作为 shell，-c参数后面是完整的命令行。成功创建文件。结论：k8s 对命令行的重定向解析貌似有问题。使用 sh -c &lt;command args&gt; 代替。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod hook]]></title>
    <url>%2Fp%2Fkubernetes-pod-hook%2F</url>
    <content type="text"><![CDATA[pod hookKubernetes 为容器提供了生命周期钩子。 对于 Pod，有以下 2 个 hook：PostStart：这个钩子在容器创建后立即执行。但是，并不能保证钩子将在容器 ENTRYPOINT 之前运行，因为没有参数传递给处理程序（异步执行）。主要用于资源部署、环境准备等。 如果钩子运行或挂起的时间太长，则容器无法达到 running 状态。PreStop：这个钩子在容器终止之前立即被调用。它是阻塞的，意味着它是同步的， 所以它必须在删除容器的调用发出之前完成。主要用于优雅关闭应用程序、通知其他系统等。针对容器，有两种类型的钩子处理程序可供实现：Exec - 执行一个特定的命令，例如 pre-stop.sh，在容器的 cgroups 和名称空间中。 命令所消耗的资源根据容器进行计算。HTTP - 对容器上的特定端点执行 HTTP 请求。实验 在 postStart 操作执行完成之前，kubelet 会锁住容器，不让应用程序的进程启动，只有在 postStart 操作完成之后容器的状态才会被设置成为 RUNNING。123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-hook-demospec: containers: - name: pod-hook-demo image: nginx lifecycle: postStart: exec: command: ["/bin/sh", "-c", "sleep 30"] preStop: exec: command: ["/bin/sh", "-c", "echo Hello from the preStop handler &gt; /usr/share/message"]马上查看 pod 状态，返回 pending。1234567891011121314151617181920212223242526root@iZwz9h8m2chowowqckbcy0Z:~/k8s# kubectl describe pod pod-hook-demoName: pod-hook-demoNamespace: defaultPriority: 0Node: izwz9h8m2chowowqckbcy0z/172.18.151.35Start Time: Mon, 07 Oct 2019 19:23:28 +0800Labels: &lt;none&gt;Annotations: &lt;none&gt;Status: PendingIP: IPs: &lt;none&gt;Containers: pod-hook-demo: Container ID: Image: nginx Image ID: Port: &lt;none&gt; Host Port: &lt;none&gt; State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-rpvtx (ro)日志 钩子处理程序的日志不会在 Pod 事件中公开。 如果处理程序由于某种原因失败，它将播放一个事件。对于 PostStart，这是 FailedPostStartHook 事件，对于 PreStop，这是 FailedPreStopHook 事件。修改 yml123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-hook-demospec: containers: - name: pod-hook-demo image: nginx lifecycle: postStart: exec: command: ["/bin/sh", "-c", "exit 1"] preStop: exec: command: ["/bin/sh", "-c", "echo Hello from the preStop handler &gt; /usr/share/message"]12345678910Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/pod-hook-demo to izwz9h8m2chowowqckbcy0z Normal Killing 3s kubelet, izwz9h8m2chowowqckbcy0z FailedPostStartHook Normal Pulled 3s kubelet, izwz9h8m2chowowqckbcy0z Successfully pulled image &quot;nginx&quot; Normal Created 3s kubelet, izwz9h8m2chowowqckbcy0z Created container pod-hook-demo Normal Started 3s kubelet, izwz9h8m2chowowqckbcy0z Started container pod-hook-demo Warning FailedPostStartHook 3s kubelet, izwz9h8m2chowowqckbcy0z Exec lifecycle hook ([/bin/sh -c exit 1]) for Container &quot;pod-hook-demo&quot; in Pod &quot;pod-hook-demo_default(3e094b95-d182-4938-be5b-c021d9df1dd2)&quot; failed - error: command &apos;/bin/sh -c exit 1&apos; exited with 1: , message: &quot;&quot; Normal Pulling 1s (x2 over 6s) kubelet, izwz9h8m2chowowqckbcy0z Pulling image &quot;nginx&quot;对比 init container一些初始化操作可以在 init container 或者 postStart hook 执行。init container 在 main container 之前执行。PostStart hook 在 main container 之中执行。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes volume 简介]]></title>
    <url>%2Fp%2Fkubernetes-volume-intro%2F</url>
    <content type="text"><![CDATA[volume 简介 Kubernetes 抽象出 Volume 对象来解决这两个问题： 容器中的文件在磁盘上是临时存放的，当容器崩溃时，容器内的文件将会丢失 当在一个 Pod 中同时运行多个容器时，常常需要在这些容器之间共享文件。Volume 的生命周期比 Pod 中运行的任何容器要持久，在容器重新启动时能可以保留数据。使用 volumeVolume 的核心是包含一些数据的目录，Pod 中的容器可以访问该目录。使用 Volume 时, Pod 声明中需要提供卷的类型 (.spec.volumes 字段)和卷挂载的位置 (.spec.containers.volumeMounts 字段)。卷不能挂载到其他卷，也不能与其他卷有硬链接。Pod 中的每个容器必须独立地指定每个卷的挂载位置。init 容器的实验就使用了 volume：kubernetes pod init container1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: ["/bin/sh", "-c", "echo hello world &gt; /work-dir/index.html"] volumeMounts: - name: workdir mountPath: "/work-dir" volumes: - name: workdir emptyDir: &#123;&#125;这里定义了一个类型为 emptyDir、名字为 wordir 的 volume。在 nginx 容器中，把 wordir 挂载到 /usr/share/nginx/html。在 install 容器中，把 workdir 挂载到 /work-dir。接下来简单介绍几个 volume type。emptyDir使用 emptyDir，当 Pod 分配到 Node 上时，将会创建 emptyDir，并且只要 Node 上的 Pod 一直运行，Volume 就会一直存。当 Pod（不管任何原因）从 Node 上被删除时，emptyDir 也同时会删除，存储的数据也将永久删除。注：删除容器不影响 emptyDir。也可以将 emptyDir.medium 字段设置为 “Memory”，以告诉 Kubernetes 安装 tmpfs（基于 RAM 的文件系统）。emptyDir 的一些用途：缓存空间，例如基于磁盘的归并排序。为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。hostPathhostPath 允许挂载 Node 上的文件系统到 Pod 里面去。如果 Pod 需要使用 Node 上的文件，可以使用 hostPath。hostPath 的一些用法有：运行一个需要访问 Docker 引擎内部机制的容器；请使用 hostPath 挂载 /var/lib/docker 路径。在容器中运行 cAdvisor 时，以 hostPath 方式挂载 /sys。允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。一旦这个 pod 离开了这个宿主机，hostPath 中的数据虽然不会被永久删除，但数据也不会随 pod 迁移到其他宿主机上。由于各个宿主机上的文件系统结构和内容并不一定完全相同，所以相同 pod 的 hostPath 可能会在不同的宿主机上表现出不同的行为。localLocal Storage 同 HostPath 的区别在于对 Pod 的调度上，使用 Local Storage 可以由 Kubernetes 自动的对 Pod 进行调度，而是用 HostPath 只能人工手动调度 Pod，因为 Kubernetes 已经知道了每个节点上 kube-reserved 和 system-reserved 设置的本地存储限制。1234567891011121314151617181920212223apiVersion: v1kind: PersistentVolumemetadata: name: example-pvspec: capacity: storage: 100Gi # volumeMode field requires BlockVolume Alpha feature gate to be enabled. volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - example-node使用 local 卷时，需要使用 PersistentVolume 对象的 nodeAffinity 字段。 它使 Kubernetes 调度器能够将使用 local 卷的 Pod 正确地调度到合适的节点。secretKubemetes 提供了 Secret 来处理敏感数据，比如密码、Token 和密钥，相比于直接将敏感数据配置在 Pod 的定义或者镜像中，Secret 提供了更加安全的机制（Base64 加密），防止数据泄露。Secret 的创建是独立于 Pod 的，以数据卷的形式挂载到 Pod 中，Secret 的数据将以文件的形式保存，容器通过读取文件可以获取需要的数据。参考Volumes]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod init container]]></title>
    <url>%2Fp%2Fkubernetes-pod-init-container%2F</url>
    <content type="text"><![CDATA[init 容器 Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。Init 容器与普通的容器非常像，除了如下两点：Init 容器总是运行到成功完成为止。 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动。init 容器和 pod 启动过程 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。 每个容器必须在下一个容器启动之前成功退出。 如果由于运行时或失败退出，导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。 然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。 Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将条件 Initializing 设置为 true。（图片来源：blog.openshift.com/kubernetes-pods-life/）init 容器用途 它们可以包含并运行实用工具，出于安全考虑，是不建议在应用容器镜像中包含这些实用工具的。它们可以包含用于安装的工具和定制化代码，这些都是在应用镜像中没有的。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。应用镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。它们使用 Linux Namespace，所以对应用容器具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用容器不能够访问。它们在应用容器启动之前运行完成，然而应用容器并行运行，所以 Init 容器提供了一种简单的方式来阻塞或延迟应用容器的启动，直到满足了一组先决条件。init 容器实验 1init-demo.yml会创建一个 Pod，以 nginx 容器作为 main container，并且使用 init container 初始化一个 index 文件。1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox command: ["/bin/sh", "-c", "echo hello world &gt; /work-dir/index.html"] volumeMounts: - name: workdir mountPath: "/work-dir" volumes: - name: workdir emptyDir: &#123;&#125;12345# kubectl create -f init-demo.yml# kubectl get podNAME READY STATUS RESTARTS AGEinit-pod 0/1 Init:0/1 0 147m如果在 pod 初始化完成之前 get pod，可以看到 STATUS 字段是 Init:0/1，表明有总共 1 个 init 容器操作，已经完成了 0 个。 当 pod 初始化完毕后，检查容器文件 12root@iZwz9h8m2chowowqckbcy0Z:~/k8s# kubectl exec -it init-demo cat /usr/share/nginx/html/index.htmlhello worldinit 容器实验 2 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。这里在 init container 中尝试 nslookup 一个不存在的域名，使其一直失败。123456789101112131415apiVersion: v1kind: Podmetadata: name: init-demo labels: app: initspec: containers: - name: init-container image: busybox command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] initContainers: - name: init-myservice image: busybox command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']123456root@iZwz9h8m2chowowqckbcy0Z:~/k8s# kubectl get podNAME READY STATUS RESTARTS AGEinit-demo 0/1 Init:CrashLoopBackOff 1 55sroot@iZwz9h8m2chowowqckbcy0Z:~/k8s# kubectl logs init-demoError from server (BadRequest): container "init-container" in pod "init-demo" is waiting to start: PodInitializing主容器等待 init container 执行完毕，因此处于 PodInitializing 状态。参考Init 容器Kubernetes: A Pod’s Life]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pod lifecycle]]></title>
    <url>%2Fp%2Fkubernetes-pod-lifecycle%2F</url>
    <content type="text"><![CDATA[Pod lifecycle &amp; status(图片来源：https://godleon.github.io/blog/Kubernetes/k8s-Pod-Overview/)Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。下面是 phase 可能的值（复制粘贴自: Pod 的生命周期 ）： 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。查看 pod 的状态：123456789101112131415161718192021# kubectl describe pod mytomcat-78c89857d6-428ggName: mytomcat-78c89857d6-428ggNamespace: defaultPriority: 0Node: izwz9h8m2chowowqckbcy0z/172.18.151.35Start Time: Mon, 30 Sep 2019 13:32:05 +0800Labels: app=tomcat pod-template-hash=78c89857d6Annotations: &lt;none&gt;Status: RunningIP: 10.1.1.15IPs: IP: 10.1.1.15Controlled By: ReplicaSet/mytomcat-78c89857d6Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True每个 Pod 都拥有一个 PodStatus，里面包含 PodConditions 数组。Type 类型如下：Type描述 PodScheduledPod 已被调度到一个节点ReadyPod 能够提供请求，应该被添加到负载均衡池中以提供服务Initialized 所有 init containers 成功启动 Unschedulable 调度器不能正常调度容器，例如缺乏资源或其他限制 ContainersReadyPod 中所有容器全部就绪status 字段是一个字符串，可能的值有 True、False 和 Unknown。 容器探针 (container probe)Probe 是在容器上 kubelet 的定期执行的诊断，kubelet 通过调用容器实现的 Handler 来诊断。目前有三种 Handlers ：ExecAction：在容器内部执行指定的命令，如果命令以状态代码 0 退出，则认为诊断成功。TCPSocketAction：对指定 IP 和端口的容器执行 TCP 检查，如果端口打开，则认为诊断成功。HTTPGetAction：对指定 IP + port + path 路径上的容器的执行 HTTP Get 请求。如果响应的状态代码大于或等于 200 且小于 400，则认为诊断成功。每次探测可能有如下之一的结果：Success：容器诊断通过 Failure：容器诊断失败Unknown：诊断失败，因此不应采取任何措施kubelet 可以选择性地对运行中的容器进行两种探测器执行和响应：livenessProbe：指示容器是否正在运行，如果活动探测失败，则 kubelet 会杀死容器，并且容器将受其 重启策略 的约束。如果不指定活动探测，默认状态是 Success。readinessProbe：指示容器是否已准备好为请求提供服务，如果准备情况探测失败，则控制器会从与 Pod 匹配的所有服务的端点中删除 Pod 的 IP 地址。初始化延迟之前的默认准备状态是 Failure，如果容器未提供准备情况探测，则默认状态为 Success。 如果集成了 springboot actuator，可以这样设置 probe：12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: mytomcatspec: selector: matchLabels: app: tomcat replicas: 1 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080 readinessProbe: # ---- 准备状态检查 ---- httpGet: path: /actuator/health port: 8080 timeoutSeconds: 2 # 探测超时时长，单位：秒 initialDelaySeconds: 60 # 初始化时间，单位：秒 livenessProbe: # ---- 健康状态检查 ---- httpGet: port: 8080 path: /actuator/info failureThreshold: 3 # 最大失败次数 timeoutSeconds: 2 # 探测超时时长，单位：秒 initialDelaySeconds: 60 # 初始化时间，单位：秒 periodSeconds: 5 # 探测时间间隔，单位：秒 successThreshold: 1 # 失败后探测成功的最小连续成功次数 使用 ExecAction 的例子：123456789101112131415spec: containers: - name: liveness image: innerpeacez/k8s.gcr.io-busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 20; rm -rf /tmp/healthy; sleep 60 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5使用 TCPSocketAction 的例子：1234567891011spec: containers: - name: liveness-tcpsocket image: innerpeacez/k8s.gcr.io-goproxy:0.1 ports: - containerPort: 8080 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20Pod readiness gate在 kubernetes 1.14 中 readiness gate 发布了 stable 版本。可以在 pod spec 中定义要额外诊断的 status.conditions(预设值为 False)，而 pod condition 的定义必须是以 key/value 的型式。12345678910111213141516171819Kind: Pod...spec: readinessGates: - conditionType: "www.example.com/feature-1"status: conditions: - type: Ready status: "True" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: "www.example.com/feature-1" status: "False" lastProbeTIme: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true...开启 readinessGates 之后，同时满足以下条件，pod 才会被诊断为 ready：pod 中所有的 container 状态皆为 Ready所有在 pod spec 中定义的 ReadinessGates 的状态皆为 True重启策略 PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 需要注意：restart policy 套用的范围是 pod 中的所有 container，而不是某一个。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10 秒，20 秒，40 秒…）重新启动，并在成功执行十分钟后重置。一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。容器探针 &amp; 重启策略的使用 容器启动后，需要经历预热、初始化操作，才能对外服务，则配置 readinessProbe。容器运行的业务有健康度检查，只有处于健康状态才能对外服务（比如自动更新状态则不接受外部流量），则配置 readinessProbe。如果容器崩溃会不会卡死，自己 crash 掉，则不需要 livenessProbe，由 k8s 根据 restartPolicy 处理。pod 包含多个容器，可以使用 readinessGates 自定义就绪状态检查。参考Pod 的生命周期[Kubernetes] Pod 的設計 &amp; 相關運作機制kuberbetes Pod 健康检查]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux cgroup 简介]]></title>
    <url>%2Fp%2Flinux-cgroup%2F</url>
    <content type="text"><![CDATA[linux namespace 对进程组实现了资源隔离和共享。但是没有实现资源限额、统计等功能。cgroup 技术弥补了 namespace 的不足。cgroup 简介 cgroup 是 Control Groups 的缩写，是 Linux 内核提供的一种可以限制、记录、隔离进程组 (process groups) 所使用的资源 (如 cpu、内存、i/o 等等) 的机制。cgroup 和 namespace 类似，也是将进程进行分组，但它的目的和 namespace 不一样，namespace 是为了隔离进程组之间的资源，而 cgroup 是为了对一组进程进行统一的资源监控和限制。cgroup 的核心组成是 subsystem 和 hierarchy。cgroup subsystemcgroups 为每种可以控制的资源定义了一个子系统（subsystem），又被称为资源管理器（resource controller）。cpu 子系统，主要限制进程的 cpu 使用率。cpuacct 子系统，可以统计 cgroups 中的进程的 cpu 使用报告。cpuset 子系统，可以为 cgroups 中的进程分配单独的 cpu 节点或者内存节点。memory 子系统，可以限制进程的 memory 使用量。blkio 子系统，可以限制进程的块设备 io。devices 子系统，可以控制进程能够访问某些设备。net_cls 子系统，可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块（traffic control）对数据包进行控制。freezer 子系统，可以挂起或者恢复 cgroups 中的进程。ns 子系统，可以使不同 cgroups 下面的进程使用不同的 namespace。 子系统必须附加（attach）到一个层级（hierarchy）上才能起作用。cgroup hierarchy一个 hierarchy 可以理解为一棵 cgroup 树，树的每个节点就是一个进程组，每棵树都会与零到多个 subsystem 关联。cgroup 功能 Cgroups 提供了以下功能：1. 限制进程组可以使用的资源数量（Resource limiting）。2. 进程组的优先级控制（Prioritization）。3. 记录进程组使用的资源数量（Accounting）。4. 进程组隔离（Isolation）。5. 进程组控制（Control）。cgroup 操作cgroup 相关的所有操作都是基于内核中的 cgroup virtual filesystem。一般情况下都是挂载到 /sys/fs/cgroup 目录下 通过查看 /proc/cgroups 知道当前系统支持哪些 subsystem123456789101112131415# cat /proc/cgroups#subsys_name hierarchy num_cgroups enabledcpuset 12 13 1cpu 5 79 1cpuacct 5 79 1blkio 6 73 1memory 3 1673 1devices 11 73 1freezer 8 14 1net_cls 4 13 1perf_event 2 13 1net_prio 4 13 1hugetlb 7 13 1pids 9 81 1rdma 10 1 1查看进程属于哪些 cgroup，/proc/[pid]/cgroup1234567891011121314# cat /proc/1/cgroup12:cpuset:/11:devices:/10:rdma:/9:pids:/8:freezer:/7:hugetlb:/6:blkio:/5:cpu,cpuacct:/4:net_cls,net_prio:/3:memory:/2:perf_event:/1:name=systemd:/init.scope0::/init.scopeTODO： 更多 cgroup 的实验。参考Linux Cgroup 系列（01）：Cgroup 概述Linux containers – next gen virtualization for cloud (atl summit)Container 内核原理介绍Linux 资源管理之 cgroups 简介]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes pause 容器]]></title>
    <url>%2Fp%2Fkubernetes-pod-pause-container%2F</url>
    <content type="text"><![CDATA[记得第一次在 microk8s 上运行容器，遇到 pause 容器无法拉取镜像的问题。今天了解下这个 pause 容器的用途。pod &amp; pause container我们知道，Pod 是 kubernetes 中最基本的应用执行单元。代表一个在 k8s 集群运行的进程。在 Kubernetes 集群中 Pod 有如下两种使用方式：一个 Pod 中运行一个容器。“每个 Pod 中一个容器”的模式是最常见的用法。在一个 Pod 中同时运行多个容器。一个 Pod 中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。Pod 将这些容器的存储资源作为一个实体来管理。可见，Kuberentes 管理的是 Pod 而不是直接管理容器。容器化的好处之一是资源隔离，不同容器的进程不会相互干扰；底层实现技术是 linux namespace 和 cgroup。但是另一方面，不同容器的进程可能需要访问其他容器的资源，才能协同工作。在 kubernetes 中，pause 容器作为 pod 中其他容器的父容器，实现了 2 个功能：共享 namespace作为每个 pod 的 PID=1 的进程，用于回收僵尸进程 linux namespace sharing 在 linux 中，新建一个进程，默认继承父进程的 namespace。如果要在新的 namespace 中运行进程，可以使用 unshare 命令：1sudo unshare --pid --uts --ipc --mount -f chroot rootfs /bin/sh当进程运行后，可以使用 setns 系统调用，把其他进程添加到这个 namespace，从而形成一个 pod。docker 提供了管理 namespace 的便捷方法。首先新建一个 pause 容器。1docker run -d --name pause -p 8080:80 gcr.io/google_containers/pause-amd64:3.0然后运行 ghost 容器，并且加入到 pause 容器的 namespace。1docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost这里的 --net、--ipc、--pid 作用与 unshare 命令类似。僵尸进程，孤儿进程 僵尸进程：一个进程使用 fork 创建子进程，如果子进程退出，而父进程并没有调用 wait 或 waitpid 获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。孤儿进程：父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被 init 进程 (进程号为 1) 所收养，并由 init 进程对它们完成状态收集工作。僵尸进程的危害：僵尸进程会占用进程号，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程。处理僵尸进程 / 孤儿进程的方法：父进程可调用 wait/waitpid 函数回收其僵尸子进程。init 进程回收法。父进程回收的问题：需要父进程去等待子进程。通常情况下，父进程要执行自己的逻辑，不会阻塞等待。另外，父进程 crash，导致不能回收。init 进程避免了父进程回收的缺陷：如果父进程先于子进程结束，那么子进程的父进程自动改为 init 进程。如果 init 的子进程结束，则 init 进程会自动回收其子进程的资源而不是让它变成僵尸进程。pause 容器 在 linux 系统中，由 init 进程处理僵尸进程问题。在 kubernetes 中，容器进程可能 fork 子进程，进而导致潜在的僵尸进程问题。因此引入了 pause 容器来执行 init 进程的职责。同时也承担了 namespace sharing 的作用。pause 容器源码很简单，核心功能是休眠（pause）。1234567891011121314151617181920212223242526272829303132333435#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;static void sigdown(int signo) &#123; psignal(signo, "Shutting down, got signal"); exit(0);&#125;static void sigreap(int signo) &#123; while (waitpid(-1, NULL, WNOHANG) &gt; 0);&#125;int main() &#123; if (getpid() != 1) /* Not an error because pause sees use outside of infra containers. */ fprintf(stderr, "Warning: pause should be the first process\n"); if (sigaction(SIGINT, &amp;(struct sigaction)&#123;.sa_handler = sigdown&#125;, NULL) &lt; 0) return 1; if (sigaction(SIGTERM, &amp;(struct sigaction)&#123;.sa_handler = sigdown&#125;, NULL) &lt; 0) return 2; if (sigaction(SIGCHLD, &amp;(struct sigaction)&#123;.sa_handler = sigreap, .sa_flags = SA_NOCLDSTOP&#125;, NULL) &lt; 0) return 3; for (;;) pause(); fprintf(stderr, "Error: infinite loop terminated\n"); return 42;&#125;参考 The Almighty Pause Container 处理僵尸进程的两种经典方法 孤儿进程与僵尸进程[总结]]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 学习札记]]></title>
    <url>%2Fp%2Fkubernetes-collection%2F</url>
    <content type="text"><![CDATA[kubernetes 学习札记，持续更新中。前置 linux 知识：linux namespace 简介 linux cgroup 简介kubernetes：microk8s 之 k8s.gcr.io 访问问题kubernetes 访问 podkubernetes 删除 pod，自动重启kubernetes Pod、ReplicationSet、Deployment、Service 初体验kubernetes pause 容器kubernetes pod lifecyclekubernetes pod init containerkubernetes pod hookkubernetes pod presetkubernetes volume 简介 杂项：kubernetes yaml 文件工具kubernetes yaml 中 command 重定向的写法kubernetes swap memorykubernetes 的 label 和 annotation]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux namespace 简介]]></title>
    <url>%2Fp%2Flinux-namespace%2F</url>
    <content type="text"><![CDATA[linux namespaceLinux Namespace 是 linux 内核在不同进程间实现的一种 环境隔离机制 。 Linux 内核实现 namespace 的一个主要目的就是实现轻量级虚拟化(容器) 服务。在同一个 namespace 下的进程可以感知彼此的变化，而对外界的进程一无所知。Linux 一共构建了 6 种不同的 Namespace，用于不同场景下的隔离：Mount - isolate filesystem mount pointsUTS - isolate hostname and domainnameIPC - isolate interprocess communication (IPC) resourcesPID - isolate the PID number spaceNetwork - isolate network interfacesUser - isolate UID/GID number spaces主要是三个系统调用 clone() – 实现线程的系统调用，用来创建一个新的进程，并可以通过设计上述参数达到隔离unshare() – 使某进程脱离某个 namespacesetns() – 把某进程加入到某个 namespace 具体的例子参见陈老师的文章：DOCKER 基础技术：LINUX NAMESPACE（上）。UTS NamespaceUTS: UNIX Time-sharing System。UTS namespace 用来隔离系统的 hostname 以及 NIS domain name。hostname 是用来标识一台主机的。NIS（Network Information Service）来自 wiki 的介绍 The Network Information Service, or NIS (originally called Yellow Pages or YP), is a client–server directory service protocol for distributing system configuration data such as user and host names between computers on a computer network. 就是一台账号主控服务器来管理网络中所有主机的账号，当其他的主机有用户登入的需求时，才到这部主控服务器上面请求相关的账号、密码等用户信息， 如此一来，如果想要增加、修改、删除用户数据，只要到这部主控服务器上面处理即可。UTS namespace 没有嵌套关系。IPC NamespaceIPC 全称 Inter-Process Communication，是 Unix/Linux 下进程间通信的一种方式，IPC 有共享内存、信号量、消息队列等方法。IPC Namespace 只有在同一个 Namespace 下的进程才能相互通信。PID NamespacePID namespaces 用来隔离进程的 ID 空间，使得不同 pid namespace 里的进程 ID 可以重复且相互之间不影响。PID namespace 可以嵌套，也就是说有父子关系，在当前 namespace 里面创建的所有新的 namespace 都是当前 namespace 的子 namespace。每个 PID namespace 的第一个进程的 ID 都是 1。在 linux 系统种，PID 为 1 的进程是 init。init 进程，它是一个由内核启动的用户级进程。当系统中一个进程的父进程退出时，内核会指定 init 进程成为这个进程的新父进程，而当 init 进程退出时，系统也将退出。因此内核会帮 init 进程屏蔽掉其他任何信号，这样可以防止其他进程不小心 kill 掉 init 进程导致系统挂掉。PID namesapce 对容器类应用特别重要， 可以实现容器内进程的暂停 / 恢复等功能，还可以支持容器在跨主机的迁移前后保持内部进程的 PID 不发生变化。Mount NamespaceMount namespace 用来隔离文件系统的挂载点, 使得不同的 mount namespace 拥有自己独立的挂载点信息，不同的 namespace 之间不会相互影响。当前进程所在 mount namespace 里的所有挂载信息可以在这些路径找到：/proc/[pid]/mounts/proc/[pid]/mountinfo/proc/[pid]/mountstats有另一个场景，需要一个 mount point 在各个 namespace 共享。由于 mount namespace 是隔离的，以前的做法是，在各个 mount namespace 都执行一次挂载，才可以访问共享的 mount point。Shared subtree 允许在 mount namespace 之间自动地或者是受控地传播 mount 和 umount 事件。Network NamespaceNetwork namespace 在逻辑上是网络堆栈的一个副本，它有自己的路由、防火墙规则和网络设备。Network namespace 之间是相互隔离的，我们可以使用 veth 设备把两个 network namespace 连接起来进行通信。veth 设备是虚拟的以太网设备。它们可以充当 network namespace 之间的通道，也可以作为独立的网络设备使用。ip netns 命令用来管理 network namespace。User NamespaceUser namespace 是 Linux 3.8 新增的一种 namespace，用于隔离安全相关的资源。一个用户可以在一个 user namespace 中是普通用户，但在另一个 user namespace 中是超级用户。User namespace 可以嵌套。todounshare 和 ip netns 命令的实验。参考Linux Namespace : PIDLinux Namespace : MountLinux Namespace : User]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes Pod、ReplicationSet、Deployment、Service 初体验]]></title>
    <url>%2Fp%2Fkubernetes-pod-deployment-service-intro%2F</url>
    <content type="text"><![CDATA[A Kubernetes Deployment managed ReplicaSet. Each one represents a different version of the deployed application. Each ReplicaSet manages a set of identically versioned Pods. (图片来源：www.weave.works)小实验 通过一个实验来加深理解。tomcat.yml 配置如下：12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: mytomcatspec: selector: matchLabels: app: tomcat replicas: 2 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080然后执行 kubectl create -f tomcat.yml。PodPod 是 kubernetes 中最基本的应用执行单元。代表一个在 k8s 集群运行的进程。 当 Pod 被创建后，都会被 Kubernetes 调度到集群的 Node 上。通常不会直接使用 Pod，而是通过 controller 操作。1234567891011121314151617181920# kubectl get podNAME READY STATUS RESTARTS AGEmytomcat-78c89857d6-428gg 1/1 Running 0 7h54mmytomcat-78c89857d6-rxx8v 1/1 Running 0 7h38m# kubectl describe pod mytomcat-78c89857d6-428ggName: mytomcat-78c89857d6-428ggNamespace: defaultPriority: 0Node: izwz9h8m2chowowqckbcy0z/172.18.151.35Start Time: Mon, 30 Sep 2019 13:32:05 +0800Labels: app=tomcat pod-template-hash=78c89857d6Annotations: &lt;none&gt;Status: RunningIP: 10.1.1.12IPs: IP: 10.1.1.12Controlled By: ReplicaSet/mytomcat-78c89857d6Controlled By字段表明由 ReplicaSet 管理 pod。另外，Deployment controller 会自动为 Pod 添加 pod-template-hash label。这样做的目的是防止 Deployment 的子 ReplicaSet 的 pod 名字重复。ReplicaSetA ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number.123456789101112131415161718192021222324252627282930# kubectl describe rs mytomcat-78c89857d6Name: mytomcat-78c89857d6Namespace: defaultSelector: app=tomcat,pod-template-hash=78c89857d6Labels: app=tomcat pod-template-hash=78c89857d6Annotations: deployment.kubernetes.io/desired-replicas: 2 deployment.kubernetes.io/max-replicas: 3 deployment.kubernetes.io/revision: 1Controlled By: Deployment/mytomcatReplicas: 2 current / 2 desiredPods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template: Labels: app=tomcat pod-template-hash=78c89857d6 Containers: tomcat: Image: tomcat Port: 8080/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 26m replicaset-controller Created pod: mytomcat-78c89857d6-428gg Normal SuccessfulCreate 26m replicaset-controller Created pod: mytomcat-78c89857d6-k5nh6 Normal SuccessfulDelete 10m replicaset-controller Deleted pod: mytomcat-78c89857d6-k5nh6 Normal SuccessfulCreate 9m54s replicaset-controller Created pod: mytomcat-78c89857d6-rxx8vDeploymentDeployment 为 Pod 和 Replica Set（下一代 Replication Controller）提供声明式更新。123456789101112131415161718192021222324252627282930313233# kubectl describe deployment mytomcatName: mytomcatNamespace: defaultCreationTimestamp: Mon, 30 Sep 2019 13:32:04 +0800Labels: &lt;none&gt;Annotations: deployment.kubernetes.io/revision: 1Selector: app=tomcatReplicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: app=tomcat Containers: tomcat: Image: tomcat Port: 8080/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: mytomcat-78c89857d6 (2/2 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set mytomcat-78c89857d6 to 1 Normal ScalingReplicaSet 11m (x2 over 27m) deployment-controller Scaled up replica set mytomcat-78c89857d6 to 2Deployment 操作 ReplicationSet 来调节 pod 数量到指定的 replicas（见 OldReplicaSets、NewReplicaSet）。Service 一个服务后端的 Pods 可能会随着生存灭亡而发生 IP 的改变，service 的出现，给服务提供了一个固定的 IP，而无视后端 Endpoint 的变化。Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。service 涉及 port、nodePort、targetPort 概念，容易混淆。portExpose the service on the specified port internally within the cluster.&lt;cluster ip&gt;:port是集群 内部 客户访问 service 的入口。service 中对应 spec.type 为ClusterIP。nodePortThis setting makes the service visible outside the Kubernetes cluster by the node’s IP address and the port number declared in this property.&lt;nodeIP&gt;:nodePort是集群 外部 客户访问 service 的一种入口（另一个种是 loadbalance）。service 中对应 spec.type 为NodePort。 端口范围只能是 30000-32767。targetPortThis is the port on the pod that the request gets sent to. Your application needs to be listening for network requests on this port for the service to work.容器的端口。外部流量经过 port、nodePort 最终流向 targetPort。关联 pod 和 serviceservice 通过 selector 和 pod 建立关联。k8s 会根据 service 关联到 pod 的 podIP 信息组合成一个 endpoint。tomcat-svc.yml 内容如下：12345678910111213apiVersion: v1kind: Servicemetadata: name: mytomcat-svcspec: selector: app: tomcat type: NodePort ports: - name: http port: 8080 targetPort: 8080 protocol: TCP执行 kubectl creat -f tomcat-svc.yml。 Endpoints 字段指向 pod。123456789101112131415# kubectl describe service mytomcat-svcName: mytomcat-svcNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=tomcatType: NodePortIP: 10.152.183.224Port: http 8080/TCPTargetPort: 8080/TCPNodePort: http 31672/TCPEndpoints: 10.1.1.12:8080,10.1.1.13:8080Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt;servcie 和端口的常见问题 为了能够在 kubernetes 外面访问到 service，会暴露 nodePort。一个常见的问题是，只在 service 的 yaml 文件设置了 spec.ports.nodePort，没有更新spec.type 为NodePort（默认为 ClusterIP），导致出现下面报错：1The Service “nacos-headless” is invalid: spec.ports[0].nodePort: Forbidden: may not be used when type is &apos;ClusterIP&apos; 小结 使用 Deployment 来创建 ReplicaSet，ReplicaSet 在后台创建 pod。port 和 nodePort 都是 service 的端口，前者暴露给集群内客户访问服务，后者暴露给集群外客户访问服务。从这两个端口到来的数据都需要经过反向代理 kube-proxy 流入后端 pod 的 targetPod，从而到达 pod 上的容器内。service 通过 selector 关联 pod。参考Why you need Istio, Kubernetes, and Weave Cloud for Distributed ApplicationsDeploymentskubernetes 中 port、target port、node port 的对比分析，以及 kube-proxy 代理]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes yaml 文件工具]]></title>
    <url>%2Fp%2Fkubernetes-yaml-export%2F</url>
    <content type="text"><![CDATA[kubernetes 推荐使用 yaml 文件定义对象，而非直接使用 kubectl 执行定义 create/run 命令。导出已有资源的 yamlkubectl get &lt;resource-type&gt; &lt;resource&gt; --export -o yaml以 yaml 格式导出系统中已有资源描述。12# kubectl get pod mytomcat-7d68ffdbfb-xjcz4 --export -o yaml &gt; mytomcat.ymlFlag --export has been deprecated, This flag is deprecated and will be removed in future.注意，未来版本的 kubernetes 会去掉 --export 参数。这里 有相关的争论，感兴趣可以阅读。如果没有 --export 参数，那么 get 资源会显示 status 相关的输出，对于定义 kubernetes 对象来说是多余的。--dry-run--dry-run仅打印这个对象，而不会执行命令。123456789101112131415161718# kubectl run mytomcat --image=tomcat --port=8080 --generator=run-pod/v1 --replicas=2 --dry-run -o yamlapiVersion: v1kind: Podmetadata: creationTimestamp: null labels: run: mytomcat name: mytomcatspec: containers: - image: tomcat name: mytomcat ports: - containerPort: 8080 resources: &#123;&#125; dnsPolicy: ClusterFirst restartPolicy: Alwaysstatus: &#123;&#125;komposeKompose 是 Kubernetes 社区开发的一个转换工具，可以方便地将简单的 Docker Compose 模板转化成为 Kubernetes 的 Yaml 描述文件，并在 Kubernetes 集群上部署和管理应用。由于已经跳过 docker compose 阶段，以后有需要再尝试这个工具。yaml 文件常见错误error: yaml: line 2: mapping values are not allowed in this contextkey: value，注意在 value 和“:”之间要有一个空格。error: yaml: line 3: found character that cannot start any tokenYAML 文件里面不能出现 tab 键。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 访问 pod]]></title>
    <url>%2Fp%2Fkubernetes-access-pod%2F</url>
    <content type="text"><![CDATA[现在已经创建了一个 tomcat 的 pod，接下来练习访问 pod。tomcat 默认是 8080 端口提供服务。1kubectl run mytomcat --image=tomcat --port=8080这里 --port 暴露容器的 8080 端口。要想访问 pod 的 8080 端口，有多种方法。port-forward通过端口转发映射本地端口到指定的应用端口。123## kubectl port-forward &lt;pod_name&gt; &lt;local_port&gt;:&lt;container_port&gt;kubectl port-forward mytomcat-7d68ffdbfb-qbp6p 7001:8080这里把本地 7001 端口映射到容器的 8080 端口。再使用 curl http://localhost:7001 测试返回。expose把 pod 暴露为 service。以 service 的方式提供对外访问。1kubectl expose deployment/mytomcat --type=&quot;NodePort&quot; --port=7000 --target-port=8080其中：--port是本地端口 --target-port 是容器暴露的端口 type 有多个选项：ClusterIP： 通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。NodePort： 通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务。LoadBalancer： 使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。ExternalName： 通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns 才支持 这里使用 NodePort 类型做实验。执行 kubectl expose 之后，会新增一个 service1234# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.152.183.1 &lt;none&gt; 443/TCP 25dmytomcat NodePort 10.152.183.222 &lt;none&gt; 7000:31364/TCP 69mkubernetes 使用 iptables 创建了路由规则 123# iptables-save | grep 10.152.183.222-A KUBE-SERVICES ! -s 10.152.183.0/24 -d 10.152.183.222/32 -p tcp -m comment --comment &quot;default/mytomcat: cluster IP&quot; -m tcp --dport 7000 -j KUBE-MARK-MASQ-A KUBE-SERVICES -d 10.152.183.222/32 -p tcp -m comment --comment &quot;default/mytomcat: cluster IP&quot; -m tcp --dport 7000 -j KUBE-SVC-GUOWFPQH6PYKZNE2 使用 curl http://&lt;cluster-ip&gt;:&lt;port&gt; 测试，成功返回首页内容 1curl http://10.152.183.222:7000 参考Use Port Forwarding to Access Applications in a Cluster]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 删除 pod，自动重启]]></title>
    <url>%2Fp%2Fkubernetes-delete-pod%2F</url>
    <content type="text"><![CDATA[问题 使用 kubectl run 命令创建 pod，发现漏了参数，想直接删除 pod，再创建。但是 kubernetes 会自动用之前的 pod 定义再次生成 pod。删除 pod1kubectl delete pod &lt;pod_name&gt;解决 直接删除 deployment123456789# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEmytomcat 1/1 1 1 36m# kubectl delete -n default deployment mytomcatdeployment.apps &quot;mytomcat&quot; deleted# kubectl get podNo resources found in default namespace-n: 指定 namespace。不指定的话是 default。原因 对 kubernetes 对象理解欠缺。spec 是必需的，它描述了对象的期望状态（Desired State）。status 描述了对象的 实际状态（Actual State） ，它是由 Kubernetes 系统提供和更新的。一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。后来发现更好的方式是使用更新 spec，让 k8s 自动更新 pod。123kubectl edit -f xxx.ymlkubectl apply -f xxx.yml]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[microk8s 之 k8s.gcr.io 访问问题]]></title>
    <url>%2Fp%2Fmicrok8s-gcr-image%2F</url>
    <content type="text"><![CDATA[为了学习 k8s，在 ecs 上安装单机版。网上查资料对比了 minikube 和 microk8s，最终使用 microk8s。安装 microk8s1snap install microk8s --classic安装的是 v1.15。设置别名（可选）microk8s 的工具都带有 microk8s. 前缀，可以增加别名简化 kubectl 使用：123vi .bashrcalias kubectl=/snap/bin/microk8s.kubectlkubectl run为了最基本体验，先启动一个 tomcat 试试。1kubectl run mytomcat --image=tomcat查看 pod 状态，发现一直是 ContainerCreating 状态 1kubectl get pod 于是使用 describe 命令查看 pod1kubectl describe pod xxx最后面日志如下 1234Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 22m (x33 over 69m) kubelet, izwz9h8m2chowowqckbcy0z Failed create pod sandbox: rpc error: code = Unknown desc = failed to get sandbox image &quot;k8s.gcr.io/pause:3.1&quot;: failed to pull image &quot;k8s.gcr.io/pause:3.1&quot;: failed to resolve image &quot;k8s.gcr.io/pause:3.1&quot;: no available registry endpoint: failed to do request: Head https://k8s.gcr.io/v2/pause/manifests/3.1: dial tcp 108.177.97.82:443: i/o timeout 这是 pod 的 sandbox 镜像拉取失败。网上查资料，k8s.gcr.io/pause:3.1 是存放在 google cloud 上的镜像，由于众所周知的原因，访问失败了。解决的方法有：科学爱国 手动下载镜像 第 1 种方法就不多说了。这里采用第 2 种方法。安装 docker1apt-get install docker.io感谢微软 azure 提供 gcr 镜像下载：地址 12docker pull gcr.azk8s.cn/google_containers/pause:3.1docker tag gcr.azk8s.cn/google_containers/pause:3.1 k8s.gcr.io/pause:3.1 下载镜像后，再手动修改 tag。但是再次 kubectl run 依然报错。网上查资料，microk8s 使用自己内置的容器服务 microk8s.docker，而非系统的 docker。 但是 12# microk8s.dockermicrok8s.docker: command not foundv1.14 之后 microk8s 使用 containerd 代替 dockerd，具体可见这个issueIndeed in the 1.14 release contanerd replaced dockerd. 要么使用私有仓库 registry，要么手动把 docker 镜像导入到 containerd。microk8s 官网提供了例子：Working with locally built images without a registry。 这里先使用手动操作，以后再建立私有仓库 12docker save k8s.gcr.io/pause:3.1 &gt; pause.tarmicrok8s.ctr -n k8s.io image import pause.tar-n 是指定 namespace。microk8s.ctr -n k8s.io image ls，看到导入的镜像了：1k8s.gcr.io/pause:3.1 application/vnd.oci.image.manifest.v1+json sha256:3efe4ff64c93123e1217b0ad6d23b4c87a1fc2109afeff55d2f27d70c55d8f73 728.9 KiB linux/amd64 io.cri-containerd.image=managed终于启动成功了 12345678Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/mytomcat-75b679fc45-gftfw to izwz9h8m2chowowqckbcy0z Normal Pulling 33m kubelet, izwz9h8m2chowowqckbcy0z Pulling image &quot;tomcat&quot; Normal Pulled 33m kubelet, izwz9h8m2chowowqckbcy0z Successfully pulled image &quot;tomcat&quot; Normal Created 33m kubelet, izwz9h8m2chowowqckbcy0z Created container mytomcat Normal Started 33m kubelet, izwz9h8m2chowowqckbcy0z Started container mytomcat 这里 有另一种解决 k8s.gcr.io 访问问题的思路，就是更换仓库名：修改 /var/snap/microk8s/current/args/kubelet。 添加–pod-infra-container-image=s7799653/pause:3.1修改 /var/snap/microk8s/current/args/containerd-template.toml 的 plugins -&gt; plugins.cri -&gt; sandbox_image 为 s7799653/pause:3.1小结 手动下载、tag、导入 k8s.gcr.io 镜像到 microk8s 的 contianerd：1234docker pull gcr.azk8s.cn/google_containers/&lt;imagename&gt;:&lt;version&gt;docker tag gcr.azk8s.cn/google_containers/&lt;imagename&gt;:&lt;version&gt; k8s.gcr.io/&lt;imagename&gt;:&lt;version&gt;docker save k8s.gcr.io/&lt;imagename&gt;:&lt;version&gt; &gt; &lt;imagename&gt;.tarmicrok8s.ctr -n k8s.io image import &lt;imagename&gt;.tar]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>microk8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟团队协作经历杂谈]]></title>
    <url>%2Fp%2Fvirtual-team-collaboration%2F</url>
    <content type="text"><![CDATA[最近业余时间在做一个公益项目，以虚拟团队方式协作。虚拟团队协作，和平常在公司工作干活，有很多差别。思维方式 一起干活的人，可能来自不同行业，有传统行业，也有互联网行业。行业背景不同，思维方式大相径庭。比如要做一个功能，面向是广大普通用户。传统行业的人倾向于一开始就想把方方面面都想好，做的很完美才觉得可以给用户去使用。这种想法容易陷入闭门造车的困境。花了大量时间做自认为用户会喜欢的功能，自己也累，结果用户用脚投票，数据惨淡，积极性被打压。常年在做 C 端产品的互联网人，就会小步快跑。先做个简单的功能，投放给种子用户群，收集反馈，改进，再发一版给用户，逐步完善。功能好与不好，由数据说了算。做事情的方法论 这些年在互联网公司工作，追求效率，做事情的方式是：这个事情是否值得去做，是长期收益还是短期收益，是现在就做，还是以后排期再做 对这个事情进行任务拆解，找到关键路径，识别风险点，确定时间节点，工作量量化预估 任务分工，每天汇报进度，及时同步问题，使得总体进度可控 这些方法论已经潜移默化，我以为是理所当然的，任何一个有心做事情的人，都应该如此。现实却打脸啪啪响：今天有一个想法，明天有一个想法，后天又有一个想法，每个想法都想立即去做，结果时间都耗费在一堆想法之中，一个都没有做好。缺乏时间节点意识，佛系做事情。要拍一个时间，觉得很困难。确定工作量，拍一个定量的工作指标，比如一天整理录入多少条资料，觉得很困难。没有主动进度同步的习惯，处于挤牙膏状态，不主动去催，就会失联，不知道任务进展情况。驱动力 在公司工作，有来自组织结构的约束。组织结构的 kpi 压力、来自老板的压力，会在最坏情况下逼迫一个人按时按量去完成任务。这是一种无形的驱动力量。可是来自不同组织的虚拟团队，会缺少这种外在驱动力。都是贡献业余时间做项目，没有直接的经济利益，没有公司的 KPI，做事情靠的是自愿和自觉。但惰性是人的天性，劲头一过，内在驱动力减少，也没有外在驱动力，干劲就少很多。反思 这个公益项目开始的时候，主要精力花在做什么产品对用户有价值、怎么去实现，并没有去思考虚拟团队的协作问题。结果是功能做出来并且审核通过上线了，运营和推广工作却严重落后，质量和数量都跟不上。既然付出了时间和精力，就期望做得更好。目前想到的点有：没有公司的 KPI，但我们有初衷和愿景。缺乏外在驱动力，就靠内在驱动力补偿。惰性人人都有，需要定时聚会，回想最初想做这个项目的初衷，以及对未来的愿景。在热情冷却之前，打打鸡血，继续战斗。在项目启动之初，尽早识别虚拟团队成员的思维方式、做事方法论、执行力。假设在项目一开始，就有意识去分配一些任务，观察成员的完成情况，可以加快发现这些思维、做事、执行力的不同，尽早调整，保持团队步伐一致。如果每天都有不同想法，每天都觉得最新的想法最值得去做，那么鼓励先记录下来，每个迭代完成后再一起 review，决定接下来做什么。如果做事随性，缺少规划，就要引导做任务拆分，并且随时跟踪执行情况。不过是短期协助，长期需要自驱动。如果没有进度更新的习惯，那就设置日历提醒任务，强制提醒，直至养成习惯。……改变已有的思维方式、方法论、执行力，并不是简单的事情，因此要尽早发现，预留时间调整。不要等项目进行到关键点才发现被这些差异阻塞，到时候调整的时间成本就很高。也不要一下子施加太大的压力，令人不舒服。大家步伐一致，虚拟团队协作才更加高效，最终做出一点事情。]]></content>
      <categories>
        <category>项目管理</category>
      </categories>
      <tags>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序 bugfix 经验]]></title>
    <url>%2Fp%2Fminapp-bugfix-case%2F</url>
    <content type="text"><![CDATA[记录 2 个小程序开发 bugfix case。软键盘弹起导致布局上移 输入框父容器使用 fixed 布局。但是软键盘弹起导致布局上浮，并且产生了一块空白区域。如下图所示 试了几种方案，最后解决如下：输入框的父容器保持 fixed 布局，另外动态调整 bottom 属性 1&lt;view class='inputRoom' style='bottom: &#123;&#123;inputBottom&#125;&#125;'&gt; 输入框增加 focus、blur 事件，更新 bottom。强制减去 60 是为了去掉多余的空白区域。手上机型不多，减去 60 高度还凑合，暂时不做机型适配。1234567891011focus: function(e) &#123; // 修复软键盘弹起，导致 fixed 布局失效 keyHeight = e.detail.height; // 兼容开发者工具 if (keyHeight &gt; 0) &#123; keyHeight -= 60 &#125; this.setData(&#123; inputBottom: keyHeight + 'px' &#125;)&#125;, 调整后的效果 view 标签换行符 原来用 view 标签展示文本。后台录入的换行符 \n 并没有展示生效，只展示为一个空格，导致文字挤成一团。解决：text 标签才支持 \n 换行。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的负载均衡算法简介]]></title>
    <url>%2Fp%2Fload-balance%2F</url>
    <content type="text"><![CDATA[随机 随机从服务器端地址选取一个来处理请求。在统计学角度看，请求会分摊到各个服务器。但是随机算法不会考虑服务器的性能和负载情况，不能充分利用异构服务器集群的性能。另外，当后端服务器集群数量很大，使用随机算法，会导致客户端与多个服务器端建立连接、维护连接，产生额外的资源消耗。优点：简单 缺点：没有考虑服务器负载和性能 源地址哈希法， IP hash通常根据客户端的 IP 地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。当服务器列表不变时，客户端每次都访问到相同服务器 ip。减少要维护连接的数量。如果服务器数量经常发生变化，可以使用一致性哈希算法，对服务器建立虚拟节点，减少客户端和服务器端连接漂移。优点：简单 在不支持会话保持的场景中，可以使用 ip_hash 实现简单的会话保持 缺点：没有考虑服务器负载和性能 轮询， Round Robbin太公分猪肉，人人有份。请求依次发送到服务器列表调度处理。因为要按顺序发送请求，显然是有状态的。不过从高可用的状态来看，这个状态即使丢失影响也不大，不需要额外持久化。优点：简单 缺点：没有考虑服务器负载和性能 加权轮询， Weighted Round Robbin给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载。解决服务器间性能不一的情况，它用相应的权值表示服务器的处理性能。然而加权轮询也不是银子弹。在请求服务时间变化较大或每个请求消耗时间不一致的情况下，容易导致服务器间的负载不平衡。机器配置是静态的，负载是动态的。简单的 RR 实现，使用机器配置作为权重。高级的 RR 实现，根据负载实现动态权重也是可行的，需要增加负载统计，向监控平台汇报 metrics，定时更新权重。另外，加权的实现要考虑权重的公约数问题，否则当权重的数值很大，会导致更新缓慢，具体以后再聊。优点：简单版本的 RR 实现相对简单。考虑到机器性能差异。适合大量短请求 缺点：如果不考虑负载，在请求服务时间变化较大或每个请求消耗时间不一致的情况下，容易导致服务器间的负载不平衡。如果考虑负载，则需要增加监控模块。加权最小连接数， Weighed Least Connections以连接数来衡量服务器负载情况。通过服务器当前所活跃的连接数来估计服务器的负载情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器时，其连接数加一; 当连接中止或超时，其连接数减一。优点：适合长时处理的请求服务，如 FTP 等应用 小结 选择负载均衡算法，通常使用加权 RR 或者加权最小连接数。加权能够根据机器性能和负载分摊请求。加权最小连接数适合长时间处理的请求。加权轮询更加适合大量短时间的请求。]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 环境相关]]></title>
    <url>%2Fp%2Fpython-env%2F</url>
    <content type="text"><![CDATA[dist-packages vs site-packagesdist-packages is a Debian-specific convention that is also present in its derivatives, like Ubuntu. Modules are installed to dist-packages when they come from the Debian package manager into this locationdist-packages instead of site-packages. Third party Python software installed from Debian packages goes into dist-packages, not site-packages. This is to reduce conflict between the system Python, and any from-source Python build you might install manually.Debian 类操作系统的包管理器，会使用 dist-packages 保存安装的第三方库。用户自己安装的 python，第三方库默认会保存在 site-packages。pipenvpipenv 支持 pipfile，可以替代 requirements.txt 文件。一个项目对应一个 Pipfile，支持开发环境与正式环境区分。默认提供 default 和 development 区分。提供版本锁支持，存为 Pipfile.lock。1pip install pipenv使用 12$ cd project_folder$ pipenv install requests 执行 1pipenv run python main.pyvirtualenvvirtualenv 用于创建独立的 Python 环境，多个 Python 相互独立，互不影响 安装 1pip install virtualenv 创建 1virtualenv [虚拟环境名称 - 也是目录名称] 启动 12cd ENVsource ./bin/activate 退出 1deactivatevirtualenvwrapperVirtaulenvwrapper 是 virtualenv 的扩展包： 将所有虚拟环境整合在一个目录下 快速切换虚拟环境 参考Pipenv &amp; Virtual EnvironmentsPython 新利器之 pipenvPython 三神器之 virtualenv、virtualenvwrapper]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装 spacy 遇到 linux oom killer 问题]]></title>
    <url>%2Fp%2Finstall-spacy-with-low-memory-vps%2F</url>
    <content type="text"><![CDATA[开发机配置从 2c4g 降配到 1c2g，在新 vps 上安装 spacy 遇到内存不足的问题。安装 spacy 命令如下 1234apt-get updateapt-get install build-essential python3-dev gitapt install python3-pippip3 install -U spacy 过了几十秒之后 12Installing collected packages: spacyKilledKilled？ 重试之后，还是同样问题。没有报错信息，应该不是程序问题。使用 tail -f /var/log/kern.log 查看内核日志 1Sep 19 22:07:19 iZwz9h8m2********** kernel: [1376339.542163] Out of memory: Kill process 25349 (pip3) score 610 or sacrifice child 恍然大悟！linux oom killer 问题，很久没有遇到过了。先看看现在的配置 才 1g 不到的 swap 文件，太小了。解决方法是增大 swap 文件，先增加到 3g：123456789101112$ create swap file of 2048 MBdd if=/dev/zero of=/swapfile.new bs=1024 count=2097152$ modify permissionschown root:root /swapfile.newchmod 0600 /swapfile.new$ setup swap areamkswap /swapfile.new$ turn swap onswapon /swapfile.new再次安装 spacy，同时打开 top 命令观察 还是很消耗资源，不过最终安装成功12Installing collected packages: spacySuccessfully installed spacy-2.1.8]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>spacy</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https 专题]]></title>
    <url>%2Fp%2Fhttps-collection%2F</url>
    <content type="text"><![CDATA[一些 https 相关原理，并且结合抓包、nginx 配置实践。nginx 配置 SSL 证书，以及 ssl_ciphers 选择 nginx 开启 TLSv1.3nginx 配置多个 TLS 证书，以及 TLS SNI 简介nginx 配置 hstsnginx 开启 http2nginx 开启 ocsp staplingnginx 配置 dhparam，以及聊聊 forward secrecyhttps tls v1.2 握手过程 使用 wireshark 抓包 tls 1.2 握手 未完待续。]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 多线程系列]]></title>
    <url>%2Fp%2Fjava-collection-multi-thread%2F</url>
    <content type="text"><![CDATA[Java 多线程文章汇总，未完待续：java wait()和 sleep()的区别 聊聊 sleep(0)和 Thread.yield()聊聊 java 内存模型 java volatileJava CAS 原理，以及 CAS ABA 问题java synchronized 实现原理java 线程池原理wait、notify 和生产者消费者模式ThreadLocal 原理Java AbstractQueuedSynchronizer 笔记Java ReentrantLock 原理Condition 原理，以及实现生产者消费者模式Java Semaphore 原理Java CountDownLatch 原理Java CyclicBarrier 原理Java ConcurrentHashMap 原理Java 8 HashMap 详解 四个线程循环输出 ABCDJava Thread join 详解Java Striped64 原理Java LongAdder 原理]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 8：cloudflare 防爬方案]]></title>
    <url>%2Fp%2Fanti-crawler-part-8-tuicool-again-cloudflare%2F</url>
    <content type="text"><![CDATA[又是推酷爬虫 不幸的消息，推酷爬虫又来了。打开 html，发现字体反爬已经被破😭，直接解析为文字了：不过图片保护还能用。另外，至今推酷官方邮件 biz@tuicool.com 一直没有回复。现在的结论是：推酷爬虫对我的小站应该是一周更新一次。文章链接还是暴露。猜测是从首页遍历。简单的一套转换字体映射名的方式，不保险。低估了这个多年文章搬运工。应对措施：先把还没被爬的文章撤下 买了新域名，赶紧搞备案。直接 nginx + iptables 封杀 cloudflare 防火墙策略防爬 更新于 2019.9.6：在阿里云买了域名之后，发现备案要求续费主机至少 3 个月以上。。。如果使用另一个开发账号的 ECS 去备案，又涉及到备案人和域名持有人的问题。域名和备案，只是为了能在 ecs 上部署站点，并且使用 nginx 做 waf。如果有另外的方式能够实现 waf，就可以防爬，不需要这么折腾。搜索资料，发现 cloudflare 提供了一些 waf 功能。于是新的方案出来：登录阿里云域名控制台，新域名 ycwu314.top 域名解析从万网，改为 cloudflare nameserver。12ines.ns.cloudflare.comivan.ns.cloudflare.comdns 解析转移，要等待一段时间才生效。123# whois ycwu314.top | grep &apos;Name Server&apos;Name Server: ines.ns.cloudflare.comName Server: ivan.ns.cloudflare.com在 cloudflare 中增加 A 记录或者 CNAME，指向 github。Custom domains configured with A recordsIf you configured your custom domain using an A record, your A record must point to one of the following IP addresses for HTTPS to work:185.199.108.153185.199.109.153185.199.110.153185.199.111.153GitHub pages 指向新的域名 ycwu314.top。 开启 https。cf 默认给站点提供了免费证书，并且开启了 https，也支持强制 https 传输、hsts 等特性。在 cf 中配置 waf纳尼，要付费用户才能使用。。。其实有隔壁的入口是免费的，可以配置 5 条规则 使用 github pages 自定义域名的另一个好处是，由 github 直接返回 301 做重定向，加快 google 索引转移权重。解决 cloudflare access log 问题 拦截爬虫请求，可以根据 ip、user-agent 拦截，通常是在 nginx/apache 的 access log 获取。但是，cloudflare 只有企业版用户才能获取（人民币玩家😂）。不过我想到一个方法，增加一个防火墙 rule，用于记录 access log。再从这个 rule 观察可疑的 ip 和 user-agent。在 firewall、tools 这个入口也可以配置 ip、user-agent 拦截，但是只能一个一个添加。Firewall Rules 页面可以使用 cf 的语法，结合脚本生成拦截规则。但是生成的 cf 表达式要小于 4KB，因此 UA 不能无限配置。具体语法使用很简单，看下官网介绍就可以了。最大问题是不支持下载 access log，目前只能人工看和分析。以后写个脚本，直接提取 access log 日志，方便做行为分析。另外，使用 cloudflare 之后，速度比直连 github page 要慢一些。logflare.app研究 cloudflare 功能的时候在 app 页面发现了好东西：logflare.app。logflare 使用 cloudflare worker（serverless 应用），异步采集请求日志，不对页面请求发生影响。logflare.app 收集请求日志，并且和 Google Data Studio 集成。这就是我想要的 access log，还有数据分析工具可以使用，太赞了！不过只有 IP 地址，没有地理信息，可以去 IPInfo.io 注册一个 key，free plan 一个月 5w requests。捕捉爬虫 剩下的就是发几篇占位符文章，尝试捕捉推酷爬虫了。github pages 自定义域名设置丢失 使用 travis ci 构建后，发现 GitHub pages 设置的自定义域名没了。改为使用 CNAME 文件，还是没有生效。发现 17 年的时候就有人汇报了这个问题：Pushing changes to GitHub Pages branch removes custom domain setting #7538。 travis ci 官网 fqdn 选项：deployment12deploy: - fqdn: Optional, sets a custom domain for your website, defaults to no custom domain support.然而也没有用。因为 deploy 是使用 hexo d 命令操作的。并非 travis ci 的 deploy 任务。后来想起来，我是在源码目录建立的 CNAME 文件，构建完之后当然没有拷贝到 master 分支的根目录。因为使用了 next 主题，直接把 CNAME 文件丢到 themes\next\source。 后续更新 屏蔽 IDC 机房、xxx 云 更新于 2019.9.10：经过几天的观察，屏蔽了一些 IDC 机房、xxx 云、疑似代理的 ip。后来 ip 太多，用 C 段地址屏蔽也麻烦，对于 IDC 机房地址，直接屏蔽 ASN。用到 3 个工具：根据名字或者 asn 查找 https://hackertarget.com/as-ip-lookup/ 输入 idc、cloud 之类关键字得到一些 asn。根据 asn 查找 ip 段 https://traceroute-online.com/ip-asn-lookup/ip 地址查询，包括 asn、idc 机房等https://www.ipip.net/ip.html 小心骨干网的 asn，不要手抖屏蔽😀。主要屏蔽云主机厂商的 asn 就可以。比较难对付的是代理 ip，只能见一个封一个。国外机房的爬虫 除了国内几大云厂商外，爬虫站点还使用了国外，统计下来法国、俄罗斯的机房请求普遍可疑，直接增加一个规则，对国家级别拦截并且弹出验证码 1ip.geoip.country in &#123;&quot;RU&quot; &quot;FR&quot;&#125;atom.xml 和站点根目录 之前猜测，站点链接泄露，是首页、atom.xml、sitemap、归档页面。于是加了规则观察这些地址。最后发现 /atom.xml 和站点根目录是重灾区：这种在一个云平台短时间更换 ip（国内、日本等，观察 ASN）反复查询 /atom.xml、/ 的地址，基本是爬虫。另外，linux 桌面端比例很少，user agent 带有“Linux x86_x64”很有可能是爬虫。之前提到分类目录也可能泄露 url 转码之后是“分布式”。 爬虫新手 不要以为这年头做爬虫的都会改个 user-agent 话说这是很早之前暴露的图片地址，早就换链接了。肯定是爬了另一个网站，恰好又是爬了之前我的文章。顺带又屏蔽了一个 idc。初级爬虫最大的危害是不限速。分分钟可以拖垮原站点 这是是很久以前暴露出去的链接了。打开详情，原来是 wordpress 采集插件。我的策略是 block IP 优先，并且包括几个常见云服务 IDC 的 ASN，因此顶住了。顺便 block UA 策略也更新了。遇到高级爬虫 有的爬虫比较高级。单个 ip 访问频次很低，并且是使用未被发现的代理 ip。但是由于访问时间间隔太长反而暴露了。一篇短文章，有几张图，正常人阅读两三分钟就读完了，但是图片的地址却是间隔 5、6 分钟才下载一张。对于这种类型的爬虫，静态的 waf 是很难防住的。只能根据文章特征和访问行为特征做建模😂图片保护 cloudflare 支持图片保护（referer 检查），具体功能叫“Hotlink Protection”：Protect your images from off-site linking.Supported file extensions: gif, ico, jpg, jpeg, and png. 注意暂时不支持 webp，落后了。以后图片优先使用 png。效果 距离上次被爬取 10 天了 加速 cloudflare 访问 （updated at 2019.9.11）cloudflare 免费账号，只能使用 NS 方式接入，dns 记录生效慢，而且不能指定 cf 节点；只有付费版才可以使用 cname 记录方式接入。cloudflare 尽管有全球各地的 cdn 节点，但是因为众所周知的原因，国内访问访问 cloudflare 站点，默认访问美国西岸的节点。so，这个 cdn 加速还不如不加速。付费用户可以选择 cloudflare 域名节点。 其实 GitHub pages 直连访问速度比 cf 美国西岸要快。网上查到其他人的解决方案：使用第三方的 cloudflare 管理后台服务。第三方服务应该是付费的 cloudflare 账号，再使用 API 更新 cname 记录。具体可以参照这个文章：张戈博客使用 CloudFlare CDN 加速的经验技巧分享 。 温馨提示：注册的时候，提供 cf 账号和密码。 所以我没有使用。后来无意中看到这个文章：基于 Hexo 的 GitHub Pages 配置 CloudFlare CDN。 cf 提供 CNAME flatten 技术，支持根域名直接 cname 到另一个域名，并且 dns 查询的时候，直接返回对应 CNAME 的 A 记录或者 AAAA 记录。Understand and configure CNAME Flattening之前是在 cf 中配置域名直接走 A 记录从 ycwu314.top 到 github.com 的 ip 地址，访问很慢；现在改为 cname 方式从 ycwu314.top 到 github pages 的地址，结果速度和之前差不多，防火墙功能也正常。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列]]></title>
    <url>%2Fp%2Fanti-crawler-collection%2F</url>
    <content type="text"><![CDATA[站点被爬之后，才意识到站点虽小，防爬工作不能少。下面是我的反爬经历，持续更新：反爬虫系列之 1：小站被爬 反爬虫系列之 2：字体反爬 反爬虫系列之 3：图片保护 反爬虫系列之 4：重命名图片 反爬虫系列之 5：headless 浏览器检测 反爬虫系列之 6：nginx 屏蔽 user-agent反爬虫系列之 7：其他方案 反爬虫系列之 8：cloudflare 防爬方案]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 7：其他方案]]></title>
    <url>%2Fp%2Fanti-crawler-part-7-others%2F</url>
    <content type="text"><![CDATA[在实施站点反爬的过程，思考了一些方案，虽然这次没有实施，但也值得记录下来。iframe不少爬虫程序处理 iframe 都比较弱鸡，尤其是嵌套的情况。因此最初考虑使用 iframe 嵌套方式保护页面内容。思路是把正文放在另一个 html，通过 iframe 加载。默认 src=””，通过动态脚本加载替换 src 地址。问题是工作量大，对 hexo 侵入太深。另外嵌入 iframe 之后，比较丑，样式不好调。css contentCSS 的 content 属性用于在元素的 ::before 和::after伪元素中插入内容。伪元素不会出现在 dom 结构，而是出现在 css 结构中。因为一般的页面解析是操作 dom 树，因此 css content 有很强的反爬能力。可以使用 python 脚本先扫描正文，再抽取个别的字，用 span 占位符，把真实内容写入 css content 属性。以后有需要可以实现。pdf一般文章爬虫会忽略 pdf 资源。但是 hexo 有 pdf 插件，可以预览。可以考虑把代码块生成 pdf 文件再显示，实现保护代码正文。但是实现成本高，需要在 python 脚本生成 pdf 文件，并且要带上语法颜色。另一个问题是 pdf 文件体积大太多了。因此不考虑。不显示的随机元素 在文章中随机插入不可见的元素。文章爬虫做数据清理，通常会干掉 css 样式，结果这些隐藏元素显示出来，导致页面混乱。以前投机取巧的 SEO，会在页面里面塞入一堆关键字，并且不显示，搜索引擎傻傻的就把排名弄上去。新版本的搜索引擎有识别页面不可显示元素的能力了。有需要可以实现，不复杂。svg 显示数字 把数字替换为 svg 文件，再显示。不过我已经实现了字体防爬，就没有必要了。css 拼接图片和元素位移 通常用于数字敏感的内容，例如价格。具体参见：爬虫笔记之自如房屋价格图片识别（价格字段 css 背景图片偏移显示）。 不适合我的静态站点。这里有一些例子，就不再重复实验：反击爬虫，前端工程师的脑洞可以有多大？。菲闭合的 html 标签 非闭合的 html 标签，通常浏览器页面显示是正常的。但是 python 的 lxml 库处理起来有坑。不过用途不大，pthon 的 html5lib 兼容性强很多。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 6：nginx 屏蔽 user-agent]]></title>
    <url>%2Fp%2Fanti-crawler-part-6-nginx-block-useragent%2F</url>
    <content type="text"><![CDATA[等过段时间部署到 ecs，就可以使用 nginx 提高防爬能力。最简单的方式是检查 ua，因为使用 HttpClient、urllib 之类的 ua 非常容易识别。在 server 块增加判断 user-agent1234567891011if ($http_user_agent = "") &#123; return 403; &#125;# 禁止 Scrapy 等工具的抓取 if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) &#123; return 403;&#125;# 禁止指定 UA 及 UA 为空的访问if ($http_user_agent ~*"FeedDemon|Indy Library|Alexa Toolbar|AskTbFXTV|AhrefsBot|CrawlDaddy|CoolpadWebkit|Java|Feedly|UniversalFeedParser|ApacheBench|Microsoft URL Control|Swiftbot|ZmEu|oBot|jaunty|Python-urllib|lightDeckReports Bot|YYSpider|DigExt|HttpClient|MJ12bot|heritrix|EasouSpider|DotBot|Ezooms|^$") &#123; return 403; &#125; 网上找到一些常见的爬虫站点的 user-agentApache/Nginx/PHP 屏蔽垃圾 UA 爬虫的方法 网上坏蜘蛛搜索引擎 bot/spider 等 HTTP USER AGENT 关键字一览(无重复, 持续更新)ua 本身就不可靠，更好的方式，根据 ip 访问频率查找异常 ip 进行封杀，直接使用 iptables 封掉 ip 段。1iptables -I INPUT -s 54.36.148.0/24 -j DROP]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 5：headless 浏览器检测]]></title>
    <url>%2Fp%2Fanti-crawler-part-5-headless-browser%2F</url>
    <content type="text"><![CDATA[检测 headless 浏览器 静态站点比较少使用 headless 浏览器爬取。不过也顺带整理了一些资料。代码见附录。navigator.user-agentuser-agent 是最基本的检查字段，也是最不可靠的。结合 platform 一起判断。参照 fingerprintjs2（以下简称为 fp）的 getHasLiedOs 和 getHasLiedBrowser 一起使用。chrome如果 ua 测试为 chrome，则有 chrome 对象，且至少有：chrome.appchrome.cslchrome.loadTimeschrome headlessChrome Headless 默认的 ua 包含 HeadlessChrome：1Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/59.0.3071.115 Safari/537.36navigator.webdriver桌面浏览器 navigator.webdriver 返回为 undefined。这是个只读属性。参照 Navigator.webdriverThe navigator.webdriver property is true when in:ChromeThe –enable-automation or the –headless flag is used.FirefoxThe marionette.enabled preference or –marionette flag is passed.navigator.languagenavigator.languages、navigator.language 必须存在。 并且 navigator.languages[0]等于 navigator.language。参照 fp 的 getHasLiedLanguages。navigator.plugins 和 navigator.mimeTypes正常浏览器 plugins 和 mimeTypes 不为空，并且 plugin 的 mimeTypes 对于 navigator.mimeTypes。headless 浏览器默认 plugins 为空。window 的存储属性： sessionstorage localStorage indexedDB测试 window.sessionStorage、window.localStorage、window.indexedDB 是否存在。屏幕尺寸 测试 1window.screen.width &lt; window.screen.availWidth || window.screen.height &lt; window.screen.availHeightwebgl 来自 [译] 如何检测 Chrome Headless（无头浏览器）？ 当使用 Linux 下的普通 Chrome 时，我获得的渲染器和供应商为：“Google SwiftShader” 和 “Google Inc.”。在无头模式下，我获得的渲染器值为 “Mesa OffScreen”，这是一种不渲染任何窗口的技术。供应商则为 “Brian Paul”，这个项目孵化了开源图形库 Mesa。12345678910var canvas = document.createElement('canvas');var gl = canvas.getContext('webgl'); var debugInfo = gl.getExtension('WEBGL_debug_renderer_info');var vendor = gl.getParameter(debugInfo.UNMASKED_VENDOR_WEBGL);var renderer = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL); if(vendor == "Brian Paul" &amp;&amp; renderer == "Mesa OffScreen") &#123; console.log("Chrome headless detected");&#125;Modernizr 库检测浏览器特性 Modernizr 库可以测试浏览器中是否支持各种 HTML 和 CSS 功能123if(!Modernizr["hairline"]) &#123; console.log("It may be Chrome headless");&#125; 加载失败的图片 来自 [译] 如何检测 Chrome Headless（无头浏览器）？ 对于普通的 Chrome，这些加载失败的图片仍然具有宽度和高度，其具体值取决于浏览器的缩放情况，总之肯定不会是零。但是，在 Chrome Headless 中，这种图片的宽度和高度全部都为零。12345678910var body = document.getElementsByTagName("body")[0];var image = document.createElement("img");image.src = "http://ycwu314dottop.jg";image.setAttribute("id", "fakeimage");body.appendChild(image);image.onerror = function()&#123; if(image.width == 0 &amp;&amp; image.height == 0) &#123; console.log("Chrome headless detected"); &#125;&#125;识别 headless 浏览器之后 一旦发现是 headless 浏览器，可以考虑：跳转到默认处理页面 或者，使用验证码，避免误杀 google reCaptcha 提供了免费的验证码服务，但是要有服务器端获取结果。 其实静态站点也可以使用，通过 serverless 函数方式，只要不超出免费额度。这里提供一份参考：Protecting Your Website Forms With Serverless CAPTCHA。文章中的架构图如下：接入 Google reCaptcha 方式见 recaptcha 选择 v3 版本，可以在国内使用。未来有需要再搭建。参考 无头浏览器反爬与反反爬 反爬虫中 chrome 无头浏览器的几种检测与绕过方式 [译] 如何检测 Chrome Headless（无头浏览器）？ 爬虫如何隐藏 Headles-Chrome 不被检测出来 附录 整理了上文 headless 浏览器检测代码备用。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224/** * begin copy from https://github.com/Valve/fingerprintjs2/blob/master/fingerprint2.js */var getHasLiedLanguages = function () &#123; // We check if navigator.language is equal to the first language of navigator.languages // navigator.languages is undefined on IE11 (and potentially older IEs) if (typeof navigator.languages !== 'undefined') &#123; try &#123; var firstLanguages = navigator.languages[0].substr(0, 2) if (firstLanguages !== navigator.language.substr(0, 2)) &#123; return true &#125; &#125; catch (err) &#123; return true &#125; &#125; return false&#125;var getHasLiedResolution = function () &#123; return window.screen.width &lt; window.screen.availWidth || window.screen.height &lt; window.screen.availHeight&#125;var getHasLiedOs = function () &#123; var userAgent = navigator.userAgent.toLowerCase() var oscpu = navigator.oscpu var platform = navigator.platform.toLowerCase() var os // We extract the OS from the user agent (respect the order of the if else if statement) if (userAgent.indexOf('windows phone') &gt;= 0) &#123; os = 'Windows Phone' &#125; else if (userAgent.indexOf('win') &gt;= 0) &#123; os = 'Windows' &#125; else if (userAgent.indexOf('android') &gt;= 0) &#123; os = 'Android' &#125; else if (userAgent.indexOf('linux') &gt;= 0 || userAgent.indexOf('cros') &gt;= 0) &#123; os = 'Linux' &#125; else if (userAgent.indexOf('iphone') &gt;= 0 || userAgent.indexOf('ipad') &gt;= 0) &#123; os = 'iOS' &#125; else if (userAgent.indexOf('mac') &gt;= 0) &#123; os = 'Mac' &#125; else &#123; os = 'Other' &#125; // We detect if the person uses a mobile device var mobileDevice = (('ontouchstart' in window) || (navigator.maxTouchPoints &gt; 0) || (navigator.msMaxTouchPoints &gt; 0)) if (mobileDevice &amp;&amp; os !== 'Windows Phone' &amp;&amp; os !== 'Android' &amp;&amp; os !== 'iOS' &amp;&amp; os !== 'Other') &#123; return true &#125; // We compare oscpu with the OS extracted from the UA if (typeof oscpu !== 'undefined') &#123; oscpu = oscpu.toLowerCase() if (oscpu.indexOf('win') &gt;= 0 &amp;&amp; os !== 'Windows' &amp;&amp; os !== 'Windows Phone') &#123; return true &#125; else if (oscpu.indexOf('linux') &gt;= 0 &amp;&amp; os !== 'Linux' &amp;&amp; os !== 'Android') &#123; return true &#125; else if (oscpu.indexOf('mac') &gt;= 0 &amp;&amp; os !== 'Mac' &amp;&amp; os !== 'iOS') &#123; return true &#125; else if ((oscpu.indexOf('win') === -1 &amp;&amp; oscpu.indexOf('linux') === -1 &amp;&amp; oscpu.indexOf('mac') === -1) !== (os === 'Other')) &#123; return true &#125; &#125; // We compare platform with the OS extracted from the UA if (platform.indexOf('win') &gt;= 0 &amp;&amp; os !== 'Windows' &amp;&amp; os !== 'Windows Phone') &#123; return true &#125; else if ((platform.indexOf('linux') &gt;= 0 || platform.indexOf('android') &gt;= 0 || platform.indexOf('pike') &gt;= 0) &amp;&amp; os !== 'Linux' &amp;&amp; os !== 'Android') &#123; return true &#125; else if ((platform.indexOf('mac') &gt;= 0 || platform.indexOf('ipad') &gt;= 0 || platform.indexOf('ipod') &gt;= 0 || platform.indexOf('iphone') &gt;= 0) &amp;&amp; os !== 'Mac' &amp;&amp; os !== 'iOS') &#123; return true &#125; else &#123; var platformIsOther = platform.indexOf('win') &lt; 0 &amp;&amp; platform.indexOf('linux') &lt; 0 &amp;&amp; platform.indexOf('mac') &lt; 0 &amp;&amp; platform.indexOf('iphone') &lt; 0 &amp;&amp; platform.indexOf('ipad') &lt; 0 if (platformIsOther !== (os === 'Other')) &#123; return true &#125; &#125; return typeof navigator.plugins === 'undefined' &amp;&amp; os !== 'Windows' &amp;&amp; os !== 'Windows Phone'&#125;var getHasLiedBrowser = function () &#123; var userAgent = navigator.userAgent.toLowerCase() var productSub = navigator.productSub // we extract the browser from the user agent (respect the order of the tests) var browser if (userAgent.indexOf('firefox') &gt;= 0) &#123; browser = 'Firefox' &#125; else if (userAgent.indexOf('opera') &gt;= 0 || userAgent.indexOf('opr') &gt;= 0) &#123; browser = 'Opera' &#125; else if (userAgent.indexOf('chrome') &gt;= 0) &#123; browser = 'Chrome' &#125; else if (userAgent.indexOf('safari') &gt;= 0) &#123; browser = 'Safari' &#125; else if (userAgent.indexOf('trident') &gt;= 0) &#123; browser = 'Internet Explorer' &#125; else &#123; browser = 'Other' &#125; if ((browser === 'Chrome' || browser === 'Safari' || browser === 'Opera') &amp;&amp; productSub !== '20030107') &#123; return true &#125; // eslint-disable-next-line no-eval var tempRes = eval.toString().length if (tempRes === 37 &amp;&amp; browser !== 'Safari' &amp;&amp; browser !== 'Firefox' &amp;&amp; browser !== 'Other') &#123; return true &#125; else if (tempRes === 39 &amp;&amp; browser !== 'Internet Explorer' &amp;&amp; browser !== 'Other') &#123; return true &#125; else if (tempRes === 33 &amp;&amp; browser !== 'Chrome' &amp;&amp; browser !== 'Opera' &amp;&amp; browser !== 'Other') &#123; return true &#125; // We create an error to see how it is handled var errFirefox try &#123; // eslint-disable-next-line no-throw-literal throw 'a' &#125; catch (err) &#123; try &#123; err.toSource() errFirefox = true &#125; catch (errOfErr) &#123; errFirefox = false &#125; &#125; return errFirefox &amp;&amp; browser !== 'Firefox' &amp;&amp; browser !== 'Other'&#125;var getWebglCanvas = function () &#123; var canvas = document.createElement('canvas') var gl = null try &#123; gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl') &#125; catch (e) &#123; /* squelch */ &#125; if (!gl) &#123; gl = null &#125; return gl&#125;var isCanvasSupported = function () &#123; var elem = document.createElement('canvas') return !!(elem.getContext &amp;&amp; elem.getContext('2d'))&#125;var isWebGlSupported = function () &#123; // code taken from Modernizr if (!isCanvasSupported()) &#123; return false &#125; var glContext = getWebglCanvas() return !!window.WebGLRenderingContext &amp;&amp; !!glContext&#125;/** * end copy from https://github.com/Valve/fingerprintjs2/blob/master/fingerprint2.js */var hasLiedUserAgent = function () &#123; var ua = window.navigator.userAgent if (ua == undefined || ua == '') &#123; return true; &#125; return false;&#125;var hashLiedChrome = function () &#123; if (/Chrome/.test(window.navigator.userAgent)) &#123; if (!window.chrome || !window.chrome.runtime || !window.chrome.app || !window.chrome.csi || !window.chrome.loadTimes) &#123; // headless... return true; &#125; &#125; return false;&#125;var isWebDriver = function () &#123; return !(navigator.webdriver == undefined);&#125;var hasLiedStorage = function () &#123; return !window.sessionStorage || !window.localStorage || !window.indexedDB;&#125;var hasLiedPlugins = function () &#123; return (!navigator.plugins || navigator.plugins.length == 0)&#125;var hasLiedMimeTypes = function () &#123; return navigator.mimeTypes.length == 0&#125;var hasLiedWebGL = function () &#123; var canvas = document.createElement('canvas'); var gl = canvas.getContext('webgl'); var debugInfo = gl.getExtension('WEBGL_debug_renderer_info'); var vendor = gl.getParameter(debugInfo.UNMASKED_VENDOR_WEBGL); var renderer = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL); return vendor == "Brian Paul" &amp;&amp; renderer == "Mesa OffScreen";&#125;// TODO test on failed img// TODO test on css hairline]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>web</tag>
        <tag>网络安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Travis CI 使用 cache 加速构建]]></title>
    <url>%2Fp%2Ftravis-ci-speed-up-building-with-cache%2F</url>
    <content type="text"><![CDATA[travis ci cache这几天在测试构建，一直觉得 travis ci 有点慢。于是打开日志研究 每次构建安装 hexo 相关插件，花掉 30s 时间！发现.travis.yml 竟然是 12345install: - npm install -g hexo --save - npm install hexo-lazyload-image --save - npm install hexo-neat --save - npm install hexo-autonofollow --save 导致每次都会下载安装包。对于不经常变化的库和文件夹，travis ci 提供了 cache 机制，加速构建速度。1234cache: apt: true directories: - node_modules # 缓存不经常更改的内容 修改为 12install: - npm install 构建日志 12345npm WARN prepare removing existing node_modules/ before installation&gt; fsevents@1.2.9 install /home/travis/build/[secure]/[secure].github.io/node_modules/fsevents&gt; node installadded 789 packages in 6.407s 缓存有过期时间，参见 Caching Dependencies and Directories:Cache archives are currently set to expire after 28 days for repositories on https://travis-ci.org and 45 days for those on https://travis-ci.com. This means a specific cache archive will be deleted if it wasn’t changed after its expiration delay.travis ci 缓存的读取和更新Before the build, we check if a cached archive exists. If it does, we download it and unpack it to the specified locations.After the build we check for changes in the directory, create a new archive with those changes, and upload it to the remote storage.travis ci cache 效果优化 新的构建平均减少了 30s。]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>devops</tag>
        <tag>travis ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 4：重命名图片]]></title>
    <url>%2Fp%2Fanti-crawler-part-4-rename-image%2F</url>
    <content type="text"><![CDATA[重命名图片 上次提到图片保护的问题 反爬虫系列之 3：图片保护 但是 8 月份才开始使用 hexo-lazyload-image 插件，导致之前的图片链接被爬了。如果爬虫站点自己保存一份图片，也就算了，否则会一直使用原来的图片地址。因此要全量更新图片地址了。更新图片地址，最大影响是图片爬虫。google 的图片爬虫更新比较久，旧的失效图片 url 要经过一段时间才会被去除索引。但因为不是图片站点，这点影响可以忽略。hexo 全量重命名文章图片 hexo 项目的_config.yml 提供了一个选项1post_asset_folder: true 开启之后，每个文章都有资源文件夹。我在里面存放图片。并且在文章中使用插入图片。1&#123;% asset_img img [title] %&#125;全量重命名图片，工作有：设计新图片名字的命名规范 遍历每个文章的资源文件夹，找到图片列表 对于有图片的文章，找到对应的 markdown 文件，然后重命名引用图片 保存修改后的 markdown 文件 重命名图片 命名规范是修改前缀，v1_、v2_，这样匹配最简单，以后也方便再次重命名。是一次性修改图片后，提交到 git，还是每次构建之前先做重命名呢？显然前者最高效。但是会导致原来 md 文件看起来有点别扭。还是交给 travis ci 干活好了。附上我的脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import osimport sys# author by ycwu314# ATTENTION: change these before start the scriptOLD_VERSION_PREFIX = ''NEW_VERSION_PREFIX = 'v1_'def img_rename(path): if not path: return if not os.path.isdir(path): return folders = [path + '/' + f for f in os.listdir(path) if os.path.isdir(path + '/' + f)] for folder in folders: imgs = [x for x in os.listdir(folder)] if len(imgs) == 0: continue md_file = folder + '.md' processed_lines = [] # key: old_img ; value: new_img img_map = &#123;&#125; with open(md_file, 'r', encoding='UTF-8') as r: for line in r.readlines(): new_line = line for img in imgs: if line.find('asset_img') &gt; -1 and line.find(img) &gt; -1: new_img = get_new_img(img) new_line = line.replace(img, new_img) img_map[img] = new_img continue processed_lines.append(new_line) with open(md_file, 'w', encoding='UTF-8') as r: r.writelines(processed_lines) r.flush() for img, new_img in img_map.items(): os.rename(folder + '/' + img, folder + '/' + new_img)def get_new_img(img: str): if img.find(OLD_VERSION_PREFIX) == 0: new_img = img.replace(OLD_VERSION_PREFIX, NEW_VERSION_PREFIX, 1) else: new_img = NEW_VERSION_PREFIX + img return new_img# img_rename(r'C:\workspace\ycwu314.github.io\source\_posts')if __name__ == '__main__': if OLD_VERSION_PREFIX == NEW_VERSION_PREFIX: print('error: old version must not be the same as new version') sys.exit(-1) if len(sys.argv) &lt; 2: print('usage: input _posts folder') sys.exit(-1) input_dir = sys.argv[1] print('folder:', input_dir) img_rename(input_dir)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改 hexo image title alt]]></title>
    <url>%2Fp%2Fhexo-image-title-alt%2F</url>
    <content type="text"><![CDATA[hexo image title 样式 hexo 插入图片的语法1&#123;% asset_img slug [title] %&#125; 展现效果是 title 字段浮到图片上面，导致看不清楚。查看生成的代码 123&lt;a class="fancybox fancybox.image" href="/p/anti-crawler-part-3-image/ 你可能是爬虫文章的受害者.webp" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url" data-fancybox="default" rel="default" title=" 你可能是爬虫文章的受害者 " data-caption=" 你可能是爬虫文章的受害者 "&gt;&lt;img src="/p/anti-crawler-part-3-image/ 你可能是爬虫文章的受害者.webp" data-original="/p/anti-crawler-part-3-image/ 你可能是爬虫文章的受害者.webp" title=" 你可能是爬虫文章的受害者 "&gt; &lt;p class="image-caption"&gt; 你可能是爬虫文章的受害者 &lt;/p&gt;&lt;/a&gt; 发现文字由 image-caption 控制样式。全文查找 hexo、next 的源码，最后定位到这个文件：themes\next\source\css\_common\components\post\post.styl123456789.post-body .image-caption,.post-body .figure .caption &#123; margin: -20px auto 15px; text-align: center; font-size: $font-size-base; color: $grey-dark; font-weight: bold; line-height: 1;&#125;默认是上浮 20px，修改为：1margin: 20px auto 15px;这样文字就不会挡住图片了。hexo image alt 默认值 title 属性是 html 的公共属性。通常用法是元素的说明。就像上面 hexo 会在图片下面显示 title 属性。alt 属性是 img 标签的特有属性，”Alternative text for images”。当不能正常加载图片，浏览器可以在占位符显示这个文字。由于是文字，也可以给 screen reader 朗读，方便视力障碍的人使用。详细可以参考这篇 Wiki：Alt_attribute。 图片的 title 和 alt 都是对 seo 友好的。既然都写了 title 属性，我想让 alt 属性默认也使用 title，不用再写一遍。修改 next 源文件：\themes\next\scripts\tags\full-image.js123456789function fullImage(args) &#123;// more code var alt = args[1] || ''; var title = args[2] || ''; var width = args[3] || '';// 增加默认使用 title 属性 if(alt.length ==0 ) &#123; alt = title; &#125;]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 3：图片保护]]></title>
    <url>%2Fp%2Fanti-crawler-part-3-image%2F</url>
    <content type="text"><![CDATA[图片保护 上次讲到使用字体反爬保护文章文本内容。文章上还有图片资源需要保护。这些爬虫站点爬图片的效果：那个大大的 loading 位置，其实是一张图。显然爬虫没有拿到真实的图片。文章爬虫站点通常会直接读取 img 标签的 src 属性，获取图片 url。有的会转存到自己的存储，避免图片失效。但这会增加存储和带宽成本，因此很多爬虫站点会直接使用原站的图片地址，消耗原站的带宽资源。图片保护的方式有：水印 防盗链 保护图片链接 给图片打水印 水印有 2 种，明水印和盲水印。明水印是肉眼可见的水印。随着抠图工具越来越智能，明水印的保护作用也变低。盲水印，也叫数字水印，是把水印信息写入到图片的高频区域，肉眼不可见，也对原图片不产生明显的质量下降，即使被编辑（加滤镜效果），还能保留。需要用特殊方法提取。明水印是基础的，数字水印以后我再专门展开。这是我加水印的命令，需要安装 imagemagick 工具 1ls *.png *.PNG *.webp *.gif *.jpg -1 | xargs -i convert "&#123;&#125;" (-size 150x -background none -fill grey -pointsize 16 -gravity center label:"ycwu314.github.io" -trim -rotate -30 -bordercolor none -border 30 -write mpr:wm +delete +clone -fill mpr:wm -draw "color 0,0 reset" ) -compose over -composite "&#123;&#125;" 替换 label 文本即可。注意：直接覆盖原图！把图片复制到另一个文件操作。（ps. 大写 PNG 后缀是 windows 自带截图工具的后缀名）这个脚本并不完美，没有考虑背景色，还有图片尺寸的问题。对于长条形状或者背景色很深（intellij 的截图），手动打水印。另外，加上水印图片尺寸变大，要再拖到 tinypng 压缩，或者转换为 webp。cdn 图片防盗链 如果有图片 cdn，那么可以在 cdn 上配置简单的防盗链。常见的图片防盗链方式是检查 referer 字段。假设 xxx 站点爬了我的图片链接，浏览器打开页面，如果不做特殊处理，那么发出图片请求的 referer 是 xxx 站点、而非 cdn 允许的站点 referer，那么 cdn 可以直接拒绝请求。但是我的小站目前没有使用图片 cdn，因为国内 cdn 提供商都要求备案了。另外还要插件支持，不然写作体验不好。hexo-lazyload-image 保护图片链接 保护图片链接是另一个手段。既然爬虫直接保存 img 标签的 src 属性，那么 src 属性就放一个假的地址，然后通过 js 增加一个事件，把真实图片加载回去。上面 loading 截图也正是这个原理。之前使用了 hexo-lazyload-image 插件实现图片懒加载，会把 src 属性替换掉。默认图片是 loading 图片，并且通过 data:image 方式直接写到 html 文件里面。默认图片 base64 编码大概 3KB，每多一个图片，html 文件大小就要增加 3KB，显然不太合适。更好的做法是自定义图片，并且保存在图床。hexo 项目_config.yml 配置 1234lazyload: enable: true onlypost: true loadingImg: &lt; 懒加载图片地址 &gt; 源码不复杂，把真实图片地址保存在图片的 data-original 属性。1234567return htmlContent.replace(/&lt;img(\s*?)src="(.*?)"(.*?)&gt;/gi, function (str, p1, p2) &#123; // might be duplicate if(/data-original/gi.test(str))&#123; return str; &#125; return str.replace(p2, loadingImage + '"data-original="' + p2);&#125;);加载的时候，注册 scroll 事件，新建图片对象并且从 data-origina 加载真正的图片。具体在 simple-lazyload.js。 这是最新的效果：但是 hexo-lazyload-image 会导致 hexo 生成 open graph 标签生成图片信息不正确，图片都是懒加载的图：1&lt;meta property="og:image" content="https://s2.ax1x.com/2019/09/01/npDr38.png"&gt;在 hexo 源码全文查找 og:image，最后定位到这个文件：hexo/lib/plugins/helper/open_graph.js 原来只读取 src 属性 1234$('img').each(function() &#123; const src = $(this).attr('src'); if (src) images.push(src);&#125;); 改为优先使用 data-original 属性 12345678910$('img').each(function() &#123; // hexo-lazyload-image plugin const original = $(this).attr('data-original'); if(original) &#123; images.push(original); &#125; else &#123; const src = $(this).attr('src'); if (src) images.push(src); &#125;&#125;); 最终效果 1&lt;meta property="og:image" content="https://ycwu314.github.io/p/anti-crawler-part-3-image/ 你可能是爬虫文章的受害者.webp"&gt; 由于修改了 hexo 框架，因此在 travis ci 增加更新 open_graph.js 动作123script: - rm -f node_modules/hexo/lib/plugins/helper/open_graph.js - cp .travis/open_graph.js node_modules/hexo/lib/plugins/helper/]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 2：字体反爬]]></title>
    <url>%2Fp%2Fanti-crawler-part-2-font%2F</url>
    <content type="text"><![CDATA[字体反爬介绍 上一篇文章 反爬虫系列之 1：小站被爬 打开来看，显示如下 但是查看源码 1&amp;#xe177;&amp;#xf74d;&amp;#xe1ce; 想做 &amp;#xe23f; 小站 &amp;#xe3da; 在浏览器看内容好好的，但是源码却是一团乱七八糟的转码。这就是反爬中经常使用的一个技巧：字体反爬。字体反爬原理 字体文件是每个字符区域可视化的描述。现在基本是矢量字体，就是字符每个笔画的 x、y 坐标描述。页面采用了 utf8 编码。其中有一个自定义区域：U+E000-F8FF。 把一些字符，比如数字、某些缺少了就会影响阅读理解的汉字（例如高、低、大、小）、一些关键符号（&gt;、&lt;），映射到这个区域，再替换原文的对应字符。可是正常人要怎么看呢？创造一种字体，能够显示这些自定义的字，比如 &amp;#xe177;。 爬虫如果只爬了我的文章，却没有对应字体的话，那么显示就是一团乱七八糟。这样能够保护原有内容。 字体反爬在小说站点上使用很久了。字体反爬的核心思路是对人友好，对爬虫不友好。还有另一种类似思路的方式：css content，以后有机会再介绍。但是字体反爬不是 silver bullet。可能对 seo 造成影响。搜索引擎爬虫原文章之后，会做文章质量打分。字体反爬产生了一堆计算机无法用语义模型去理解的内容，导致评分变低。为了显示这些字体，需要额外加载字体文件，影响页面性能。因此替换的字符是有讲究的，不是随便找。字体反爬在实现上主要 2 种：只改变字符编码。例如，我 的 uft 编码是 0x6211，把它映射到U+E000-F8FF 其中一个槽位。但是 我字体每个笔画 xy 坐标不改变。这样最简单，只需修改映射即可。改变字符编码，同时改变笔画的 xy 坐标。真·创造字体了。为什么要改变字体笔画呢？因为自造字体，通常会基于某个已有字体文件提取目标字符。虽然我不知道 0xe177 是什么，但是根据笔画 xy 坐标做映射，就可以反推过去这个原来是哪个字了。如果给笔画 xy 坐标加上扰动，那么这个反推过程就不能直接通过反向映射实现。此时的反字体反爬，就要使用机器学习方式，使用 LR 或者 GAN 方式识别，得到映射的码表。当然，还有一种方式是用 OCR，但是大规模爬取的话，呵呵。实施字体反爬 对于我的静态站点，字体反爬是比较合适的。能够应付一堆爬虫站点，并且我的实施成本也低。实施字体反爬，要解决的问题：选择哪些字做替换 生成新的字体 hexo 怎样应用自定义字体 怎么在 hexo 生成静态 html 的时候做替换 选择要替换的字符 粗糙的做法是，把经常出现的字符替换掉。但这个做法真的很粗糙。因为排在前面的，肯定有“的”之类的字。去掉的字要对理解产生困难，并且字数越少越好。字数太多，导致字体文件增大。特殊编码的字越多，对原文语义缺失越多，搜索引擎机器打分模型也会有影响。第一版先做粗糙点，快速上线后再优化。具体实现也简单，写一个 python 脚本，读取所有文章 markdown，做个 word count，再根据计数排序就 ok 了。以后还可以针对每个文章自定义字体。使用 fonttools 创建字体 接下来就给替换字符创造字体文件。python 上常用的字体工具是 fonttools。1pip3 install fonttoolswindows 上有个好用的工具，”High-Logic FontCreator”，可以方便查看字体文件，测试的时候很方便，可以申请评估版使用。我打算从 windows 上的某个字体作为基础。系统上有 2 种类型的字体文件：ttc，ttf：ttf 是单个 true type 字体的文件。ttc 包含多个 true type 字体。有些字体大部分字符相同，但是个别字形不同，使用 ttc 方式存储，可以共享相同编码的字符，节省字体文件存储空间。FontCreator 可以从 ttc 文件提取单个 ttf 文件。然后丢给 fonttools 处理。核心的流程是：导入 ttf 字体 生成新的映射 name生成映射空间：U+E000-F8FF保存映射表 具体实现在这篇文章上做了修改：使用 fonttools 自定义字体实现 WebFont 反爬虫 。 得到的字符映射表不能暴露，否则就白干了1zip -P &lt;a_long_random_password&gt; cmap.json 如果是 winrar 操作，记得勾选“zip 传统加密”否则 linux 上解密报错 1unsupported compression method 99 密码可以保存到 travis ci。具体可以参照 travis ci 部署 github 和 gitee 码云hexo 应用自定义字体 修改 themes\next\source\css\_custom\custom.styl123456789@font-face &#123; font-family: myfont; src: url("/fonts/myfont.ttf"); unicode-range: U+E000-F8FF;&#125;p &#123; font-family: myfont;&#125;css 的 font-face 可以描述自定义字体。 一开始在 body 上修改，发现不生效，索性直接改在 p 上。替换 markdown 文件字符 有了自定义字体文件、字符映射表，就剩下怎么在 html 文件替换字符了。替换 markdown 文件比替换生成的 html 要简单。静态站点使用 travis ci 构建部署，因此可以放心操作 markdown 文件，不影响源码分支。把字符映射表加密提交 git密码交给 travis ci 托管 travis ci 构建步骤，在 hexo g 之前，先解压缩字符映射表，再执行 python 脚本，替换所有的 markdown 文件。 剩下的细节点是替换脚本。一个典型的 hexo markdown 是 123456789101112131415161718192021---title: 反爬虫系列之 2：字体反爬 date: 2019-08-31 18:09:59tags: [爬虫]categories: [爬虫]keywords: [字体反爬, fonttools]description:---# xxx&#123;% asset_img a.png %&#125;[]()![]()\`\`\`javapublic static void main(String[] args) &#123;&#125;\`\`\` 如果无脑全局替换就会挖坑了。链接可能打不开，代码块也有问题（因为 html 生成为 &lt;pre&gt; 标签，里面的内容不做转义），甚至 seo 也很糟糕。结论是 meta 信息、标题、代码块、超链接、图片链接等都不能替换。因此脚本要识别当前行是否处于 meta 块、代码块之内，这一行是不是标题，是不是处于图片、超链接等。参考Recommendations for OpenType FontsPython 爬虫六：字体反爬处理（猫眼 + 汽车之家）-2018.10]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫系列之 1：小站被爬]]></title>
    <url>%2Fp%2Fanti-crawler-part-1%2F</url>
    <content type="text"><![CDATA[反爬虫：故事的起源 以前就想做个小站点，记录学习点滴。苦于 996 的生活，一直没动手。今年决定要改变现状，于是就有了这个小站，纯属自娱自乐。期间学习 SEO，也在这个站点上做实验，接入 google analysis、google search console。为了研究外链的影响，我在 csdn、cnblogs 也注册了账号，找了几篇文章贴过去，带上站点链接。然后，就给自己挖坑了。 都是些什么站点呢，打开一个看看 http://www.ishenping.com/ArtInfo/1377053.html 还有外链呢？别傻了，这是我保存在原文里面的。如果把这个链接去掉，就是彻底的文章搬运工了。不过这个站点已经是比较友（ruou）好（zhi）了，更加恶心的爬虫，会把文章内部的超链接都去掉，比如这个 http://www.liuhaihua.cn/archives/596486.html 更大的麻烦在于推酷也来了，爬了好几十篇文章 推酷会保留完整的格式和链接出处，这一点倒是真的。但是这个爬虫网站，反爬能力却战五渣。通过 tuicool 让更多文章搬运站点复制了这个小站。爬虫文章的受害者，不是瞎说的：这个文章是预先提交的，准备填坑。连基本的数据清洗、机器文章质量打分都没有就直接入库 。爬虫文章基本不会更新的（纯属利益问题，不需要做更新），如果文中有误，即使作者修复了，也只会无脑传播出去原始爬到的版本。 于是就有了这个反爬虫系列文章。爬虫站点的经济利益 爬虫站点是典型的流量经济。它们本身不做内容的产出，只做内容的搬运工。文章只是个占位符，至于内容好坏和是否更新，they dont care。有的甚至连图片 cdn 也省了，直接使用原站的图片链接，消耗原站的流量。初级的爬虫限流不做好，容易对原站点造成流量压力，影响正常访问。由搜索引擎引入流量，然后通过各种广告联盟获利。在微信发达的今天，这种玩法也搬到了公众号，不过公众号平台对原创内容的保护，情况要比互联网要好一些。反爬虫思路 脱离业务背景的解决方案就是耍流氓。先说背景：这站点是个托管到 github 的静态页面，连域名都是 github 提供的二级域名。。。之前考虑过买个域名，但是备案流程长，当时觉得用 github 托管也没什么问题，于是就愉快的使用 hexo 撸了几十篇文章，直到被爬虫发现。没有服务器端能力，做反爬真心蛋疼。但，人生在于折腾。首先要承认，随着爬和反爬的对抗，识别爬虫越来越艰难。但是技术实施有成本问题，防住大部分低级爬虫就是了。换位思考，爬虫站点是怎么工作的：发现新链接 静态爬取：直接下载 html 文章，再做静态解析。适用于大部分静态站点，例如 Hexo、Jekyll、Hugo。动态爬取：使用 headless 浏览器，模拟打开站点，解析完整的 dom 树，提取文章内容。反爬虫的方向：行为检测 客户端特征检查 页面内容做混淆 动态加载内容 爬虫陷阱，返回过期内容 针对我的站点，长远规划：购买域名，备案，迁移站点 使用 nginx 加流量分析、指纹分析屏蔽爬虫请求 图片接入 cdn，简单的防盗链 短期任务：联系删除已有侵权文章 修改文章模板，增加版权和来源声明，方便维权 尽量阻止爬虫获取新文章 （静态反爬）发布过时内容（静态反爬）字体反爬（动态反爬）增加检查客户端的 js，识别爬虫客户端 已经暴露的图片链接换名 图片加水印 这个系列文章主要分享我在做静态网站反爬的经历。联系爬虫网站删除 内容都进入别人的数据库，还能怎样呢？直到看到这篇文章，感谢作者分享维权经历： 博客园原创文章防剽窃、反爬虫指南（持续更新…..）于是我也照着做 推酷官网邮件根本不鸟人。dig 一下发现站点是部署在 aliyun，1234567891011121314151617181920# dig tuicool.com; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.8-Ubuntu &lt;&lt;&gt;&gt; tuicool.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 59465;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 65494;; QUESTION SECTION:;tuicool.com. IN A;; ANSWER SECTION:tuicool.com. 10 IN A ** 这里是 ip 地址 **;; Query time: 33 msec;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: Sat Aug 31 16:03:17 CST 2019;; MSG SIZE rcvd: 56拿到 ip 地址之后，再用 whois 查询。看样子要通过 阿里云举报中心 投诉了。这些爬虫站点的一大流量来源是搜索引擎。首先去全球最大的百家号搜索引擎举报侵权：如果你真的点击进去 深感维权艰难。虽然由于不可描述的原因，谷歌不能正常访问。但是版权制度在国外比较完善。我向 google 提起侵权删除页面请求： 举报涉嫌侵犯版权的行为：Google 网页搜索 一旦确认通过，在 google 搜索页面底部可以看到（LumenDatabase 是寒蝉效应的数据库）为了回应用户根据美国数字千年版权法案向我们提交的多项投诉，我们已从此页上移除了 2 个结果。如果需要，您可以访问 LumenDatabase.org，查看导致结果遭到移除的 DMCA 投诉内容： 投诉, 投诉.至少还有 google 会认真对待我的诉求❤。阻止爬虫获取新文章通知 我这个不知名站点，为什么文章更新不久之后，就会被爬虫站点发现并且爬取呢？从经济的角度就直到，它们不会一直盯着各个站点，看有无更新，太消耗资源了。它们也不需要这样操作。因为有太多简单的方式可以获取。知名站点，例如 cnblogs、csdn，就爬热门的、新发的、推荐的。普通静态站点，一般使用模板引擎构建。爬取 sitemap、归档页面就可以获取获取全量的站点文章，然后根据模板引擎的不同，获取内容数据。例如，hexo 的生成的静态站点，默认会在 meta 标签增加一行 1&lt;meta name="generator" content="Hexo 3.9.0"&gt; 就如 nginx 配置，通常会隐藏版本号提高安全性一样，这一行可以屏蔽掉。文件位置是 themes\next\layout_layout.swig 。模板引擎还有其他特征暴露身份，此处不展开了。不过我观察发现，有的爬虫站点是从某一个时间开始爬文章，并非全量文章都爬取。很有可能是使用 rss feed 订阅功能，直接关注链接获取增量更新。有了上面的分享，就简单了。更改 sitemapsitemap 告诉搜索引擎可以被爬取的文章地址。能够批量提交 url。对于垃圾爬虫文章站点来说，sitemap 就相当于站点脱裤子了。sitemap 是一个约定，默认地址是 /sitemap 或者 /sitemap.xml 绝大数的站点都会遵循这样的约定。当然这个路径是可以自定义的。关闭归档页面 遍历归档页面也可以简单获取所有内容。至少目前来看，归档页面对我作用不大，直接在 hexo 的项目_config.yml 关闭入口。同时修改目录路径（也可以在部署脚本直接 rm -f 删除整个归档目录）。动态修改归档页面路径 如果想保留归档页面，可以考虑每次部署的时候动态修改归档页面路径。这样旧的爬虫拿不到更新。修改 next 主题的_config.yml1234menu: home: / || home about: /about/ || user archives:结合 travis ci，可以方便实现每次构建动态更新 tags 目录名字。不过产生另一个问题：搜索引擎会索引大量的无效链接。本身归档页面主要是给人看，而不是机器看的。页面上也只是其他文章的链接，因此不需要被索引。虽然是动态生成归档路径，但是如果是一个 pattern 命名，可以直接写到 robots.txt 告诉爬虫不要抓取。另一种方法是在归档页面使用 noindex 标记。原理参考 使用“noindex”阻止搜索引擎将您的网页编入索引 。1&lt;meta name="robots" content="noindex"&gt; 修改 themes\next\layout_layout.swig：1234&lt;head&gt; &#123;% if noindex %&#125; &lt;meta name="robots" content="noindex"&gt; &#123;% endif %&#125;修改 themes\next\layout\archive.swig，在开头增加 1&#123;% set noindex = true %&#125; 还可以应用到 tags、categories。关闭 rss流量也没有，rss 也是多余的。直接在 hexo 的 next 主题关闭 1rss: false 效果 发了几个测试文章，观察发现爬虫站点没有爬取。下一步 遍历站点首页，也可以获取全部文章链接。下次在 hexo 模板上做个优化，不把链接写在超链接的 href 属性，改成 JavaScript 触发，增加静态解析的难度。对于 google 爬虫来说，没有影响。爬虫投毒：发布过时内容 如果觉得关闭 sitemap、rss 不方便，可以尝试发布无效内容，也叫爬虫投毒。在工作中，爬虫投毒更多用于竞争对手的防爬。先新建一个文章，空的内容，提交，等垃圾爬虫收录之后，再编辑原来的文章，写入真正的内容。这个过程，需要几天时间。前面提到，这类文章搬运工只会收录，不管更新。因此保护了站点内容。但是写作体验就不好了。文章正文增加版权声明 hexo next 自带版权插件。但是爬虫根本不会抓取。要把版权内容写入到文章 body 才有用。 打开 themes\next\layout\_macro\post.swig，搜索post-body，在这个 div 里面然后新增一个&lt;p&gt; 来存放版权声明，写明作者、地址、授权问题。1本文作者 &#123;&#123; config.author &#125;&#125;，未经允许请勿转载 &lt;a href="&#123;&#123; post.permalink &#125;&#125;"&gt;&#123;&#123; post.title &#125;&#125;&lt;/a&gt; : &#123;&#123; post.permalink &#125;&#125;爬虫识别这是 hexo 构建的站点，爬取的时候，就无脑解析 &lt;div class=&quot;post-body&quot;&gt;。自然把这个版权声明也爬取进去了。 总结 下次聊聊字体反爬。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java LongAdder 原理]]></title>
    <url>%2Fp%2Fjava-longadder%2F</url>
    <content type="text"><![CDATA[有了 Striped64 的基础，LongAdder 就见多了。相关文章：Java Striped64 原理 LongAdder add 源码分析 因为继承自 Striped64，因此 LongAdder 使用了 123transient volatile Cell[] cells;transient volatile long base;transient volatile int cellsBusy; 前情回顾：Striped64 有 3 个核心变量，都是 volatile 修饰：base：计数字段 cells：是一个 Cell 数组。使用 lazy init 方式。第一次访问时候初始化。cellsBusy：用于自旋锁，表明 cells 数组正在初始化或者扩容。0 表示无 cells 竞争，1 表示有线程在操作 cells。LongAdder 核心是 add()12345678910public void add(long x) &#123; Cell[] as; long b, v; int m; Cell a; if ((as = cells) != null || !casBase(b = base, b + x)) &#123; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125;&#125; 第一个 if：如果 cells 不为空，则直接进入内层判断。（Striped64 优先使用分段锁 cells 更新计数）否则，cells 为空，此时正在初始化，则尝试 CAS 更新 base 字段。如果成功则退出，否则进入内层判断。第一个 if 虽然简单，但是值得细细品味。然后乐观假设没有竞争 uncontended=true，进入第二个 if。第二个 if：如果 cells 为空，或者未初始化完成（(m = as.length - 1) &lt; 0），直接进入 Striped64.longAccumulate()否则，cells 已经初始化，随机找一个 cells 的槽位（），且未被使用（(a = as[getProbe() &amp; m]) == null），也是槽位没有竞争，直接进入 Striped64.longAccumulate()如果上一步随机探测的 cell 已经存在，则尝试在这个槽位 CAS 更新计数（a.cas(v = a.value, v + x)）。如果更新失败，则表明这个槽位有竞争，同时更新 uncontended=true，进入 Striped64.longAccumulate()具体的 Striped64.longAccumulate()就不在重复了。LongAdder sum 源码分析 显然计数存在于 base 和 cells 数组。真实的计数 =base + sum (cells)1234567891011public long sum() &#123; Cell[] as = cells; Cell a; long sum = base; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125;因为这个方法没有加锁，不是并发安全的。javadoc 上就有温馨提示：The returned value is NOT anatomic snapshotLongAdder reset 源码分析 resetfangf 重置 base 和 cells。可以复用 LongAdder 对象。同样，这个方法也不是线程安全的。个人感觉用途不大。12345678910public void reset() &#123; Cell[] as = cells; Cell a; base = 0L; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) a.value = 0L; &#125; &#125;&#125;LongAdder SerializationProxy 从类图可以看到，LongAdder 实现了 Serializable 接口。但是 LongAdder 本身没有数据变量，所有属性来自父类 Striped64。123transient volatile Cell[] cells;transient volatile long base;transient volatile int cellsBusy;Striped64 中这几个变量都是 non-public。不应该暴露这几个变量。同时，真正想要序列化的是真实的计数 (当然只是 snapshot)。因此 LongAdder 设计了内部类 SerializationProxy，用于序列化和反序列化。1234567891011121314151617private static class SerializationProxy implements Serializable &#123; private static final long serialVersionUID = 7249069246863182397L; /** * The current value returned by sum(). * @serial */ private final long value; SerializationProxy(LongAdder a) &#123; value = a.sum(); &#125; private Object readResolve() &#123; LongAdder a = new LongAdder(); a.base = value; return a; &#125; &#125; 直接 sum 计算当前计数。Serializable 接口的类支持使用 readObject 提供自定义的反序列化方式。这里直接返回异常，强调要使用 SerializationProxy 操作。1234private void readObject(java.io.ObjectInputStream s) throws java.io.InvalidObjectException &#123; throw new java.io.InvalidObjectException("Proxy required");&#125;LongAdder vs AtomicLongAtomicLong 是乐观锁设计，CAS 更新计数 value。在多读少写的情况下性能好。但是写竞争激烈的情况，乐观锁的前提就不存在了，导致大量更新线程自旋等待。LongAdder 继承自 Striped64，使用了分段锁的设计、缓冲行填充优化，并发度更大，尽可能分散竞争。写并发不高的情况下，AtomicLong 已经够用了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Contended 避免伪共享]]></title>
    <url>%2Fp%2Fjava-contended-avoid-false-sharing%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Striped64 原理]]></title>
    <url>%2Fp%2Fjava-striped64%2F</url>
    <content type="text"><![CDATA[Striped64 是 Java8 新增的、64bit 高性能累加器。Striped64 是 LongAdder、LongAccumulator 等类的基类。ConcurrentHashMap 的计数就使用了 Striped64 和 LongAdder 的设计。Striped64 初探 Striped64 是个抽象类，为子类提供工具方法支持。Striped64 有 3 个核心变量，都是 volatile 修饰：base：计数字段cells：是一个 Cell 数组。使用 lazy init 方式。第一次访问时候初始化。cellsBusy：用于自旋锁，表明 cells 数组正在初始化或者扩容。0 表示无 cells 竞争，1 表示有线程在操作 cells。1234567891011121314151617abstract class Striped64 extends Number &#123; /** * Table of cells. When non-null, size is a power of 2. */ transient volatile Cell[] cells; /** * Base value, used mainly when there is no contention, but also as * a fallback during table initialization races. Updated via CAS. */ transient volatile long base; /** * Spinlock (locked via CAS) used when resizing and/or creating Cells. */ transient volatile int cellsBusy;&#125;Striped64 设计思路： 优先把计数更新到 cells 数组。每次遇到竞争，则扩容为 2 倍，直到等于 cpu 核数，或者比 cpu 核数大的、最小的 2 的整数幂。使用 cellsBusy 自旋锁。最后才尝试 CAS 更新 baseStriped64 Cell12@sun.misc.Contended static final class Cell &#123; volatile long value;Cell 核心是一个 long 容器。注意被 Contended 注解修饰。在 Java 8 中，提供了 @sun.misc.Contended 注解来避免缓存伪共享。具体可以看看：Java Contended 避免伪共享 cells 数组扩容是有限制的。原因是使用随机探针方式试探 cells 的冲突，只有 CAS 失败才知道。更大的 cells 会影响收敛时间。cells 数组不考虑回收问题，因为在常时间运行、高并发环境，这些 cells 最终会有机会使用。1234567891011121314151617181920/* * The table size is capped because, when there are more threads * than CPUs, supposing that each thread were bound to a CPU, * there would exist a perfect hash function mapping threads to * slots that eliminates collisions. When we reach capacity, we * search for this mapping by randomly varying the hash codes of * colliding threads. Because search is random, and collisions * only become known via CAS failures, convergence can be slow, * and because threads are typically not bound to CPUS forever, * may not occur at all. However, despite these limitations, * observed contention rates are typically low in these cases. * * It is possible for a Cell to become unused when threads that * once hashed to it terminate, as well as in the case where * doubling the table causes no thread to hash to it under * expanded mask. We do not try to detect or remove such cells, * under the assumption that for long-running instances, observed * contention levels will recur, so the cells will eventually be * needed again; and for short-lived ones, it does not matter. */Striped64 PROBE / threadLocalRandomProbe 发生竞争时候，使用 cells 数组存储计数。那么有个问题，选择 cells 的哪个位置存储呢？答案是 Thread 类的 threadLocalRandomProbe 变量。Thread.java123/** Probe hash value; nonzero if threadLocalRandomSeed initialized */@sun.misc.Contended("tlr")int threadLocalRandomProbe;Striped64 使用 unsafe 操作 Thread 类的 threadLocalRandomProbe 变量 123456789private static final long PROBE; static &#123; try &#123; // more code Class&lt;?&gt; tk = Thread.class; PROBE = UNSAFE.objectFieldOffset (tk.getDeclaredField("threadLocalRandomProbe")); &#125; 每次计算 PROBE 的位置，都使用 ThreadLocalRandom 的伪随机算法。1234567891011121314151617/** * Pseudo-randomly advances and records the given probe value for the * given thread. * Duplicated from ThreadLocalRandom because of packaging restrictions. */static final int advanceProbe(int probe) &#123; probe ^= probe &lt;&lt; 13; // xorshift probe ^= probe &gt;&gt;&gt; 17; probe ^= probe &lt;&lt; 5; UNSAFE.putInt(Thread.currentThread(), PROBE, probe); return probe;&#125;// 返回当前线程的 probe 值 static final int getProbe() &#123; return UNSAFE.getInt(Thread.currentThread(), PROBE);&#125;Striped64 longAccumulate 详解 有了前面的准备分析，可以深入了解 Striped64 实现。Striped64 提供了 longAccumulate 和 doubleAccumulate 两个工具方法（因为 long 和 double 是 64bit）。这里研究 longAccumulate。12final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended)其中：x：要更新的值。fn：更新函数。null 则直接增加 x。为 LongAdder 提供支持。wasUncontended：无冲突标记。如果调用之前 CAS 操作失败，则为 false。如果当前线程的 probe 为 0，则未初始化，初始化之，并且更新 wasUncontended。1234567int h;if ((h = getProbe()) == 0) &#123; ThreadLocalRandom.current(); // force initialization h = getProbe(); // 没有冲突 wasUncontended = true;&#125;ThreadLocalRandom.current 初始化当前线程的探针：12345public static ThreadLocalRandom current() &#123; if (UNSAFE.getInt(Thread.currentThread(), PROBE) == 0) localInit(); return instance;&#125; 接下来是重头戏，在循环中更新计数。if-else 分支很多，直接在代码上做解析：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788for (;;) &#123; Cell[] as; Cell a; int n; long v; // 如果有 cells 数组可用 if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; // 当前线程哈希到 cells 数组，且未被使用，则尝试新建一个 cell（lazy init） if ((a = as[(n - 1) &amp; h]) == null) &#123; // 尝试乐观锁方式更新 if (cellsBusy == 0) &#123; // Try to attach new Cell Cell r = new Cell(x); // Optimistically create // 再次检查 cellsBusy，尝试 cas 更新 if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; boolean created = false; try &#123; // Recheck under lock Cell[] rs; int m, j; if ((rs = cells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; // 释放自旋锁 cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; // 已经有 cell 存在，如果之前是没有竞争，就要更新为有竞争，且在下一次循环继续 else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash // 既然有 cell，尝试在该 cell cas 更新计数，如果成功则跳出循环 else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // CAS 更新该 cell 失败了，有竞争 // 如果 cells 数组长度到达最大值，或者已经改变，则进入下一轮循环的初始状态是没有冲突（是下一轮！） else if (n &gt;= NCPU || cells != as) collide = false; // At max size or stale // 如果之前检查没有冲突，则在进入下一轮循环的初始状态是有冲突 else if (!collide) collide = true; // 到达这个分支的条件：cells 有竞争，且容量未到最大值 // 尝试获取乐观锁，并对 cells 扩容 else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; try &#123; if (cells == as) &#123; // Expand table unless stale Cell[] rs = new Cell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; cells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; // 进入下一轮循环之前，更新探针 h = advanceProbe(h); &#125; // cells 数组未初始化。进行 lazy init else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; boolean init = false; try &#123; // Initialize table if (cells == as) &#123; Cell[] rs = new Cell[2]; rs[h &amp; 1] = new Cell(x); cells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; // 进入此分支的条件： // 1. cells 数组为 null // 2. 无法初始化 cells 数组 // 则尝试 CAS 更新 base 变量 else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base&#125;Striped64 总结Striped64 的核心思路是，尽量使用分段锁，提高并发度。只有在 fallback 情况才更新 base 计数器，并且是 CAS 方式。Cell 数组使用 @Contended 注解，避免缓存伪共享。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Thread join 详解]]></title>
    <url>%2Fp%2Fjava-thread-join%2F</url>
    <content type="text"><![CDATA[Thread join 例子 需求：一个线程去干点活，等工作结束后，通知主线程。1234567891011121314public static void main(String[] args) &#123; Thread t = new Thread(() -&gt; &#123; System.out.println("I am doing my job!"); try &#123; TimeUnit.SECONDS.sleep(6); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("job is done"); &#125;); t.setName("my-job-thread"); t.start(); System.out.println("all is done");&#125;输出 123all is doneI am doing my job!job is done 显然，不符合要求。其实只要增加一行代码即可：12345678910111213141516public static void main(String[] args) throws InterruptedException &#123; Thread t = new Thread(() -&gt; &#123; System.out.println("I am doing my job!"); try &#123; TimeUnit.SECONDS.sleep(6); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("job is done"); &#125;); t.setName("my-job-thread"); t.start(); // 划重点 t.join(); System.out.println("all is done");&#125; 输出 123I am doing my job!job is doneall is doneThread join 原理 加上 join()方法后，main 线程等待线程 t 结束后才继续执行。如果 jstack 做 thread dump：main 线程被挂起，等待唤醒。Thread.join()的说明很简单明了：等待线程死亡。123456/*** Waits for this thread to die.*/public final void join() throws InterruptedException &#123; join(0);&#125;再看看实际调用的 join(long millis)：1234567891011121314151617181920212223242526272829303132/*** This implementation uses a loop of this.wait calls conditioned on this.isAlive. * As a thread terminates the this.notifyAll method is invoked. * It is recommended that applications not use wait, notify, or notifyAll on Thread instances.*/public final synchronized void join(long millis)throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125;&#125;/** Tests if this thread is alive. A thread is alive if it has been started and has not yet died.*/public final native boolean isAlive();join(long millis)被 synchronized 修饰。实例方法级别的 synchronized，表明要获取这个实例的监视器锁。因为里面的方法使用了 wait()，因此要先获取监视器锁。isAlive()返回线程是否死亡。只要线程还活着，就 wait(0)一直等待。通常 wait 要通过 notify 来唤醒，但是 t 线程死亡后，自动唤醒了 main 线程，why？ps. 关于 synchronized、wait、notify，可以参见：java synchronized 实现原理 wait、notify 和生产者消费者模式 明明是 main 线程调用 t 线程的 join 方法，好像要挂起的应该是 t 线程，但实际挂起的是 main 线程，有点反直觉？其实不然。看看 Object.wait()的 javadocCauses the current thread to wait until another thread invokes the notify() method or the notifyAll() method for this object.重点是 current thread。 sychronized 只是为了获取监视器锁而已，挂起的是当前线程(上面例子，就是 main 线程)。 最大的疑问是，t 线程死亡后，为什么能够唤醒 main 线程？Java 代码明明没有使用 notify 啊。java 代码没有 notify，那么可能就是底层 jvm 代码做了这个操作。最终找到 openjdk8 thread.cpp：123456789101112131415161718192021222324252627void JavaThread::exit(bool destroy_vm, ExitType exit_type) &#123;// more code // Notify waiters on thread object. This has to be done after exit() is called // on the thread (if the thread is the last thread in a daemon ThreadGroup the // group should have the destroyed bit set before waiters are notified). ensure_join(this);// more code&#125;static void ensure_join(JavaThread* thread) &#123; // We do not need to grap the Threads_lock, since we are operating on ourself. Handle threadObj(thread, thread-&gt;threadObj()); assert(threadObj.not_null(), "java thread object must exist"); ObjectLocker lock(threadObj, thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception(); // Thread is exiting. So set thread_status field in java.lang.Thread class to TERMINATED. java_lang_Thread::set_thread_status(threadObj(), java_lang_Thread::TERMINATED); // Clear the native thread instance - this makes isAlive return false and allows the join() // to complete once we've done the notify_all below java_lang_Thread::set_thread(threadObj(), NULL); lock.notify_all(thread); // Ignore pending exception (ThreadDeath), since we are exiting anyway thread-&gt;clear_pending_exception();&#125;线程 exit 的时候，会调用 ensure_join()通知这个线程对象等待队列的 waiter。ensure_join 最终调用 notify_all 唤醒 waiter。因此，t 线程结束，main 线程被唤醒。Thread join 一个细节 join 上的注释写明：It is recommended that applications not use wait, notify, or notifyAll on Thread instances. 因为使用了 wait 机制，因此当前线程最好使用 wait，notfiy 等方法。Thread.join 使用了 while 检查，避免虚假唤醒 123while (isAlive()) &#123; wait(0);&#125;Thread join 小结Thread.join() 挂起当前线程。目标线程结束后再唤醒当前线程。可以用于简单的线程等待。最好使用 FutureTask.get()之类的方法代替。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四个线程循环输出 ABCD]]></title>
    <url>%2Fp%2Fexam-java-4-threads-write-abcd-to-4-files%2F</url>
    <content type="text"><![CDATA[题目 四个线程 A、B、C、D 向四个文件写入数据。要求 A 线程只写入 A，B 线程只写入 B……最终达到的效果：A.txt 内容为： A B C D A B C D……B.txt 内容为： B C D A B C D A……C.txt 内容为： C D A B C D A B……D:txt 内容为： D A B C D A B C……分析 每个线程的职责：A 线程只写 A，B 线程只写 B，etc。使用一个锁，保护共享资源即可。怎样使 4 个线程按顺序写入？很简单，一开始 4 个线程都处于阻塞状态，然后先唤醒 A。这样 A 结束后，自动唤醒 B，etc。本质是 线程间通信 。可以使用 Object 的 wait、notfiy，也可以使用 Condition 的 await、signal。具体可以看看：Condition 原理，以及实现生产者消费者模式wait、notify 和生产者消费者模式 怎样控制结束？假设执行 X 次后自动结束。增加一个变量 X，到达指定次数后结束。但是有个细节点，后面的线程阻塞了，依赖前面的线程唤醒，才能结束。因此前一个线程结束的时候，要唤醒下一个线程。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109public class PrintABCDBySequence &#123; // 执行次数 static final int RUN_COUNT = 10; static volatile boolean stop = false; static volatile boolean hasInit = false; static AtomicInteger round = new AtomicInteger(); static int size = 4; // 模拟向 size 个文件输出 static List&lt;String&gt;[] output = new ArrayList[size]; // 等待 size 个线程结束 static CountDownLatch latch = new CountDownLatch(size); static ReentrantLock lock = new ReentrantLock(); static Condition waitForA = lock.newCondition(); static Condition waitForB = lock.newCondition(); static Condition waitForC = lock.newCondition(); static Condition waitForD = lock.newCondition(); public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; size; i++) &#123; output[i] = new ArrayList&lt;&gt;(); &#125; new Thread(new PrintTask("A", waitForD, waitForA)).start(); new Thread(new PrintTask("B", waitForA, waitForB)).start(); new Thread(new PrintTask("C", waitForB, waitForC)).start(); new Thread(new PrintTask("D", waitForC, waitForD)).start(); // 等待 4 个线程启动 Thread.sleep(500); hasInit = true; lock.lock(); // 先唤醒 A try &#123; waitForD.signal(); &#125; finally &#123; lock.unlock(); &#125; latch.await(); for (List&lt;String&gt; a : output) &#123; System.out.println(a); &#125; &#125; static class PrintTask implements Runnable &#123; private String ch; private Condition prevCondition; private Condition selfCondition; public PrintTask(String ch, Condition prevCondition, Condition selfCondition) &#123; this.ch = ch; this.prevCondition = prevCondition; this.selfCondition = selfCondition; &#125; @Override public void run() &#123; while (true) &#123; lock.lock(); // 初始化，线程进入阻塞等待 if (!hasInit) &#123; try &#123; prevCondition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; try &#123; // attention: stop 的时候，要先唤醒后面的线程，否则后面的线程不能结束 if (stop) &#123; selfCondition.signal(); break; &#125; int r = round.getAndIncrement(); for (int i = 0; i &lt; output.length; i++) &#123; if (i &lt;= r) &#123; output[i].add(ch); &#125; else &#123; break; &#125; &#125; if (r &gt;= RUN_COUNT) &#123; stop = true; &#125; selfCondition.signal(); if (!stop) &#123; prevCondition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; latch.countDown(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 HashMap 详解]]></title>
    <url>%2Fp%2Fjava-hashmap%2F</url>
    <content type="text"><![CDATA[HashMap 是一个常用的 key-value 工具。这里以 java8 版本介绍。Java8 HashMap 简介 Java8 HashMap 采用数组（Java8 是Node&lt;K,V&gt;[] table，Java8 之前是Entry[] table）+ 链表 + 红黑树的方式实现。 对于 key 做 hash，得到所在的数组位置。hash 可能发生冲突，即多个 key 都 hash 到同一个数组位置（桶，bucket）。HashMap 采用拉链法处理冲突，也就是数组位置存放链表，所有冲突的 key 都在这个链表。通过遍历可以找到目标 key 和 value。但是有些情况下，可能大量的 key 都冲突到同一个 bucket，导致链表很长。这样遍历查找 key 耗时长，时间复杂度为 O(N)。为了解决这个问题，如果一个 bucket 的链表长度超过一定长度，则转换为红黑树。HashMap NodeHashMap 包含了内部类 Node 作为基本节点结构。123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125;上面提到链表长度达到阈值（TREEIFY_THRESHOLD），会触发转为红黑树。因此 HashMap 也定义了红黑树 Node1234567static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red;&#125;两者 Node 关系见下图 HashMap loadfactortable 数组越满，哈希发生冲突的概率就越高。一旦发生冲突，就要遍历链表或者从红黑树查找 key。提高 hashmap 性能的方式是，不要写满、保留适当的空间。loadfactor 描述 hashmap 的负载情况，默认是 0.75f。可以在构造函数指定。1234567static final float DEFAULT_LOAD_FACTOR = 0.75f;public HashMap(int initialCapacity, float loadFactor)&#123; // more code this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125;HashMap 初始化大小 上面的构造函数 HashMap(int initialCapacity, float loadFactor) 可以传入 initialCapacity 作为初始化容量。但这不是真正的初始化容量，HashMap 会做调整，调整为比 initialCapacity 大的 2 整数幂（见 tableSizeFor 函数）。比如 initialCapacity=14，则 threshold 调整为 16。如果不指定，默认为初始化大小为 16（DEFAULT_INITIAL_CAPACITY）。123456789101112131415161718192021/** * The next size value at which to resize (capacity * load factor). * * @serial */int threshold;static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;为什么要做初始化大小调整呢？前面提到 loadfactor，保持未用空间比例。如果发生大小调整，2 的整数幂做重新哈希会相当方便。ps. 阿里巴巴 Java 开发手册其中一条规定是，使用 HashMap，要指定初始化容量大小。顺带提下，HashMap 的最大容量限制是 1static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;HashMap put() 向 hashmap 添加 key-value，先计算这个 key 的 hash 值，最终调用 putVal 方法。123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // lazy init if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // table[]对应槽位为 null，直接把 key 放在这个位置 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果该槽位第一个元素是目标 key，返回该节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果该槽位第一个元素是 TreeNode，则放到红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 否则，是链表结构，则遍历 for (int binCount = 0; ; ++binCount) &#123; // 没有找到已有元素，则插入到链表尾部 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 链表增加新元素后，可能到达 treeify 的下限 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 在链表中找到已有元素，返回该节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 找到已有节点，更新 value if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 新增元素，检查触发扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125;putTreeVal、treeifyBin 是红黑树相关操作，这里不做展开。HashMap 源码还有 afterNodeXXX 方法，是留给 LinkedHashMap 使用，同样先不做展开。1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125;HashMap get()get() 底层使用 getNode1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;12345678910111213141516171819202122final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 先检查第一个槽位的元素 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 访问红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; // 遍历链表 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125;HashMap removeremove 从 HashMap 中查找并且删除元素。1234567891011121314151617181920212223242526272829303132333435363738394041424344final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 先检查槽位的第一个元素 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 查找红黑树 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; // 链表查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // 找到节点，从红黑树或者链表删除 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125;HashMap hash 深入 哈希函数的 2 个指标：计算速度 冲突量 常见的 hash 方式是取模，但是 HashMap 实现并没有使用，而是设计了一个 hash 函数。先看一眼 Java8 的 hash1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;这个 hash 函数对比简单的取模（x % y），好处在于：位运算比取模要快。计算 hash 的目的是为了找到 table[]中的槽位：1p = tab[index = (n - 1) &amp; hash]n 是 table 数组的长度。之前讲到，扩容的时候，table[]的大小是 2 的整数幂。其中包含一个数学特性 X % 2^n = X &amp; (2^n – 1)2^n 表示 2 的 n 次方，也就是说，一个数对 2^n 取模 == 一个数和(2^n – 1) 做按位与运算 。假设 n 为 3，则 2^3 = 8，表示成 2 进制就是 1000。2^3 = 7 ，即 0111。此时 X &amp; (2^3 – 1) 就相当于取 X 的 2 进制的最后三位数。从 2 进制角度来看，X / 8 相当于 X &gt;&gt; 3，即把 X 右移 3 位，此时得到了 X / 8 的商，而被移掉的部分 (后三位)，则是 X % 8，也就是余数。 看一个例子就很直观 精心设计数组长度为 2 的整数幂，用位运算代替取模，解决了哈希计算的速度问题。哈希函数的另一个问题是冲突率。看下面的例子 对于取模运算，冲突常见的是高位不同、低位相同。解决冲突的思路是，进行扰动操作，把高位特征加入到 hash 当中。Java8 的实现很简单，把 h 和 h 的高 16 位做异或（XOR）运算。1(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)Java8 之前的 hash 更加复杂 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125;HashMap resize 扩容机制 有了上面 hash 设计的基础，就方便理解 HashMap 扩容机制。Java8 的 resize()负责扩容（double by 2）或者初始化 table[]。扩容涉及的操作：计算新的 threshold、newCap初始化新的 newTab[]把原来 table 的元素，重新哈希到 newTab因为 table[]中的结构，可能是链表，也可能是红黑树，需要分开处理。这里只分析链表情况，比较难理解的是重新 hash 过程。12345678910111213141516171819202122232425262728293031323334353637383940414243444546for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 拆分红黑树 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表情况 // 拆分为 lo 链表和 hi 链表 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 划重点： (e.hash &amp; oldCap) == 0 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 划重点 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 划重点 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125;&#125; 分为 lo 链表、hi 链表 12Node&lt;K,V&gt; loHead = null, loTail = null;Node&lt;K,V&gt; hiHead = null, hiTail = null; 然后遍历这个槽位，把 (e.hash &amp; oldCap) == 0 放入到 lo 链表，其他放到 hi 链表。如果 lo 链表不为 null，则把 lo 链表放到 newTab[j]。如果 hi 链表不为 null，则把 hi 链表放到 newTab[j + oldCap]。为什么呢？逐步分析：hashmap 的大小是 2 的整数幂。假设 oldCap=2^n，newCap 是 oldCap 的 2 倍，则 newCap=2^(n+1)。又因为 X % 2^n = X &amp; (2^n – 1)，则 X 对 newCap 求模，等于计算 X 的 2 进制的最后 n+1 位数。为了计算新槽位的位置，就要 e.hash &amp; newCap 。不管计算 e.hash &amp; oldCap 还是 e.hash &amp; newCap，都可以转化为计算 e.hash 的二进制的低 n 位或者 n+1 位（newCap 比 oldCap 多取一位）。假设 e.hash &amp; oldCap = xyz，那么 e.hash &amp; newCap 的结果，只能是 0xyz1xyz 这 2 个结果相差一个 oldCap（这里举例用 oldCap=8）。也就是说，计算新槽位，可以使用 e.hash &amp; oldCap 代替。因此 e.hash &amp; oldCap = 0，则在 newTab[j]e.hash &amp; oldCap ≠ 0，则在 newTab[j+oldCap] 比较 keyHashMap 比较 key 是否相同，实现是 1p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))) 虽然简单，但是值得品味。hash 不同，肯定不是同一个 key，直接返回 false否则，比较 key 的引用（==），引用相同，肯定是同一个 key，返回 true使用 key 的 equals 方法比较 可以直接使用 key 的 equals 方法吗？可以的，但是 equals 方法通常比较重型，比较 hash、引用，则更加轻便（fast path）。HashMap modCount 和 fail-fast修改（get、put、remove）方法，都对 modCount 字段进行自增操作。先看介绍 12345678/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */transient int modCount;modCount 是为了支持 fail-fast 机制，能在第一时间感知集合的内部结构被修改。在 Java 中，fail-fast 机制通常使用比较操作前后的 modCount 计数和抛出 ConcurrentModificationException 实现。 以 HashMap.EntrySet.forEach()为例：12345678910111213141516public final void forEach(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; // 保存旧的 modCount int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e); &#125; // 当前 modCount 和旧的 modCount 不相同，则发生了并发修改。 if (modCount != mc) throw new ConcurrentModificationException(); &#125;&#125;并发问题 HashMap 不是线程安全容器！HashMap 不是线程安全容器！HashMap 不是线程安全容器！ 有并发需求，使用 ConcurrentHashMap。HashMap 的并发问题，纯属使用不当的问题。发生问题的代码是 Java8 之前。那时候的设计是数组 + 链表存储。并且扩容方式如下 12345678910111213141516171819202122232425262728293031323334353637void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; // rehash int i = indexFor(e.hash, newCapacity); // 并发条件下，可能在此处形成环形 e.next = newTable[i]; newTable[i] = e; e = next; // 因为形成环，所以永远 e != null，死循环 &#125; while (e != null); &#125; &#125;&#125;resize 的时候直接把所有元素 rehash，然后搬运到 newTable。 问题发生在 1e.next = newTable[i] 并发情况下，多个线程同时各自引发了 resize，可能出现 next=e.next; e.next=newTalbe[i]=e。 具体阅读这个文章： 疫苗：JAVA HASHMAP 的死循环 HashMap 小结java8 之前采用数组 + 链表的方式实现，通过链表处理冲突java8 之后采用数组 + 链表 + 红黑树的方式实现HashMap 不是线程安全的，使用 modCount 字段实现 fail-fast 机制HashMap 的扩容方式是 double by 2HashMap 支持 null 作为 key，实现上把 null 放在 table[0] 的第一个元素 java8 之前的 HashMap 有并发情况下死循环的问题。 参考 openjdk 6 HashMap 深入理解 HashMap(四): 关键源码逐行分析之 resize 扩容]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ConcurrentHashMap 原理]]></title>
    <url>%2Fp%2Fjava-concurrenthashmap%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap 是并发安全的 HashMap。ConcurrentHashMap 设计比较复杂，涉及的知识比较多，建议先看看：Java 8 HashMap 详解 Java Striped64 原理Java LongAdder 原理 同样以 java8 作为分析。ConcurrentHashMap 简介 类似 HashMap，使用数组 + 链表 + 红黑树的方式实现。负载因子 loadfactor，初始化大小 initialCapacity，默认容量 DEFAULT_CAPACITY，链表转红黑树 TREEIFY_THRESHOLD 都和 HashMap 一样。ConcurrentHashMap 数组 table（非扩容时使用）、nextTable（扩容时使用）都使用 volatile 修饰，保证并发修改的可见性，关于 volatile，见java volatile12transient volatile Node&lt;K,V&gt;[] table;private transient volatile Node&lt;K,V&gt;[] nextTable; 为了操作 table、nextTable，使用 Unsafe 类提供的 getObjectVolatile、putObjectVolatile 等方法。12345678910111213@SuppressWarnings("unchecked")static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125;ConcurrentHashMap Node123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next;&#125;Node 的结构和 HashMap.Node 相似，但是 val 和 next 都使用 volatile 修饰，保证并发修改的可见性。hash 字段存放节点的哈希值。ConcurrentHashMap 定义了几个特殊值：1234567/* * Encodings for Node hash fields. See above for explanation. */static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashMOVED、TREEBIN、RESERVED 是为了对应的特殊节点使用。HASH_BITS 是普通节点计算哈希时使用。ForwardingNodeForwardingNode，扩容的时候使用。hash 值为 MOVED（-1）。12345678910/** * A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125;TreeBinTreeBin 是红黑树，对应 hash 值为 TREEBIN（-2）。ReservationNodeReservationNode 在 computeIfAbsent 时候使用，对应 hash 为 RESERVED（-3）。因为是占位符，不需要具体的 key、value。12345678/** * A place-holder node used in computeIfAbsent and compute */static final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; ReservationNode() &#123; super(RESERVED, null, null, null); &#125;&#125;ConcurrentHashMap 计算 hash同 HashMap 一样，为了定位 key 在 table[]哪个位置，先做哈希计算。计算 hash 的方式和 HashMap 类似，都会使用 (h ^ (h &gt;&gt;&gt; 16))，兼顾速度和降低冲突；但是 ConcurrentHashMap 多了一次对 HASH_BITS 按位与。123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 上面提到，HASH_BITS 是普通节点使用。ConcurrentHashMap put()key 或者 value 不能为 null1if (key == null || value == null) throw new NullPointerException();计算 hash1int hash = spread(key.hashCode());遍历 table，定位 key 的位置。table[]和 nextTable[]都是 volatile 修饰，这里使用 Unsafe 类直接访问指定槽位的内存数据 如果指定槽位为 null，那么尝试 cas 方式插入节点 12345if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; 如果正在 resize，则参与扩容（扩容细节先按下不表）12if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f);否则，对该槽位加锁。然后根据链表或者红黑树方式更新 value。留意这里使用 synchronized 方式加锁，并且再次检查节点的 hash1234567891011121314151617181920212223242526272829303132333435363738394041V oldVal = null;// 使用 synchronized 方式加锁 synchronized (f) &#123; // 如果槽位已经不是节点 f，则不做处理 if (tabAt(tab, i) == f) &#123; // fh &gt; 0，是普通节点，并且是链表形式 if (fh &gt;= 0) &#123; // 在链表分支中，binCount 统计已经查找的节点数量 // 如果该链表访问的节点数量到达 TREEIFY_THRESHOLD，则进行转换 binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 否则，是特殊节点，检查是否红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125;&#125; 检查是否进行链表转换红黑树 1234567if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break;&#125; 更新计数，可能发生扩容。具体在后面再说 1addCount(1L, binCount); 小结：ConcurrentHashMap 并发修改，如果 table[x]是 null 则尝试 CAS 方式放入新节点，否则通过对 table[x]槽位同步（synchronized）来实现线程安全。标记 table[x]正在扩容的方式是，table[x]节点的 hash 修改为 MOVEDConcurrentHashMap 支持多个线程参与 table[x]的扩容 helpTransfer()ConcurrentHashMap get()12345678910111213141516171819202122public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 槽位的第一个节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 该节点处于特殊状态，（MOVED、RESERVED），或者是红黑树 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 普通节点，且是链表，遍历该槽位的所有节点 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125;ConcurrentHashMap 支持并发修改，因此 get 的时候，某个槽位可能处于特殊状态（MOVED、RESERVED），或者是红黑树，因此使用 Node 提供的 find 方法遍历。1234567891011121314151617static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125;Node 的子类会覆盖 find。TreeNode 使用红黑树方式查找12345static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125;&#125;ReservationNode 用在 computeIfAbsent，只是占位符，因此直接返 null。12345static final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; Node&lt;K,V&gt; find(int h, Object k) &#123; return null; &#125;&#125;ForwardingNode 表明该位置正在发生 resize。遍历的时候可能遇上节点也在做 resize，Java 实现上使用循环 + 代码块跳转，避免陷入多层递归。12345678910111213141516171819202122232425262728293031323334static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; // 特殊状态的节点 if (eh &lt; 0) &#123; // 遇到 ForwardingNode，则跳出当前循环，避免递归 if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125;&#125;get 方法小结： 不需要加锁 ConcurrentHashMap computeIfAbsent()computeIfAbsent: 如果没有对应的 key，则执行 mappingFunction 计算， 并且保证最多只计算一次 。1public V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) 考虑并发线程使用 computeIfAbsent 对同一个 key 操作。那么正常情况下，mappingFunction 只被计算一次。从执行 mappingFunction 得到 value，到新建 node 保存 key-value，不是原子化操作，因此要有机制实现。ConcurrentHashMap 的设计是：目标槽位为 null：CAS 设置一个占位符，使用了 ReservationNode。1234567891011121314if ((f = tabAt(tab, i = (n - 1) &amp; h)) == null) &#123; Node&lt;K,V&gt; r = new ReservationNode&lt;K,V&gt;(); synchronized (r) &#123; if (casTabAt(tab, i, null, r)) &#123; binCount = 1; Node&lt;K,V&gt; node = null; try &#123; if ((val = mappingFunction.apply(key)) != null) node = new Node&lt;K,V&gt;(h, key, val, null); &#125; finally &#123; setTabAt(tab, i, node); &#125; &#125;&#125;目标槽位不为 null：对这个槽位加锁 123synchronized (f) &#123; // 分开链表、红黑树操作&#125; 小结：ReservationNode 用于 computeIfAbsent，当目标槽位为 null 时作为占位符。ConcurrentHashMap 扩容机制： transfer &amp; helpTransferConcurrentHashMap 的扩容机制相对 HashMap 更加复杂。ConcurrentHashMap 的扩容，也是新建 newTable，然后把原来 table 元素搬运到到 newTable。但是这个搬运动作是并发！默认并发度是 16（MIN_TRANSFER_STRIDE）。1234567891011121314private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;/** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */private static int RESIZE_STAMP_BITS = 16;/** * Minimum number of rebinnings per transfer step. Ranges are * subdivided to allow multiple resizer threads. This value * serves as a lower bound to avoid resizers encountering * excessive memory contention. The value should be at least * DEFAULT_CAPACITY. */private static final int MIN_TRANSFER_STRIDE = 16;除了上面 3 个和并发度相关的参数，还有 sizeCtl 变量 123456789/** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */private transient volatile int sizeCtl;sizeCtl:-1：正在初始化 其他负数：有 N-1 个线程正在进行扩容操作 0 或者正数：table 没有初始化；或者下一轮 resize 要调整的数量 和扩容相关的方法是 transfer 和 helpTransfer。TODO: 具体扩容比较复杂，先挖个坑，以后单独文章介绍。ConcurrentHashMap 获取大小 获取容量大小有 2 个方法：size：java 集合类覆盖的方法。返回 intmappingCount：java8 以后新增的。返回 long。推荐使用此方法。123456789101112131415161718192021public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125;/** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values&#125;不管是 size 还是 mappingCount，都会使用 sumCount1234567891011final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125;如果 counterCells 不为 null，则遍历 counterCells，把 value 加上到 baseCount。否则直接返回 baseCount。疑问：baseCount 的用途？在什么时候更新？counterCells 是什么？在什么时候更新？先看 baseCount，在没有竞争的情况下使用，通过 CAS 方式更新。BASECOUNT 在 addCount 和 fullAddCount 使用。123456789/** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */private transient volatile long baseCount;BASECOUNT = U.objectFieldOffset (k.getDeclaredField("baseCount"));CounterCell 用来做发生冲突时的计数。还有个关联的 cellsBusy 变量，表示正在使用自旋锁更新 counterCells。12345678/** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */private transient volatile int cellsBusy;/** * Table of counter cells. When non-null, size is a power of 2. */private transient volatile CounterCell[] counterCells;CounterCell 的数据结构很简单，只有一个 long 类型的 value 保存计数。但是使用了 @sun.misc.Contended 注解，同时 javadoc 表示从 LongAdder 和 Striped64 改造。这里先不做扩展。12345678/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125;&#125;在 ide 中查找引用，发现只在 fullAddCount 方法修改 counterCellsfullAddCount 又有温馨提示了，具体要看 LongAdder 的设计。12// See LongAdder version for explanationprivate final void fullAddCount(long x, boolean wasUncontended) &#123;Striped64、LongAdder 文章：Java Striped64 原理 Java LongAdder 原理fullAddCount 被 addCount 调用。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if &lt;0, don't check resize, if &lt;= 1 only check if uncontended */private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // 如果有可用的 counterCells，并且 CAS 更新 baseCount 失败， if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || // 检查 counterCells 是否可用 (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // 随机找一个可用（即 null）的 counterCell 槽位 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 并且在该槽位上更新失败 fullAddCount(x, uncontended); // 则执行 fullAddCount return; &#125; // 执行此处，表明已经 CAS 更新了 counterCell if (check &lt;= 1) return; s = sumCount(); &#125; // 需要 resize。这里先不深入 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125;addCount 小结： 尝试 CAS 更新 baseCount。如果有竞争，则使用 fullAddCount。fullAddCount 参考了 LongAdder 的设计 ConcurrentHashMap 并发度ConcurrentHashMap 大量使用 CAS 更新，只有失败才进行同步。从上面代码可见，同步的位置是 table[x]。因此并发度是 table 的长度。Java7 的 ConcurrentHashMapHashMap、ConcurrentHashMap 都在 Java8 增加了链表转红黑树，设计复杂了。 对比 Java 7 ConcurrentHashMap.java，Java8 的 Java7 中的 Segment 数组对应 Java8 中 table 数组。但是真实数据存放在 HashEntry 类型的 table 数组。1234final Segment&lt;K,V&gt;[] segments;static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; transient volatile HashEntry&lt;K,V&gt;[] table;&#125; 每个 Segment 就是一个 ReentrantLock。如果要加锁，直接使用 ReentrantLock.lock。在 Java8，要对槽位加锁，使用 synchronized。我的理解是：Java 对 synchronized 优化，和 ReentrantLock 相差越来越少 随着 ConcurrentHashMap 扩容，并发度越大，单个槽位的竞争变少 ConcurrentHashMap 把所有节点（链表节点、红黑树、特殊状态节点）都抽象统一为 Node 类型，槽位也能直接存放数组，并且优先使用 CAS 更新。CAS 要比 ReentrantLock 更加轻量。 如果使用继续使用 ReentrantLock，那么消耗大量存储空间（ReentrantLock 底层使用 AQS。同时所有 Node 都变成了 ReentrantLock，完全没有这个必要）。ConcurrentHashMap 小结 ConcurrentHashMap 不支持 key 或者 value 为 null。ConcurrentHashMap 的并发度是 table[] 的长度，默认是 16。背后的设计思路是优先 CAS 更新、失败则使用 synchronized 加锁（缩小加锁范围）。ConcurrentHashMap 并发扩容使用 sizeCtl 控制 ConcurrentHashMap 有 3 种特殊类型的节点：TreeNode（红黑树），ReservationNode（用于 computeIfAbsent 的占位符），ForwardingNode（resize 标记）ConcurrentHashMap 可以使用 size 或者 mappingCount 获取容量大小。java8 以后推荐使用 mappingCount 方法。CounterCell 保存在竞争条件下 CAS 更新 baseCount 失败的计数 计数更新：使用 addCount()尝试 CAS 更新 baseCount，如果失败则使用 fullAddCount()。fullAddCount 设计参考了 LongAdderConcurrentHashMap 值得反复品味的设计细节很多。参考ConcurrentHashMap 扩容源码介绍1.8 版本的 ConcurrentHashMap 分析]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java CAS 原理，以及 CAS ABA 问题]]></title>
    <url>%2Fp%2Fjava-cas%2F</url>
    <content type="text"><![CDATA[Java CAS 简介 不管是 synchronized 还是 Lock，都是悲观的操作，假设每个操作都会竞争，因此要先加锁保护，导致吞吐量上不去。然而很多情况，竞争并不是很激烈，每次加锁保护临界区域并不是必须的。其中一种优化方式是，循环检查旧值是否改变。如果没有，就更新为新值；否则重新读取新值。这里有个隐式要求，检查–更新必须是原子化操作，需要硬件级别的支持。这就是 CAS(compare and swap)的思想，它是一种乐观锁，认为大多数情况下竞争不激烈，使用硬件原子化比较–更新的能力实现线程安全。AtomicIntegerJUC 包中 AtomicXXX 是使用 CAS 方式实现的类。以 AtomicInteger 为例子介绍。123456789101112131415public class AtomicInteger extends Number implements java.io.Serializable &#123; private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value;计数器 value 使用 volatile，保证多线程修改的可见性。关于 volatile，参见 java volatile 底层 CAS 操作，依赖 Unsafe 实现。AtomicInteger 常见的 getAndIncrement、getAndIncrement，底层都会使用 Unsafe 提供的 compareAndSwapInt1234567public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125;Unsafe12345678public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 依靠 unsafe.compareAndSwapInt 实现比较 - 交换的原子化操作。unsafe.cpp123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper("Unsafe_CompareAndSwapInt"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_ENDatomic.cpp12345678910111213141516171819jbyte Atomic::cmpxchg(jbyte exchange_value, volatile jbyte* dest, jbyte compare_value) &#123; assert(sizeof(jbyte) == 1, "assumption."); uintptr_t dest_addr = (uintptr_t)dest; uintptr_t offset = dest_addr % sizeof(jint); volatile jint* dest_int = (volatile jint*)(dest_addr - offset); jint cur = *dest_int; jbyte* cur_as_bytes = (jbyte*)(&amp;cur); jint new_val = cur; jbyte* new_val_as_bytes = (jbyte*)(&amp;new_val); new_val_as_bytes[offset] = exchange_value; while (cur_as_bytes[offset] == compare_value) &#123; jint res = cmpxchg(new_val, dest_int, cur); if (res == cur) break; cur = res; new_val = cur; new_val_as_bytes[offset] = exchange_value; &#125; return cur_as_bytes[offset];&#125;最终使用处理器的 cmpxchgl 指令原子化地 compare-and-swap。CAS ABA 问题 CAS 可能产生 ABA 问题。来自 wikiIn multithreaded computing, the ABA problem occurs during synchronization, when a location is read twice, has the same value for both reads, and “value is the same” is used to indicate “nothing has changed”. However, another thread can execute between the two reads and change the value, do other work, then change the value back, thus fooling the first thread into thinking “nothing has changed” even though the second thread did work that violates that assumption.ABA 问题发生过程T1 线程从共享的内存地址读取值 A；T1 线程被抢占，线程 T2 开始运行；T2 线程将共享的内存地址中的值由 A 修改成 B，然后又修改回 A；T1 线程继续执行，读取共享的内存地址中的值仍为 A，认为没有改变然后继续执行；（图片来源：https://lumian2015.github.io/lockFreeProgramming/aba-problem.html） 由于 ABA 问题带来的隐患，各种乐观锁的实现中通常都会用版本戳 version 来对记录或对象标记，避免并发操作带来的问题。A common workaround is to add extra “tag” or “stamp” bits to the quantity being considered.AtomicStampedReference 是 java 提供解决 CAS ABA 问题的工具。AtomicStampedReferenceAtomicStampedReference 给引用打了 stamp，作为版本号。123456789101112131415161718public class AtomicStampedReference&lt;V&gt; &#123; private static class Pair&lt;T&gt; &#123; final T reference; final int stamp; private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125; &#125; private volatile Pair&lt;V&gt; pair;&#125;AtomicStampedReference 进行 CAS 更新，先判断引用和 stamp 发生变化。如果未变，则利用 unsafe 进行原子化 CAS 更新，注意包括新的 stamp。12345678910111213141516public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return expectedReference == current.reference &amp;&amp; expectedStamp == current.stamp &amp;&amp; ((newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)));&#125;private boolean casPair(Pair&lt;V&gt; cmp, Pair&lt;V&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, pairOffset, cmp, val);&#125;参考 ABA problemWriting a Lock-free Stackhttps://lumian2015.github.io/lockFreeProgramming/aba-problem.html 用 ATOMICSTAMPEDREFERENCE 解决 ABA 问题]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java CyclicBarrier 原理]]></title>
    <url>%2Fp%2Fjava-cyclicbarrier%2F</url>
    <content type="text"><![CDATA[CyclicBrrierCyclicBrrier 是一个有趣的工具，能够让一组线程阻塞等待彼此 allows a set of threads to all wait for each other to reach a common barrier point.Cyclic 的意思是“循环”，即 CyclicBrrier 可以使用多此。（ps. 之前提到 CountDownLatch 也可以使一组线程阻塞等待，但是 CountDownLatch 只能使用一次。）(图片来源：https://www.geeksforgeeks.org/java-util-concurrent-cyclicbarrier-java/)CyclicBrrier 例子12345678910111213141516171819202122public class TestCyclicBarrier &#123; public static void main(String[] args) &#123; ExecutorService es = Executors.newCachedThreadPool(); final int PLAYER_COUNT = 5; CyclicBarrier barrier = new CyclicBarrier(PLAYER_COUNT, () -&gt; System.out.println("bang!")); for (int i = 0; i &lt; PLAYER_COUNT; i++) &#123; int pid = i; es.submit(() -&gt; &#123; try &#123; System.out.println("player[" + pid + "] is ready"); barrier.await(); System.out.println("player[" + pid + "] is done"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; es.shutdown(); &#125; 输出 1234567891011player[0] is readyplayer[4] is readyplayer[1] is readyplayer[3] is readyplayer[2] is readybang!player[2] is doneplayer[1] is doneplayer[3] is doneplayer[4] is doneplayer[0] is done 初始化 CyclicBarrier12public CyclicBarrier(int parties)public CyclicBarrier(int parties, Runnable barrierAction)parties：需要多少个线程到达屏障 barrierAction：可选，到达屏障触发一个动作 每个线程执行 barrier.await()，表示到达屏障并且等待。CyclicBrrier 源码 从上面来看，入口是 CyclicBrrier.await()方法。不过在深入之前，先了解下 CyclicBrrier 的结构。123456789101112131415161718192021private static class Generation &#123; boolean broken = false;&#125;/** The lock for guarding barrier entry */private final ReentrantLock lock = new ReentrantLock();/** Condition to wait on until tripped */private final Condition trip = lock.newCondition();/** The number of parties */private final int parties;/* The command to run when tripped */private final Runnable barrierCommand;/** The current generation */private Generation generation = new Generation();/** * Number of parties still waiting. Counts down from parties to 0 * on each generation. It is reset to parties on each new * generation or when broken. */private int count;CyclicBrrier 有个内部类 Generation。之前提到 CyclicBrrier 可以反复使用：每次复用 CyclicBrrier，generation 就会 reset。为了屏障的安全性，使用了 ReentrantLock 保护。从后面的代码看出，需要保护的操作是，更新剩余等待线程数，即 count 变量。ReentrantLock 上绑定了一个等待队列 trip。所有进入屏障等待的线程，都会进入 trip 等待队列。回到 await()方法，实际调用的是 dowait()检查 generation 合法性 如果发生了中断，则这个屏障已经坏掉，唤醒所有等待的线程 123456789/** * Sets current barrier generation as broken and wakes up everyone. * Called only while holding lock. */private void breakBarrier() &#123; generation.broken = true; count = parties; trip.signalAll();&#125; 如果最后一个线程进入了屏障，则执行 barrierCommand。nextGeneration 会唤醒所有线程，同时更新 generation 实例。每次复位 CyclicBrrier，即生成新的 Generation。1234567891011121314151617181920212223int index = --count;if (index == 0) &#123; // tripped boolean ranAction = false; try &#123; final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) breakBarrier(); &#125;&#125;private void nextGeneration() &#123; // signal completion of last generation trip.signalAll(); // set up next generation count = parties; generation = new Generation();&#125;加入 trip 条件队列等待，或者发生超时、屏障坏掉、中断等异常而退出 123456789101112131415161718192021222324252627// loop until tripped, broken, interrupted, or timed outfor (;;) &#123; try &#123; if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to //"belong"to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125;&#125; 最后在 finally 释放 ReentrantLockCyclicBrrier vs CountDownLatchCyclicBrrier 设计了 generation 字段，因此可以重复使用，通过 nextGeneration()重置屏障。CountDownLatch 只能使用一次。CyclicBrrier 支持在唤醒线程之前，执行自定义的命令（barrierCommand）。CountDownLatch 不支持。小结 CyclicBrrier 使用 ReentrantLock 保护屏障 在屏障处更新 count（剩余等待线程计数）所有等待线程都会加入 trip 条件队列，并且阻塞 等 count==0，唤醒所有等待的线程]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java CountDownLatch 原理]]></title>
    <url>%2Fp%2Fjava-countdownlatch%2F</url>
    <content type="text"><![CDATA[CountDownLatchCountDownLatch，字面上是倒计数的门闩，也就是倒计数结束的时候，开门干事情。javadoc 介绍很清楚 A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.CountDownLatch 可以让一组线程阻塞，直到收到唤醒。CountDownLatch 底层使用 AQS 实现。相关文章见：Java AbstractQueuedSynchronizer 笔记CountDownLatch 源码分析 回忆 AQS 的节点：1234567private transient volatile Node head;private transient volatile Node tail;/** * The synchronization state. */private volatile int state;子类使用 state 字段存储同步状态。CountDownLatch 使用 AQS.state 字段存储可用的计数。1234567891011private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125;&#125;留意 state 只有在初始化的时候可以设置。也就是说，CountDownLatch 只能使用一次，没有复位操作 。CountDownLatch.await()CountDownLatch.await()123public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125; 调用 AQS 的 acquireSharedInterruptibly12345678public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // tryAcquireShared 是模板方法，由子类覆盖 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125; 最终使用 Sync.tryAcquireShared123protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125;只要计数≠0，就使用 AQS.doAcquireSharedInterruptibly()阻塞等待 12345678910111213141516171819202122232425private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125;CountDownLatch.countDown()CountDownLatch.countDown() 减少计数，实际是释放共享锁 12345678/*** Decrements the count of the latch, releasing all waiting threads if the count reaches zero.* If the current count is greater than zero then it is decremented. If the new count is zero then all waiting threads are re-enabled for thread scheduling purposes.* If the current count equals zero then nothing happens.**/public void countDown() &#123; sync.releaseShared(1);&#125; 使用了 AQS.releaseShared()12345678public final boolean releaseShared(int arg) &#123; // tryReleaseShared 是模板方法，由子类覆盖 if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 最终使用 Sync.tryReleaseShared12345678910111213protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); // 计数为 0，则返回 false，由 AQS.doReleaseShared()唤醒所有等待的线程 if (c == 0) return false; int nextc = c-1; // CAS 更新 if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; 当 state==0，唤醒所有等待的线程 12345678910111213141516171819private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 唤醒线程 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125;CountDownLatch 例子12345678910111213141516171819202122232425262728public class TestCountDownLatch &#123; public static void main(String[] args) &#123; final int SIZE = 5; AtomicInteger ready = new AtomicInteger(0); CountDownLatch latch = new CountDownLatch(1); for (int i = 0; i &lt; SIZE; i++) &#123; int pid = i; new Thread(() -&gt; &#123; try &#123; System.out.println("thread[" + pid + "] is ready"); ready.incrementAndGet(); latch.await(); System.out.println("thread[" + pid + "] done."); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; // spin while (ready.get() &lt; SIZE) &#123; &#125; latch.countDown(); &#125;&#125; 在性能测试里，可以使用 CountDownLatch 模拟多线程瞬间并发。小结 CountDownLatch 内部类 Sync 继承了 AQSCountDownLatch 使用 AQS.state 保存计数。countDown() 减少计数。当计数 ==0，唤醒所有等待的线程CountDownLatch 只能使用一次，不能复位]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Semaphore 原理]]></title>
    <url>%2Fp%2Fjava-semaphore%2F</url>
    <content type="text"><![CDATA[前言 Semaphore 信号量用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。 操作时首先要获取到许可，才能进行操作，操作完成后需要释放许可。如果没有获取许可，则阻塞到有许可被释放。如果设置允许的信号量为 1，则退化为互斥锁（mutex）。之前分析了 AQS 和 ReentrantLock，Semaphore 就简单了。相关文章：Java AbstractQueuedSynchronizer 笔记 Java ReentrantLock 原理Semaphore.Sync 源码分析Semaphore 和 ReentrantLock 类似，支持公平、非公平策略。也有 Sync 内部类继承自 AQS。1234567abstract static class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 1192457210091910933L; Sync(int permits) &#123; setState(permits); &#125;&#125; 回想 AQS 的属性 123456private transient volatile Node head;private transient volatile Node tail;/** * The synchronization state. */private volatile int state; 子类使用 state 字段存储同步状态。这里使用 state 记录剩余许可数量。Sync 类提供了非公平模式获取许可、以及释放许可的方法。12345678910final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; // AQS.state 保存许可数量 int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) // CAS return remaining; &#125;&#125; 在无限循环中尝试获取许可。CAS 方式更新许可剩余数量。释放许可方式也是类似，但是增加溢出检查 12345678910protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error("Maximum permit count exceeded"); if (compareAndSetState(current, next)) return true; &#125;&#125;NonfairSync 非公平模式 NonfairSync，直接使用 Sync 提供的方法。FairSync公平模式，则是获取许可之前，先检查 AQS sync 队列有无等待的节点。1234567891011121314151617181920static final class FairSync extends Sync &#123; private static final long serialVersionUID = 2014338818796000944L; FairSync(int permits) &#123; super(permits); &#125; protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 检查 AQS sync 队列有无前驱 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125; &#125;&#125;Semaphore 例子 只允许 3 个线程并发打印。123456789101112131415161718192021222324252627282930313233343536373839404142434445public class TestSemaphore &#123; private static int MAX_SIZE = 23; private static int SEMAPHORE_SIZE = 3; private static AtomicInteger count = new AtomicInteger(MAX_SIZE); private static Semaphore semaphore = new Semaphore(SEMAPHORE_SIZE); public static void main(String[] args) &#123; ExecutorService es = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; es.submit(new PrintTask(i)); &#125; es.shutdown(); try &#123; es.awaitTermination(5, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; static class PrintTask implements Runnable &#123; private int id; public PrintTask(int id) &#123; this.id = id; &#125; @Override public void run() &#123; int curr; while ((curr = count.getAndDecrement()) &gt; 0) &#123; try &#123; semaphore.acquire(); System.out.println("thread[" + id + "] got semphore @" + curr); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125; &#125; &#125;&#125;小结Semaphore 支持非公平和公平模式。Semaphore 使用 AQS 的 state 字段存放剩余许可数量。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Condition 原理，以及实现生产者消费者模式]]></title>
    <url>%2Fp%2Fjava-condition%2F</url>
    <content type="text"><![CDATA[Condition 简介 java 线程间通信，除了可以使用 Object 提供的 wait/notify/notifyAll 之外，还可以使用 java.util.concurrent.locks.Condition 接口。Condition 接口的 await/signal/signalAll 分别对应 Object 的 wait/notify/notifyAll。 相比 Object 提供的线程间通信机制，Condition 接口支持在一个对象上创建多个等待队列（having multiple wait-sets per object）；而 Object 机制使用隐式锁，只有一个等待队列。Condition 实例和一个 Lock 对象绑定。因此在使用 Condition 实例方法之前，要先获得 Lock。ObjectCondition使用方式 synchronizedlock.newCondition();lock.lock() 等待队列数量 1 个 多个 支持超时等待 noyes 支持中断 noyes 使用 Condition 实现生产者消费者模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class TestCondition &#123; private int MAX_SIZE = 10; private ReentrantLock lock = new ReentrantLock(); // having multiple wait-sets per object private Condition notFull = lock.newCondition(); private Condition notEmpty = lock.newCondition(); private LinkedList&lt;Integer&gt; holders = new LinkedList&lt;&gt;(); @Test public void test() &#123; new Thread(new Producer()).start(); new Thread(new Consumer()).start(); try &#123; Thread.currentThread().join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; while (true) &#123; try &#123; lock.lock(); while (holders.isEmpty()) &#123; notEmpty.await(); &#125; int x = holders.removeFirst(); System.out.println("consumes:" + x); notFull.signal(); Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; &#125; class Producer implements Runnable &#123; @Override public void run() &#123; Random random = new Random(); while (true) &#123; try &#123; lock.lock(); // 在循环中检查条件，处理虚假唤醒的情况 while (holders.size() == MAX_SIZE) &#123; notFull.await(); &#125; int x = random.nextInt(100); holders.addLast(x); System.out.println("produce:" + x); notEmpty.signal(); Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; &#125;&#125;Condition 相关源码Condition 要与 Lock 对象绑定。以 ReentrantLock 为例，12345678public class ReentrantLock implements Lock, java.io.Serializable&#123; abstract static class Sync extends AbstractQueuedSynchronizer &#123; final ConditionObject newCondition() &#123; return new ConditionObject(); &#125; &#125;&#125; 实际使用的 AQS.ConditionObject 类。ConditionObject 使用 CLH node 构建双向链表，是 FIFO 队列。123456public class ConditionObject implements Condition, java.io.Serializable &#123; private static final long serialVersionUID = 1173984872572414699L; /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter;addConditionWaiter添加等待线程的操作发生在 lastWaiter（链表尾部）。注意 Condition 类型的等待队列，节点的 waitStatus 是 Node.CONDITION。12345678910111213141516171819/** * Adds a new waiter to wait queue. * @return its new wait node */private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125;因为等待队列中的节点可能取消等待，因此要从头开始检查。触发 unlinkCancelledWaiters 的时机有 2 个：等待队列的 lastWaiter 已经取消 发送 await，这时候已经获取了锁 1234567891011121314151617181920private void unlinkCancelledWaiters() &#123; Node t = firstWaiter; Node trail = null; while (t != null) &#123; Node next = t.nextWaiter; // 因为是条件队列，waitStatus != Node.CONDITION 都认为是取消等待。 if (t.waitStatus != Node.CONDITION) &#123; t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; &#125; else trail = t; t = next; &#125;&#125;signal 从链表头部开始遍历，唤醒一个节点 12345678910111213141516public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125;private void doSignal(Node first) &#123; do &#123; if ((firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125;transferForSignal 方法，把条件队列的节点丢到同步队列，这样线程被“唤醒”，可以参与竞争锁12345678910111213141516171819202122232425/*** Transfers a node from a condition queue onto sync queue.*/final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ // 把节点插入到 AQS 同步队列的 tail，这样线程可以参与竞争锁 // 并且返回它的前驱 Node p = enq(node); int ws = p.waitStatus; // 当前前驱的 ws 为 SIGNAL，才需要唤醒后面的节点。 if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125;signalAll 也是类似的思路，在此不重复。awaitCondition 的 await 对应 Object 的 wait，因此等待的时候会释放 lock。1234567891011121314151617181920212223public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 释放锁后要再次等待锁，因此加入到 Condition 等待队列 Node node = addConditionWaiter(); // 释放锁 int savedState = fullyRelease(node); int interruptMode = 0; // 自旋检查并且阻塞，直到进入 sync 队列，或者被中断 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125;1234567891011121314151617final int fullyRelease(Node node) &#123; boolean failed = true; try &#123; // 获取 AQS 的 state 字段，保存的是同步状态 int savedState = getState(); // release 会调用模板方法 tryRelease if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 回忆：AQS 的 state 字段用来保存同步状态。在 ReentrantLock 实现中，state 字段用来记录当前 lock 被获取了几次。isOnSyncQueue 检查刚放进 condition 队列的节点，是否在 sync 队列。isOnSyncQueue 方法做的检查比较多 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Returns true if a node, always one that was initially placed on * a condition queue, is now waiting to reacquire on sync queue. * @param node the node * @return true if is reacquiring */final boolean isOnSyncQueue(Node node) &#123; // CONDITION 表明在 condition 队列 // node.prev == null : 进入 sync 队列，是在 tail 入队，且 node.prev 设置为之前的 tail // 因此 null 表明肯定没有进入队列 if (node.waitStatus == Node.CONDITION || node.prev == null) return false; // 进入 sync 队列，则在 tail 入队，node.next 被设置为 null // 因此 next != null 肯定已经在 sync 队列 if (node.next != null) // If has successor, it must be on queue return true; /* * node.prev can be non-null, but not yet on queue because * the CAS to place it on queue can fail. So we have to * traverse from tail to make sure it actually made it. It * will always be near the tail in calls to this method, and * unless the CAS failed (which is unlikely), it will be * there, so we hardly ever traverse much. */ // 这时候 node.next==null &amp; node.prev!=null // 可以认为 node 正处于放入 Sync 队列的执行 CAS 操作执行过程中。而这个 CAS 操作有可能失败 // 遍历 sync 队列检查 return findNodeFromTail(node);&#125;/** * Returns true if node is on sync queue by searching backwards from tail. * Called only when needed by isOnSyncQueue. * @return true if present */private boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125;&#125; 小结 await() 唤醒 Condition 队列中第一个 Nodesignal 就是唤醒 Condition 队列中的第一个非 CANCELLED 节点线程，而 signalAll 就是唤醒所有非 CANCELLED 节点线程。遇到 CANCELLED 线程就需要将其从 FIFO 队列中剔除。参考Java 多线程之 JUC 包：Condition 源码学习笔记Java 多线程（九）之 ReentrantLock 与 Condition]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java ReentrantLock 原理]]></title>
    <url>%2Fp%2Fjava-reentrantlock%2F</url>
    <content type="text"><![CDATA[有了 AQS 的基础，可以了解 ReentrantLock 的原理。相关文章 Java AbstractQueuedSynchronizer 笔记ReentrantLock 简介ReentrantLock 支持公平、非公平方式获取互斥锁锁。 公平锁（Fair）：加锁前检查是否有排队等待的线程，优先排队等待的线程，先来先得 非公平锁（NonFair）：加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待 ReentrantLock 正如其名，是可重入的 可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。ReentrantLock 底层使用了 AQS 工具类。ReentrantLock 和 AQS 的关系如下：ReentrantLock 的内部类 Sync 继承自 AQS，还有 FairSync、NonFairSync 继承于 Sync 类。ReentrantLock 源码 ReentrantLock 内部包含一个 Sync 实例，核心的 lock、unlock 操作交给 Sync 实例负责。Sync 实例可以是 NonFairSync 或者 FairSync。1234567891011public class ReentrantLock implements Lock, java.io.Serializable &#123; /** Synchronizer providing all implementation mechanics */ private final Sync sync; public void lock() &#123; sync.lock(); &#125; public void unlock() &#123; sync.release(1); &#125;ReentranLock.SyncAbstractQueuedSynchronizer 提供了 tryXXX 模板方法，供子类覆盖。Sync 覆盖了 tryRelease()，并且提供了 nonfairTryAcquire()1234567891011121314151617181920212223242526272829303132333435363738394041424344abstract static class Sync extends AbstractQueuedSynchronizer &#123; abstract void lock(); /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 使用 AQS.state 字段保存 // 对于非公平锁，一旦发现锁可以占有，则当前线程抢占锁 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; // 可重入锁，更新当前获取的许可数 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; // 检擦当前线程是否拥有互斥锁 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 因为是可重入，一个线程可以多次获取锁 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 回想 AQS 的属性 123456private transient volatile Node head;private transient volatile Node tail;/** * The synchronization state. */private volatile int state; 子类使用 state 字段存储同步状态。这里使用 state 记录线程获取锁的次数。ReentranLock 是互斥锁，每次获取的许可数都是 1。释放锁则可以一次释放多个许可。NonfairSync1234567891011121314static final class NonfairSync extends Sync &#123; final void lock() &#123; // fast-path if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;FairSync123456789101112131415161718192021222324252627282930static final class FairSync extends Sync &#123; final void lock() &#123; acquire(1); &#125; /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 公平锁，在抢占锁之前，先检查有无等待的前驱节点 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125;ReentrantLock 实践 一定要释放锁。通常在 finally 释放。1234567ReentrantLock lock=new ReentrantLock();try&#123; lock.tryLock(); // do something&#125;finally&#123; lock.unlock();&#125;公平锁 vs 非公平锁 公平锁按照请求资源的先后顺序获取锁。会导致频繁切换线程上下文。吞吐量比非公平锁低。非公平锁获取锁的时候不会检查等待队列。可能发生一个线程反复获取锁，导致其他线程发生“饥饿”。ReentrantLock vs synchronizedReentrantLock 支持公平锁、非公平锁模式，synchronized 是非公平锁。ReentrantLock 和 synchronized 都是可重入锁。ReentrantLock 底层使用 AQS 实现，synchronized 使用 java 的隐式锁实现。ReentrantLock 支持超时等待（tryLock）。在 java6 之前，synchronized 性能要比 ReentrantLock 低。ReentrantLock 和 ConditionCondition 是线程间通信的方式。Condition 要绑定 Lock。ReentrantLock 提供了 Condition 支持。具体参见 Condition 原理，以及实现生产者消费者模式 小结ReentrantLock 使用了 AQS 的互斥锁能力。使用 state 变量记录线程获取锁的次数。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BASE 理论]]></title>
    <url>%2Fp%2Fbase-theory%2F</url>
    <content type="text"><![CDATA[BASE 理论是分布式系统设计的一种指导思想。BASE 是 Basically Available(基本可用)、Soft State（软状态）和 Eventually Consistent（最终一致性）的缩写。 虽然无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。Basically Available 出现故障，系统虽然能够使用，但是打了折扣，比如响应时间变长、部分功能不可用。Basically Available 对系统设计的指导意义是： 识别系统的关键功能。比如账号系统，有注册、登录、登录态校验、修改用户信息等功能。最关键的功能，应该是日常流量最大的登录、登录态校验。识别系统的关键功能，投入更多资源做系统优化，保证关键功能模块可用性。 服务降级形式和影响。发生故障的时候，这个服务是允许响应变长、还是返回拒绝服务？一旦服务降级，会发生怎样的级联影响？Soft State 软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。 相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。 业务能够接受 Soft State 状态为多长时间，这将直接影响设计方案的复杂度。Eventually Consistent 软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。Eventually Consistent 是一致性模型的一种。CAP 理论表明网络分区无法表明，系统只能在一致性（C）和可用性（A）之间做出取舍。但这是忽略了时间维度。一旦分区恢复，数据必须回归一致性状态。数据追尾、业务优先原则覆盖都是常见的修复非一致性数据的方式。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java AbstractQueuedSynchronizer 笔记]]></title>
    <url>%2Fp%2Fjava-aqs%2F</url>
    <content type="text"><![CDATA[AbstractQueuedSynchronizer 是 JUC 包的基础，解决了同步器的细节问题（同步状态、FIFO 队列），ReentrantLock、Semaphore、CountDownLatch 等类都使用了 AQS。CLH lock12345 +------+ prev +------+ prev +------+ | | &lt;---- | | &lt;---- | | head | Node | next | Node | next | Node | tail | | ----&gt; | | ----&gt; | | +------+ +------+ +------+CLH 锁通常用于自旋锁。AQS 使用 CLH 的变种作为同步器。CLH 节点数据结构如下。12345678910111213141516171819202122232425262728293031static final class Node &#123; /** Marker to indicate a node is waiting in shared mode */ static final Node SHARED = new Node(); /** Marker to indicate a node is waiting in exclusive mode */ static final Node EXCLUSIVE = null; /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor's thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; /** * The thread that enqueued this node. Initialized on * construction and nulled out after use. */ volatile Thread thread; // 存储 condition 队列中的后继节点 Node nextWaiter;&#125;waitStatus 是节点的状态。SIGNAL：-1。这个节点释放、或者取消的时候，要通过 unpark 操作通知后继节点。CANCELLED：1。节点由于超时或者中断，因此取消了。CONDITION：-2。这个节点处于条件队列。PROPAGATE：-3。releaseShared 操作的时候要传递到其他节点。0：其他。 对于普通同步节点，waitStatus 初始化为 0，如果是条件队列节点，则初始化为 CONDITION。AQS 是由 CLH node 构建的双向链表，head、tail 分别指向链表的头部、尾部。1234567private transient volatile Node head;private transient volatile Node tail;/** * The synchronization state. */private volatile int state;留意 state 字段，子类使用 state 字段存储同步状态。AQS 入队 1234567891011121314151617181920/** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 入队操作发生在 tail：node.prev = pred。这里做了优化，假设入队竞争不大，先尝试检查和更新 tail，失败才走完整的 enq 流程。enq 逻辑如下 12345678910111213141516171819202122232425262728293031323334private Node enq(final Node node) &#123; for (;;) &#123; // 入队发生在 tail Node t = tail; if (t == null) &#123; // Must initialize // 由于 AQS 的 head、tail 节点使用了 lazy init 方式， // 所以要先检查节点是否为 null，并做初始化。 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 设置入队节点的前驱为之前的 tail // 并且尝试原子化检查和更新 tail node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125;/** * CAS head field. Used only by enq. */private final boolean compareAndSetHead(Node update) &#123; return unsafe.compareAndSwapObject(this, headOffset, null, update);&#125;/** * CAS tail field. Used only by enq. */private final boolean compareAndSetTail(Node expect, Node update) &#123; return unsafe.compareAndSwapObject(this, tailOffset, expect, update);&#125; 在一个无限循环中不停检查和重试更新 tail，直至入队成功。底层使用了 Unsafe 类，直接使用底层硬件提供的原子化操作，这里先不展开。获取互斥锁 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;acquire 是互斥方式获取锁，不支持中断。tryAcquire 是模板方法，由子类覆盖（ReentrantLock、Semaphore 等）。 当尝试获取锁失败，则使用 addWaiter 把当前线程添加到 CLH 队列，并在循环中尝试。12345678910111213141516171819202122232425final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 当前节点的前驱是 head，才尝试获取同步器 // tryAcquire 是模板方法，由子类覆盖 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 更新 node 为 head setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 判断是否要阻塞线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 有意思的是获取锁失败后，判断是否需要阻塞线程。1234567891011121314151617181920212223242526272829private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 检查前驱节点的 waitStatus int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 需要被阻塞 return true; if (ws &gt; 0) &#123; // ws &gt; 0 即 CANCELLED，往回找到非 CANCELLED 节点 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ // 其余情况设置为 SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125;private final boolean parkAndCheckInterrupt() &#123; // 使用 LockSupport 阻塞当前线程 LockSupport.park(this); return Thread.interrupted();&#125; 释放互斥锁 123456789101112131415161718192021222324252627282930313233343536373839public final boolean release(int arg) &#123; // tryRelease 是模板方法，由子类覆盖 if (tryRelease(arg)) &#123; // 在 head 释放锁 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; // 没有后继节点，或者已经 CANCELLED // 则需要从 tail 往回遍历，寻找当前节点的后继 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 获取共享锁 12345678910111213141516171819202122232425262728293031323334public final void acquireShared(int arg) &#123; // tryAcquireShared 是模板方法 if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125;private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 前驱是 head，才尝试获取锁 if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 因为是 shared 模式，获取锁之后，可能要通知后面的节点 123456789101112131415161718192021222324252627282930313233343536/** * Sets head of queue, and checks if successor may be waiting * in shared mode, if so propagating if either propagate &gt; 0 or * PROPAGATE status was set. * * @param node the node * @param propagate the return value from a tryAcquireShared */private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don't know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ // propagate &gt; 0：还有剩余，需要传递 // h.waitStatus &lt; 0 ：SIGNAL or PROPAGATE if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 释放共享锁 12345678public final boolean releaseShared(int arg) &#123; // tryReleaseShared 是模板方法 if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;1234567891011121314151617181920212223242526272829303132private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; // 在 head 释放锁 Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // SIGNAL 需要唤醒后续节点 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; // why 转为 PROPAGATE 状态？ else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125;Node.PROPAGATEAQS 源码比较难理解的是 Node.PROPAGATE。javadoc 对 Node.PROPAGATE 的解释12345PROPAGATE: A releaseShared should be propagated to other nodes. This is set (for head node only) in doReleaseShared to ensure propagation continues, even if other operations have since intervened. 在共享模式下，可以认为资源有多个，因此当前线程被唤醒之后，可能还有剩余的资源可以唤醒其他线程。该状态用来表明后续节点会传播唤醒的操作。需要注意的是只有头节点才可以设置为该状态 找到一篇文章，对此思考比较深入，推荐阅读：AbstractQueuedSynchronizer 源码解读 在 AQS 的共享锁中，一个被 park 的线程，不考虑线程中断和前驱节点取消的情况，有两种情况可以被 unpark：一种是其他线程释放信号量，调用 unparkSuccessor；另一种是其他线程获取共享锁时通过传播机制来唤醒后继节点。可能会有队列中处于等待状态的节点因为第一个线程完成释放唤醒，第二个线程获取到锁，但还没设置好 head，又有新线程释放锁，但是读到老的 head 状态为 0 导致释放但不唤醒，最终后一个等待线程既没有被释放线程唤醒，也没有被持锁线程唤醒。PROPAGATE 的引入是为了解决共享锁并发释放导致的线程 hang 住问题。setHeadAndPropagate()123456if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) doReleaseShared();&#125;doReleaseShared()123else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CASAQS 和 ConditionAQS 提供了 ConditionObject，实现了条件队列。具体参见 Condition 原理，以及实现生产者消费者模式 小结 AQS 使用 CLH 节点，构建双向链表AQS 提供了模板方法（tryXXX，例如 tryRelease、tryAcquire），由子类覆盖AQS 支持互斥锁、共享锁、条件队列 在 tail 增加等待线程 在 head 获取锁、释放锁 使用 unsafe 类 CAS 更新 head、tail释放锁且 Node.SIGNAL，则要通过 unparkSuccessor 唤醒后续节点 对于共享锁，为了解决并发释放导致线程 hang 的情况，增加了 Node.PROPAGATE。只有 head 节点才可以设置为 PROPAGATE 状态 参考【Java 并发】详解 AbstractQueuedSynchronizerAbstractQueuedSynchronizer 源码解读]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal 原理]]></title>
    <url>%2Fp%2Fjava-threadlocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal 是什么 ThreadLocal 的字面意思是“线程本地变量”，即每个线程都能独立初始化、访问、修改这个变量，因此才叫 thread local。javadoc 对 ThreadLocal 类的简介：This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. 因为 ThreadLocal 变量是每个线程独自拥有、相互隔离，因此 ThreadLocal 变量不是共享变量，不存在线程安全的问题，不需要同步。ThreadLocal 应用场景javadoc 提及了 ThreadLocal 的一些应用场景ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). 隐式传参。例如一个线程处理周期中，会反复使用的变量，例如用户 Id、事务 Id，直接放在 ThreadLocal，要比每个使用方法增加 userId、transactionId 要简洁。事务处理也经常使用 ThreadLocal，参见 TransactionSynchronizationManager12345678910111213141516171819public abstract class TransactionSynchronizationManager &#123; private static final ThreadLocal&lt;Map&lt;Object, Object&gt;&gt; resources = new NamedThreadLocal&lt;&gt;("Transactional resources"); private static final ThreadLocal&lt;Set&lt;TransactionSynchronization&gt;&gt; synchronizations = new NamedThreadLocal&lt;&gt;("Transaction synchronizations"); private static final ThreadLocal&lt;String&gt; currentTransactionName = new NamedThreadLocal&lt;&gt;("Current transaction name"); private static final ThreadLocal&lt;Boolean&gt; currentTransactionReadOnly = new NamedThreadLocal&lt;&gt;("Current transaction read-only status"); private static final ThreadLocal&lt;Integer&gt; currentTransactionIsolationLevel = new NamedThreadLocal&lt;&gt;("Current transaction isolation level"); private static final ThreadLocal&lt;Boolean&gt; actualTransactionActive = new NamedThreadLocal&lt;&gt;("Actual transaction active");日志框架的 MDC 机制，使用 ThreadLocal 保存线程相关的上下文信息。ThreadLocal 源码 ThreadLocal 的 get() set() setInitialValue() 从 ThreadLocal 读取数据，先检查 ThreadLocalMap 是否为空，再从 map 中查找。如果 map 为 null，或者没有这个 key，则执行初始化（setInitialValue）。12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;setInitialValue 也很单，如果 map 为 null 则新建 map，否则把默认值放进 map。注意 map.set(this, value)。上面提到 ThreadLocalMap 的 key 是ThreadLocal&lt;?&gt;，这里的 this 正是 ThreadLoca 变量。12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 修改的思路也是类似的。12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;从上面可以看到，ThreadLocal 的 get、set 操作都交给 ThreadLocalMap 处理。ThreadLocalMapThreadLocal 底层依靠内部类 ThreadLocalMap 实现。12345678910111213141516171819202122232425262728293031/** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */static class ThreadLocalMap &#123; /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as "stale entries" in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; // more code&#125;ThreadLocalMap 的特点是，以 ThreadLocal&lt;?&gt; 作为存储的 key，并且 Key 继承了 WeakReference。ThreadLocal 本身不存储数据，真正的数据存储在 Thread 类。12345678910public class Thread implements Runnable &#123; /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;留意 Thread 类和 ThreadLocal 类都在 java.lang 包下面，并且 Thread 类的 threadLocals 和 inheritableThreadLocals 的访问级别是 default，即包级别。因此可以直接被 ThreadLocal 类访问。ThreadLocalMap.set()和 replaceStaleEntry()ThreadLocalMap 是一个定制版的 HashMap，专门为 ThreadLocal 服务。123456789101112131415161718192021222324252627282930private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125;使用开链表的方式实现。真正有意思的是 key 为 null 的情况 1234if (k == null) &#123; replaceStaleEntry(key, value, i); return;&#125;replaceStaleEntry 寻找过时的 key，并且启动清理。无论是否找到过是的 key，新的 value 都会写入该槽位。123456789101112131415161718192021// If we find key, then we need to swap it// with the stale entry to maintain hash table order.// The newly stale slot, or any other stale slot// encountered above it, can then be sent to expungeStaleEntry// to remove or rehash all of the other entries in run.if (k == key) &#123; e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; // Start expunge at preceding stale entry if it exists if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return;&#125;// If we didn't find stale entry on backward scan, the// first stale entry seen while scanning for key is the// first still present in the run.if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i;replaceStaleEntry 是为了避免内存泄露问题。ThreadLocal 内存泄露ThreadLocalMap 的介绍有这么一行：To help deal with very large and long-lived usages, the hash table entries use WeakReferences for keys. 可见 java 使用 WeakReference 作为 Entry 的初衷，是为了应对 ThreadLocalMap 生命周期很长的情况。由于是 WeakReference，有可能出现 ThreadLocal 被回收了，但是底层的 Entry[]数组却是强引用，最后导致 value 的内存无法被回收。为了避免内存泄露，java 做了如下措施：ThreadLocalMap.set()发现 key 为 null，触发 replaceStaleEntry()ThreadLocalMap.getEntry()发现 key 为 null，触发 expungeStaleEntry()ThreadLocalMap.remove()触发 expungeStaleEntry()expungeStaleEntry()使用 rehash 的方式，寻找 key 为 null 的位置，并且释放对应的 value。1234567891011121314151617181920212223242526272829303132private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125;ThreadLocal 和上下文 ThreadLocal 可以用来保存上下文信息。 但是一个线程处理多个请求，就有可能导致这个请求读取到了上个请求遗留的上下文信息。ThreadPoolExecutor 线程池提供了入口 12protected void beforeExecute(Thread t, Runnable r) &#123; &#125;protected void afterExecute(Runnable r, Throwable t) &#123; &#125; 可以在此执行 ThreadLocal 的清理操作。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wait、notify 和生产者消费者模式]]></title>
    <url>%2Fp%2Fjava-wait-notify%2F</url>
    <content type="text"><![CDATA[wait、notify、notifyAll 定义在 Object 类，是 java 提供的线程间通信机制。wait、notify、notifyAll 使用了监视器锁实现同步，因此使用之前要先获取监视器锁（即 synchronized），否则抛出 IllegalMonitorStateException 异常。生产者消费者模式 线程间通信典型的例子是生产者消费者模式。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class WaitAndNotify &#123; private static int MAX_SIZE = 8; public static void main(String[] args) &#123; LinkedList&lt;Integer&gt; holders = new LinkedList(); new Thread(new Producer(holders)).start(); new Thread(new Consumer(holders)).start(); &#125; static class Consumer implements Runnable &#123; private LinkedList&lt;Integer&gt; holders; public Consumer(LinkedList&lt;Integer&gt; holders) &#123; this.holders = holders; &#125; @Override public void run() &#123; while (true) &#123; synchronized (holders) &#123; if (holders.size() == 0) &#123; holders.notify(); &#125; else &#123; System.out.println("consumes:" + holders.removeFirst()); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125; static class Producer implements Runnable &#123; private LinkedList&lt;Integer&gt; holders; public Producer(LinkedList&lt;Integer&gt; holders) &#123; this.holders = holders; &#125; @Override public void run() &#123; Random random = new Random(); while (true) &#123; synchronized (holders) &#123; if (holders.size() &lt; MAX_SIZE) &#123; int x = random.nextInt(100); System.out.println("produce:" + x); holders.add(x); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; try &#123; holders.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125;&#125;notify vs notifyAllnotify 随机唤醒一个等待的线程。notifyAll 唤醒全部等待的线程。虚假唤醒 spurious wakeup来自 https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#wait%28%29 ：A thread can also wake up without being notified, interrupted, or timing out, a so-called spurious wakeup. While this will rarely occur in practice, applications must guard against it by testing for the condition that should have caused the thread to be awakened, and continuing to wait if the condition is not satisfied. In other words, waits should always occur in loops, like this one:这里有篇扩展的资料，讲述 linux 上 PTHREAD_COND_TIMEDWAIT 和虚假唤醒的问题，PTHREAD_COND_TIMEDWAIT：Some implementations, particularly on a multi-processor, maysometimes cause multiple threads to wake up when the conditionvariable is signaled simultaneously on different processors.In general, whenever a condition wait returns, the thread has to re-evaluate the predicate associated with the condition wait todetermine whether it can safely proceed, should wait again, or shoulddeclare a timeout. A return from the wait does not imply that theassociated predicate is either true or false.一些 obj.wait()会在除了 obj.notify()和 obj.notifyAll()的其他情况被唤醒，而此时是不应该唤醒的。为了处理虚假唤醒的 case，要在 while 中循环判断条件是否成立，不能直接 if 判断就了事。12345synchronized (obj) &#123; while (&lt;condition does not hold&gt;) obj.wait(timeout); ... // Perform action appropriate to condition&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 线程池原理]]></title>
    <url>%2Fp%2Fjava-threadpool%2F</url>
    <content type="text"><![CDATA[频繁地创建和销毁线程，会浪费资源。因此产生了线程池，缓存并重用线程，提高性能。使用 java 线程池，最简单的方式是 Executors 和 ExecutorService。(图片来源：https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-pools.html)Executors 和 ExecutorServiceExecutorService 提供了线程、线程池相关管理的抽象，包括提交任务（submit、execute）、关闭线程池。Executors 是一个工具类，提供了创建线程池的入口。1ExecutorService es = Executors.newCachedThreadPool();常见创建线程池 newCachedThreadPool：创建缓存型线程池。适用于大量短生命周期的任务。newFixedThreadPool：创建固定线程数量的线程池。newScheduledThreadPool：创建延迟 / 定时任务的线程池。newSingleThreadScheduledPool：模拟单个线程操作的线程池。java8 以后新增 newWorkStealingPool，创建工作窃取类型的线程池，以后再专门讨论 working steal，这里先不深入展开。ThreadPoolExecutor 真实创建线程池，依靠 ThreadPoolExecutor。Executors 只是简单的封装。12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); &#125;ThreadPoolExecutor 的构造函数，有一堆定制参数。12345678910public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; &#125;&#125;参数意义如下：corePoolSize：核心线程数，默认情况下核心线程会一直存活，即使处于闲置状态也不会受存 keepAliveTime 限制。除非将 allowCoreThreadTimeOut 设置为 true。maximumPoolSize：线程池所能容纳的最大线程数。超过这个数的线程将被阻塞。当任务队列为没有设置大小的 LinkedBlockingDeque 时，这个值无效。keepAliveTime：非核心线程的闲置超时时间，超过这个时间就会被回收。unit：指定 keepAliveTime 的单位，如 TimeUnit.SECONDS。workQueue：任务队列。常用的有三种队列：SynchronousQueue，LinkedBlockingDeque，ArrayBlockingQueue。threadFactory：线程工厂，可以对创建的线程进行定制。handler：超出 workQueue 容量的任务的处理方式。DiscardPolicy当缓存任务数量大于 workQueue 容量，且工作线程数到达 maxPoolSize，会触发 DiscardPolicy 处理，有 4 种处理策略 DiscardOldestPolicy：丢弃 workQueue 中最老的任务，并且尝试再次把当前任务加入到 workQueue。AbortPolicy：丢弃任务，且抛出异常。CallerRunsPolicy：有发起任务的线程去执行。这就从源头上限流。DiscardPolicy：丢弃任务，但是不抛出异常。 为什么建议直接使用 ThreadPoolExecutor从 ThreadPoolExecutor 构造函数可以看到多个控制参数，最为核心的是线程池大小、阻塞队列、拒绝策略。举个例子，打开 Executors，查看 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;那么当线程数到达 corePoolSize，新提交的任务将会堆积到 workQueue，由于类型是 LinkedBlockingQueue，没有做长度限制，就会有可能堆积大量的任务，消耗大量内存，引发频繁 gc、甚至 OOM 问题。如果手动使用 ThreadPoolExecutor 创建线程池，那么新建线程池的时候就要考虑清楚适合业务场景的线程池配置，可控性更高。这也是阿里推荐使用 ThreadPoolExecutor 创建线程池的原因。接下来深入了解 ThreadPoolExecutor 的代码。添加任务到线程池 调用的是 execute()。1234567891011121314151617181920212223242526272829303132333435363738394041private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn't, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125;如果当前线程数 &lt; corePoolSize，则新建线程。当前线程数 &gt;= corePoolSize，且小于 workQueue 容量，则添加到 workQueue，缓存任务。如果 workQueue 已满，并且未达到 maxPoolSize，则新建线程并且执行任务。否则，已经达到 maxPoolSize，reject 这个任务。addWorker()是底层实现添加任务的类。核心是在循环内进行 CAS 检查，避免使用锁。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125;ctl 变量 阅读 ThreadPoolExecutor 源码，需要留意 ctl 变量。ctl 变量把 workerCount 和 runState 打包到一个 int 类型。很巧妙的做法：In order to pack them into one int, we limit workerCount to* (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2* billion) otherwise representable.定义 ctl 变量 1234private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 32 - 3 = 29private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; 把运行状态保存在高位，其余位数表示线程数。注意 running 状态，ctl 是负数。123456// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;接下来就容易理解了。123456// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 和 CAPACITY 按位与，丢弃了高位（状态位）private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java synchronized 实现原理]]></title>
    <url>%2Fp%2Fjava-synchronized%2F</url>
    <content type="text"><![CDATA[前言 上次讲到 volatile 只保证多线程对共享对象修改的可见性。多线程需要同步的操作，就要使用锁机制。java volatile锁机制可以控制关键区域代码的访问。只有获得锁，才能执行关键区域代码。synchronized 是 java 内置的隐式锁机制。synchronizedsynchronized 可以作用于方法级别，或者代码块级别。synchronized 用于方法：静态方法 非静态方法 synchronized 用于代码块：synchronized(this)、synchronized(object)synchronized(XXX.class) 不管是用在方法、还是代码块，本质是使用的锁不同。ps. synchronized 关键字不能继承。java 的锁 在了解 synchronized 实现机制之前，先了解 java 的锁。内置锁 / 监视器锁 每个 java 对象的实例，都有一个关联的锁，称为监视器锁 (monitor lock) ，又叫内置锁（intrinsic lock） 。 类锁 每一个类都有一个对应的 Class 对象，这个 Class 对象的锁叫类锁。和 synchronized 的关系 synchronized 修饰静态方法，使用的是类锁。synchronized 修饰非静态方法，使用的是实例的监视器锁。synchronized(this) 使用的是实例的监视器锁。synchronized(XXX.class)使用的是 XXX 对象的类锁。对象锁和类锁是相互独立 。静态方法和非静态方法同时执行，会各自竞争需要的锁。synchronized 的底层实现 为了探究 synchronized 的底层实现，首先写个简单的类。123456789101112public class TestSynchronized &#123; public synchronized int test(int x) &#123; return x + 1; &#125; public int test2(int x) &#123; synchronized (this) &#123; return x + 1; &#125; &#125;&#125;然后使用 javap 查看反编译的字节码 12345678910111213141516171819202122232425262728293031323334&gt;javap -c TestSynchronized.classCompiled from &quot;TestSynchronized.java&quot;public class com.example.threads.TestSynchronized &#123; public com.example.threads.TestSynchronized(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public synchronized int test(int); Code: 0: iload_1 1: iconst_1 2: iadd 3: ireturn public int test2(int); Code: 0: aload_0 1: dup 2: astore_2 3: monitorenter 4: iload_1 5: iconst_1 6: iadd 7: aload_2 8: monitorexit 9: ireturn 10: astore_3 11: aload_2 12: monitorexit 13: aload_3 14: athrow&#125;synchronized 修饰方法，字节码并没有特别指令。synchronized 修饰代码块，则会在进入和离开 synchronized 块处插入monitorenter、monitorexit。synchronized 修饰代码块 关于 monitorenter、monitorexit 的具体介绍，参考 monitorenter/monitorexit。 简单来说，monitorenter、monitorexit是获得、释放 objref 关联的监视器（monitor）。之前提到，获取监视器锁的实例，才能执行关键区域的代码。Monitor 是依赖于底层的操作系统的 Mutex Lock（互斥锁）来实现的线程同步。因此是一个重量级的锁。在 java6 之前，性能比较低。java6 对 synchronized 做了优化，把锁分为 4 种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。synchronized 修饰方法 这次使用 javap -verbose 再次分析 123456789public synchronized int test(int); descriptor: (I)I flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=2, args_size=2 0: iload_1 1: iconst_1 2: iadd 3: ireturn 发现 synchronized 方法，flags 多了 ACC_SYNCHRONIZED。 在 Class 文件的方法表中将该方法的 access_flags 字段中的 synchronized 标志位置 1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的 Class 在 JVM 的内部对象表示 Klass 做为锁对象。java 对象头，锁状态 java 对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。Mark Word 用于存储对象自身的运行时数据。Klass Point 是对象指向它的类元数据的指针。 这里面核心的是 mark word。Mark word 为了节省存储空间，会根据 state 的值重用之前的空间。锁状态 存储内容 存储内容 无锁 对象的 hashCode、对象分代年龄、是否是偏向锁（0）01偏向锁 偏向线程 ID、偏向时间戳、对象分代年龄、是否是偏向锁（1）01轻量级锁 指向栈中锁记录的指针 00 重量级锁 指向互斥量（重量级锁）的指针 10 关于锁的状态，强烈推荐阅读这篇文章：不可不说的 Java“锁”事 。现在简单摘录主要内容。 无锁 无锁不会锁定资源。线程在循环内尝试修改共享资源，如果没有冲突，则修改成功，否则修改失败、在下一次循环继续尝试。CAS 操作是典型的无锁方式。偏向锁 一段代码一直被一个线程访问，那么该线程自动获取锁。这样不存在锁的竞争。线程只需要在获得偏向锁的时候，使用 CAS 操作，更新 thread id，无需要在进入、离开关键区域再进行 CAS 操作。偏向锁优化了无多线程竞争的情况，减少不必要的获取轻量级锁。获得偏向锁的线程，不会主动释放锁。只会在其他线程竞争获取锁的时候，才会释放锁。偏向锁 java6 以后默认是打开的，由 jvm 参数 XX:+UseBiasedLocking 控制。轻量级锁 当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。重量级锁 升级为重量级锁时，锁标志的状态值变为“10”，此时 Mark Word 中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。4 种锁状态的小结 偏向锁通过对比 Mark Word 解决加锁问题，避免执行 CAS 操作。轻量级锁是通过用 CAS 操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。其他锁优化措施 锁粗化（Lock Coarsening）：也就是减少不必要的紧连在一起的 unlock，lock 操作，将多个连续的锁扩展成一个范围更大的锁。锁消除（Lock Elimination）：通过运行时 JIT 编译器的逃逸分析来消除一些没有在当前同步块以外被其他线程共享的数据的锁保护，通过逃逸分析也可以在线程本地 Stack 上进行对象空间的分配（同时还可以减少 Heap 上的垃圾收集开销）。自适应自旋锁（Adaptive Spinning）：根据 jvm 运行时的统计，调整自旋的等待时间。参考 [Class 文件结构]3——方法表、属性表out-ofmemoryerror-what-is-the-cost-of-java-objects 不可不说的 Java“锁”事]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java volatile]]></title>
    <url>%2Fp%2Fjava-volatile%2F</url>
    <content type="text"><![CDATA[上次简单讲了 Java 内存模型，有了铺垫，这次可以聊聊 volatile 关键字。聊聊 java 内存模型 线程安全问题 原子性 多个操作，要么全部执行，要么都不执行。数据库事务的原子性很常见。多线程操作也有原子性的要求。可见性 一个线程修改了共享变量，结果能够立即被其他线程看到。回忆 java 内存模型文章，现代多核 cpu，每个核心有独立的寄存器、缓存，并且共享内存。当一个线程修改了缓存、但结果没有回写到内存，就有可能导致另一个核心上线程使用了该核心上旧缓存数据，需要一个机制通知数据更新。有序性 编译器、cpu 能够对代码重排序，提高执行性能。如果不做限制多线程的重排序，就有可能导致错误的执行结果。JMM 定义了 as-if-serial 语义，对重排序做了约束。volatile 和使用场景happens-before 规则有一条：volatile 变量规则：被 volatile 修饰的变量，写操作 happens-before 后续的读操作volatile 保证共享变量的可见性。被 volatile 修饰的变量，不会写入到 cpu cache，只会存储在内存，每次访问该变量，都直接从内存读取， 解决多线程对共享变量可见性的问题 。 因为 volatile 只提供可见性支持，使用 volatile 的前提是：对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 volatile 最适合应用在简单的标记。1234567private volatile boolean hasInit = false;public void init()&#123; if(!hasInit)&#123; hasInit = true; // do sth else &#125;&#125;hasInit 变量的写操作不依赖于当前 hashInit 的值。volatile 底层实现 cpu 指令中，有 load、store 指令。load：将内存数据复制到 cpu 的缓存。store：将 cpu 缓存的数据刷新到内存中。 有专门的 load store unit，负责 load/store 指令的处理。在此基础上，有 2 种内存屏障：Load Barrier 和 Store Barrier。内存屏障的作用 禁止内存屏障两侧的指令重排序 强制把写缓冲区的脏数据写回主内存 (dirty cache pages write back to memory)，使得其他核心对应的缓存行失效 根据 load、store 操作组合的不同，java 有 4 种内存屏障。LoadLoad：该屏障确保 Load1 数据的装载先于 Load2 及其后所有装载指令的的操作 123Load1;LoadLoad;Load2StoreStore：该屏障确保 Store1 立刻刷新数据到内存(使其对其他处理器可见) 的操作先于 Store2 及其后所有存储指令的操作 123Store1;StoreStore;Store2LoadStore：确保 Load1 的数据装载先于 Store2 及其后所有的存储指令刷新数据到内存的操作123Load1;LoadStore;Store2StoreLoad：该屏障确保 Store1 立刻刷新数据到内存的操作先于 Load2 及其后所有装载装载指令的操作。它会使该屏障之前的所有内存访问指令(存储指令和访问指令) 完成之后, 才执行该屏障之后的内存访问指令 123Store1;StoreLoad;Load2StoreLoad 是最强的屏障语义，开销也是最大。不同架构的 cpu，一般都会支持 StoreLoad。volatile 在变量读、写之前分别插入内存屏障，禁止重排序。普通变量、volatile 变量的操作顺序和插入内存屏障关系如下（图片来源：https://stackoverflow.com/questions/51700223/a-puzzle-on-how-java-implement-volatile-in-new-memory-model-jsr-133）volatile 和 synchronized 完整的线程安全，要包括原子性、可见性、有序性。volatile 只解决了可见性。需要完整线程安全的场景，应该用 synchronized。volatile 底层指令实现比 synchronized 简单。但是由于不使用 cpu 缓存，变量的访问要慢一些。小结 volatile 解决多线程对共享变量 可见性 的问题。volatile 不能替代 synchronized 实现锁的能力。参考 正确使用 Volatile 变量 一文解决内存屏障]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>多线程</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊 java 内存模型]]></title>
    <url>%2Fp%2Fjmm-java-memory-model%2F</url>
    <content type="text"><![CDATA[硬件架构 存储器的速度：缓存 &gt; 内存。容量：缓存 &lt; 内存。价格：缓存 &gt; 内存。现实情况是在速度、容量、价格之间平衡。cpu 要读取数据，数据会经过内存、缓存。如果发生修改，则更新路径缓存、内存。从图上可见，读取、修改数据，要经历多级存储器，如果多线程操作，就有可能产生数据不一致，需要有机制处理线程安全。java 内存结构 最简化的 java 内存结构，是分为堆内存（heap）和栈内存（stack）。stack 是每个线程独占内存，用于存放本地变量和引用。heap 是各个线程共享的内存，创建对象将存放在这个区域。下面这张图更为细致的展现了 java 内存结构 Local variable 是本地变量，可以是原始类型 (boolean, byte, short, char, int, long, float, double)，也可以是指向堆中对象的引用。java 内存模型和硬件架构的鸿沟 硬件架构不会区分 java 的 stack 和 heap。Java 内存模型中 stack、heap 数据也可能出现在硬件的寄存器、缓存、内存。数据跨越多个区域存储，就有可能产生问题：共享变量被多个线程修改后的可见性 读、检查和写共享变量的竞争条件 可见性 考虑这样的情况，2 个线程运行在 2 个 cpu 核心上，一开始都读取了 obj.count，并且缓存到各自的 cpu 的 cache。那么其中一个 cpu 修改了缓存中obj.count 的值，在没有回写到内存之前，另一个 cpu 缓存中 obj.count 的值依然是旧的。Java 解决可见性问题，使用 volatile 关键字。竞争条件 多个线程对共享变量修改，会出现竞争条件 (race condition)。 竞争条件指多个线程或者进程在读写一个共享数据时结果依赖于它们执行的相对时间的情形。2 个线程同时对 obj.count 进行加 1 操作。线程 A 在缓存中更新了 obj.count，此时还没有回写到内存，如果这时候线程 B 也进行加 1 操作，它不知道obj.count 已经发生变化。导致 2 个线程操作的最终结果是 2，并非期望的 3。Java 解决竞争条件，是对共享变量的读写操作使用同步或者锁的机制，即 synchronized 或者各种 Lock 的变种。重排序 为了优化代码执行，计算机实际执行指令的顺序，可能和源码中指令顺序不一致。1234x=1y=2z=3x=x+1一个可能的执行顺序是 1234x=1z=3y=2x=x+1 重排序可能发生在编译阶段、处理器执行阶段。Java 使用了 as-if-serial 语义，限制某些场景的重排序。happens-before 和 as-if-serial 语义 因为 jvm 内存结构和硬件架构不一致，会产生并发线程对共享变量的可见性和竞争条件问题，所以 JVM 规范描述了 java 内存模型，解决多线程对共享变量的读取、修改问题。JMM 定义了 happens-before 规则，保证一个线程的操作对另一个线程是可见的（Happens-before relationship is a guarantee that action performed by one thread is visible to another action in different thread.）。happens-before 有多条规则，核心的有：单个线程规则：单个线程内按照代码顺序，写在前面的 happens-before 后面的 监视器锁规则：unlock 监视器锁的操作，happens-before 后续的获取监视器操作 volatile 变量规则：被 volatile 修饰的变量，写操作 happens-before 后续的读操作 传递性：A happens-before B，B happens-before C，那么 A happens-before C对于重排序的问题，JMM 提出了 as-if-serial 语义：不管怎么重排序，单线程下的执行结果不能被改变。 参考Java Memory ModelJSR-133 Java Memory Model and Thread Specification 1.0 Proposed Final DraftJava - Understanding Happens-before relationship]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html 链接 rel 属性介绍：external nofollow noreferer noopener]]></title>
    <url>%2Fp%2Fhtml-link-rel-external-nofollow-noreferer-noopener%2F</url>
    <content type="text"><![CDATA[背景 rel 字段用于描述链接和当前 URL 的关系。 学习 seo 的时候，发现链接的 rel 字段有几个属性，记录下来。external告诉爬虫这是一个外部链接。标准的 html doctype 写法。效果等同于 target=&quot;_blank&quot;。nofollownofollow 告诉爬虫不要跟踪这个链接，不会分享 pagerank 权重。最初由 google 提出，用于解决垃圾链接。比如热门站点留言区发一条留言指向自己的网站（从热门站点产生了外链到不知名站点）。noreferernoreferer告诉浏览器，打开该链接时不发送当前站点路径作为 referer 字段。在 html 规范中，referer 字段用于来源跟踪。因此，使用 noreferer 能够提高打开外链时的隐私安全。noopener在新窗口打开链接，通常的写法 1&lt;a href="example.com" target="_blank"&gt; 然而，这样有很大的安全和性能隐患！引用 google developers 上的文章：网站使用 rel=”noopener” 打开外部锚 当您的页面链接至使用 target=”_blank” 的另一个页面时，新页面将与您的页面在同一个进程上运行。 如果新页面正在执行开销极大的 JavaScript，您的页面性能可能会受影响。此外，target=”_blank” 也是一个安全漏洞。新的页面可以通过 window.opener 访问您的窗口对象，并且它可以使用 window.opener.location = newURL 将您的页面导航至不同的网址。这篇文章讲到 noopener 的好处：The performance benefits of rel=noopenerHowever, due to the synchronous cross-window access the DOM gives us via window.opener, windows launched via target=”_blank” end up in the same process &amp; thread. The same is true for iframes and windows opened via window.open.rel=”noopener” prevents window.opener, so there’s no cross-window access. Chromium browsers optimise for this and open the new page in its own process.noopener使得外部链接在独立的进程中打开。新窗口 window.opener 会为 null。实践 使用 hexo-autonofollow 插件 1npm install hexo-autonofollow --save 也有另一个插件 hexo-nofollow，但是截至 2019.8.15，这个插件的 html 转义有问题，如果结合 neat-html 就会报错。因此使用hexo-autonofollow。 修改项目的 _config.yml12345nofollow: enable: true exclude: - 'ycwu314.github.io' - 'ycwu314.gitee.io' 查看生成的链接 1&lt;a href="https://developers.google.com/web/tools/lighthouse/audits/noopener" rel="external nofollow noopener noreferrer" target="_blank"&gt; 网站使用 rel=”noopener” 打开外部锚&lt;/a&gt;]]></content>
      <categories>
        <category>html</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>html</tag>
        <tag>网路安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊 sleep(0) 和 Thread.yield()]]></title>
    <url>%2Fp%2Fjava-sleep-0-and-yield%2F</url>
    <content type="text"><![CDATA[看到一个问题“sleep(0)的作用是什么”，发现底层牵涉还挺多内容。以下基于 openjdk8 的源码，hg id 87ee5ee27509。回顾线程状态状态机图：java sleep 和 yield 的源码 Thread.sleep() 实现：/src/share/vm/prims/jvm.cpp#l3038123456789101112131415if (millis == 0) &#123; // When ConvertSleepToYield is on, this matches the classic VM implementation of // JVM_Sleep. Critical for similar threading behaviour (Win32) // It appears that in certain GUI contexts, it may be beneficial to do a short sleep // for SOLARIS if (ConvertSleepToYield) &#123; os::yield(); &#125; else &#123; ThreadState old_state = thread-&gt;osthread()-&gt;get_state(); thread-&gt;osthread()-&gt;set_state(SLEEPING); os::sleep(thread, MinSleepInterval, false); thread-&gt;osthread()-&gt;set_state(old_state); &#125;&#125;// more codeThread.yield()实现：/src/share/vm/prims/jvm.cpp#l300212345678910111213141516JVM_ENTRY(void, JVM_Yield(JNIEnv *env, jclass threadClass)) JVMWrapper("JVM_Yield"); if (os::dont_yield()) return;#ifndef USDT2 HS_DTRACE_PROBE0(hotspot, thread__yield);#else /* USDT2 */ HOTSPOT_THREAD_YIELD();#endif /* USDT2 */ // When ConvertYieldToSleep is off (default), this matches the classic VM use of yield. // Critical for similar threading behaviour if (ConvertYieldToSleep) &#123; os::sleep(thread, MinSleepInterval, false); &#125; else &#123; os::yield(); &#125;JVM_ENDsleep(0)和 yield 的底层实现行为，由变量 ConvertSleepToYield 和ConvertYieldToSleep控制。并且使用平台相关的 os::yield() 或者 os::sleep()。linxu 平台os::sleepos::sleep 源码：/src/os/linux/vm/os_linux.cpp#l3792原码 100 来行，不贴了，简单说下：先判断是否支持中断 interruptible。 以可中断分支为例，退出条件是：被中断 os::is_interrupted(thread, true)，或者时间到millis &lt;= 0。否则通过ParkEvent.park(millis) 阻塞等待 millis 之后返回，进入下一次循环。1234567891011121314151617181920 ParkEvent * const slp = thread-&gt;_SleepEvent ;// more code prevtime = newtime; &#123; assert(thread-&gt;is_Java_thread(), "sanity check"); JavaThread *jt = (JavaThread *) thread; ThreadBlockInVM tbivm(jt); OSThreadWaitState osts(jt-&gt;osthread(), false /* not Object.wait() */); jt-&gt;set_suspend_equivalent(); // cleared by handle_special_suspend_equivalent_condition() or // java_suspend_self() via check_and_wait_while_suspended() slp-&gt;park(millis); // were we externally suspended while we were waiting? jt-&gt;check_and_wait_while_suspended(); &#125;以后再对 ParkEvent 做介绍，此处不展开。os::yieldos::yield实现同样在 os_linux.cpp12345678int os::naked_sleep() &#123; // %% make the sleep time an integer flag. for now use 1 millisec. return os::sleep(Thread::current(), 1, false);&#125;void os::yield() &#123; sched_yield();&#125;找到 SCHED_YIELDsched_yield() causes the calling thread to relinquish the CPU. The thread is moved to the end of the queue for its static priority and a new thread gets to run. 小结 linux 平台上：java sleep 最终使用 ParkEvent.park(mills)java yield 使用 sched_yield()windows 平台 源码连接： /src/os/windows/vm/os_windows.cpp比较有意思的是 Windows 系统的 yield 和 sleep 的实现。sleep见 Sleep functionA value of zero causes the thread to relinquish the remainder of its time slice to any other thread that is ready to run. If there are no other threads ready to run, the function returns immediately, and the thread continues execution.Windows XP: A value of zero causes the thread to relinquish the remainder of its time slice to any other thread of equal priority that is ready to run. If there are no other threads of equal priority ready to run, the function returns immediately, and the thread continues execution. This behavior changed starting with Windows Server 2003.sleep(0) 会使该 thread 放弃拥有的剩余时间片。yield.net 4.0 版本以后增加了 Thread.Yield MethodIf this method succeeds, the rest of the thread’s current time slice is yielded. The operating system schedules the calling thread for another time slice, according to its priority and the status of other threads that are available to run.Yielding is limited to the processor that is executing the calling thread. The operating system will not switch execution to another processor, even if that processor is idle or is running a thread of lower priority. If there are no other threads that are ready to execute on the current processor, the operating system does not yield execution, and this method returns false.This method is equivalent to using platform invoke to call the native Win32 SwitchToThread function. You should call the Yield method instead of using platform invoke, because platform invoke bypasses any custom threading behavior the host has requested. 注意 yield 的时间片只能被同一个处理的线程使用。小结 Windows 平台：sleep(0) 和 yield()都会尝试释放当前线程剩余的时间片，并且通知调度器让合适的线程使用该时间片。yield 限制是当前处理器的其他线程才能使用剩余的时间片，sleep 没有这个限制。总结 sleep(0) 和 yield 的实现，受具体的 jvm 版本和 os 影响。yield 通常会使用 os::yield() 实现，通知 scheduler 让其他线程使用时间片。sleep(0)则不一定会触发 yield 的语义。要释放时间片，通常用 yield 就好了。参考Java 线程源码解析之 yield 和 sleepThread.Sleep(0) vs Sleep(1) vs Yeild]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java wait() 和 sleep() 的区别]]></title>
    <url>%2Fp%2Fjava-wait-vs-sleep%2F</url>
    <content type="text"><![CDATA[方法所属类 sleep() 是 Thread 类的方法。wait()是 Object 类的方法。调用地方 sleep 可以在任何地方调用。wait 必须在 synchronized 中调用，否则抛出IllegalMonitorStateException123456Object a = new Object();try &#123; a.wait();&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 输出 1234java.lang.IllegalMonitorStateException at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at com.example.threads.TestSleepAndWait.testWait(TestSleepAndWait.java:11) 正确使用 wait：1234567891011121314151617181920@Testpublic void testWait() &#123; Object lock = new Object(); synchronized (lock) &#123; try &#123; lock.wait(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@Testpublic synchronized void testWait2() &#123; try &#123; wait(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;是否释放监视器锁 sleep 不会释放监视器锁。wait 释放监视器锁（因此 wait 必须在 synchronized 中使用）sleep 的 javadoc1234567/** * Causes the currently executing thread to sleep (temporarily cease * execution) for the specified number of milliseconds, subject to * the precision and accuracy of system timers and schedulers. The thread * does not lose ownership of any monitors. */public static native void sleep(long millis) throws InterruptedException;wait 的 javadoc，原文太长做了截断1234567891011/* * The current thread must own this object's monitor. The thread * releases ownership of this monitor and waits until another thread * notifies threads waiting on this object's monitor to wake up * either through a call to the &#123;@code notify&#125; method or the * &#123;@code notifyAll&#125; method. The thread then waits until it can * re-obtain ownership of the monitor and resumes execution.*/public final void wait() throws InterruptedException &#123; wait(0);&#125; 测试 sleep12345678@Testpublic void testSleepForMonitor()&#123; try &#123; Thread.sleep(60000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;jps 找到 pid，再 jstack 打印 thread dump。12345678C:\Users\ycwu&gt;jps33127744 RemoteMavenServer13352 Jps1884 Launcher8476 JUnitStarterC:\Users\ycwu&gt;jstack 8476输出 1234&quot;main&quot; #1 prio=5 os_prio=0 tid=0x00000000029f3000 nid=0x2fdc waiting on condition [0x00000000028ae000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.example.threads.TestSleepAndWait.testSleepForMonitor(TestSleepAndWait.java:42) 测试 wait12345678@Testpublic synchronized void testWaitForMonitor()&#123; try &#123; wait(60000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;jps 找到 pid，再 jstack 打印 thread dump。12345678C:\Users\ycwu&gt;jps13408 JUnitStarter33127744 RemoteMavenServer13220 Launcher13224 JpsC:\Users\ycwu&gt;jstack 13408输出 123456&quot;main&quot; #1 prio=5 os_prio=0 tid=0x0000000002ae3000 nid=0x91c in Object.wait() [0x000000000294e000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x000000076c56cc18&gt; (a com.example.threads.TestSleepAndWait) at com.example.threads.TestSleepAndWait.testWaitForMonitor(TestSleepAndWait.java:50) - locked &lt;0x000000076c56cc18&gt; (a com.example.threads.TestSleepAndWait) 从下往上看，先获取了锁（locked &lt;0x000000076c56cc18&gt;），然后释放并且等待（waiting on &lt;0x000000076c56cc18&gt;）。唤醒wait 能够被 notify 和 notifyAll 唤醒。但是 sleep 不能。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux select epoll poll 简介]]></title>
    <url>%2Fp%2Flinux-select-epoll-poll%2F</url>
    <content type="text"><![CDATA[在这篇文章简单介绍了 linux IO 模型。io 模型 linux 的 select、poll、epoll 函数是 IO 多路复用的基础。这次聊聊这几个函数。 内容来自以前的有道笔记，当时没有记录参考文章。select12345678910typedef struct &#123;#ifdef __USE_XOPEN __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];#define __FDS_BITS(set) ((set)-&gt;fds_bits)# else __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS];#define __FDS_BITS(set) ((set)-&gt;__fds_bits)#endif &#125; fd_set ;(一个 fd 占用一个 bit)由数组 fds_bits[FD_SETSIZE / __NFDBITS]的定义可以看出，它将数组 fds_bits 的长度从通常的 FD_SETSIZE 缩短到了（FD_SETSIZE / __NFDBITS），数组的元素的每个位表示一个描述符，那么一个元素就可以表示NFDBITS 个描述符，整个数组就可以表示（__FD_SETSIZE / __NFDBITS）* __NFDBITS = __FD_SETSIZE 个描述符了。__FDS_BITS 的定义是为了便于直接引用该结构中的 fds_bits，而不用关心内部具体的定义。int select(int maxfd,fd_set *rdset,fd_set *wrset,fd_set *exset,struct timeval *timeout); 参数 maxfd 是需要监视的最大的文件描述符值 +1；rdset,wrset,exset 分别对应于需要检测的可读文件描述符的集合，可写文件描述符的集合及异常文件描述符的集合。struct timeval 结构用于描述一段时间长度，如果在这个时间内，需要监视的描述符没有事件发生则函数返回，返回值为 0。基于 数组 的实现。优点：posix 定义的，可移植性好。适用于少量连接。缺点：单个进程可以监控的 fd 数量限制 FD_SETSIZE。cat /proc/sys/fs/file-max 察看。32 位机默认是 1024 个。64 位机默认是 2048.对 socket 扫描是线性的，要扫描 fd_size 个，不管 socket 是否就绪。epoll，kqueue 改进了此处，使用回调函数，不盲目扫描 socket 浪费 cpu 时间 维护一个重型的 fd 数据结构，并且从内核空间拷贝到用户空间。1234/**********************************************************//* Copy the master fd_set over to the working fd_set. *//**********************************************************/memcpy(&amp;working_set, &amp;master_set, sizeof(master_set));poll12345678struct pollfd&#123; int fd; /* 文件描述符 */ short events; /* 等待的事件 */ short revents; /* 实际发生了的事件 */&#125;;int poll(struct pollfd *ufds, unsigned int nfds, int timeout);poll 函数使用 pollfd 类型的结构来监控一组文件句柄，ufds 是要监控的文件句柄集合，nfds 是监控的文件句柄数量，timeout 是等待的毫秒数，这段时间内无论 I/O 是否准备好，poll 都会返回。timeout 为负数表示无线等待，timeout 为 0 表示调用后立即返回。执行结果：为 0 表示超时前没有任何事件发生；-1 表示失败；成功则返回结构体中 revents 不为 0 的文件描述符个数 优点：使用链表结构。不受 fd 最大数量限制 缺点：检查 socket 还是要遍历整个数据。水平触发：如果这次报告 fd 已经可以操作但是没有处理，那么下次该 fd 还会被报告 。epoll 通常 1g 内存可以打开 10w 个 epoll 连接。int epoll_create(int size);创建一个 epoll 的句柄，size 用来告诉内核需要监听的数目一共有多大。当创建好 epoll 句柄后，它就是会占用一个 fd 值，在 linux 下如果查看 /proc/ 进程 id/fd/，是能够看到这个 fd 的，所以在使用完 epoll 后，必须调用 close() 关闭，否则可能导致 fd 被耗尽。int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);epoll 的事件注册函数，第一个参数是 epoll_create() 的返回值，第二个参数表示动作，使用如下三个宏来表示：123EPOLL_CTL_ADD // 注册新的 fd 到 epfd 中；EPOLL_CTL_MOD // 修改已经注册的 fd 的监听事件；EPOLL_CTL_DEL // 从 epfd 中删除一个 fd；第三个参数是需要监听的 fd，第四个参数是告诉内核需要监听什么事，struct epoll_event 结构如下：123456789101112typedef union epoll_data&#123; void *ptr; int fd; __uint32_t u32; __uint64_t u64;&#125; epoll_data_t;struct epoll_event &#123;__uint32_t events; /* Epoll events */epoll_data_t data; /* User data variable */&#125;;events 可以是以下几个宏的集合：1234567EPOLLIN // 表示对应的文件描述符可以读（包括对端 SOCKET 正常关闭）；EPOLLOUT // 表示对应的文件描述符可以写；EPOLLPRI // 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR // 表示对应的文件描述符发生错误；EPOLLHUP // 表示对应的文件描述符被挂断；EPOLLET // 将 EPOLL 设为边缘触发 (Edge Triggered) 模式，这是相对于水平触发 (Level Triggered) 来说的。EPOLLONESHOT// 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个 socket 的话，需要再次把这个 socket 加入到 EPOLL 队列里。当对方关闭连接 (FIN), EPOLLERR，都可以认为是一种 EPOLLIN 事件，在 read 的时候分别有 0，-1 两个返回值。int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 参数 events 用来从内核得到事件的集合，maxevents 告之内核这个 events 有多大，这个 maxevents 的值不能大于创建 epoll_create() 时的 size，参数 timeout 是超时时间（毫秒，0 会立即返回，-1 将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回 0 表示已超时。epoll 支持两种 fd 扫描方式，水平触发和边缘触发 LT(level triggered，水平触发模式) 是缺省的工作方式，并且同时支持 block 和 non-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。ET(edge-triggered，边缘触发模式)是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，等到下次有新的数据进来的时候才会再次出发就绪事件。ET 模式减少了 epoll 事件被重复触发的问题。但是可能错过事件：如果一直不对这个 fd 作 IO 操作 (从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)。（redis 使用 LT，nginx 使用 ET） 优点：没有描述符数量限制。支持水平触发和边缘触发 使用事件回调处理就绪的 fd，而非遍历 fd。epoll_ctl 首先注册了文件描述符。一旦该 fd 就绪，就由内核激活该 fd，当进程调用 epoll_wait()时便得到通知。select，poll，epoll 对比 selectpollepoll 单个进程打开连接数限制 受 FD_SETSIZE 限制 无限制 无限制 事件检查方式 遍历 fd 数组 同 select由内核调用事件的 callback打开大量 FD 后的性能表现 因为线性遍历检查 fd 数组，所以性能线性下降 同 select只有活跃的 socket 才会触发 epoll。因此性能只受活跃 socket 数量影响 消息传播 从内核空间复制到用户空间 同 select内核与用户空间 mmap 同一块内存，无需复制 事件触发模式 水平触发 水平触发 水平触发 or 边缘触发 pselect, ppoll 解决的问题 在检查 signal 后和调用 select()之前，丢失了 signal。12int pselect(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, const struct timespec *timeout, const sigset_t *sigmask);1234struct timespec &#123; long tv_sec; /* seconds */ long tv_nsec; /* nanoseconds */ &#125;;它和 select() 函数基本相同，区别在于两个不同的参数，一个是 struct timespec *timeout，另一个是 sigset_t *sigmask。和 select() 不同，每次超时后，pselect() 并不会去修改这个时间参数，也就是说，没有必要再次对这个时间参数进行初始化。对于最后一个参数 sigmask 表示信号屏蔽掩码。该参数允许程序先禁止递交某些信号，再测试由这些当前被禁止的信号处理函数设置的全局变量，然后调用 pselect，告诉它重新设置信号掩码。使用 pselect() 函数一个最大的原因正是它可以防止信号的干扰。比如说，如果你只是使用 select() 函数，在超时时间内很可能还会受到时钟信号 (SIGALARM) 的打断，从而影响 select() 的正常使用。1234567891011121314151617181920while (1)&#123; if(need_to_quit) // 1 break; if(select(...) == -1) // 2 &#123; if(errno == EINTR) continue; ... &#125; ...&#125;int need_to_quit = 0; // handler interruptvoid sigquit_handler()&#123; need_to_quit = 1;&#125;假设 need_to_quit 接收 SIGINT 信号。如果在 1）和 2）之间发生 SIGINT，那么会丢失 SIGINT 信号的处理，结果可能一直在 select 调用中阻塞等待。posix 的解决方法：step 1 : you block all signals and save the current sigmaskstep 2 : check the event condition and do what is requiredstep 3 : call pselect() and pass it a signal mask to enable all the signals that would provide you the events. when pselect() returns, it will restore the sigmask it had when it was entered(ie, here all the signals masked).step 4 : you restore the old signal mask1234567pselect(..)&#123; // enable the signals to be received sigprocmask(.., &amp;new_mask, &amp;old_mask); // 1 select(); // 2 sigprocmask(.., &amp;old_mask, null);&#125;pselect 类似上面的代码。但是用户模式下自己定义一个这样的函数，会在 1）和 2）之间产生竞争条件。posix 提供的 pselect 会进入内核模式，不会产生竞争。最终的代码流程如下：123456789101112131415161718192021222324sigset_t new_set, old_set;int ret;sigfillmask(&amp;new_set);sigprocmask(.., null, &amp;old_set);while (1)&#123; sigprocmask(.., &amp;new_set, null); if(need_to_quit) break; ret = pselect(.., &amp;old_set) ; sigprocmask(.., &amp;old_set, null); if(ret == -1) &#123; if(errno == EINTR) continue; ... &#125; ... sigprocmask(.., &amp;old_set, null);&#125;kqueueBSD 系统上独有，类似 epoll。但是更复杂。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>高并发</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reactor 模式]]></title>
    <url>%2Fp%2Freactor-pattern%2F</url>
    <content type="text"><![CDATA[在这篇文章简单介绍了 linux IO 模型。io 模型 其中一种模型是 IO 多路复用，并且产生了Reactor 模式。这次就继续聊聊 Reactor 模式。 阻塞 IO 模型的性能瓶颈 首先回顾下 blocking io 的处理流程 如果 IO 操作不能及时返回，那么整个线程就会被阻塞。1234while(true)&#123; socket = accept(); handle(socket) &#125;在阻塞模式，要提升高并发性能，只能增加线程数，使用 thread per connection 的模式。1234while(true)&#123; socket = accept(); new handler_thread(socket); &#125;可是线程的创建和销毁带来性能损耗。于是使用线程池来管理线程的生命周期，尽量重用线程。但是线程切换上下文的开销是无法避免的。再高的并发就扛不住了。reactor 模式 正因为阻塞 IO 的缺点，演化出非阻塞 IO、IO 多路复用等模型。reactor 模式使用了 IO 多路复用模型。回顾 IO 多路复用流程 IO 多路复用使用 select 函数或者 epoll 函数，这 2 个函数也是阻塞调用，但是一次能够检查多个 IO 操作。 那么，可以只使用一个主线程来检查 IO 状态，如果发现就绪，就交给另一个线程去处理底层的 IO 操作；主线程继续检查其他 IO 操作状态。这就是 reactor 模式的核心思想。典型的 reactor 模式包含以下组件：dispatcher。派分器。注册、移除事件处理器。synchronous event demultiplexer。同步事件分离器。使用 select 之类的方法，阻塞等待资源就绪。一旦资源就绪，通知事件处理器处理。handle。资源句柄。例如 socket。event handler。通用的事件处理器。concret event hanler。具体的某种类型的事件处理器。组件之间的交互如下：时序图如下：3 种 reactor 模式 Reactor 单线程模型 对应 NioEventLoop。所有的 I/O 操作都在同一个 NIO 线程上面完成。这个 NIO 线程，负责了和 client 建立连接，处理读写请求。问题 单个 NIO 线程不可靠。如果该 IO 线程挂了，那么就全挂了。单个 NIO 线程的处理能力有限，即使该 NIO 线程 100% cpu，也没有发挥多核 cpu 的资源。读取、解码、编码、发送跟不上。单个 IO 线程处理过载后，导致新的请求等待 (so_backlog 队列)，然后不能被 accept、不能建立连接，然后又重新发送请求，加剧 NIO 线程负载。Reactor 多线程模型 一个专门的 NIO 线程，负责和 client 建立连接（acceptor 线程）。专用的 NIO 线程池，负责 IO 读写操作。一个 NIO 线程可以处理多个链路，但是一个链路由一个 NIO 线程处理，避免发生竞争问题。 1 个 connection=1 个 channel=1 个 thread（解决并发的线程安全问题）这种模式能够适合大多数场景了。但是在极高的并发下，单个 acceptor 线程成为了瓶颈。尤其是在建立连接的时候执行耗时的操作，比如鉴权。主从 Reactor 多线程模型 对应 netty 的 NioEventLoopGroup。NIO 线程池 +acceptor 线程池。acceptor 线程池，处理客户端连接。特点 acceptor 线程池中的一个线程被指派监听端口。 监听线程接收客户端请求，建立 SocketChannel，并且注册到 acceptor 线程池的其他线程上，完成认证等耗时的操作。认证等操作通过后，该 SocketChannel 从 acceptor 线程上取消注册，再注册到 IO 线程池上完成后续操作。reactor 场景分析 reactor 的事件驱动模型，适合大量并发的、处理时间短的请求。一旦单个请求处理事件长，将会整体导致性能急剧下降。 像文件传输这样的场景就不适合使用 reactor 模式，传统的 thread per connection 更为合适。对比生产者 - 消费者模式 生产者–消费者模式用 queue 去缓冲消息。消费者主动 pull 来获取消息。reactor 模式没有 queue。synchronous demultiplexer 收到 event 后立即分发。小结 Reactor 模式： 适用场景：处理大量小数据量、耗时少的请求 哪些组件（synchronous demultiplexer，handle，dispatcher，event handler）核心：1 个 connection=1 个 channel=1 个 thread，没有锁，顺序执行 优点（少量线程；分离关注点；事件驱动）缺点（io 模型复杂了）多个变种（单 NIO 线程；多 NIO 线程；多 acceptor）参考资料reactor-siemens.pdfjava 使用 netty 的模型总结]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>IO</tag>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[io 模型]]></title>
    <url>%2Fp%2Fio-model%2F</url>
    <content type="text"><![CDATA[用户空间，内核空间，系统调用 CPU 将指令分为特权指令和非特权指令。特权指令可以操作 IO、设置或者忽略中断、设置时钟等操作。由于特权指令可以执行高危操作，一旦被不恰当使用，会导致系统崩溃。 从系统安全和稳定性的角度出发，特权指令只允许被操作系统执行（内核空间），普通用户进程只能执行非特权指令（用户空间）。当进程运行在内核空间时就处于内核态，而进程运行在用户空间时则处于用户态。系统调用（system call）是操作系统提供给应用程序的接口。 用户通过调用系统调用来完成那些需要操作系统内核进行的操作， 例如硬盘， 网络接口设备的读写等。同步，异步 POSIX 对同步、异步的定义A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completesAn asynchronous I/O operation does not cause the requesting process to be blocked 异步 IO 在实现上，通过通知用户进程，或者调用用户进程注册的回调函数。阻塞，非阻塞 阻塞是指 I/O 操作需要彻底完成后才返回到用户空间；非阻塞是指 I/O 操作被调用后立即返回给用户一个状态值 (EWOULDBOLCK)，无需等到 I/O 操作彻底完成。Linux IO 模型 以读操作为例子，主要包含 2 个阶段 Waiting for the data to be ready. This involves waiting for data to arrive on the network. When the packet arrives, it is copied into a buffer within the kernel.Copying the data from the kernel to the process. This means copying the (ready) data from the kernel’s buffer into our application buffer 翻译过来是 等待数据就绪。就绪数据保存在内核空间 把就绪数据从内核空间复制到进程 这 2 个阶段处理上的不同，linux 演化了 5 种 io 模型 blocking I/Ononblocking I/OI/O multiplexing (select and poll)signal driven I/O (SIGIO)asynchronous I/O (the POSIX aio_ functions) 先上对比图，在后面的介绍中可以返回来比较 阻塞 IO，blocking IO步骤 等待数据就绪 把就绪数据从内核空间复制到进程 如果数据未 ready，就一直阻塞。最简单直接的模型。但是一个用户线程只能阻塞一个 IO 操作。伪代码如下 1234while(true)&#123; socket = accept(); handle(socket) &#125; 如果要同时阻塞多个 IO 操作，只能使用多线程。1234while(true)&#123; socket = accept(); new handler_thread(socket); &#125;线程的创建、销毁产生额外的性能损耗。使用线程池可以减少线程创建、销毁的开销。但是太多线程数依然会引起上下文切换开销占比变大，导致高并发性能瓶颈。非阻塞 IO，non blocking IO步骤 socket 设置为 non blocking。当数据未就绪，操作系统不会挂起进程，内核向用户进程返回错误码(EWOULDBLOCK) 尽管用户进程没有被阻塞，但是因为不知道数据是否就绪，只能不断轮询，消耗大量的 cpu 资源 数据就绪，把数据从内核空间复制到进程 在实际中，很少直接使用非阻塞 IO 模型。IO 多路复用，IO multiplexing多路复用使用 select 函数或者 epoll 函数。这 2 个函数也是阻塞调用，但是一次调用能够检查多个 IO 操作的状态 ，一旦有就绪数据，才真正执行底层阻塞 IO 操作。 多路复用最大的特点是，一个用户线程可以注册检查多个 IO 请求。传统的 blocking io 模型，一个用户线程只能检查一个 IO 请求。在高并发的情况下，多路复用能够减少 IO 线程数量，提高了高并发性能。多路复用模型使用了 Reactor 设计模式实现了这一机制。netty 使用了 Channel 这个概念。一个 channel 监听一个 IO 操作。一个用户线程注册多个 Channel，从而关注多个 IO 操作。 最大的好处是，一个线程可以处理多个 IO 操作：一个用户线程不停地 select，获取已经就绪的 Channel。相关资料参见：reactor 模式 linux select epoll poll 简介 信号驱动 IO，signal driven IO通过一个 system call，注册信号 handler，并且立即返回 当数据就绪，由系统发送 SIGIO 信号，触发信号 handler把数据从内核空间复制到进程 异步 IO，asynchronous IOWe call aio_read (the POSIX asynchronous I/O functions begin with aio_ or lio_) and pass the kernel the following:descriptor, buffer pointer, buffer size (the same three arguments for read),file offset (similar to lseek),and how to notify us when the entire operation is complete.This system call returns immediately and our process is not blocked while waiting for the I/O to complete.调用 aio_read 函数，告诉内核 fd，缓冲区指针，缓冲区大小，文件偏移以及通知的方式，然后立即返回。当内核将数据拷贝到缓冲区后，再通知应用程序。异步 I/O 模型使用了 Proactor 设计模式实现了这一机制。对比信号驱动 IO，AIO 由内核自动完成复制数据到用户空间这个操作。TODO：proactor 设计模式 小结 阻塞 IO 一直等待，直到数据就绪 非阻塞 IO 会返回 EWOULDBOLCK 错误码，用户进程要不停检查，消耗 cpu 资源。很少单独使用非阻塞 IO 信号驱动 IO 安装信号 handler 之后立即返回。一旦数据就绪，内核通知用户进程，需要进程自行复制数据。异步 IO 告诉系统 fd、缓冲区地址、复制字节数，由内核完成就绪数据复制到用户空间的动作后，再通知用户进程。非阻塞 IO，信号驱动 IO，异步 IO，这 3 者的明显区别：非阻塞 IO 返回的是状态值，数据未就绪；后两者返回的是就绪数据。参考资料Linux 内核空间与用户空间Chapter 6. I/O Multiplexing: The select and poll Functionsjava Selector is asynchronous or non-blocking architecture]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>高并发</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务治理：服务注册和发现]]></title>
    <url>%2Fp%2Fmicro-service-governance-service-registry-and-discovery%2F</url>
    <content type="text"><![CDATA[微服务架构的系统，由多个以业务边界为划分的服务编排或者协同组成。这些服务在独立进程上运行，服务之间通过进程级别的通信机制（rpc、http）进行交互。这就涉及微服务的服务治理问题，首先要解决的是服务注册和服务发现。需求 最简单的服务注册和发现，需要解决的问题：一个进程可以对外提供服务，怎么让其他进程知道？怎么调用其他进程的服务？这要求有 服务注册中心 ，存储服务列表和路由。 一个服务通常由多个实例进程提供。于是带来新的需求:提供者：实例上线、下线，要通知注册中心 消费者：这次服务调用要使用哪个实例？（负载均衡）注册中心：下发服务变化的通知到消费者 注册中心：检查服务提供者是否可用（心跳检查）当服务提供者和消费者越来越多，对注册中心提出新的需求：性能 高可用 组件 从上面需求来看，服务注册和发现，至少有以下组件。registry center。存储服务列表和路由信息。discovery。服务提供者暴露服务，服务消费者获取服务列表。health check。心跳检查，服务中心检查服务是否可用。load balance。请求在服务实例之间负载均衡。服务治理需要哪些信息 在注册、调用之前，先想想服务治理需要哪些信息：协议。比如 http，dubbo，thrift机房标识。处理跨机房调用，以及同机房优先的策略。服务地址。可以是 ip + host + port 描述，也可以是域名。服务名字。版本。一个服务可以有多个版本共存。服务的元数据：方法名，参数列表，返回结果，抛出异常类型，同步 / 异步响应等 与负载均衡相关的信息，比如超时、限流、权重 服务鉴权。一种常见的实现方式是，使用 URL 来描述服务 123dubbo://192.168.1.100:20880/com.example.HelloServicehttp://demo-service/HelloService?version=1 服务注册、服务发现的流程 以 dubbo 服务注册和发现为例，看看服务注册、服务发现的流程 （图片来源：https://dubbo.apache.org/img/architecture.png） 注册中心启动 服务提供者在启动时，向注册中心注册自己提供的服务。服务消费者在启动时，向注册中心订阅自己所需的服务。注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。客户端发现 vs 服务端发现 服务发现有 2 种模式：客户端发现和服务端发现。上面提到的 dubbo 框架使用的是客户端发现。Consul、AWS ELB 则是服务端发现。客户端发现，client side discovery服务提供者启动后向注册中心登记，服务下线前向注册中心注销。注册中心定时检查服务实例是否在线，并且踢掉异常实例。服务消费者从注册中心获取服务地址列表，直接向服务实例发起请求。（图片来源：https://www.nginx.com/wp-content/uploads/2016/04/Richardson-microservices-part4-2_client-side-pattern.png）服务端发现，server side discovery服务提供者启动后向注册中心登记，服务下线前向注册中心注销。注册中心定时检查服务实例是否在线，并且踢掉异常实例。服务消费者发起的服务调用请求，经过服务路由器、或者负载均衡器处理，由它们去访问注册中心，并且找到服务实例。（图片来源：https://www.nginx.com/wp-content/uploads/2016/04/Richardson-microservices-part4-3_server-side-pattern.png）客户端发现对比服务端发现：服务请求路径：client side 比 server side 少了一次网络跳转，效率更高。服务发现逻辑：client side 模式在服务消费者端实现。对于不同编程语言的服务，要有不同的客户端框架，否则可能无法通信（rpc）。server side 由路由器 / 负载均衡器完成，简化了客户端依赖，但是增加了服务路由器，对基础建设要求更高。AP or CP分布式系统逃不开 CAP 理论。以下摘自 wiki一致性（Consistency） （等同于所有节点访问同一份最新的数据副本）可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。）（图片来源：https://www.researchgate.net/figure/Visualization-of-CAP-theorem_fig2_282679529）因为网络分区无法避免，因此实际上是 CP 或者 AP 的选择。Eureka 是 AP 设计。节点之间的服务注册信息不做强一致性同步，依赖最终一致性实现节点间注册信息同步。也有基于 zookeeper 设计的注册中心。由于 zookeeper 是强一致性的设计，因此对应的服务中心是 CP 类型。那么注册中心应该使用 AP 还是 CP？这就见仁见智。个人认为，对于服务注册这种类型，同一机房使用 AP 更为合适。服务的优雅上线、优雅下线 服务的优雅上线实现比较简单，当服务提供者初始化完毕后，再向服务注册中心注册。实现服务治理框架可以提供接口，判断是否已经完成注册。例如 Thrift 框架的 Server.isServing()。 也可以自行对外提供 http 接口，查询是否已经完成注册。 服务的优雅下线则相对复杂。如果是正常的主动下线，可以先从注册中心下线，处理完现有请求后，再进行服务自身的下线操作。考虑非正常的下线情况。进程被杀掉。如果是 kill -15，则会触发应用的关闭钩子，例如 java 的addShutdownHook()。只要在 shutdown hook 执行下线操作，也是可以顺利主动下线。 如果是 kill -9，则不会触发 shutdown hook。此时要依赖注册中心进行 health check，或者消费者上报调用失败统计，再对该服务实例下线。 消费者接收服务变更通知 注册中心 push 方式通知消费者，更新及时。但是如果网络瞬间故障，会丢失更新信息。消费者 pull 方式主动向注册中心拉取更新，即使单次访问失败，最终也会保持一致。缺点是效率低，接收更新不及时。因此实际上是推拉结合方式。服务的心跳检查 通过心跳检查，剔除不在线的服务，从而维护可用服务列表。服务提供者和注册中心建立 health check 机制，服务提供者定时发送心跳信息。注册中心的定时任务检查超时未上报服务实例，再根据心跳策略决定是否对失联服务实例下线。如果服务实例规模相当大，那么心跳机制也会对注册中心产生流量压力。可以考虑其他的通信机制，例如 gossip。注册中心的性能 以下内容复制自“聊聊微服务的服务注册与发现”对于那些采用了类 Paxos 协议的强一致性的组件，如 ZooKeeper，由于每次写操作需要过半的节点确认。水平扩容不能提升整个集群的写性能，只能提升整个集群的读性能。而对于采用最终一致性的组件来说，水平扩容可以同时提升整个集群的写性能和读性能。注册中心故障 注册中心故障后，新的服务不能注册，旧的服务无法及时下线、增加或者减少服务实例。对于消费者，通常会缓存服务路由信息（内存、本地文件）注册中心故障，不会影响已有消费者。参考资料 聊聊微服务的服务注册与发现 service-discovery-using-etcd-consul-and-kubernetesService Discovery in a Microservices Architecture 谈服务发现的背景、架构以及落地方案 微服务架构中基于 DNS 的服务发现]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>服务治理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[travis ci 跳过触发构建]]></title>
    <url>%2Fp%2Ftravis-ci-skip-trigger-some-build%2F</url>
    <content type="text"><![CDATA[问题背景 自从博客使用 travis ci 做构建和部署之后，更新博客非常方便。不过带来一个问题，那就是每次提交都触发了构建，但是有的提交并不是我想触发更新：因为要切换电脑使用，本地内容提交到 github或者想 hexo new draft，暂时保存为草稿，这也不需要触发 travis ci 构建 只是改很小的地方，没必要触发构建，在以后一起部署就好，不浪费公共资源 解决 解决问题的思路是，通过在构建之前检查 git commit message，自己写脚本判断是否继续构建。或者 travis ci 自己可以自动识别特定 git commit message，不做构建。TRAVIS_COMMIT_MESSAGE如果要手动判断 git commit message，首先要找到这个消息。查阅 官网资料 ，travis ci 提供了内置的环境变量支持TRAVIS_COMMIT_MESSAGE: The commit subject and body, unwrapped. 自定义的检查脚本大概是 1if [[$TRAVIS_COMMIT_MESSAGE != "skip" ]]; then hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ; fi ;[skip &lt;keyword&gt;] 事实上跳过构建是一个很通用的需求，travis ci 提供了内置支持。参见 Skipping-a-buildThe command should be one of the following forms:[skip]or[skip] whereis either ci, travis, travis ci, travis-ci, or travisci. For example,[skip travis] Update README只要 git commit message 是以 [skip &lt;KEYWORD&gt;] 形式开头，travis 可以自动跳过构建。推荐使用 [skip &lt;KEYWORD&gt;]，太方便了。 实践 12345678910111213141516171819202122232425262728C:\workspace\ycwu314.github.io\source\_posts (hexo -&gt; origin)λ git add .warning: LF will be replaced by CRLF in source/_posts/travis-deploy-both-github-and-gitee.md.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in source/_posts/travis-ci-igonre-trigger-some-build.md.The file will have its original line endings in your working directoryC:\workspace\ycwu314.github.io\source\_posts (hexo -&gt; origin)λ git commit -m &quot;[skip ci] test skip ci&quot;[hexo 68cb27a] [skip ci] test skip ci 2 files changed, 46 insertions(+), 2 deletions(-) create mode 100644 source/_posts/travis-ci-igonre-trigger-some-build.mdC:\workspace\ycwu314.github.io\source\_posts (hexo -&gt; origin)λ git push origin hexoEnumerating objects: 10, done.Counting objects: 100% (10/10), done.Delta compression using up to 8 threadsCompressing objects: 100% (6/6), done.Writing objects: 100% (6/6), 1.73 KiB | 1.73 MiB/s, done.Total 6 (delta 3), reused 0 (delta 0)remote: Resolving deltas: 100% (3/3), completed with 3 local objects.remote:remote: GitHub found 6 vulnerabilities on ycwu314/ycwu314.github.io&apos;s default branch (1 high, 5 moderate). To find out more, visit:remote: https://github.com/ycwu314/ycwu314.github.io/network/alertsremote:To github.com:ycwu314/ycwu314.github.io.git 9c5e76a..68cb27a hexo -&gt; hexo 然后上 travis 观察，嗯，nothing happens.]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>devops</tag>
        <tag>travis ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置多个 TLS 证书，以及 TLS SNI 简介]]></title>
    <url>%2Fp%2Fhttps-sni-nginx-config%2F</url>
    <content type="text"><![CDATA[背景 原来申请的正式域名备案通过，TLS 证书也申请了。之前使用的临时域名和证书作为测试环境使用。于是要在单个 ECS 主机上配置 nginx 多个证书和多个域名。实践 nginx 部署多个 TLS 证书很简单，在不同的 virtual host 分别配置证书就搞定了。比如我有a.com 和b.com两个域名，在 nginx.conf 分别配置 2 个 server 就可以了 123456789101112131415161718192021server &#123; listen 443 ssl http2; server_tokens off; server_name a.com; ssl_certificate cert/a.com.pem; # 将 domain name.pem 替换成您证书的文件名。 ssl_certificate_key cert/a.com.key; # 将 domain name.key 替换成您证书的密钥文件名。 # more config&#125;server &#123; listen 443 ssl http2; server_tokens off; server_name b.com; ssl_certificate cert/b.com.pem; # 将 domain name.pem 替换成您证书的文件名。 ssl_certificate_key cert/b.com.key; # 将 domain name.key 替换成您证书的密钥文件名。 # more config&#125;ps. 推荐一个 nginx conf 配置在线美化工具：Nginx Beautifier 问题再深入 在一个主机（IP）上配置多个域名，使用虚拟主机（virtual host）就可以解决了。但是申请 TLS 证书的时候，是绑定了具体的域名和 IP 地址。建立 TLS 连接的时候，服务器要下发哪个证书呢？等等，证书明明和域名绑定，都知道了域名，为什么会有选择选择证书的问题？那是因为，域名是 http 的概念。client 和 server 先建立了 tcp 连接，再经过 TLS 握手，才能实现 https 通信。在最初，这个过程，是没有域名的概念的！为了解决当个主机部署多个 TLS 证书的问题，带来了 TLS 的 SNI 扩展。（图片来源：https://blogs.akamai.com/2017/03/reaching-toward-universal-tls-sni.html）SNI 介绍 摘自 wiki，”服务器名称指示”服务器名称指示（英语：Server Name Indication，缩写：SNI）是 TLS 的一个扩展协议，在该协议下，在握手过程开始时客户端告诉它正在连接的服务器要连接的主机名称。这允许服务器在相同的 IP 地址和 TCP 端口号上呈现多个证书，并且因此允许在相同的 IP 地址上提供多个安全（HTTPS）网站（或其他任何基于 TLS 的服务），而不需要所有这些站点使用相同的证书。TLS SNI 在 client hello 握手阶段，增加了一个扩展字段，表明想要和哪个域名建立 TLS 连接（注意是 明文 ）。服务器根据 SNI，选择证书并且下发。以下是 TLS v1.2 握手抓包 这个扩展字段，OpenSSL 0.9.8 版本开始支持。现代的浏览器和服务器都支持了。TLS v1.2 的 SNI 是明文传输，这就可以被第三方直接截获，暴露了域名，甚至阻止建立 https 连接（等等，有人在敲门，我先收个快递）。。。这就是 censorship 问题了。TLS ESNI 扩展 TLS v1.2 的 SNI 是明文传输引发安全隐患，那么加密不就可以了吗？不行，因为 TLS 连接还没有建立。这就是先有鸡还是先有蛋的问题。google、amazon、microsoft 等大厂想出一个办法，既然 SNI 是明文，我就传输无公害的 host，比如他们自家的域名，等 tls 连接建立之后，再把真实域名传输过去。 这就是 TLS ESNI 扩展，使用的技术叫 域前置（英语：Domain fronting）。如果要 censorship，那么就要一刀切都关闭！怎么样，是不是很狂拽酷炫吊炸天！不过，理想很丰满，现实很骨感。截至目前，Google、Amazon 都由于不可描述的原因，关闭了此项服务。在此只聊聊技术上的问题。即使有了 TLS v1.3 ESNI，在建立连接后返回真实域名，但是传统的 DNS 查询是明文的！解决方式就是加密 DNS，DNS over TLS (DoT) and DNS over HTTPS (DoH)。具体以后再研究。小结 单个主机上 nginx 多个证书的配置，在不同的 server 上配置 ssl_certificate 和ssl_certificate_key即可。背后的原理涉及到 TLS 的 SNI 扩展。TLS v1.2 的 SNI 扩展字段是明文的，并且由于在 client hello 阶段传输，会被第三方直接获取，带来安全隐患。TLS v1.3 引入 ESNI 扩展，技术上叫 domian fronting，使用安全的 host 建立 TLS 连接，之后再传输目标域名。结合加密 DNS，实现安全通信。参考资料Encrypt it or lose it: how encrypted SNI works]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 wireshark 抓包 tls 1.2 握手]]></title>
    <url>%2Fp%2Ftls-handshake-wireshark-capture%2F</url>
    <content type="text"><![CDATA[前言 上次聊了 tls 1.2 握手流程，内容比较多，缺乏具体的例子 https tls v1.2 握手过程 这次以访问 baidu 首页为例，使用 wireshark 抓包，对其中的步骤进行观察。无图无真相，先上图，点击放大。查询 dns为了方便，关闭了浏览器，使用 curl 发起网络请求。1curl https://www.baidu.comseq=9，发起了 dns 查询。seq=10，dns 服务器返回了 baidu.com 的 ip 地址。tcp 三次握手 接下来 3 个数据包是熟悉的建立 tcp 连接三次握手。seq=11，本机向 baidu 服务器发起 tcp 请求。发送的 syn 包。seq=12，baidu 服务器向本机回应 syn ack 包。seq=13，本机发送 ack，正式建立 tcp 连接。client hello从第 14 个包开始，正式进入 tls 握手阶段。留意 protocol=TLSv1.2。核心的字段是 TLS 版本，client random，客户端支持的 cipher suites。还有其他扩展字段，比如之前提到的 SNI，签名算法等。server hello第 16 个包开始是 server hello 阶段。首先发送 server random，以及协商使用的 cipher suite1Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256的意义是:TLS：SSL/TLS 协议 ECDHE：key exchange 算法RSA：authentication 算法AES_128_GCM：encryptation 算法SHA256： MAC 算法，用于创建消息摘要。后面的 encrypted handshake message 阶段用到。 第 20 个包发送了服务器证书。注意这里是 2 个证书，是 baidu 的证书，以及签发 baidu 的上级机构的证书 打开 baidu 的证书。可以看到几个关键信息：这个证书的制作，sha256WithRSAEncryption，sha256 是验签方式，使用的非对称加密方式是 RSAvalidity记录证书的有效期 这个证书的公钥 public keyserver key exchange第 20 个包有意思。上个文章提到，tls 握手有 2 种模式：RSA 握手、DH 握手。这里再重复一下。RSA 握手模式，client 自己生成 premaster secret，并且使用证书的 public key 去加密，再发送给 server。一旦服务器私钥泄露，未来所有的 tls 握手都不安全了。DH 握手模式，premaster secret 由 client、server 根据 DH parameter 协商生成，安全性更高。从抓包情况看，baidu 使用了 DH 握手模式了。使用 ECDH，返回了 server dh param。server hello done这个包很简单，没什么好说的。client exchange 阶段 第 24 个数据包内容很丰富。client key exchange因为使用 DH 模式握手，client 要回复它生产的 client dh paramchange cipher spec（client）这个包只有一个字节，但是干嘛呢？发现对这个数据包理解不到位，最后找到 Cisco 的一篇文章：SSL Introduction with Sample Transaction and Packet ExchangeThe message is sent by both the client and server in order to notify the receiving party that subsequent records are protected under the most recently negotiated Cipher Spec and keys.client 和 server 都会发送 change cipher spec 报文 这个报文的意义是，通知对方，使用最近协商的 cipher 和 key，并且后续的报文都是加密的。encrypted handshake messagetls 握手成功之后，双方都会发送一条 encrypted handshake message 报文。这个报文的作用是校验握手是否成功，数据没有被中间人篡改。Both client and server send the Finished message but the first to do it is the client. If the server receives the message and could decrypt and understand it, it means the the server is reading the encrypted information in the right way. Now the only missing part is that client could decrypt the information sent by the server. To do that the server must send a Change Cipher Spec message too followed by the Finished message in the encrypted way. Exactly the same as client did. Again if the client could decrypt the Finished message it means that both parties are in frequency and they can talk to each other protecting all the data in transit.client 和 server 都会发送这个报文，但是首先由 client 发送 发送的内容是什么呢，又是怎么验证？查到一个文章，讲的清楚 TLS v1.2 handshake overview - apoorv munshi - Medium这个消息体的内容是一个 hash，哈希方法使用的是 server hello 协商的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 中的 SHA256。哈希的内容是 master_secrethash of all the previous handshake messages (from ClientHello up to Finished message, not including this Finished Message)finished_label string (“client finished” for client message and “server finished” for server message)123verify_data PRF(master_secret, finished_label, Hash(handshake_messages))(quoted from section 7.4.9 of RFC 5246) 对方用同样方式计算，如果和哈希值一致，则表明安全建立加密通信。new session ticketserver 生成会话 id。change cipher spec（server）类似上面 client 的 change cipher spec，不重复了。application data从这个阶段开始，client 和 server 使用 session key 加密、解密数据通信。其他：[TCP Spurious Retransmissions]spurious的意思是虚假的。baidu 服务器认为可能发生超时或者丢包，提前发起重传。具体以后再研究。]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https tls v1.2 握手过程]]></title>
    <url>%2Fp%2Ftls-handshake-v-1-2%2F</url>
    <content type="text"><![CDATA[前言 http 是明文传输，因此对于中间人攻击很脆弱。于是诞生了 https 加密传输。在 tcp 三次握手建立连接后，再进行 ssl/tls 握手，升级为 https。(图片来源：cloudflare.com)https 使用的握手协议，发展了多个版本，从 ssl 到 tls。目前广泛使用的是 tls 1.2。tls 1.3 也开始推广铺开。本文讨论 tls 1.2 握手过程。tls 设计哲学 在深入 tls 握手过程之前，先思考下如何在公开网络上实现安全通信。显然要对传输内容加密。而为了实现通信加密，双方要使用一致的加解密方式、密码。在密码学中，加密算法分为 对称加密 和非对称加密 两大类别。对称加密：速度快 支持任意长度内容 如果密码长度太短，容易被攻击 非对称加密：速度慢 不能加密超过密钥长度的内容（除非分组）攻击难度高 非对称加密分为公钥和私钥部分。顾名思义，公钥可以公开传输给任何人使用。私钥要保密存放，不能泄露。非对称加密一个特性是，使用公钥加密的内容，只能用对应私钥解密。反之亦然。如果只用非对称加密，那么安全性很高，但是对传输内容长度有限制，并且性能损耗大。这不是一个工程上的好方案。如果使用对称加密，那么传输内容长度不受限制，性能也好。密码长度足够，安全性也高。但是产生另一个问题：怎么安全传输对称加密的密码？tls 设计哲学是：使用对称加密传输内容，这样传输内容长度不受限、性能也好 双方协商使用的加密方式、并且生成对称加密的密码，不会在网络中明文传输 为了安全生成对称加密的密码，使用非对称加密方式传输密码的一个部分 tls 握手相关概念 深入了解 tls 握手过程之前，先了解几个概念。session id服务器生成的会话 id，用于开启或者恢复会话状态。client random客户端生成的 32 字节随机数据，每个 connection 都有唯一的 client random。server random类似 client random，但是由服务器生成。premaster secretserver 和 client 共享的 48-byte 密码。session keypremaster secret、client random、server random 一起用于生成 session 对称加密的密码。cipher spec加密策略，包括生成加密算法（例如 AES）和 MAC 算法（例如 HMAC-SHA256）tls 握手过程概览 rfc5246 section-7.3 描述了 tls 1.2 握手过程123456789101112131415161718Client Server ClientHello --------&gt; ServerHello Certificate* ServerKeyExchange* CertificateRequest* &lt;-------- ServerHelloDone Certificate* ClientKeyExchange CertificateVerify* [ChangeCipherSpec] Finished --------&gt; [ChangeCipherSpec] &lt;-------- Finished Application Data &lt;-------&gt; Application Data Figure 1. Message flow for a full handshakeClientHello 客户端发送支持的 ciphers 组合，生成 client random，session id（如果有的话）、扩展字段例如 SNI。SNI 是服务器名称指示（英语：Server Name Indication，缩写：SNI）。具体可以参照 nginx 配置多个 TLS 证书，以及 TLS SNI 简介ServerHello 服务器端根据 SNI 选择要发送的证书，决定本次会话使用的 cipher，生成 server random、session id。ClientKeyExchange客户端校验证书的有效性，方式有 CRL、OCSP。具体可以参照 nginx enable ocsp 如果证书检查不通过，则浏览器发出警告。如果校验通过，客户端生成 48byte 的 premaster secret（ps. 这里是 RSA 握手模式，DH 握手模式有不同）。client random、server random、premaster secret 生成 session key（key generation）。使用服务器证书中的公钥对 premaster secret 进行加密，发送给 server。（ps. 这里是 RSA 握手模式，DH 握手模式有不同）另一方面，服务器使用私钥解密得到 premaster secret。通用使用 client random、server random、premaster secret 生成 session key（key generation）。此时双方有一致的 session key。tls 握手完毕，后续通信使用 session key 进行对称加解密。tls 握手模式和 forward secrecytls 握手模式分为分为 RSA 模式和 DH 模式（ Diffie-Hellman）。(图片来源：cloudflare.com)authentication 是身份验证，是指证书校验。key establishment 是 session key 生成，具体是 premaster secret 部分。使用 RSA 制作的证书，RSA 可以用于签名，以及 premaster secret 的加密。使用 DSA 制作的证书，DSA 只能用于前面，premaster secret 需要使用 DH 算法协商。对于 RSA 握手，身份验证使用 RSA，premaster secret 由客户端生成，并且使用证书的公钥进行 premaster secret 的加密，服务器端使用证书的私钥进行解密，得到 premaster secret。(图片来源：cloudflare.com)这就有个问题，RSA 既用于身份验证，又用于 session key 生成。一旦私钥泄露，未来所有的 tls 都不安全，因为 1session key = f(client random, server random, pre master secret) 其中 client random、server random 是明文，pre master secret 能够用泄露的私钥解密！这是 forward secrecy 问题。针对 RSA 握手的问题，诞生了 DH 握手模式。具体来说，premaster secret 不再由 client 生成并且加密传输。而是使用 DH 算法，双方协商生成 premaster secret。(图片来源：cloudflare.com)server hello 阶段，额外发送 server DH parameter，并且使用 privae key 对 client random，server random，DH 参数加密 （密文）。client 用 public key 解密 server hello 的密文，得到 server DH param。client 回复 client DH param。 现在 client 和 server 都有 client DH param，server DH param，分别计算，得到 premaster secret。因为每次会话建立使用的 dh 参数是随机的，即使泄露了 server 的私钥，要攻击未来的会话也是很困难。具体可以参照 nginx 配置 dhparam，以及聊聊 forward secrecytls 握手性能简单分析 从上面可以看到，tls 1.2 握手需要 2 个阶段。第一阶段，明文传 client random、server random、证书、cipher。占用一个 RTT。第二阶段，client 生成 premaster secret，用 server 证书的公钥加密 client 生成的 premaster secret 发送给 server（RSA 握手模式）；或者双方协商 DH 参数生成 premaster secret（DH 握手模式）；占用一个 RTT。握手完毕后双方得到一致 session key。从网络传传输角度看，tls 1.2 握手 至少 增加了 2 个 RTT。根据服务器配置和浏览器实现，客户端验证 server 证书有效性，可能要发送一次完整的 http 请求，额外增加一个 RTT；如果是多级证书，那就更多的 RTT 了。从计算资源角度看，协商 premaster secret 阶段要使用非对称加密（RSA 或者 DH），是 cpu 密集计算。对服务器 cpu 消耗更大。这额外的 2 到 3 个 RTT，以及使用非对称加密方式处理 premaster secret，导致了 https 建立连接要比直接使用 http 慢。对于 tls 1.2，优化握手性能的方式有：提高服务服务器 cpu 性能，或者使用专门硬件处理非对称加密 tls session 缓存，具体参照nginx 配置 SSL 证书，以及 ssl_ciphers 选择 优化证书验证流程，浏览器可以预载吊销列表 CRL，同时服务器代替客户端查询 OCSP，在 server hello 阶段一并返回，具体可以参照 nginx 开启 ocsp staplingtls 握手安全性简单分析 以 RSA 握手为例，tls 握手的核心是双方各自计算出本次会话使用的 session key1session key = f(client random, server random, pre master secret)其中 client random，server random 是明文传输，能够被抓包截获。pre master secret 经过非对称加密，即使被被抓包截获，也很难解密。因此 tls 安全性比较高。但是例外总是有的，那就是 tls 中间人攻击。攻击的时机是 tls 握手。常见方式有 中间人伪造了证书，并且得到客户端信任。sslsplit中间人代替 client 向 server 发起 tls 握手请求。client 和中间人进行 http 连接，中间人和 server 进行 http/https 连接。sslstrip1client --&gt; 中间人 --&gt; server那么 client random, server random, premaster secret 对于中间人来说都是透明的。不过实施 tls 中间人的攻击有难度。对于 sslsplit 攻击 中间人要伪造 CA 证书难，通常是自签名证书 client 要信任伪造的证书 浏览器的证书检查机制，对于伪造证书，会有“证书不可信”的警告信息 对于 sslstrip 攻击 不会有“证书不可信”的警告信息，但是客户端显示的 URL 会由 https:// 变成 http://因为 sslstrp 比较棘手，于是诞生了 HSTS 机制，强制客户端对目标网站进行 https 连接。关于 hsts，曾经做过介绍 nginx 配置 hsts 结合证书校验机制和 HSTS 机制，tls 握手阶段安全性得到加强。（TODO：以后补上 ssl 攻击实验）证书安全 除了自签名证书外，证书由受信任机构颁发 证书分等级认证，存在门槛 每个证书都有效期 可以被吊销（revoke）tls 握手阶段，client 会验证证书有效性 参考资料The Transport Layer Security (TLS) Protocol Version 1.2Keyless SSL: The Nitty Gritty Technical Details]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http 307 重定向]]></title>
    <url>%2Fp%2Fhttp-307%2F</url>
    <content type="text"><![CDATA[刚才在做 hexo 页面优化，发现了本地测试返回 http 307。以前没见过这个响应码，于是做一下调研。相关文章：hexo 页面优化 http 307 在 rfc 规范中，http 307 Temporary Redirect 是临时重定向 。 平时常见的重定向是：301：Permanently Moved，永久重定向 302：Temporarily Moved，临时重定向http 307 和 302 的区别在于：307 要求客户端不改变原先的请求方法，对在 Location 头部中规定的 URI 进行访问。对于 302，很多客户端的实现是，直接使用 GET 方式访问重定向地址。 例子 客户端请求 12POST /index.php HTTP/1.1Host: www.example.org 服务器端响应 12HTTP/1.1 307 Temporary RedirectLocation: https://www.example.org/ 那么客户端必须以 POST 方式重定向访问 https://www.example.org/。 本地测试产生 http 307next 的 _config.yml 配置 123456789# Internal version: 2.1.5 &amp; 3.5.7# See: https://fancyapps.com/fancybox# Example:# fancybox: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js# fancybox: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.js# fancybox_css: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css# fancybox_css: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.cssfancybox: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.jsfancybox_css: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.css 平时写 url 地址，一般是 http 或者 https，但是 next 里面的例子都是 //。// 的意义是，沿用当前页面的协议。如果当前页面是 http 协议，那么发出去的请求就是 http；如果是当前页面是 https，那么发出去走 https。//写法的好处是，不需要关注协议，只需要关注 URI 路径。如果哪一天发生协议变更，比如 http 升级为全站 https，那么代码完全都不用修改。但是这没有解释为什么返回了 http 307。仔细看看 response header，除了 Location 指示重定向地址外，还有 1Non-Authoritative-Reason: HSTSHSTS 是 HTTP 严格传输安全（英语：HTTP Strict Transport Security），之前的文章提到过：nginx 配置 hsts 因为本地测试，使用 http://localhost:4000 访问，所以 // 的页面协议是 http。但是 cloudflare.com 开启了 HSTS，所有请求都必须是 https 协议。对 cloudflare.com 原来的 http 请求必须升级为 https。 事实上，这个 307 响应不是 cloudflare.com 产生的，是 chrome 浏览器干的好事。The way Chrome shows this in the network tab is by creating a dummy 307 response with a redirect to the https version of the address. But that’s a fake response and is not generated by the server - the reality is Chrome did that internally before the request even went to the server.注意到，rfc 定义 http 307 是 Temporary Redirect，而截图显示的是Internal Redirect。 回想到 HSTS 只在第 1 次 http 访问之后才会生效。如果 chrome 不做这个返回，会是怎样的流程呢： 本地客户端 http 方式访问 cloudflare.com服务器表示要以 https 方式访问资源 于是本地客户端以 https 方式再次访问 cloudflare 的资源 中途多了一次网络请求。因为 chrome 维护了一份 HSTS 站点列表，知道 cloudflare 必须要 https 方式请求。于是截获 http 请求后，直接以 https 方式访问，同时做出 dummy 307 响应。小实验 把 next 的 _config.yml 从//修改为 https://，再测试 直接就是 http 200 了。小结 // 比写死具体 http、https 更加灵活，推荐使用http 307 Temporary Redirect，临时重定向，客户不能改变请求方式chrome 知道 HSTS 站点，会自动把这些站点的 http 请求改写为 https，同时在 response header 增加Non-Authoritative-Reason: HSTS，并且把 307 响应码解析为Internal redirect]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>web</tag>
        <tag>https</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 页面优化]]></title>
    <url>%2Fp%2Fhexo-optimize-speed%2F</url>
    <content type="text"><![CDATA[博客打开有点慢，于是做优化。页面优化工具使用 PageSpeed Insights。 思路 优化 web 页面的常见思路 压缩文本、图片等资源 静态资源使用 cdn合并 js、css 文件，把多个请求资源请求合并为一个 设置适当的静态资源缓存时间 资源懒加载 优化资源加载顺序]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>hexo</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 开启 google analytics 多域名支持]]></title>
    <url>%2Fp%2Fhexo-enable-GA%2F</url>
    <content type="text"><![CDATA[问题 Google 分析是一个由 Google 所提供的网站流量统计服务。既然是免费，就接进来研究下。hexo next 已经集成了一系列统计插件，开启很方便。打开 next 的_config.yml1234# Google Analyticsgoogle_analytics: tracking_id: localhost_ignored: truetrack_id 是“媒体资源设置”的“跟踪 ID”。如果只有一个站点，那么一切都很好。但是现在有 https://ycwu314.github.io/ 和 https://ycwu314.gitee.io/ 两个站点。如果流量归一的话，用一个 track_id 就可以。但是我想分别查看流量数据。只用一个 track_id 是不方便的。于是开始折腾一下。hexo next 接入 GA 源码分析 在 next 项目全文查找 google_analytics，最后定位接入 GA 统计的具体文件：themes\next\layout\_third-party\analytics\google-analytics.swig123456789101112&#123;% if theme.google_analytics.tracking_id %&#125; &lt;script async src="https://www.googletagmanager.com/gtag/js?id=&#123;&#123; theme.google_analytics.tracking_id &#125;&#125;"&gt;&lt;/script&gt; &lt;script&gt; var host = window.location.hostname; if (host !== "localhost" || !&#123;&#123;theme.google_analytics.localhost_ignored&#125;&#125;) &#123; window.dataLayer = window.dataLayer || []; function gtag()&#123;dataLayer.push(arguments);&#125; gtag('js', new Date()); gtag('config', '&#123;&#123; theme.google_analytics.tracking_id &#125;&#125;'); &#125; &lt;/script&gt;&#123;% endif %&#125; 根据 track_id 判断是否加载脚本，并且脚本 id 包含 track_id。数据打标签的时候，把 track_id 也打进去。解决 思路：在 next 的 _config.yml 增加 github_tracking_id 和 gitee_tracking_id修改 google-analytics.swig，根据 host，动态加载脚本 修改 _config.yml123456# Google Analyticsgoogle_analytics: enable: true github_tracking_id: gitee_tracking_id: localhost_ignored: true 修改 google-analytics.swig第一版 1234567891011121314&lt;script async src="" id="ga"&gt;&lt;/script&gt;&lt;script&gt; var host = window.location.hostname; var track_id; if(host == 'ycwu314.gitee.io')&#123; track_id = '&#123;&#123; theme.google_analytics.gitee_tracking_id &#125;&#125;'; &#125;else if(host == 'ycwu314.github.io')&#123; track_id = '&#123;&#123; theme.google_analytics.github_tracking_id &#125;&#125;'; &#125; if(track_id)&#123; ga.src = "https://www.googletagmanager.com/gtag/js?id=" + track_id; &#125;&lt;/script&gt; 以为直接修改 src 就会触发动态加载脚本。然而 chrome F12 观察，并没有加载脚本。查 html 相关资料 Changing the src, type, charset, async, and defer attributes dynamically has no direct effect; these attribute are only used at specific times described below. 于是改为动态插入 script 标签，并且在 head 插入标签。1234567891011121314151617181920212223242526272829303132&#123;% if theme.google_analytics.enable %&#125; &lt;script&gt; var host = window.location.hostname; var track_id; if(host == 'ycwu314.gitee.io')&#123; track_id = '&#123;&#123; theme.google_analytics.gitee_tracking_id &#125;&#125;'; &#125;else if(host == 'ycwu314.github.io')&#123; track_id = '&#123;&#123; theme.google_analytics.github_tracking_id &#125;&#125;'; &#125; if(track_id)&#123; var script = document.createElement("script"); script.src = "https://www.googletagmanager.com/gtag/js?id=" + track_id; script.async = "async"; document.head.appendChild(script); &#125; &lt;/script&gt; &lt;script&gt; var host = window.location.hostname; if (host !== "localhost" || !&#123;&#123;theme.google_analytics.localhost_ignored&#125;&#125;) &#123; window.dataLayer = window.dataLayer || []; function gtag()&#123;dataLayer.push(arguments);&#125; gtag('js', new Date()); if(host == 'ycwu314.gitee.io')&#123; gtag('config', '&#123;&#123; theme.google_analytics.gitee_tracking_id &#125;&#125;'); &#125;else if(host == 'ycwu314.github.io')&#123; gtag('config', '&#123;&#123; theme.google_analytics.github_tracking_id &#125;&#125;'); &#125; &#125; &lt;/script&gt;&#123;% endif %&#125;script async 表示异步加载。测试 测试的时候发现报错 net::ERR_BLOCKED_BY_CLIENTadblock 插件会拦截 GA 统计。关闭 adblock 之后就可以了。 原来的博文路径是hexo-enable-google-analytics，触发 adblock 规则了😱ps. 对于 https://ycwu314.gitee.io/ , 关闭网络工具之后发现 ga js 也能正常加载，略表惊讶。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次 maven offline mode 引发的问题]]></title>
    <url>%2Fp%2Fmaven-offline-mode%2F</url>
    <content type="text"><![CDATA[问题描述 前 2 天，老铁对接讯飞语音转文字 api。讯飞使用 gson 做 json 转换，并且不兼容我们项目的 FastJSON，必须引入 gson 包。但是神奇的事情发生了。老铁电脑上 gson 依赖包是红色的。我本地是正常拉到 jar 包。解决过程 对于 maven 拉取不到 jar 包，通常是 网络不好，访问 maven central repo 慢成狗 下载过程中出现异常，导致 jar 包损坏 首先检查 .m2 目录的 settings.xml 文件，发现使用的是默认中央仓库，肯定慢啦。改为使用阿里云镜像，在 mirrors 节点增加一个镜像 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; 再去 idea 构建，发现还是不成功。检查了 .m2\repository 目录，没有对应的 gson 目录。肯定是某个环境配置的问题，但是一时半会想不到是哪里。。。先让他本地引入 gson jar 开发。等开发完毕后，回头开始解决这个奇怪的问题。他有 2 台电脑，另一台电脑能够正常拉取 gson 包。由于在 idea 操作 maven，怀疑是 idea maven 配置问题。打开有惊喜 不能拉取 gson 包的电脑，开启了 work offline。取消这个选项，重新拉取依赖，问题就解决。maven offline mode 以前没有接触 maven 离线模式，于是查资料了解。maven offline mode，只使用本地仓库，不会从远程仓库拉取 jar 包。这就要求本地仓库要有所有的 jar 包！maven offline mode 的场景对于大多数人来说都是不适用的。我想下面的场景可能有用：严格控制上网的地方，比如 min gan 行业的开发，只能使用本地仓库 连接 central repo 网路不好，不如直接本地仓库快。可以是一台能够快速连接 central repo 的电脑做代理，然后 jar 包都由它下载，并且放到共享的本地仓库。没有网络，但是要构建项目，这时候强制使用本地仓库。比如高铁上。dependency:go-offline来自《Apache Maven Cookbook》The go-offline goal of the Maven Dependency plugin downloads all the required dependencies and plugins for the project, based on the pom file. The –o option tells Maven to work offline and not check the Internet for anything.正常使用 maven offline mode 的前提是，所有依赖包已经下载到本地。dependency:go-offline提供了这个功能 12345678910111213141516171819202122C:\workspace\medical&gt;mvn dependency:go-offline[INFO] Scanning for projects...Downloading from alimaven: http://maven.aliyun.com/nexus/content/groups/public/org/apache/johnzon/johnzon-maven-plugin/1.1.11/johnzon-maven-plugin-1.1.11.pomDownloaded from alimaven: http://maven.aliyun.com/nexus/content/groups/public/org/apache/johnzon/johnzon-maven-plugin/1.1.11/johnzon-maven-plugin-1.1.11.pom (3.6 kB at 2.7 kB/s)// 省略一堆输出[INFO] --------------------------------[jar]---------------------------------[INFO][INFO] &gt;&gt;&gt; maven-dependency-plugin:3.1.1:go-offline (default-cli) &gt; :resolve-plugins @ medical &gt;&gt;&gt;[INFO][INFO] --- maven-dependency-plugin:3.1.1:resolve-plugins (resolve-plugins) @ medical ---Downloading from alimaven: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/reporting/maven-reporting-impl/2.3/maven-reporting-impl-2.3.pomDownloaded from alimaven: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/reporting/maven-reporting-impl/2.3/maven-reporting-impl-2.3.pom (5.0 kB at 10 kB/s)// 省略一堆输出[INFO] &lt;&lt;&lt; maven-dependency-plugin:3.1.1:go-offline (default-cli) &lt; :resolve-plugins @ medical &lt;&lt;&lt;[INFO][INFO][INFO] --- maven-dependency-plugin:3.1.1:go-offline (default-cli) @ medical ---[INFO] Resolved: spring-boot-starter-aop-2.1.5.RELEASE.jar[INFO] Resolved: spring-boot-starter-2.1.5.RELEASE.jar// 省略一堆输出 构建的时候，使用 -o 参数指示 maven 离线工作，并且不检查网络 12&gt; mvn help -o,--offline Work offline 友情提示 不要手抖点击这个按钮，会开启 offline mode小结 以后遇到 maven 依赖下载问题，要考虑 central repo 镜像配置 本地 jar 包损坏maven 离线模式]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>故障案例</tag>
        <tag>devops</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs selenium 自动登录并且更新 pages]]></title>
    <url>%2Fp%2Fnodejs-selenium-auto-login-and-deploy-pages%2F</url>
    <content type="text"><![CDATA[上次文章留了个尾巴，免费版的码云 gitee pages 更新文件后，不会自动重新部署，只有 pro 版才支持。于是思考怎么自动更新网站。travis ci 部署 github 和 gitee 码云 没想到又一番折腾开始😭思路 为了自动更新 gitee pages，需要模拟用户登录后点击 pages 页面的“更新”按钮。有 2 种方式：抓包看登录、更新接口的 api，以及附加的安全参数，通常有 cookies、token 之类 使用浏览器模拟用户操作 平常使用第一种比较多。这次决定使用第二种方式。seleniumselenium 是一个自动化浏览器框架，支持多种浏览器。selenium 通过 WebDriver 操作浏览器。安装 selenium1npm install selenium-webdriver --save后来发现安装了 4.0 alpha。对 alpha 版不感兴趣，于是改为 1npm install selenium-webdriver@3.6.0 --save 安装浏览器和 webdriver浏览器有 gui 界面，在服务器没有显示器的情况下，是通过 headless 模式使用。目前 chrome 和 firefox 都支持。12# which firefox /usr/bin/firefox发现 vps 上已经有 firefox，于是直接使用。查资料发现 firefox 对应的 WebDriver 是 geckodriver1npm install geckodriver --save然后从官网拷贝一个简单的例子测试，发现跑不通 12Error: The geckodriver executable could not be found on the current PATH. Please download the latest version from https://github.com/mozilla/geckodriver/releases/ and ensure it can be found on your PATH. at findGeckoDriver (/root/ycwu/webtest/node_modules/selenium-webdriver/firefox/index.js:354:11) 需要把 geckodriver 添加到 $PATH。后来才发现这里要把 geckodriver 所在目录添加到$PATH，但是手抖把文件名也粘贴进去了。。。😥Mozilla 官网还特意提醒了Note: Just to reiterate, the path you add to PATH needs to be the path to the directory containing the drivers, not the paths to the drivers themselves! This is a common mistake. 接着发现浏览器和 webdriver 有版本对应关系。。。算了，索性手动安装 chrome 和 chromedriver 算了 123# apt install chromium-browser# chromium-browser --versionChromium 75.0.3770.142 Built on Ubuntu , running on Ubuntu 18.04 然后去官网 ChromeDriver - WebDriver for Chrome 下载对应的 driver。1234567# wget https://github.com/mozilla/geckodriver/releases/download/v0.24.0/geckodriver-v0.24.0-linux64.tar.gz--2019-08-06 20:45:19-- https://github.com/mozilla/geckodriver/releases/download/v0.24.0/geckodriver-v0.24.0-linux64.tar.gzResolving github.com (github.com)... 13.250.177.223Connecting to github.com (github.com)|13.250.177.223|:443... connected.ERROR: cannot verify github.com's certificate, issued by ‘CN=DigiCert SHA2 Extended Validation Server CA,OU=www.digicert.com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer's authority.To connect to github.com insecurely, use `--no-check-certificate'.由于是 https 且系统没有对应的证书，因此 wget 要加上 --no-check-certificate 参数。12wget https://chromedriver.storage.googleapis.com/75.0.3770.140/chromedriver_linux64.zip --no-check-certificateunzip chromedriver_linux64.zip然后把 chromedriver 加入到 PATH 变量 123echo "export CHROME_DRIVER_PATH=`pwd`" &gt;&gt; ~/.profileecho "export $PATH=$PATH:$CHROME_DRIVER_PATH" &gt;&gt; ~/.profilesource ~/.profile 到此，selenium 和 webdriver 安装完毕。gitee 登录页面分析 chrome 按 F12，再打开 gitee 的登录页面 关键点：用户名：id=user_login密码：id=user_password登录按钮：input name=sumbit，有 2 个。但是由于密码输入框有监听 enter 按键事件，直接忽略登录按钮。 为了调试方便，我在 Windows 上开发。123456789101112131415161718192021222324var webdriver = require('selenium-webdriver'), By = webdriver.By, until = webdriver.until, Key = webdriver.Key;var chrome = require('selenium-webdriver/chrome');var options = new chrome.Options();//options.addArguments("--headless");options.addArguments("--no-sandbox");// chrome driver 要放在 PATH// chrome exe 路径 options.setChromeBinaryPath("C:\\Users\\ycwu\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe");(async function example() &#123; let driver = await new webdriver.Builder().forBrowser('chrome').setChromeOptions(options).build(); try &#123; await driver.get('https://gitee.com/login'); await driver.findElement(&#123; id: 'user_login' &#125;).sendKeys("&lt;myaccount&gt;"); await driver.findElement(&#123; id: 'user_password' &#125;).sendKeys("&lt;mypassword&gt;", Key.ENTER); await driver.sleep(1000); &#125; finally &#123; await driver.quit(); &#125;&#125;)(); 这里有几个点要注意：–headless：不显示 gui。为了调试方便，不启动。等到正式环境再开启。–no-sandbox：解决 root 用户的权限问题，具体和 chrome 的 sandbox 机制有关。setChromeBinaryPath：因为 chrome 不在系统 PATH，要手动指定。注意 Windows 的目录分隔符要转义 \\。 由于 node 是全异步操作，为了实现同步等待，要使用 async/await/promise 机制，以后再深入了解。12// wait 3 secondsawait new Promise((resolve, reject) =&gt; setTimeout(resolve, 3000));注意 await 只能用在 async 方法之内。selenium 选择页面元素，支持 id、tag、name、class 等选择器。官网看一下就是了。gitee pages 部署页面分析 更新按钮没有 id，直接写在 div。不过试了可以用 update_deploy 来定位 1await driver.findElement(By.className('update_deploy')).click(); 结果出现弹窗。。。见到弹窗莫慌，发现类型是 alert，不是自定义模态窗口。selenium 轻松解决 1await driver.switchTo().alert().accept();ps. accept() 改为 dismiss() 则是取消。然后就开始更新静态网站了。最终 windows 版脚本 123456789101112131415161718192021222324252627282930var webdriver = require('selenium-webdriver'), By = webdriver.By, until = webdriver.until, Key = webdriver.Key;var chrome = require('selenium-webdriver/chrome');var options = new chrome.Options();options.addArguments("--headless");options.addArguments("--no-sandbox");options.addArguments("--disable-gpu");// chrome driver 要放在 PATH// chrome exe 路径options.setChromeBinaryPath("C:\\Users\\ycwu\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe");(async function example() &#123; let driver = await new webdriver.Builder().forBrowser('chrome').setChromeOptions(options).build(); try &#123; await driver.get('https://gitee.com/login'); await driver.findElement(&#123; id: 'user_login' &#125;).sendKeys('&lt;myaccount&gt;'); await driver.findElement(&#123; id: 'user_password' &#125;).sendKeys('&lt;mypassword&gt;', Key.ENTER); await driver.sleep(2000); await driver.get('https://gitee.com/ycwu314/ycwu314/pages'); await driver.sleep(3000); await driver.findElement(By.className('update_deploy')).click(); await driver.sleep(3000); await driver.switchTo().alert().accept(); &#125; finally &#123; await driver.quit(); &#125;&#125;)(); 更新 travis 配置 travis 配置要修改的内容：travis 的 addons 支持 chrome，具体是google-chrome-stable。 把 gitee 用户、密码分别保存到 gitee_user、gitee_pwd，交给 travis 托管 为了能让 node 读取到 gitee_user、gitee_pwd，使用process.env.gitee_user 安装 chromedriver，并且设置 $PATH 把 windows 版修改为 linux 版，并且保存到 .travis/deploy_gitee.jsdeploy 之后，执行node .travis/deploy_gitee.js 都是体力活，不再重复。结果 激动人心的时候到了，提交 commit 触发 travis 构建。突然手机响了一下 我擦，traivs 服务器在美帝啊，触发异地登录异常验证😭。不慌，还有一台深圳的开发 vps，用来做触发 pages 更新可以不。。。不可以，还是异地告警。小结 虽然没有达到目标，但是生命不息，折腾不止！更新：2019.8.11：无意中使用 xxx 的全局流量登录，提示”登录账号验证”最后一行有惊喜~]]></content>
      <categories>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>devops</tag>
        <tag>web</tag>
        <tag>nodejs</tag>
        <tag>自动化测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csrf 攻击和防范]]></title>
    <url>%2Fp%2Fcsrf-attack-and-solution%2F</url>
    <content type="text"><![CDATA[csrf 是什么 来自 wiki跨站请求伪造（英语：Cross-site request forgery），也被称为 one-click attack 或者 session riding，通常缩写为 CSRF 或者 XSRF，是一种挟制用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法。跟跨网站脚本（XSS）相比，XSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户网页浏览器的信任。简单的身份验证只能保证请求发自某个用户的浏览器，却不能保证请求本身是用户自愿发出的。一个例子 网上很多是银行转账的例子，这里再编一个新的。用户访问 www.example.com，看到大抽奖，毫不犹豫就点击了12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt; 喜迎 5G 上市幸运大抽奖 &lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method="get" action="http://www.xx-ads.com/click.do"&gt; &lt;input type="hidden" name="ad_id" value="xxx"&gt; &lt;input type="hidden" name="from_id" value="yyy"&gt; &lt;input type="button" onclick="submit()" value="点击赢大奖"&gt;&lt;/form&gt;&lt;/body&gt; 然后就给某个广告投放者创造了一次点击（当然广告平台有反作弊机制啦）。解决 csrf 根源问题在于，服务器对请求来源不加以区分，即无条件信任来自浏览器的请求。 验证 referer 字段 http 协议定义了referer 字段，用来表示当前请求的来源，具体是 url 地址。使用 referer 字段可以简单判断请求来源。但是，referer 字段很容易伪造。自定义 csrf token 字段 因为 A 站点的页面不能获取 B 站点的页面内容、cookies，因此可以在提交页面上增加 token 字段。生成表单的时候增加 hidden 字段，用来存放服务器生成的 token。提交表单的时候一起提交 1&lt;input type="hidden" name="csrf-token" value="5QhM72KX9zjHO03V"&gt;cookies 返回 token，因为获取不到其他站点的 cookie。token 放在自定义的 header，比如X-CSRF-TOKEN。 不管 token 是放在 hidden、cookies 还是自定义 header，都要在提交的时候在服务器端验证 token 的有效性 。 服务器端可以使用 filter、切面等方式统一验证 token。用户端防范 关键站点（比如跟💴相关的）使用完之后及时退出。这样伪造请求发出去，通常会挂在被攻击站点的登录态校验。实战 nginx 限制 referer 字段nginx 使用ngx_http_referer_module 处理 referer 字段。1234567valid_referers none blocked server_names *.example.com example.* www.example.org/galleries/ ~\.google\.;if ($invalid_referer) &#123; return 403;&#125;根据 valid_referers 的结果，$invalid_referer返回 0 或者 1。valid_referers 支持的方式有：none：referer 字段为空 blocked：正常的 referer 字段是http:// 或者 https:// 开头。其他为非正常的 referer，比如 Referer:xxx。通常是伪造请求，或者被防火墙修改。server_names：指定的服务器，支持通配符。ps. nginx referer 更多是用于简单的防盗链。 使用 spring security默认是开启 csrf。12345@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .csrf().disable();&#125;然后在 form 增加 1&lt;input type="hidden" name="$&#123;_csrf.parameterName&#125;" value="$&#123;_csrf.token&#125;"/&gt; 剩下的交给 spring security 框架。注意这个是 form 表单。如果是 ajax 方式提交，要自己获取 csrf token，并且提交。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>web</tag>
        <tag>网络安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo fancybox 3 安装问题]]></title>
    <url>%2Fp%2Fhexo-install-fancybox-3%2F</url>
    <content type="text"><![CDATA[上次在搞 travis 部署码云，途中遇到过一个奇怪的问题：使用 traivs 生成的静态页少了 fancybox 的库，页面加载失败。但是我本机 hexo 生成和部署就没有这个问题。安装 fancybox3打开 hexo next 主题的 _config.yml12345# Fancybox. There is support for old version 2 and new version 3.# Choose only one variant, do not need to install both.# To install 2.x: https://github.com/theme-next/theme-next-fancybox# To install 3.x: https://github.com/theme-next/theme-next-fancybox3fancybox: true 然后参照 fancy box 3.x 版本的说明，git clone 到 next 主体的 lib 目录，相对于项目路径是 themes\next\source\lib。hexo next 的.gitigonre 文件 使用 next 主题，source目录下面的内容会拷贝到最终静态内容目录 public（位于项目顶级目录）。 但是 hexo g 的日志输出是没有 fancybox 文件。可是文件明明就在这里，为什么没有拷贝到 public 呢？只能是在某个地方被排除掉。hexo next 主题的 _config.yml 和项目的 _config.yml 搜索 fancybox，不是这个问题。怀疑是某个隐藏文件做了排除，可能是 gitignore 文件。由于使用 Windows，打开 cmder 一看，hexo next 目录.gitignore 文件有惊喜：1234567891011121314151617181920λ cat .gitignore.DS_Store.idea/*.log*.imlyarn.lockpackage-lock.jsonnode_modules/# Ignore optional external librariessource/lib/*# Track internal libraries &amp; Ignore unused verdors filessource/lib/font-awesome/less/source/lib/font-awesome/scss/!source/lib/font-awesome/*!source/lib/jquery/!source/lib/velocity/回想起来，next 主题是直接 git clone 到 theme 目录的方式安装的。.gitignore 文件当然也被下载，并且生效。其实以前也发生过一次 fancybox 文件部署之后找不到。不过当时直接拷贝一份到 public 目录，并没有认真想过这个问题。以后本地也没有 hexo clean，所以拷贝到public 的文件一直都在，没有重现过。但是，出来混，迟早是要还的😂。小结 使用 git clone 仓库安装插件，会连带下载.gitignore 文件并且生效。可以考虑直接删除.gitignore 文件。]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[travis ci 部署 github 和 gitee 码云]]></title>
    <url>%2Fp%2Ftravis-deploy-both-github-and-gitee%2F</url>
    <content type="text"><![CDATA[博客使用 GitHub Pages 服务部署，国内访问比较慢，于是考虑部署一份在国内。目标是一次构建，自动部署 2 份。于是开始了折腾之旅。选型 最初考虑使用 coding.net 的 pages 服务，速度很快，但是试用下来有个问题：coding 的默认域名，不提供顶级项目访问。导致 Github 生成的目录路径直接部署在 coding.net 会报错。这个问题可以解决，使用自己的域名指向 coding 提供的默认域名即可。但是 coding 会对自定义域名的 pages 进行流量挟持，放广告。不太想用了。后来发现 gitee 码云也提供 pages 服务，并且支持顶级项目访问路径。速度还好，于是就选定了。travis 入坑 travis 是一个自动化构建和部署平台。支持 GitHub。要注意的是，travis.org 支持 public 项目，travis.com 支持 private 项目。 使用 travis 之后有 2 个好处：在线修改 md 文件，直接提交就生效。改个做别字什么的好方便了。可以本地提交 md 文件，远程生成博客。以后文章变多之后，部署就相当方便。使用 token 部署 travis 和 github 通过 Github Token 集成，网上查一下就有。（因为最终版本没有使用 GitHub Token，所以这里不展开了） 参照 用 TravisCI 持续集成自动部署 Hexo 博客的个人实践 ，成功集成了 GitHub 构建。 于是在 gitee 申请 token，选择了部署权限，给 travis 调用，但是发现不能部署！ 坑爹了。这要改变思路。部署静态博客，本质是把生成的静态页文件 push 到指定分支。以前本地 hexo deploy 就可以了，但是 travis 的机器没有 ssh key 安装，不能直接访问。问题就变成，把一个授权的 ssh key 安装到 travis。ssh key 安装 首先，要单独一个 ssh key 用于部署，不要偷懒使用个人电脑的 ssh key。ssh key 分为公钥和私钥。用 ssh-keygen 工具生成，这里不输入密码（passphrase）1ssh-keygen -t rsa -f travis.keytravis.key 是私钥，travis.key.pub 是公钥。把公钥分别添加到 Github 和 gitee。要让 travis 能够 ssh 到 github 和 gitee，现在还需要把私钥安装到 travis 部署脚本。但是私钥是敏感数据，不应该使用明文存储到仓库。因此要对私钥文件做加密，再在部署脚本解密并且安装。网上提供的方式是 travis client，但是这货需要 ruby 运行时，在服务器折腾安装 ruby 遇到各种问题，最后放弃了。加密的话使用 openssl 就可以了。-k是指定密码。然后把密码放到 travis 管理就好啦。1234# 加密 openssl aes-256-cfb -in travis.key -out travis.key.enc -k CtbBnkohJFSW27ga # 解密openssl aes-256-cfb -in travis.key.enc -out travis.key.out -k CtbBnkohJFSW27ga -d 但是在这里掉坑了！把加密文件丢到 travis，解密，执行 ssh-add 就会提示输入密码（passphrase）。一开始没有思考为什么会提示输入密码的原因，还以为是添加新 key 都要的。于是网上试了好几种方式，怎么不用输入 passphrase。但是依然部署失败。就这样折腾了半天，有点绝望。冷静一下，网上 travis 安装 ssh 成功的例子，都没有提到过 passphrase 的问题。ssh-add 认为这个 key 是加解密了，但是 ssh-keygen 的时候明明没有输入密码啊。在 travis.yml 解密后直接打印 ids_rsa1cat ~./.ssh/id_rsa一堆乱码！解密失败了。在服务器上检查，上面的命令可以解密成功的。但是放到 travis 上就有问题了。检查 openssl version，都是 1.1.1，应该不是这个问题。后来再对比网上的 aes 加密模式，发现别人用的是 cbc，我用 cfb，修改了之后，发现可以解密了。。。不太明白其中的问题😥，以后再填 aes 知识的坑。修改后的加密解密 1234# 加密openssl aes-256-cbc -in travis.key -out travis.key.enc -k CtbBnkohJFSW27ga # 解密openssl aes-256-cbc -in travis.key.enc -out travis.key.out -k CtbBnkohJFSW27ga -dtravis.yml 这是我最后测试成功的配置 123456789101112131415161718192021222324252627282930313233343536373839404142language: node_js # 设置语言dist: bionicnode_js: 10cache: apt: true directories: - node_modules # 缓存不经常更改的内容before_install: - export TZ='Asia/Shanghai' # 更改时区 - gunzip .travis/travis.key.enc.gz - openssl aes-256-cbc -in .travis/travis.key.enc -out ~/.ssh/id_rsa -d -k $&#123;SSH_PWD&#125; - chmod 600 ~/.ssh/id_rsa - eval $(ssh-agent -s) - ssh-add - git config --global user.name 'ycwu314' - git config --global user.email 'ycwu314@foxmail.com'install: - pwd - npm install -g hexo --save - npm installscript: - hexo clean - hexo g - hexo dbranches: only: - hexo env: global:addons: ssh_known_hosts: - gitee.com - github.com 项目的根目录新建了 .travis 目录、.travis.yml文件。把加密后的私钥放到 .travis 目录，对称加密的密钥 ${SSH_PWD} 由 travis 保管即可。解压缩后的私钥放到 ~/.ssh/id_rsa，访问权限至少为 600（ssh 踩过好几次坑了）。然后 ssh-add 加载私钥。ssh 首次连接的时候，会提示是否保存公钥。这里 addons.ssh_known_hosts 插件自动把域名加到 known_hosts。 这下连 GitHub token 也不需要使用了，直接 hexo d 搞定。hexo _config.yml 配置 hexo 部署支持多种方式1234567deploy:- type: git repo: git@gitee.com:ycwu314/ycwu314.git branch: master- type: git repo: git@github.com:ycwu314/ycwu314.github.io.git branch: master 其他问题 这里有点小问题，_config.yml配置的 url 填了 https://ycwu314.github.io，会影响生成的 sitemap 和版权声明链接。 也可以在 travis 上 sed 为 https://ycwu314.gitee.io，再部署到码云。不过暂时不折腾了。 后来才发现，gitee 非 Pro 的 Pages 需要手动部署。。。不过应该可以写脚本解决，模拟登录后点击更新按钮。以后再填坑。小结 使用 travis 部署 GitHub Pages 很简单，配置 GitHub Token 就可以了。部署 gitee 的话，由于 gitee token 不能正常部署，要使用 ssh 方式提交。私钥要加密后才能提交仓库。travis client 可以加密 ssh 私钥，但是需要 ruby 运行时，可以直接使用 openssl 加密。travis 安装私钥后，使用 hexo d 即像本地一样部署 参考资料 用 TravisCI 持续集成自动部署 Hexo 博客的个人实践 使用 Github，Travis CI 自动布署 Hexo 博客到 Coding，OSChina 服务器]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>devops</tag>
        <tag>travis ci</tag>
        <tag>travis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单个阿里云 ECS 部署多个环境的应用]]></title>
    <url>%2Fp%2Faliyun-single-machine-deploy-mutiple-environement%2F</url>
    <content type="text"><![CDATA[因为刚起步，小程序项目规模小，本着勤俭持家 (qiong) 的原则，我的 java 应用在一个阿里云 ECS 上部署开发环境和生产环境。端口规划 首先要考虑 Java 应用端口规划 8000：生产环境9000：开发环境 同时在 ECS 上配置安全组规则，放开端口。nginx 配置 小程序生产环境要求 https。但是开发环境可以在“项目设置”里面选择“不校验安全域名、TLS 版本以及 HTTPS 证书”，直接使用 http 方式。因此直接暴露 9000 端口，不用原有 nginx 配置。因为 nginx 上做了 http 到 https 的 301 强制改写，因此小程序上开发环境直接使用 ip 加端口访问。中间件配置划分 mysql，redis 等配置划分 2 份，没什么好说的spring boot 配置文件支持多环境配置 用 diamond 之类可以方便管理多个环境配置。但是需要格外部署一个中间件，暂时不折腾。spring boot 的 profiles 机制对多环境配置提供了支持，现阶段使用没有问题。application 配置文件规划如下 application.properties: 公共配置，例如各种超时配置application-local.properties：本地环境专有配置，例如 mysq、redis。application-dev.properties：开发环境专有配置application-prod.properties：生产环境专有配置 启动的时候增加应用参数，是 Program Arguments，并且是 --，比如1--spring.profiles.active=local 更新云效流水线部署脚本 这个应用使用云效的流水线模式部署。登录云效，选择要修改的流水线环境，比如“日常环境”进入修改页面，点击“日常”查看部署配置 在部署配置页面更新脚本 部署配置涉及打包文件存放路径，以及解压缩和执行应用。不同环境的打包文件存放在单独路径。这里提供一份参考 1234567set -e;THIS_ENV=devBASE_DIR=/home/admin/$&#123;THIS_ENV&#125;mkdir -p $&#123;BASE_DIR&#125;/medicalrm -f $&#123;BASE_DIR&#125;/medical/medical.jartar xf $&#123;BASE_DIR&#125;/package.tgz -o -C $&#123;BASE_DIR&#125;/medical;$&#123;BASE_DIR&#125;/deploy_medical.sh $&#123;THIS_ENV&#125; start 最后调用部署脚本是 deploy_medical.sh。 部署脚本 真正的部署脚本要处理旧应用的关闭，和新应用的启动。正路来说，可以使用 spring boot 的优雅关闭，迟点再接上。野路子的方式是 kill -15。这里要注意kill -9: 没有机会响应 shutdownHookkill -15: 有机会响应 shutdownHook 要先找到原来的旧应用才能 kill1ps aux | grep java | grep medical | grep $&#123;ENV&#125; | awk '&#123; print $2 &#125;要留点时间给 shutdownHook 做清理，因此建议 kill 之后 sleep 一小段时间。然后重新启动应用 1nohup java -jar medical.jar --spring.profiles.active=$&#123;ENV&#125; &amp; 大功告成。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>springboot</tag>
        <tag>devops</tag>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ProcessBuilder waitFor 调用外部应用]]></title>
    <url>%2Fp%2Fprocessbuilder-waitfor-to-start-a-process%2F</url>
    <content type="text"><![CDATA[小程序项目最初使用 ffmpeg 转换微信录音文件为 wav 格式，再交给阿里云 asr 识别成文字。视频音频转换最常用是 ffmpeg。1ffmpeg -i a.mp3 b.wav相关文章：小程序实现语音识别转文字，坑路历程 问题变成怎样使用 java 调用系统的 ffmpeg 工具。在 java 中，封装了进程 Process 类，可以使用 Runtime.getRuntime().exec() 或者 ProcessBuilder 新建进程。从 Runtime.getRuntime().exec()说起 最简单启动进程的方式，是直接把完整的命令作为 exec() 的参数。1234567try &#123; log.info("ping 10 times"); Runtime.getRuntime().exec("ping -n 10 127.0.0.1"); log.info("done");&#125; catch (IOException e) &#123; e.printStackTrace();&#125;输出结果 1217:12:37.262 [main] INFO com.godzilla.Test - ping 10 times17:12:37.272 [main] INFO com.godzilla.Test - done 我期望的是执行命令结束后再打印 done，但是明显不是。waitFor 阻塞等待子进程返回 Process 类提供了waitFor 方法。可以阻塞调用者线程，并且返回码。0 表示子进程执行正常。1234567891011121314151617/** * Causes the current thread to wait, if necessary, until the * process represented by this &#123;@code Process&#125; object has * terminated. This method returns immediately if the subprocess * has already terminated. If the subprocess has not yet * terminated, the calling thread will be blocked until the * subprocess exits. * * @return the exit value of the subprocess represented by this * &#123;@code Process&#125; object. By convention, the value * &#123;@code 0&#125; indicates normal termination. * @throws InterruptedException if the current thread is * &#123;@linkplain Thread#interrupt() interrupted&#125; by another * thread while it is waiting, then the wait is ended and * an &#123;@link InterruptedException&#125; is thrown. */public abstract int waitFor() throws InterruptedException;123456789101112try &#123; log.info("ping 10 times"); Process p = Runtime.getRuntime().exec("ping -n 10 127.0.0.1"); int code = p.waitFor(); if(code == 0)&#123; log.info("done"); &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;输出结果 1217:15:28.557 [main] INFO com.godzilla.Test - ping 10 times17:15:37.615 [main] INFO com.godzilla.Test - done 似乎满足需要了。但是，如果子进程发生问题一直不返回，那么 java 主进程就会一直 block，这是非常危险的事情。对此，java8 提供了一个新接口，支持等待超时。注意接口的返回值是 boolean，不是 int。当子进程在规定时间内退出，则返回 true。1public boolean waitFor(long timeout, TimeUnit unit)测试代码 1234567891011121314try &#123; log.info("ping 10 times"); Process p = Runtime.getRuntime().exec("ping -n 10 127.0.0.1"); boolean exit = p.waitFor(1, TimeUnit.SECONDS); if (exit) &#123; log.info("done"); &#125; else &#123; log.info("timeout"); &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 输出结果 1217:43:47.340 [main] INFO com.godzilla.Test - ping 10 times17:43:48.352 [main] INFO com.godzilla.Test - timeout 获取输入、输出和错误流 要获取子进程的执行输出，可以使用 Process 类的 getInputStream()。类似的有getOutputStream()、getErrorStream()。12345678910111213141516try &#123; log.info("ping"); Process p = Runtime.getRuntime().exec("ping -n 1 127.0.0.1"); p.waitFor(); BufferedReader bw = new BufferedReader(new InputStreamReader(p.getInputStream(), "GBK")); String line = null; while ((line = bw.readLine()) != null) &#123; System.out.println(line); &#125; bw.close(); log.info("done")&#125; catch (IOException e) &#123; e.printStackTrace();&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; 注意，GBK 是 Windows 平台的字符编码。输出结果 1234567891018:28:21.396 [main] INFO com.godzilla.Test - ping 正在 Ping 127.0.0.1 具有 32 字节的数据:来自 127.0.0.1 的回复: 字节 =32 时间 &lt;1ms TTL=128127.0.0.1 的 Ping 统计信息: 数据包: 已发送 = 1，已接收 = 1，丢失 = 0 (0% 丢失)，往返行程的估计时间 (以毫秒为单位): 最短 = 0ms，最长 = 0ms，平均 = 0ms18:28:21.444 [main] INFO com.godzilla.Test - done 这里牵涉到一个技术细节，参考 Process 类的 javadoc123456789101112* &lt;p&gt;By default, the created subprocess does not have its own terminal* or console. All its standard I/O (i.e. stdin, stdout, stderr)* operations will be redirected to the parent process, where they can* be accessed via the streams obtained using the methods* &#123;@link #getOutputStream()&#125;,* &#123;@link #getInputStream()&#125;, and* &#123;@link #getErrorStream()&#125;.* The parent process uses these streams to feed input to and get output* from the subprocess. Because some native platforms only provide* limited buffer size for standard input and output streams, failure* to promptly write the input stream or read the output stream of* the subprocess may cause the subprocess to block, or even deadlock.翻译过来是，子进程默认没有自己的 stdin、stdout、stderr，涉及这些流的操作，到会重定向到父进程。由于平台限制，可能导致缓冲区消耗完了，导致阻塞甚至死锁！网上有的说法是，开启 2 个线程，分别读取子进程的 stdout、stderr。不过，既然说是 By default，就是有非默认的方式，其实就是使用ProcessBuilder 类，重定向流。此功能从 java7 开始支持。ProcessBuilder 和 redirect12345678try &#123; log.info("ping"); Process p = new ProcessBuilder().command("ping -n 1 127.0.0.1").start(); p.waitFor(); log.info("done")&#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace();&#125;输出结果 1234567891019:01:53.027 [main] INFO com.godzilla.Test - pingjava.io.IOException: Cannot run program "ping -n 1 127.0.0.1": CreateProcess error=2, 系统找不到指定的文件。 at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at com.godzilla.Test.main(Test.java:13)Caused by: java.io.IOException: CreateProcess error=2, 系统找不到指定的文件。 at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.&lt;init&gt;(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 1 more 此处有坑：ProcessBuilder 的 command 列表要用字符串数组或者 list 形式传入 ！ ps. 在小程序项目上，一开始把ffmpeg -i a.mp3 b.wav 传入 ProcessBuilder，却看不到生成的 wav 文件，查了日志 CreateProcess error=2, 系统找不到指定的文件。 还以为是 ffmpeg 路径问题。后来查了 api 才发现掉坑了。正确的写法 1Process p = new ProcessBuilder().command("ping", "-n", "1", "127.0.0.1").start(); 刚才说的重定向问题，可以这样写 123Process p = new ProcessBuilder().command("ping", "-n", "1", "127.0.0.1") .redirectError(new File("stderr.txt")) .start(); 工作目录 默认子进程的工作目录继承于父进程。可以通过 ProcessBuilder.directory() 修改。一些代码细节 ProcessBuilder.Redirectjava7 增加了ProcessBuilder.Redirect 抽象，实现子进程的流重定向。Redirect 类有个 Type 枚举 1234567public enum Type &#123; PIPE, INHERIT, READ, WRITE, APPEND&#125;; 其中 PIPE: 表示子流程 IO 将通过管道连接到当前的 Java 进程。 这是子进程标准 IO 的默认处理。INHERIT: 表示子进程 IO 源或目标将与当前进程的相同。 这是大多数操作系统命令解释器（shell）的正常行为。 对于不同类型的 Redirect，覆盖下面的方法 appendappendTofilefromtoRuntime.exec()的实现 Runtime 类的 exec() 底层也是用 ProcessBuilder 实现 1234567public Process exec(String[] cmdarray, String[] envp, File dir) throws IOException &#123; return new ProcessBuilder(cmdarray) .environment(envp) .directory(dir) .start();&#125;ProcessImplProcess 的底层实现类是 ProcessImpl。 上面讲到流和 Redirect，具体在 ProcessImpl.start() 方法 123FileInputStream f0 = null;FileOutputStream f1 = null;FileOutputStream f2 = null; 然后是一堆繁琐的 if…else 判断是 Redirect.INHERIT、Redirect.PIPE，是输入还是输出流。总结 Process 类是 java 对进程的抽象。ProcessImpl 是具体的实现。Runtime.getRuntime().exec() 和 ProcessBuilder.start()都能启动子进程。Runtime.getRuntime().exec()底层也是 ProcessBuilder 构造的 Runtime.getRuntime().exec() 可以直接消费一整串带空格的命令。但是 ProcessBuilder.command()必须要以字符串数组或者 list 形式传入参数 默认子进程的执行和父进程是异步的。可以通过 Process.waitFor() 实现阻塞等待。默认情况下，子进程和父进程共享 stdin、stdout、stderr。ProcessBuilder 支持对流的重定向（since java7）流的重定向，是通过 ProcessBuilder.Redirect 类实现。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序实现语音识别转文字，坑路历程]]></title>
    <url>%2Fp%2Fminiapp-speech-to-text-experience%2F</url>
    <content type="text"><![CDATA[最近为小程序增加语音识别转文字的功能，坑路不断，特此记录。微信开发者工具 开发者工具上的录音文件与移动端格式不同，暂时只可在工具上进行播放调试，无法直接播放或者在客户端上播放 debug 的时候发现，工具上录音的路径是http://tmp/xxx.mp3，客户端上录音是wxfile://xxx.mp3。 忽悠呢，不是格式不同，是映射路径不同。 其实做个兼容也不难，每次提示一行文字，很丑。采样率与编码码率限制 每种采样率有对应的编码码率范围有效值，设置不合法的采样率或编码码率会导致录音失败。详细看这个 https://developers.weixin.qq.com/miniprogram/dev/api/media/recorder/RecorderManager.start.html 一开始没有留意，导致录音不成功。试过几次后，采用这样的配置，感觉录音识别率和体积之间比较好平衡：123sampleRate: 16000, // 采样率 numberOfChannels: 1, // 录音通道数encodeBitRate: 96000, // 编码码率 单通道基本是必选的。因为 asr 只支持单通道。frameSize 也是可以的，但是要考虑截断对识别的影响。暂时没有用上。录音优化 因为可能误按，于是对小于 500ms 的录音直接忽略。另外，松开录音按键后，再延迟一点时间才真正 stop 录音。录音文件格式 微信录音文件支持 mp3 和 aac。这 2 种格式文件都比较小，aac 文件体积更小。这对上传来说是件好事情，速度更快。但是对语音识别转文字就不友好了。因为百度、阿里云 ASR、讯飞的语音转文字接口都不支持 aac 和 mp3，通常要求是 pcm 或者 wav 格式。如果微信录音能提供 wav 格式，那么就不用服务器做格式转换了，但是 wav 格式体积是 mp3、aac 的 5 到 10 倍，至少短期是没戏了，这也是很多人吐槽的地方。服务器转换录音文件格式 可以用 java 第三方库转换，也可以用 Process 调用 ffmpeg 转换。要注意的是，根据识别 API 的要求来做转换。比如阿里云 asr 的要求是：支持音频编码格式：pcm(无压缩的 pcm 文件或 wav 文件)、opus，16bit 采样位数的单声道 (mono)； 支持音频采样率：8000Hz、16000Hz；java ProcessBuilder 要使用数组传参 转换音视频，习惯用 ffmpeg。安装完 ffmpeg 之后，用 java 新建进程调用。1Process = new ProcessBuilder("ffmpeg -i in.mp3 out.wav").start();一直提示 CreateProcess error。 后来看文档才发现，要以数组的形式传入参数。1Process = new ProcessBuilder("ffmpeg", "-y", "-i", "in.mp3", "out.wav").start(); 这样就启动成功了。关于 java 启动进程，不是本文重点，以后再写篇文章总结。阿里云 asr sdk 使用问题 这个问题困扰了一天时间，回想起来真是吐血。问题表现是微信录制的语音很多都识别不了。最初是直接把录音 mp3 文件转换为 pcm 文件，本地能播放，但是 用阿里云 asr sdk 却识别不了 。 一开始以为是文件编码问题。特意查了 asr 支持的文件格式，用 ffprobe 检查，potplayer 看属性，都没有看出问题。 甚至把启动 ffmpeg 进程转换也改了，用了 java 的库去做，还是不行。后来为了方便测试问题，用 asr 的 restful 接口测试录音文件，都能识别 ！ 似乎是 sdk 的问题。于是打开官方文档例子对比。发现用的是 sdk 2.x，老铁啊你复制粘贴过来的代码竟然少了！欲哭无泪。1234// TODO 重要提示：这里是用读取本地文件的形式模拟实时获取语音流并发送的，因为 read 很快，所以这里需要 sleep// TODO 如果是真正的实时获取语音，则无需 sleep, 如果是 8k 采样率语音，第二个参数改为 8000int deltaSleep = getSleepDelta(len, sampleRate);Thread.sleep(deltaSleep); 也少了对 sampleRate 的设置。阿里云 asr token 过期 因为用的是免费版 asr，没有给福报厂充值，因此 token 一天失效，导致联调的时候突然报错。最后实在受不了，写了个定时任务每小时更新 token。这，就是 beggar VIP😎wx.uploadFile 返回值 封装了一个接口 parseResponse，统一解析查询结果（文本、语音）。发现奇怪的问题：用文本查询的，可以正常解析结果 用语音查询的，明明已经返回了结果，却解析不了！只能 console.log() 打印出来对比 第一行是 wx.request() 发起文本查询。第二行是 wx.uploadFile() 上传语音文件后直接语音转文字，并且查询。wx.request返回值是 json 对象。wx.uploadFile返回值是“字符串”！wx.uploadFile返回值是“字符串”！wx.uploadFile返回值是“字符串”！重要的事情要说 3 遍。尽管 Content-Type: &quot;application/json; charset=utf8&quot;，但是微信根本不做转换！非常坑爹！ 解决：对 wx.uploadFile 返回值进行 JSON.parse(res.data)，得到 json 对象。 更换 appid 和 secret因为正式小程序项目账号一直拖着没有申请，所以这段时间用的是我个人的 appid 和 secret 进行开发。等正式账号准备好了，更新了小程序项目的 appid，并且发出内部体验包。此时已经深夜 1 点半，头脑有点发懵。只更新了小程序 appid，竟然忘了更新服务器的 appid 和 secret。。。于是乎反复报错登录失败。过了一会才反映过来，更新服务器的 appi 的 secret，但是还是用户。才想起忘了还有 storage 缓存没有清除😂，里面放着自定义的 session。这下真机体验没问题了。但是微信开发者工具又是登录失败。反复摸索后发现：更换小程序 appid 后，清除所有数据，关闭开发者工具，重新打开，这就正常了。应该是微信开发者工具的 bug。结论：深夜不宜加班写 bug😭。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序用户协议页面实现]]></title>
    <url>%2Fp%2Fminapp-user-licence-implementation%2F</url>
    <content type="text"><![CDATA[小程序用户协议页面设计思路 新增用户协议页面 首页加载（onLoad()）的时候，检查是否已经同意过，没有的话则弹出用户协议界面。点击详情跳转到用户协议页面 (使用wx.navigateTo) 用户点击同意后，才能继续使用小程序，并且保存到 storage效果图如下 控制显示用户协议窗口 在首页新增一个 view，根据全局 userAgree 的值，决定是否显示弹窗 123456789101112&lt;view wx:if='&#123;&#123;userAgree==false&#125;&#125;'&gt; &lt;view catchtouchmove="catchtouchmove" class="tips"&gt; &lt;view class="tips_box"&gt; &lt;view class="hint_view"&gt; &lt;view class="text"&gt; &lt;view class="hint1" bindtap='goToUserLicence'&gt; 点击查看《xx 小程序》使用协议 &lt;/view&gt; &lt;/view&gt; &lt;/view&gt; &lt;button bindtap='tipAgree' class="agreeBtn" type='primary'&gt; 我已阅读并同意 &lt;/button&gt; &lt;/view&gt; &lt;/view&gt;&lt;/view&gt; 因此要在首页增加一个全局变量 12// 用户协议var userAgree = false 更新 onLoad() 事件从 storage 读取 userAgree 字段 12345var that = thisvar userAgree = wx.getStorageSync('userAgree') || falsethat.setData(&#123; userAgree&#125;) 因为用户协议很长，因此点击查看会导航到另一个页面 12345678goToUserLicence: function()&#123; wx.navigateTo(&#123; url: '/pages/licence/licence', success: function(res) &#123;&#125;, fail: function(res) &#123;&#125;, complete: function(res) &#123;&#125;, &#125;)&#125; 首页用户协议弹窗用到的 css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263.tips &#123; display: flex; justify-content: center; align-items: center; position: fixed; left: 0; top: 0; z-index: 100; width: 100%; height: 100%; background: rgba(0,0,0,0.7);&#125;.tips .tips_box &#123; display: flex; flex-direction: column; align-items: center; width: 75%; height: auto; border-radius: 45rpx; background: #fff; overflow: hidden;&#125;.tips .tips_box .hint_view &#123; display: flex; align-items: center;&#125;.tips .tips_box .hint_view .text &#123; display: flex; flex-direction: column; margin: 12rpx 24rpx;&#125;.tips .tips_box .hint1 &#123; margin-top: 38rpx; text-align: center; font-size: 30rpx; color: #1a1a1a; line-height:52rpx; border-bottom:1px solid;&#125;.agreeBtn &#123; display: flex; justify-content: center; align-items: center; margin: 32rpx 0 32px; width: 70%; line-height: 64rpx; border-radius: 80rpx; font-size: 32rpx; letter-spacing: 6rpx; color: #fff;&#125;.isTipShow &#123; display: block;&#125;.isTipHide &#123; display: none;&#125;用户协议页面设计 作为 Java 后端架构汪，写起前端页面也就 hehehe 的水平，仅供参考。css 在线调试，用到这个工具 https://tool.chinaz.com/tools/cssdesigner.aspxlicence.wxml12345678910111213&lt;view&gt; &lt;view class='title'&gt;用户授权协议 &lt;/view&gt; &lt;view class='h1'&gt; 使用条款及声明 &lt;/view&gt; &lt;view&gt; xxx &lt;/view&gt; &lt;view class='h1'&gt; 小程序用途 &lt;/view&gt; &lt;view&gt; yyy &lt;/view&gt;&lt;view&gt;licence.wxss1234567891011121314/* pages/licence/licence.wxss */.title &#123; text-align: center; font-size: 20pt; font-weight : bold; margin: 20px;&#125;.h1 &#123; text-align: left; font-size: 16pt; margin: 10px;&#125; 参考 小程序开发：在登录时弹窗用户使用协议]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postman 自动计算 md5 加密验签]]></title>
    <url>%2Fp%2Fpostman-use-prescript-to-calculate-md5-sign%2F</url>
    <content type="text"><![CDATA[接口加了加密验签逻辑，具体是 md5(salt+ 时间戳)。被某君吐槽说测试不方便啊能不能先关掉。其实没有必要打开又关闭验签功能，postman 的 pre-request script 功能完全可以模拟客户端加密过程。创建环境变量 接口使用了 tm、sign 字段，先创建环境变量 pre-request script 脚本12345var tm = new Date().getTime()var salt = 'F5ZeNjdP2IpoLYc3'var sign = CryptoJS.MD5(salt + tm).toString()postman.setEnvironmentVariable('tm', tm);postman.setEnvironmentVariable('sign', sign); 使用 CryptoJS 计算 md5 加密。然后把 tm、sign 设置为环境变量。注意 url 参数的写法，是用双花括号包住环境变量：tm=验证 点击Send、Code，可以看到 tm 和 sign 已经被替换了。1234GET /test/hello2?tm=1564422732095&amp;amp; sign=69b5e46368f3e1f3aa3be03ddd4b7dae HTTP/1.1Host: localhost:8000cache-control: no-cachePostman-Token: f8388dfc-c1d7-4c99-a5f5-5839f31da081so easy！]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 开启 http2]]></title>
    <url>%2Fp%2Fnginx-enable-http2%2F</url>
    <content type="text"><![CDATA[都 9102 年，不开启 http2 都凹凸曼了。配置要求 nginx 版本》1.9.5 且带有 http2 模块1# nginx -Vopenssl 版本》1.0.2。12# openssl versionOpenSSL 1.1.1c 28 May 2019 配置 nginx123server&#123; listen 443 ssl http2;&#125;http2 支持加密（h2）和非加密（h2c，HTTP/2 cleartext）协议。但浏览器上的实现都是 h2。因此要配置 ssl。然后 1# nginx -s reload 测试 http2chrome 浏览器，F12、Network、选择 Protocol，显示为h2 则开启成功 https://tools.keycdn.com/http2-test 提供了在线测试工具 兼容性http2 向下兼容 http1.1。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>web</tag>
        <tag>网络安全</tag>
        <tag>https</tag>
        <tag>http2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决微信小程序开发者工具输入框焦点问题]]></title>
    <url>%2Fp%2Fsolution-for-miniapp-input-box-cant-enter%2F</url>
    <content type="text"><![CDATA[Windows10 笔记本上运行微信小程序开发者工具，输入框（input，textarea）没有焦点，只能在真机调试，效率太低。后来发现是 Window10 对笔记本高分屏支持不好，要 DPI 缩放，导致兼容性问题。解决方法：显示设置 、 缩放与布局 改为 100%。这样就可以点击输入框了，但是字体变得很小，最好是外接显示器。 不修改 DPI 缩放，长按输入框即可！2019.9.1 update: 新版微信开发工具已经修复此 bug。]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 nginx stream 模块做端口转发]]></title>
    <url>%2Fp%2Fuse-nginx-stream-module-as-ssh-tunnel%2F</url>
    <content type="text"><![CDATA[之前提到使用 ssh 隧道访问隔离的 mysql。通过 ssh 端口转发访问网络隔离的 mysqlnginx 也可以实现数据转发。常见的是 http 请求的转发（7 层）。新版本的 nginx 加入了 stream 模块，可以做 tcp 转发（4 层）。检查 nginx stream 模块 nginx v1.9.0 之后新增 stream 模块，但不是默认安装。1nginx -V 如果有 --with-stream 则表明有安装 stream 模块（此处有伏笔）。配置 nginx1234567891011stream &#123; upstream rds &#123; server rm-wz9p9rpee32z381re.mysql.rds.aliyuncs.com:3306; &#125; server &#123; listen 3307; proxy_pass rds; proxy_connect_timeout 1h; proxy_timeout 1h; &#125;&#125;这里使用 3307 端口映射 RDS 的 3306 端口。和 http 模块配置类似。具体参照 ngx_stream_core_module.html。 测试 nginx stream 配置 nginx -t，发现报错123# nginx -tnginx: [emerg] unknown directive "stream" in /etc/nginx/nginx.conf:13nginx: configuration file /etc/nginx/nginx.conf test failednginx unknown directive stream？怎么不认识 stream 指令呢？nginx -V 明明有 stream 模块 1--with-stream=dynamic 参考了这篇文章 unknown-directive-stream-in-etc-nginx-nginx-conf86。dynamic……nginx 支持动态模块，需要先载入才能使用。解决方法是nginx.conf 文件开头加上：1load_module /usr/lib/nginx/modules/ngx_stream_module.so;然后 12345# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful# nginx -s reloadECS 本地测试连接 rds mysql12# mysql -h localhost -P3307 -urootERROR 2002 (HY000): Can't connect to local MySQL server through socket'/var/run/mysqld/mysqld.sock'(2)-P 是端口，-p才是密码。这里要注意，localhost作为 host 的话，会直接访问 unix socket，然而 ECS 本地没有开启 mysqld 服务，导致报错。正确的方式是使用 127.0.0.1 作为 host。12# mysql -h 127.0.0.1 -P3307 -uroot -pEnter password:更新 ECS 安全组ECS 安全组策略放开 3307 端口的访问，这样就可以让外网访问到（从安全角度，强烈不建议）。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>linux</tag>
        <tag>ssh</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过 ssh 端口转发访问网络隔离的 mysql]]></title>
    <url>%2Fp%2Faccess-mysql-by-ssh-tunnel%2F</url>
    <content type="text"><![CDATA[问题 我的 mysql 数据库使用阿里云 RDS，默认情况下只能通过阿里云内网访问，除非走白名单申请才能开放外网访问权限。如下图所示：从系统安全性来说，对敏感的数据库资源进行网络隔离是很有必要的。但是日常开发来说，每次都要在 ECS 上运行 mysql client 操作就效率太低了。既然我可以 ssh 到 ECS 机器，ECS 机器又可以直接访问到 RDS，那么先 ssh 到 ECS，把所有操作 mysql 的请求，都交给 ECS 转发到 RDS，问题就解决了。这就是 ssh 端口转发（ssh port forwarding），也叫 ssh 隧道（ssh tunnel）。虽然对 mysql 的操作来自公网，但是数据经过 ssh 隧道，是被加解密的，因此安全性还是有的。期望的效果是，mysql 客户端连接本机的 3307 端口就可以访问远程的 mysql。实际上是通过 ECS 做端口转发到 RDS 的 3306 端口。使用 ssh 命令做隧道 我本地安装了 Cmder 的完整版，带有 ssh 命令，可以直接做端口转发。12345678# sshusage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-B bind_interface] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] destination [command]头晕了吧，这么多参数。需要关注的是 -L，本地转发。Cmder 没有 man，于是从 ECS 上拷贝一份解释1234567891011121314151617-L [bind_address:]port:host:hostport-L [bind_address:]port:remote_socket-L local_socket:host:hostport-L local_socket:remote_socket Specifies that connections to the given TCP port or Unix socket on the local-L local_socket:host:hostport-L local_socket:remote_socket Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to the given host and port, or Unix socket, on the remote side. This works by allocating a socket to listen to either a TCP port on the local side, optionally bound to the specified bind_address, or to a Unix socket. Whenever a connection is made to the local port or socket, the connection is forwarded over the secure channel, and a connection is made to either host port hostport, or the Unix socket remote_socket, from the remote machine. Port forwardings can also be specified in the configuration file. Only the supe‐ ruser can forward privileged ports. IPv6 addresses can be specified by enclosing the address in square brackets. 简单来说，-L &lt; 本地端口 &gt;:&lt; 目标服务器 &gt;:&lt; 目标端口 &gt;，对于我的例子，是 -L 3307:&lt;RDS 内网地址 &gt;:&lt;RDS 端口 3306&gt;1ssh -L 3307:&lt;RDS 内网地址 &gt;:3306 ecs_user@ecs_host 表示本机的 3307 端口做本地转发到 RDS 服务器的 3306 端口，由 ecs_host 做隧道。登录之后，就可以正常使用本地 3307 端口访问远程 mysql 了。这里还有个小问题。ssh 连接到 ECS 服务器，只是为了做端口转发，并不会使用 ssh 远程命令。另外，上面的 ssh 命令会一直占用 foreground，一旦关闭 ssh 连接，隧道也会关闭。对此，ssh 提供了 -N 和-f参数。-N参数只做端口转发。1-N Do not execute a remote command. This is useful for just forwarding ports.-f参数 ssh 保留在后台状态。1234567-f Requests ssh to go to background just before command execution. This is useful if ssh is going to ask for passwords or passphrases, but the user wants it in the background. This implies -n. The recommended way to start X11 programs at a remote site is with something like ssh -f host xterm. If the ExitOnForwardFailure configuration option is set to “yes”, then a client started with -f will wait for all remote port forwards to be successfully estab‐ lished before placing itself in the background.因此，最后的命令是 1ssh -fNL 3307:&lt;RDS 内网地址 &gt;:3306 ecs_user@ecs_hostWindows：使用 xshell 做 ssh 隧道 习惯图形界面操作的也可以用 xshell 来完成端口转发。新建连接到 ECS 的 ssh 配置 SSH、 隧道 。源主机选择localhost，监听端口是 3307。 目标主机、目标端口填 RDS 的内网地址和内网端口 。xshell 打开刚才配置的连接navicat 配置。主机是 localhost，端口 3307，账号和密码填 RDS 的。 注意这种方式，每次都要先打开 xshell 才能访问 mysql。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>mysql</tag>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置 hsts]]></title>
    <url>%2Fp%2Fnginx-enable-hsts%2F</url>
    <content type="text"><![CDATA[SSL strip（SSL 剥离攻击）一个站点 xyz.com，已经开启了 https 配置。站点期望用户访问的是https://xyz.com。 考虑这样的场景： 用户在地址栏输入 xyz.com。而这个地址，默认情况下会被浏览器解析成http://xyz.com。并没有走 https 配置！ 通常我们会在 nginx 做 https 永久强制跳转。permanent 表示 301 permanently redirect1rewrite ^(.*) https://$server_name$1 permanent;因为返回了 301 状态码，以后浏览器访问 xyz.com 都会自动变成 https://xyz.com。So far so good！但是，如果 step1 和 step2 之间有个不怀好意的中间人呢？浏览器以为是直接连接上xyz.com，并且建立了 https 连接。实际上却是浏览器和中间人建立了连接，中间人代替浏览器和xyz.com 建立 https 连接。那么浏览器和 xyz.com 的所有交互都会被中间人窥探。这就是 SSL 中间人剥离攻击。 图片来源 HTTP Strict Transport SecuritySSL 剥离攻击是中间人攻击的一种，由 Moxie Marlinspike 于 2009 年发明。 他在当年的黑帽大会上发表的题为《New Tricks For Defeating SSL In Practice》的演讲中将这种攻击方式公开。 SSL 剥离的实施方法是阻止浏览器与服务器创建 HTTPS 连接。 问题的根源在于用户发起的第一请求不是 https 请求，并且被中间人截获 。RFC 小组对此提出一个新的规范，HSTS(HTTP Strict Transport Security)。 对于第一次访问站站点的 https 请求 ，可以增加返回一个 header：Strict-Transport-Security，告诉浏览器这个域名的请求在指定时间内要使用 https 去连接。 注意浏览器会忽略非 https 响应的 Strict-Transport-Security 字段，因为这个字段可以被中间人篡改。HSTS 的配置选项有：max-age=&lt;expire-time&gt;设置在浏览器收到这个请求后的 秒的时间内凡是访问这个域名下的请求都使用 HTTPS 请求。includeSubDomains 可选 如果这个可选的参数被指定，那么说明此规则也适用于该网站的所有子域名。preload 可选 使用浏览器的预加载 HSTS 机制。预加载 HSTSHSTS 机制要在目标服务器第一个 https 响应后才生效。这还是留给中间人攻击机会。更好的方式是，开启了 https 服务的 xyz.com，浏览器只要见到xyz.com 都会自动访问 https://xyz.com。 这就是浏览器的预加载 HSTS。chrome 浏览器维护了一份预加载 HSTS 名单，只要向hstspreload.org 提交了站点，那么所有浏览器会强制使用 https 访问该站点。HSTS is supported in Google Chrome, Firefox, Safari, Opera, Edge and IE我的站点托管在 GitHub Pages 上，查看状态是 换我的项目地址试试 由此可见，申请加入预加载 HSTS 的条件是：对于 http 请求，必须跳转到 https返回 Strict-Transport-Security header 字段max-age 不能少于一年 子域名也要开启 HSTS，即 includeSubDomains 是必须的 必须包含 preload 指令 更新 nginx 配置，在 server 节点增加 123456789101112server &#123; listen 80 default; server_name &lt; 域名 &gt;; return 301 https://&lt; 域名 &gt;;&#125;server &#123; listen 443 ssl; server_name &lt; 域名 &gt;; add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always; # 其他省略&#125; 再次提交，提示可以加入预加载 HSTS 名单了 副作用 开启 HSTS，会强制该站点的请求都必须是 https，会有一定的副作用：SL 证书过期了，那么就不能正常访问。该站点的静态资源例如 js，css 也必须走 https 访问，有修改成本 参考资料RFC 6797 - HTTP Strict Transport Security (HSTS)New Tricks For Defeating SSL In Practice - Black HatHTTP Strict Transport Security]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>web</tag>
        <tag>网络安全</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 开启 TLSv1.3]]></title>
    <url>%2Fp%2Fenable-tls-v1-3-with-nginx%2F</url>
    <content type="text"><![CDATA[开启 TLS v1.3 的要求：openssl 版本 &gt;=1.1.1nginx 使用对应版本 openssl 构建 ubuntu 18.04LTS 默认是 openssl 1.1.0g。因此 2 个软件都要升级。 升级 openssl1234567# wget https://www.openssl.org/source/openssl-1.1.1c.tar.gz# gunzip openssl-1.1.1c.tar.gz # tar xvf openssl-1.1.1c.tar # cd openssl-1.1.1c/# ./config# make# make install新的 openssl 放在 apps 目录。先尝试执行 1234# cd apps# ./openssl./openssl: /usr/lib/x86_64-linux-gnu/libssl.so.1.1: version `OPENSSL_1_1_1'not found (required by ./openssl)./openssl: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: version `OPENSSL_1_1_1' not found (required by ./openssl) 这是因为 so 文件没有加入到动态链接库路径。google 到这个帖子 `OPENSSL_1_1_1’ not found (required by openssl)123456# echo "export LD_LIBRARY_PATH=/usr/local/lib" &gt;&gt; ~/.bashrc# ldconfig# opensslOpenSSL&gt; versionOpenSSL 1.1.1c 28 May 2019 至此，openssl 升级成功。升级 nginx一般思路是，卸载旧的 nginx，再用源码构建，比较麻烦的是 configure 加上一堆 with_XXX module。google 发现有个 PPA 库正合适。参见 How to Easily Enable TLS 1.3 in Nginx on Ubuntu 18.10, 18.04, 16.04, 14.041234# add-apt-repository ppa:ondrej/nginx# apt update# apt remove nginx# apt install nginx 中途会询问要不要更新配置文件。因为我已经配置好，选择保留旧的。123# nginx -Vnginx version: nginx/1.16.0built with OpenSSL 1.1.1b 26 Feb 2019 (running with OpenSSL 1.1.1c 28 May 2019)nginx 配置 1ssl_protocols TLSv1.2 TLSv1.3; 验证 ssllabs.comchrome75 上要手动开启 TLS1.3 支持。地址栏输入chrome://flag，搜索tls 重启 chrome 后，打开我的项目站点]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置 dhparam，以及聊聊 forward secrecy]]></title>
    <url>%2Fp%2Fnginx-ssl-dhparam-and-forward-secrecy%2F</url>
    <content type="text"><![CDATA[Forward Secrecy，前向保密 前向保密（英语：Forward Secrecy，FS），有时也被称为完全前向保密（英语：Perfect Forward Secrecy，PFS）。长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。要理解 Forward Secrecy，要先理解 TLS 连接的建立过程。基于 RSA 的 SSL 握手 client 发送 client hello 信息，包括：协议版本，支持的加密套件，以及随机数据“client random”。（明文）server 响应，包括：SSL 证书（RSA public key），优先使用的加密套件，以及随机数据“server random”。（明文）client 创建新的随机数据，“premaster secret”，然后用 step2 返回的 SSL 证书加密 premaster secret，发送给 server。（密文）server 使用 RSA private key 解密 step3 的内容，得到明文的 premaster secret。 现在 client，server 都得到了 client random，server random 和 premaster。把这 3 者组合得到 session key，后续通讯基于此 session key。这个方案的安全性基于只有 server 知道 private key。一旦 private key 泄露，那么所有使用这个 SSL 证书的密文 premaster secret 都能被解密！这个方案的问题在于，private key 既用于 SSL 证书身份识别（authentication），又用于 premaster secret 的解密。因此解决思路是，改变交换 premaster secret 的方式。基于 DH 的 SSL 握手 Ephemeral Diffie-Hellman (DHE) 算法（ephemeral 是临时的意思，表明同一个 key 不会使用两次），是其中一种解决 premaster secret 交换安全性的算法。RSA 握手方式，premaster 由 client 生成，并且加密。但是 DHE 算法，client 和 server 使用协商的 DH param，分别计算 premaster secret。client 发送 client hello 信息，包括：协议版本，支持的加密套件，以及随机数据“client random”。（明文）server 响应，包括：SSL 证书（RSA public key），优先使用的加密套件，随机数据“server random”，server DH param。并且使用 privae key 对 client random，server random，DH 参数加密 。（密文）client 用 public key 解密 step2，验证（authentication）了服务器身份，并且得到 server DH param。client 回复 client DH param。 现在 client 和 server 都有 client DH param，server DH param，分别计算，得到 premaster secret。client 和 server 都有 client random，server random，premaster，组合得到 session key。注意到 private key 只在 step2 使用。即使 private key 泄露，也只得到一个临时的 DH param。因为 premaster secret 是每个 session 生成、不在网络上传输、并且不经过长期密钥加密 ，除非攻击每个 session，否则会话是安全的。从而实现 Forward Secrecy。DH 算法具体的数学原理此处不展开了。 支持 FS 的加密算法 基于 DHE，包括 DHE-RSA 和 DHE-DSA基于椭圆曲线迪菲 - 赫尔曼密钥交换（ECDHE）包括，ECDHE-RSA 与 ECDHE-ECDSADHE 算法的缺点是计算开销大。优先选择 ECDHE，性能更好。nginx 配置 nginx 开启 FP 的条件：cipher suite 是 ECDHE 或者 DHE(ps. TLSv1.3 只支持 PFS 的 ciphers) 配置 dhparam这里有个坑，nginx 使用 openssl 生产 DH 参数，默认是 1024bit，强度太低，至少是 2048 以上才行。生成 dhparam1openssl dhparam -out dhparam.pem 2048更新配置 server1234server &#123; ssl_dhparam cert/dhparam.pem; ssl_ecdh_curve X25519:prime256v1:secp384r1;&#125;参考资料Perfect Forward Secrecy - Why You Should Be Using ItHow Does Keyless SSL Work? | Forward Secrecy]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>网络安全</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 开启 ocsp stapling]]></title>
    <url>%2Fp%2Fnginx-enable-ocsp%2F</url>
    <content type="text"><![CDATA[OCSP stapling客户端建立 tls 连接时，会发起完整的 http 请求，查询证书的状态（有效、撤销 revoke），这个操作是阻塞的 。 开启服务器的 OCSP stapling 功能，服务器会代替客户端查询证书状态，并且缓存响应结果，直接返回给客户端，从而省去客户端主动查询 OCSP 的耗时。OCSP(Online Certificate Status Protocol): 在线证书状态协议。 一张 SSL 证书（例如 DigiCert 签发的）有有效期，但是在有效期内可能会被撤销（revoke）。因此客户端需要有办法知道证书是否有效。一种做法是 CRL（Certificate Revocation List），维护一个证书撤销列表，浏览器定时去从指定网址下载。CRL 有明显的不足：列表越来越大 更新不及时 （ps. Chrome 自己维护一份CRLSet） 就这样诞生了 OCSP，在线查询证书的状态，解决了证书有效的实时问题——客户端拿到证书，去 CA 服务器查询证书是不是有效。但是新的问题又产生了： 大家都去 CA 服务器查询 OCSP，流量太大了 客户端发起的 CA OCSP 的查询，是一个 http 同步请求，必须等待结果返回才进行下一步握手操作 隐私问题。因为客户端直接查询 CA 服务器，暴露 ip 地址 于是就诞生了 OCSP stapling。由服务器端主动向 CA OCSP 主动查询证书，缓存结果，并且返回给客户端，加快客户端 生成 OCSP 证书链 思路：获取中间证书和根证书 把证书粘贴在一起。注意顺序是 中间证书在上面，根证书在下面 可以在 Windows 上操作，也可以使用 openssl 工具。Windows 操作 打开网页，点击地址栏的证书 中间证书是 Encryption Everywhere DV TLS CA - G1，根证书是DigiCert。 使用导出证书向导，注意格式是 Base64 编码 X.509 分别得到 middle.cer，root.cer。按照 midle.cer、root.cer 的顺序，把文件内容粘贴到新文件 ocsp.cer，并且上传到 nginx 可以访问的路径。openssl对 openssl 操作不熟悉，因此参考了 从无法开启 OCSP Stapling 说起 获取证书链 证书链一般由根证书、一个或多个中间证书、站点证书组成。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122$ openssl s_client -connect &lt;your host&gt;:443 -showcerts &lt; /dev/null 2&gt;&amp;1CONNECTED(00000003)depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert Global Root CAverify return:1depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = Encryption Everywhere DV TLS CA - G1verify return:1depth=0 CN = www.ilovenancy1314.comverify return:1---Certificate chain 0 s:/CN=www.ilovenancy1314.com i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1-----BEGIN CERTIFICATE-----MIIFqTCCBJGgAwIBAgIQCaWHzDxuO5/QR+N2QZbpTjANBgkqhkiG9w0BAQsFADBuMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMS0wKwYDVQQDEyRFbmNyeXB0aW9uIEV2ZXJ5d2hlcmUgRFYgVExTIENBIC0gRzEwHhcNMTkwNzE4MDAwMDAwWhcNMjAwNzE3MTIwMDAwWjAhMR8wHQYDVQQDExZ3d3cuaWxvdmVuYW5jeTEzMTQuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmUQOTRCw+AUqxn7W3j+juu2FbIbRdDAX5NC0DOsotPVF8//w5iyafxTGz9SiBdYG6Ua9fB4YLaVYxYayn2+qIqIwQfYh9w0DXDE8F/4NNl3+dLf3HUK2PJ9qWxpNie61m3q4CTNmJjmSF37iULsS36eTyKdv3S3bqCixBz9tb9Boc8xIvwmeSma+DOvBrd7HDj2wbeDrcJ3Y9g49Fl97vu/VCHLncLJIEepJsDIH1Vm8nMeBS6z71JtC76RPUX0gUmBYdA3cLuUOWDV9zML18SMXtNa5svIfzirP0WthZ+OwF5SJsYxY9/wJRf3vfZLA6uwieJ0P4UlOHO3BxR4zkwIDAQABo4ICjjCCAoowHwYDVR0jBBgwFoAUVXRPsnJP9WC6UNHX5lFcmgGHGtcwHQYDVR0OBBYEFNpOCUesFuc9/TBD+oEYvbfFE9bbMDUGA1UdEQQuMCyCFnd3dy5pbG92ZW5hbmN5MTMxNC5jb22CEmlsb3ZlbmFuY3kxMzE0LmNvbTAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMEwGA1UdIARFMEMwNwYJYIZIAYb9bAECMCowKAYIKwYBBQUHAgEWHGh0dHBzOi8vd3d3LmRpZ2ljZXJ0LmNvbS9DUFMwCAYGZ4EMAQIBMIGABggrBgEFBQcBAQR0MHIwJAYIKwYBBQUHMAGGGGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBKBggrBgEFBQcwAoY+aHR0cDovL2NhY2VydHMuZGlnaWNlcnQuY29tL0VuY3J5cHRpb25FdmVyeXdoZXJlRFZUTFNDQS1HMS5jcnQwCQYDVR0TBAIwADCCAQQGCisGAQQB1nkCBAIEgfUEgfIA8AB2AKS5CZC0GFgUh7sTosxncAo8NZgE+RvfuON3zQ7IDdwQAAABbAVQ4ysAAAQDAEcwRQIgCRzYPNZC/ZpuT4qLLSKhAnbyeulGxR3pbi42Y41STS8CIQC0tQXg6OKZtEfssp3lf+go0qyzTiInh4u9bn/8q9XlNAB2AF6nc/nfVsDntTZIfdBJ4DJ6kZoMhKESEoQYdZaBcUVYAAABbAVQ4tYAAAQDAEcwRQIgRWyssfENvdOdSeXhMILRyGyPmFQQOjBLj7IuM07Fs7ACIQDVvY7R7V9v7VMBTA0kyiBFLlGmk1Rt2Fj/ioRPCaabyTANBgkqhkiG9w0BAQsFAAOCAQEACtbU90Lw2Ob+TbLV3bEsnm5zbcXBQa6eUVR1AANNOeTsONWwz6rm8c5svrk+51ipuHmx1DlBkE8DmcgKjb5jbY3pLl2bxqMi63jlIoBRnKIHJcNtwamyj0bIbuVTa7p9vf7UPGm12pFC5mD0ILRUB0m8xAKByrhAeYNNMLv7dAfru60HxuD9I/zmsTrgPC8/tl1V41MZnGmlodimNu487WH1P5TliSylMBn1iSs+ZmYLxbmsolnjUP+fpxl9TEcs51TrfSBJSTNVQsTo6dteKY69BlorsSTkeX4FLKFFAvTYsFsMm4WqQK6vnUuS+OfWC/tpT8KM6qDZOB5AqlRPhQ==-----END CERTIFICATE----- 1 s:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1 i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA-----BEGIN CERTIFICATE-----MIIEqjCCA5KgAwIBAgIQAnmsRYvBskWr+YBTzSybsTANBgkqhkiG9w0BAQsFADBhMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBDQTAeFw0xNzExMjcxMjQ2MTBaFw0yNzExMjcxMjQ2MTBaMG4xCzAJBgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5jb20xLTArBgNVBAMTJEVuY3J5cHRpb24gRXZlcnl3aGVyZSBEViBUTFMgQ0EgLSBHMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALPeP6wkab41dyQh6mKcoHqt3jRIxW5MDvf9QyiOR7VfFwK656es0UFiIb74N9pRntzF1UgYzDGu3ppZVMdolbxhm6dWS9OK/lFehKNT0OYI9aqk6F+U7cA6jxSC+iDBPXwdF4rs3KRyp3aQn6pjpp1yr7IB6Y4zv72Ee/PlZ/6rK6InC6WpK0nPVOYR7n9iDuPe1E4IxUMBH/T33+3hyuH3dvfgiWUOUkjdpMbyxX+XNle5uEIiyBsi4IvbcTCh8ruifCIi5mDXkZrnMT8nwfYCV6v6kDdXkbgGRLKsR4pucbJtbKqIkUGxuZI2t7pfewKRc5nWecvDBZf3+p1MpA8CAwEAAaOCAU8wggFLMB0GA1UdDgQWBBRVdE+yck/1YLpQ0dfmUVyaAYca1zAfBgNVHSMEGDAWgBQD3lA1VtFMu2bwo+IbG8OXsj3RVTAOBgNVHQ8BAf8EBAMCAYYwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMBIGA1UdEwEB/wQIMAYBAf8CAQAwNAYIKwYBBQUHAQEEKDAmMCQGCCsGAQUFBzABhhhodHRwOi8vb2NzcC5kaWdpY2VydC5jb20wQgYDVR0fBDswOTA3oDWgM4YxaHR0cDovL2NybDMuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0R2xvYmFsUm9vdENBLmNybDBMBgNVHSAERTBDMDcGCWCGSAGG/WwBAjAqMCgGCCsGAQUFBwIBFhxodHRwczovL3d3dy5kaWdpY2VydC5jb20vQ1BTMAgGBmeBDAECATANBgkqhkiG9w0BAQsFAAOCAQEAK3Gp6/aGq7aBZsxf/oQ+TD/BSwW3AU4ETK+GQf2kFzYZkby5SFrHdPomunx2HBzViUchGoofGgg7gHW0W3MlQAXWM0r5LUvStcr82QDWYNPaUy4taCQmyaJ+VB+6wxHstSigOlSNF2a6vg4rgexixeiV4YSB03Yqp2t3TeZHM9ESfkus74nQyW7pRGezj+TC44xCagCQQOzzNmzEAP2SnCrJsNE2DpRVMnL8J6xBRdjmOsC3N6cQuKuRXbzByVBjCqAA8t1L0I+9wXJerLPyErjyrMKWaBFLmfK/AHNF4ZihwPGOc7w6UHczBZXH5RFzJNnww+WnKuTPI0HfnVH8lg==-----END CERTIFICATE--------Server certificatesubject=/CN=www.ilovenancy1314.comissuer=/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=Encryption Everywhere DV TLS CA - G1---No client certificate CA names sentPeer signing digest: SHA512Server Temp Key: X25519, 253 bits---SSL handshake has read 3295 bytes and written 269 bytesVerification: OK---New, TLSv1.2, Cipher is ECDHE-RSA-AES128-GCM-SHA256Server public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONENo ALPN negotiatedSSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-AES128-GCM-SHA256 Session-ID: 0499074C6EEEDE1BB2810F7535655F965ADE964C182D2CD8E196AC1B9B187E66 Session-ID-ctx: Master-Key: A9139C9234D8DEEBA74194AF62AA52EEB942CF671B79793897F352103EC335347AB4ECEA3FF495C3404555F21C4DCD8E PSK identity: None PSK identity hint: None SRP username: None TLS session ticket lifetime hint: 300 (seconds) TLS session ticket: 0000 - 83 b7 6e 18 91 58 5f 46-f3 d5 e6 49 e5 7f 76 2a ..n..X_F...I..v* 0010 - 99 77 51 ed e6 ba b3 b1-41 e7 bd 1a 79 6f 68 be .wQ.....A...yoh. 0020 - 0c b9 60 ba 29 a1 73 e8-92 b4 71 34 eb ab cd 72 ..`.).s...q4...r 0030 - 1d 97 a1 aa c7 d3 09 2b-be ea d5 9e 31 35 82 e1 .......+....15.. 0040 - 4b d7 7a 92 b8 4e 23 05-65 18 d5 b2 39 da da 85 K.z..N#.e...9... 0050 - 8c f4 28 50 83 cb 59 0c-41 0a ec 2b 8b 2d 2e 61 ..(P..Y.A..+.-.a 0060 - 35 66 95 23 0e 25 ca f9-29 5b 6a d1 9e 32 5c 0d 5f.#.%..)[j..2\. 0070 - 1f 2e 7e 35 7e 71 6b f6-6c 63 df 06 7b 09 02 46 ..~5~qk.lc..&#123;..F 0080 - 88 ae ba 6f 77 53 1c 6c-fb 8f 26 2e fc c9 b8 e3 ...owS.l..&amp;..... 0090 - 98 cc 55 58 4e 62 52 cb-58 6f 1c 55 51 97 67 74 ..UXNbR.Xo.UQ.gt 00a0 - 4c 7b c3 02 ca d7 7f 9e-dc 05 c7 4d dd f2 6c b8 L&#123;.........M..l. Start Time: 1563886499 Timeout : 7200 (sec) Verify return code: 0 (ok) Extended master secret: yes---DONE上面的 depth 部分是证书层次。Certificate chain开始，分别把 -----BEGIN CERTIFICATE----- 到-----END CERTIFICATE-----站点证书、中间证书都保存到 site.perm、middle.pem。然后查看 Linux 系统安装的根证书 1ls /usr/share/ca-certificates/mozilla/ 找到对应的根证书，我的是 DigiCert_Global_Root_CA.crt。 验证 ocsp 响应 首先获取根证书的 ocsp 查询地址 12$ openssl x509 -in site.pem -noout -ocsp_urihttp://ocsp.digicert.com12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ openssl ocsp -issuer middle.pem -cert site.pem -no_nonce -text -url http://ocsp.digicert.comOCSP Request Data: Version: 1 (0x0) Requestor List: Certificate ID: Hash Algorithm: sha1 Issuer Name Hash: 978B4716E5B0F658BAE69DAB1689B8363AE3C3A6 Issuer Key Hash: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Serial Number: 09A587CC3C6E3B9FD047E3764196E94EOCSP Response Data: OCSP Response Status: successful (0x0) Response Type: Basic OCSP Response Version: 1 (0x0) Responder Id: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Produced At: Jul 22 19:32:58 2019 GMT Responses: Certificate ID: Hash Algorithm: sha1 Issuer Name Hash: 978B4716E5B0F658BAE69DAB1689B8363AE3C3A6 Issuer Key Hash: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Serial Number: 09A587CC3C6E3B9FD047E3764196E94E Cert Status: good This Update: Jul 22 19:32:58 2019 GMT Next Update: Jul 29 18:47:58 2019 GMT Signature Algorithm: sha256WithRSAEncryption 94:5f:bc:8b:d7:94:18:90:a0:e4:08:51:53:40:56:9f:b6:a8: 64:8a:c1:0b:e6:a6:18:c7:2c:5e:34:64:76:be:99:60:e9:72: b0:8e:e6:85:19:fa:53:72:83:8d:7d:57:67:62:3e:0d:e6:56: fd:f1:c6:f4:f0:5b:a6:44:af:c0:0d:96:c1:fa:86:35:1c:ce: a8:6c:59:a2:21:51:0c:45:e8:04:e7:b9:2f:f1:a9:61:01:bb: ce:7c:1e:a9:d4:fe:85:bf:ff:a9:44:fb:86:e9:f9:36:1d:4f: eb:fd:69:a3:c5:a4:4f:eb:f7:f1:ae:70:ad:0a:e9:a9:4e:0d: 33:ae:8f:b3:a9:af:e1:6d:77:fb:76:c1:42:33:ba:f9:ef:ed: 73:8d:ff:95:26:c6:07:2a:42:e5:a5:e5:02:95:b1:41:76:af: 34:90:53:6f:98:46:53:6a:eb:69:b0:03:ff:71:01:00:12:43: c2:55:61:52:a9:81:7c:73:00:8f:e1:0e:19:04:fe:0d:f8:bc: 30:47:d4:88:11:85:cb:ed:7d:ae:eb:bf:74:b9:cb:d6:d4:67: c0:7e:c5:a9:4d:f2:80:55:ae:95:4c:27:d2:2e:dc:f5:e3:5c: b0:99:92:a1:95:22:f4:0e:42:72:7e:49:f2:95:af:8e:c5:71: 6b:1e:fe:59Response verify OKsite.pem: good This Update: Jul 22 19:32:58 2019 GMT Next Update: Jul 29 18:47:58 2019 GMT 合成 ocsp 证书，并且验证 按照中间证书、根证书的顺序，合并成 mix.pem12cat middle.pem &gt;&gt; mix.pemcat root.pem &gt;&gt; mix.pem再次验证 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ openssl ocsp -CAfile mix.pem -issuer middle.pem -cert site.pem -no_nonce -text -url http://ocsp.digicert.comOCSP Request Data: Version: 1 (0x0) Requestor List: Certificate ID: Hash Algorithm: sha1 Issuer Name Hash: 978B4716E5B0F658BAE69DAB1689B8363AE3C3A6 Issuer Key Hash: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Serial Number: 09A587CC3C6E3B9FD047E3764196E94EOCSP Response Data: OCSP Response Status: successful (0x0) Response Type: Basic OCSP Response Version: 1 (0x0) Responder Id: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Produced At: Jul 22 19:32:58 2019 GMT Responses: Certificate ID: Hash Algorithm: sha1 Issuer Name Hash: 978B4716E5B0F658BAE69DAB1689B8363AE3C3A6 Issuer Key Hash: 55744FB2724FF560BA50D1D7E6515C9A01871AD7 Serial Number: 09A587CC3C6E3B9FD047E3764196E94E Cert Status: good This Update: Jul 22 19:32:58 2019 GMT Next Update: Jul 29 18:47:58 2019 GMT Signature Algorithm: sha256WithRSAEncryption 94:5f:bc:8b:d7:94:18:90:a0:e4:08:51:53:40:56:9f:b6:a8: 64:8a:c1:0b:e6:a6:18:c7:2c:5e:34:64:76:be:99:60:e9:72: b0:8e:e6:85:19:fa:53:72:83:8d:7d:57:67:62:3e:0d:e6:56: fd:f1:c6:f4:f0:5b:a6:44:af:c0:0d:96:c1:fa:86:35:1c:ce: a8:6c:59:a2:21:51:0c:45:e8:04:e7:b9:2f:f1:a9:61:01:bb: ce:7c:1e:a9:d4:fe:85:bf:ff:a9:44:fb:86:e9:f9:36:1d:4f: eb:fd:69:a3:c5:a4:4f:eb:f7:f1:ae:70:ad:0a:e9:a9:4e:0d: 33:ae:8f:b3:a9:af:e1:6d:77:fb:76:c1:42:33:ba:f9:ef:ed: 73:8d:ff:95:26:c6:07:2a:42:e5:a5:e5:02:95:b1:41:76:af: 34:90:53:6f:98:46:53:6a:eb:69:b0:03:ff:71:01:00:12:43: c2:55:61:52:a9:81:7c:73:00:8f:e1:0e:19:04:fe:0d:f8:bc: 30:47:d4:88:11:85:cb:ed:7d:ae:eb:bf:74:b9:cb:d6:d4:67: c0:7e:c5:a9:4d:f2:80:55:ae:95:4c:27:d2:2e:dc:f5:e3:5c: b0:99:92:a1:95:22:f4:0e:42:72:7e:49:f2:95:af:8e:c5:71: 6b:1e:fe:59Response verify OKsite.pem: good This Update: Jul 22 19:32:58 2019 GMT Next Update: Jul 29 18:47:58 2019 GMTnginx 配置 在 server 节点增加 12345server &#123; ssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate cert/ocsp/ocsp.cer;&#125;nginx -t 测试没问题后，nginx -s reload更新。验证 ssllabs.com 再次测试，查找 ocsp。 或者直接使用 openssl 验证 12345$ openssl s_client -connect &lt;your host&gt;:443 -status -tlsextdebug &lt; /dev/null 2&gt;&amp;1 | grep -i "OCSP response"OCSP response: OCSP Response Data: OCSP Response Status: successful (0x0) Response Type: Basic OCSP Response 表明配置通过。OCSP Must-Staplessllabs.com 证书页面，还有一个属性 OCSP Must-Staple。 浏览器可能不知道服务器端支持 OCSP stapling 特性，或者浏览器不响应 OCSP stapling 的结果。导致 OCSP stapling 没有发挥作用。 因此证书扩展了一个新特性，OCSP Must-Staple，强制浏览器必须对 OCSP stapling 结果做出响应。详细内容可以参考资料。参考资料 从无法开启 OCSP Stapling 说起OCSP Must-Staple]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>网络安全</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修复 intellij 打开 properties 文件乱码]]></title>
    <url>%2Fp%2Ffix-intellij-properties-file-garbled%2F</url>
    <content type="text"><![CDATA[案例 项目之前是在 windows 上开发，没问题。今天朋友在 mac 上打开 properties 文件一堆乱码。。。又是编码问题。在 intellij 配置里面搜索“encoding”，修改之前 因为我的电脑是 Windows，默认的系统编码就是 GBK。GBK 是微软的标准，在 mac 上打开当然有问题了。mac 上的默认编码一般是 utf8，具体使用 locale 命令可以查看。全部改成 utf8 即可。这样跨平台用 utf8 格式打开就不会乱码了。java 应用也能正常访问。接下来要把原来的 GBK 文件转成 utf8，因为配置只对新的文件生效！接下来要把原来的 GBK 文件转成 utf8，因为配置只对新的文件生效！接下来要把原来的 GBK 文件转成 utf8，因为配置只对新的文件生效！因为只有一个文件受影响，手动操作一下就好。用 vscode 以 GBK 编码打开原来的 properties 文件，确定中文不乱码，再复制到 idea 之中，保存即可。但是会有另一个问题。原来的 老铁 ，转码之后变成\u8001\u94c1。根本不是人看的。idea 有个相当窝心的选项，Transparent native-to-ascii conversion，打开之后会把 properties 文件的\u8001\u94c1 又重新显示成中文。最终效果如下。总结 编辑器的基础配置应该统一设置，避免产生类似问题。最近换了 2 台电脑，一时疏忽。ps. 如果是正版的 intellij，账号自带配置同步功能。。。家境贫寒，告辞。]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 配置 SSL 证书，以及 ssl_ciphers 选择]]></title>
    <url>%2Fp%2Fsetup-nginx-with-ssl-cert%2F</url>
    <content type="text"><![CDATA[新域名备案一直等待回复，只能先使用旧域名和相应 SSL 证书。准备 从 SSL 证书颁发者下载证书，不同的颁发者提供的证书和文件可能有出任，需要进行转换。通常包含 key 和 pem 两个文件。保存到服务器，并且确保 nginx 程序具有访问权限。我把证书和密钥放在 /etc/nginx/cert 目录。ssllabs.comssllabs.com是一个很好的站点工具，检查证书安全、SSL 配置安全、SSL 已知漏洞检测。强烈安利。nginx 配置 12345678910111213server&#123; listen 443 ssl; server_name "修改为您证书绑定的域名"; ssl_certificate cert/cert.pem; # 将 domain name.pem 替换成您证书的文件名。 ssl_certificate_key cert/cert.key; # 将 domain name.key 替换成您证书的密钥文件名。 ssl_session_timeout 5m; ssl_session_cache shared:SSL:10m; ssl_prefer_server_ciphers on; ssl_ciphers HIGH:!aNULL:!eNULL:!MD5:!RC4:!DES:!PSK:!EXPORT:!SHA:!SHA256; ssl_protocols TLSv1.2; &#125;ssl_session_timeout, ssl_session_cachessl 握手是一个消耗 cpu 资源的操作。因此在多个 worker 进程之间共享 ssl session 可以提升性能。1M 的缓存大概包含 4000 个 session，默认的缓存超时时间为 5 分钟。考虑到小程序用完就走的场景，通常设置 5min 够用了。 只用 shared 缓存要比 built-in 性能好 。参见ssl_protocolsbut using only shared cache without the built-in cache should be more efficient.ssl_prefer_server_ciphers 优先使用服务器端 cipher 配置。ssl_protocols指定握手协议。最新已经有 TLS 1.3，但是需要 openssl 升级 v1.1.1，以及客户端支持。以后再折腾。TLS1.0、TLS1.1 计划在 2020 年被废弃，直接忽略。于是只保留 TLS1.2。ssl_ciphersssl_ciphers 配置使用的加密套件，使用 OpenSSL 的格式，具体可以参照 ciphers.html 比如 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 代表的含义是：TLS - the protocol usedECDHE - the key exchange mechanismECDSA - the algorithm of the authentication keyAES - the symmetric encryption algorithm128 - the key size of the aboveGCM - the mode of the aboveSHA256 - the MAC used by the algorithm经常见到 HIGH:!aNULL 之类的配置。HIGH是一个 ciphers 宏，代表一系列组合。!代表不使用该 cipher。使用命令查看配置的 ciphers。1openssl ciphers -V 'HIGH:!aNULL'记住一点，NULL 相关的都加上 ! 就是了。aNULL 是没有认证阶段。The cipher suites offering no authentication. This is currently the anonymous DH algorithms and anonymous ECDH algorithms. These cipher suites are vulnerable to “man in the middle” attacks and so their use is discouraged.NULL、eNULL 是根本不加密。The “NULL” ciphers that is those offering no encryption. Because these offer no encryption at all and are a security risk they are not enabled via either the DEFAULT or ALL cipher strings.使用 HIGH:!aNULL:!eNULL 就高枕无忧吗，不是的，下面是 ssllabs.com 的测试结果，TLS1.2 页面 点击 Safari 10 / iOS 10，发现一堆 weak 提示cipher 安全性的一些经验CHACHA20 是 goolge 几年前对移动设备的优化算法，速度比 AES 快，也省电，一度是 google 极力推荐的。但是自从 ARMv7 支持硬件 AES 加密之后，chacha20 就比不上了。MD5，RC4，DES，SHA1 等都是已知不安全的选项，容易被攻击。SHA256，SHA384 要比 SHA1 安全。但是 CBC 模式的 SHA 是不安全。GCM 模式的 SHA 是安全的。 苹果旧设备默认是 CBC_SHA256，CBC_SHA384减少非安全 ciphers 能够提高安全性，但是对老的系统、设备兼容性下降，甚至握手失败。推荐一个 mozilla 的站点 Security/Server Side TLS，里面有官方推荐的 tls ciphers 组合。一般情况选择 Intermediate compatibility (recommended) 就可以了。更新后的 ciphers 配置 123server &#123; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;&#125;ps. 如果想要“Cipher Strength”得分更高，就把 128bit 相关的加密套件都去掉。 再次测试，发现对 ios8.4、osx10.10、ie11 兼容性不好。不过都是化石级的系统了，直接忽略。至此，最基本的 ssl 证书已经配置能用。接下来再做优化。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>devops</tag>
        <tag>nginx</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蓝绿部署和金丝雀发布]]></title>
    <url>%2Fp%2Fblue-green-deployment-and-canary-release%2F</url>
    <content type="text"><![CDATA[软件部署是一个重要的环节，有多重要，值得烧香保平安😀。部署主要涉及的有：不停服务发布 线上验证 失败回滚 一把梭部署 图片来源网络，侵删。自信满满、或者已有跑路路线的朋友，可以使用此方式部署（开个玩笑~）。一把梭部署看上去简单粗暴又高效，实际上相当危险：一旦发现线上的包有问题，则整个集群都被污染了（扩大了错误影响面）全量部署后才发现问题，已经没得回滚。或者回滚相当复杂 更艰难的是处理混滚期间的脏数据，让系统恢复一致性 切勿在生产环境中使用一把梭部署。蓝绿部署，blue green deployment引用 Martin Fowler 的图片 BlueGreenDeployment 蓝绿部署环境分为 2 个，一个蓝，一个绿，并且都是生产配置。假设一开始系统部署在蓝色环境，流量指向蓝色环境，现在要做升级部署。那么把最新代码部署在绿色环境，再把流量切换到绿色环境，做线上验证。如果验证通过，则全量流量切换到绿色环境，使用绿色环境做这次的正式生产环境。如果验证不通过，则流量切换回到蓝色环境。在绿色环境进行修复和验证。如此重复直到正式部署。正式上线后，另一个环境的资源可以被回收，避免浪费。优点：做切换和部署简单 缺点：资源消耗大（容器化可以简化问题）为了做线上验证，切换了整个集群 数据同步问题 在落地中，数据一致性是比较麻烦的事情。在 Martin Fowler 的示意图中，部署 2 份数据库。但是会浪费资源，而且数据同步也是坑。只部署一份数据库，由蓝绿环境共用同一份数据库，遇到数据库修改的部署，也是麻烦。因此，设计好表结构，避免日后修改，非常重要。一点小技巧是，尽量做新增，而不是修改。新的扩展表、影子表、字段，回滚相对方便。金丝雀发布（灰度发布），canary release蓝绿部署，每次操作的单位是一个集群，粒度很大。金丝雀发布（canary release），是蓝绿部署的一种改善。每次发布，只针对进群中少量的机器进行更新，以及验证。一旦线上验证通过，再对其余实例进行更新，放大流量。图片来源 CANARY DEPLOYMENTS WITH SERVERLESS AND AWS CODE PIPELINE 从图上可以看到，金丝雀发布过程，会出现 2 个服务版本共存的状态，这个共存状态可能持续好一段时间 。比如做 A/B 测试，验证新旧推荐算法对 CTR 的影响。 优点：缩小线上验证粒度，比蓝绿部署更加安全 缺点：因为每次只验证少量实例，需要有好的部署工具做滚动发布（rolling update）共同的问题 除了一把梭部署方式，其他线上部署和验证都会遇到流量切换和验证问题。现在简单聊聊。DNS。更改出口 DNS 和内部调用 DNS，把新流量引入到带验证机器上。操作简单，但是 DNS 更新有延迟问题。SLB（server side loadbalance）。在 SLB 层面切换流量。对 SLB 后边的服务切换快速快速生效。tcpcopy 流量复制。直接把线上流量复制到测试机器。但是要改造业务和框架，防止流量被多次处理。流量进入了待验证的新服务，还需要验证操作结果是否正确。怎么把特定流量和普通流量区分开来？特殊固定账号。在验证阶段，使用特殊账号产生数据，并且跟踪。特殊标记。在入口处加上特殊标记，比如 abtest=1，再利用全链路跟踪打点找到对应的结果。 如果出现问题需要回滚，要具体场景具体分析：只读场景。只需要回滚上一个版本的代码即可。已经修改了数据。如果是特殊固定账号创建的数据，通常可以直接删除。真实用户创建的数据。只能根据具体情况，做业务补偿。实践 第一次接触“蓝绿部署”的概念，是在《微服务设计》。实际工作中，使用的是 金丝雀发布（灰度发布） + 滚动发布。]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>测试</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒烟测试]]></title>
    <url>%2Fp%2Fsmoke-testing%2F</url>
    <content type="text"><![CDATA[事情的起因是，帮朋友面试一个技术主管候选人。其中问了团队输出质量管理怎么搞，问了“冒烟测试”，对方一脸懵逼，似乎没有听过，甚至还想用压力测试来糊弄过去（压力大了就会冒烟嘛🙃）……于是就就聊聊“冒烟测试”。概念 以下定义引用自 Smoke TestingSMOKE TESTING, also known as “Build Verification Testing”, is a type of software testing that comprises of a non-exhaustive set of tests that aim at ensuring that the most important functions work. The result of this testing is used to decide if a build is stable enough to proceed with further testing. 定义很简洁。几个关键点是：构建后就应该执行 不是全量的测试集合 关注的是最重要的功能 冒烟测试的结果决定要不要继续下一阶段的测试 执行时机和覆盖内容 smoke test 又叫 Build Verification Testing（BVT）。顾名思义，是构建后就应该执行。验证一个构建，至少包含： 是否编译通过 核心功能有无受影响 bug fix 内容是否生效 此次修改内容是否生效 为了节省时间，通常考虑覆盖正常路径操作即可 从上面也可以看到，冒烟测试并不是一个全量测试，不会覆盖到所有细节。强调的是快速验证核心路径是否正常。如果核心路径都由问题，那么就没有必要进行下一阶段的测试。测试用例 可以由测试和开发一起设计整理。因为每次冒烟测试都会涉及到核心路径，可以把主路径相关的测试做成自动化测试，提高效率。谁来执行 一个误区是冒烟测试交由测试人员进行。实际上应该由开发人员负责。开发人员对构建进行核心、简单的测试，一旦发现错误，就立即修复。由开发人员负责，一是对自己的产出负责，二是提高效率，减少沟通、文书的开销，这样才能快速验证。价值 冒烟测试的价值在于尽快发现 artifact 的问题。尤其是低级的、阻塞核心功能的问题。当然对于维护测试和开发的关系也是大有益处。试想提交给测试，结果根本运行不了、修复的 bug 根本不起作用，这么低级的问题都不解决就让他们干活，换谁都有意见啦。]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>测试</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 ASR 的语音审核方案]]></title>
    <url>%2Fp%2Fan-audio-review-solution-with-ASR%2F</url>
    <content type="text"><![CDATA[当前环境下，做互联网产品尤其是面向 C 端用户的，一个最不能忽视问题是做好内容监管。不能发布有害社会的内容，一旦发现被举报，轻则下架停改，重则永远 say goodbye。对于传统的文本、图片、视频，业界已经有比较好的内容审核方案。至于音频相关审核方案，目前还在成长期。项目需求 作为语音直播项目，SD 项目既然规划日活要 XXXX 万。完善的审核机制是不可以少的，否则分分钟就踩雷。在第一版主功能开发差不多的时候，我就提议要把审核方案放到日程上，安排人力去实现，只可惜未被得到重视。不过对于自己认可的事情，花点自己的时候去思考也是大有裨益的。语音识别挑战 语音识别来自多个方面的挑战：语言。普通话，广东话，四川话，东北话，……，blablabla。通常模型只针对某个语言做训练和优化。一旦是多人聊天还东一句普通话、西一句广东化，效果好不到哪里。口音。哪怕都是讲普通话，口音会有很大的差别。声纹的多样性。不同人的声纹不一样，同一个人不同状态下的声纹也不一样：平常说话的声音，跟昨晚唱 K 后沙哑的声音。环境噪音。一般人不可能在很安静的地方说话，环境噪音无法避开。语气。一句话正常说，跟用发喋的声音说，意思就很不一样了。（跑题了，跑到语义识别去~）语音审核方案 产品的审核功能设计建议 人们对隐私越来越看重，谁也不想自己做的事情被监控。因此，ugc 内容避免在产品设计上突出“审核”、尤其是“人工审核”之类的字眼。这些字眼会对令人感到不适，缺乏隐私。个人觉得觉得审核设计有 2 点注意：举报功能。由用户触发，人人都是朝阳区热心群众呢。实际上，举报的实际功效是相当分化的，要么没人用，要么被玩坏。在运营后台可以看到各式沙雕的举报理由。因此，我建议“举报”按钮，可以做成固定几个选项，让用户选择就好了。机器审核。机器是冰冷冷的，相比人工检查，更让人觉得隐私安全。可以突出全程机器审核，减轻用户的焦虑。架构设计 与 ASR 工作相关的流程：由 ASR 从语音通道获取输入 转化为文本 把文本输入到文本审核系统 审核结果通过发送消息，通知到审核控制模块，再做处理 服务器对接 ASR图片来源阿里云 ASR热词管理 为了提升 ASR 的识别率，可以增加热词。在这个项目场景，可以把常见敏感词贴上去。模型管理 ASR 支持自定义模型，提升识别率。但是目前来看，没有足够的能力去优化模型，先用默认就好。 方案风险和收益分析 没有审核方案（自动、人工）。一旦被举报，必死无疑。只有人工审核，对于一个全天候都有人使用的 app，根本处理不过来，而且质量参差不齐。哪怕是外包，人力成本相当高。只有 ASR 自动机器审核。质量也是不令人放心的。ASR+ 人工，用人工去弥补 ASR 不能很好识别的内容。质量上比较妥当，成本也可以接受。由于使用了第三方房屋，极端情况下还可以踢下球，态度诚恳些，避免被下架。另外，由于使用量大，对于第三方来说，我们可以成为一个很好的合作方，提供大量训练语料，可以用来优化模型，双方达到共赢。其他内容形式的审核 文本 从最基本的敏感词、到各种自然语言模型，已经玩得比较成熟了。以后有机会再详细说说。图片 从人工审核发展到深度学习，也是很多选择，以后有机会再详细说说。视频直播 逐帧截图，再做图片审核。通常是按图片数量收费（贵啊）。个人觉得可以按照关键关键帧截图，这样图片数量少很多。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 vi 和 vscode 中文乱码，以及聊聊字符编码]]></title>
    <url>%2Fp%2Fvi-and-visual-studio-code-fix-showing-garbled%2F</url>
    <content type="text"><![CDATA[域名备案要求显示站点内容，于是网上找了个静态页模板，再用 nginx 部署。需要在最下方需添加备案号且链接到工信部网站。vi 中文乱码 ssh 服务器，vi 加上去就好了。 但是，vi 打开乱码了。。。不用想，肯定是编码问题。我的控制台是 utf8 编码，估计是文件用了 gbk、gb2312 之类的编码。修改 ~/.vimrc 文件 123set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936set encoding=utf-8set termencoding=utf-8 保存后再打开，中文显示就正常了。配置项意义如下：encoding：vi 内部运行使用的编码 fileencoding：写入文件时采用的编码类型。termencoding：输出到 terminal 采用的编码类型。vscode 中文乱码 备案信息是加上去了，但是刷新页面看，基本看不清，得手动调下 css。索性把文件拷贝到本地编辑好了。vscode 打开又是乱码。。。尼玛，太矬了。。。于是 settings 查找 encoding 配置，发现 Auto Guess Encoding 竟然是默认关闭，太不友好了。嗯，这下终于正常了。顺便看了下右下角的文件编码：gb2312。意外收获：安装 xshell、xftp，打开 xshell 并且 ssh 登录后，Ctrl+Alt+F可以直接打开 xftp，非常方便。扩展：编码和字符集 既然今晚跟编码方式这么有缘，就顺手整理下编码相关的知识。对于字符编码，通常关注编码方式（定长、变长、编码字节数、特殊控制字符）、支持字符数、和不同字符编码的兼容性。gb 2312国家标准。1981 年 5 月 1 日实施。GB 2312 标准共收录 6763 个汉字，其中一级汉字 3755 个，二级汉字 3008 个；同时收录了包括拉丁字母、希腊字母、日文平假名及片假名字母、俄语西里尔字母在内的 682 个字符。（就是支持的字符少，不包括繁体字，但是能满足一般场景使用）GB 2312 对任意一个图形字符都采用两个字节表示。big 5维基上说：Big5 是由台湾财团法人信息产业策进会为五大中文套装软件（并因此得名 Big-5）所设计的中文共通内码，在 1983 年 12 月完成公告。那个之前还没有繁体字编码，GB2312 又不含繁体字，因此才有了 Big-5。（想起以前玩游戏转换码的岁月）gbkGBK 是微软标准但不是国家标准。GBK 向下与 GB 2312 完全兼容。GBK 即汉字内码扩展规范，K 为汉语拼音 Kuo Zhan（扩展）中“扩”字的声母。GBK 共收入 21886 个汉字和图形符号，包括：GB 2313、BIG 5、CJK 字符（中日韩）等。GBK 采用双字节表示。CJK 是中日韩统一表意文字（CJK Unified Ideographs）。在 Unicode 中，收集各国相同的汉字，并且进行合并相同的编码点（code point）上，可以避免相同文字重复编码，浪费编码空间。gb 18030GB 18030 与 GB 2312-1980 和 GBK 兼容，共收录汉字 70244 个。2000 年，取代了 GBK1.0 的正式国家标准。支持中国国内少数民族的文字，不需要动用造字区。GB 18030 编码是一二四字节变长编码。code page，cp936Windows 的内核已经采用 Unicode 编码，这样在内核上可以支持全世界所有的语言文字。但是由于现有的大量程序和文档都采用了某种特定语言的编码，例如 GBK，Windows 不可能不支持现有的编码，而全部改用 Unicode。Windows 使用代码页 (code page) 来适应各个国家和地区。code page 可以被理解为前面提到的内码。GBK 对应的 code page 是 CP936。微软也为 GB18030 定义了 code page：CP54936。但是由于 GB18030 有一部分 4 字节编码，而 Windows 的代码页只支持单字节和双字节编码，所以这个 code page 是无法真正使用的。查看 Windows 本地代码页，打开 cmd12λ chcp活动代码页: 936ISO 8859-1又称 Latin-1 或“西欧语言”，是国际标准化组织内 ISO/IEC 8859 的第一个 8 位字符集。它以 ASCII 为基础，在空置的 0xA0-0xFF 的范围内，加入 96 个字母及符号，藉以供使用附加符号的拉丁字母语言使用。unicode、ucs、utfUnicode 也是一种字符编码方法，不过它是由国际组织设计，可以容纳全世界所有语言文字的编码方案。 Unicode 的学名是 “Universal Multiple-Octet Coded Character Set”，简称为 UCS。UCS 只是规定如何编码，并没有规定如何传输、保存这个编码。UTF-8、UTF-7、UTF-16 都是被广泛接受的方案。UTF-8 的一个特别的好处是它与 ISO- 8859-1 完全兼容。UTF 是 “UCS Transformation Format”的缩写。UTF-8 是 Unicode 的一种实现!UTF-8 是 Unicode 的一种实现!UTF-8 是 Unicode 的一种实现!UTF 的字节序和 BOMUTF-8 以字节为编码单元，没有字节序的问题。UTF-16 以两个字节为编码单元，在解释一个 UTF-16 文本前，首先要弄清楚每个编码单 &gt; 元的字节序。例如“奎”的 Unicode 编码是 594E，“乙”的 Unicode 编码是 4E59。如果我们收到 UTF-16 字节流“594E”，那么这是“奎” &gt; 还是 “乙”？Unicode 规范中推荐的标记字节顺序的方法是 BOM。BOM 不是“Bill Of Material”的 BOM 表，而是 Byte Order Mark。BOM 是一个有点 &gt; 小聪明的想法：在 UCS 编码中有一个叫做”ZERO WIDTH NO- BREAK SPACE”的字符，它的编码是 FEFF。而 FFFE 在 UCS 中是不存在的字符，所以不应该出现在实际传输中。UCS 规范建议我们在传输字节流前，先传输字符”ZERO WIDTH NO-BREAK SPACE”。这样如果接收者收到 FEFF，就表明这个字节流是 Big-Endian 的；如果收到 FFFE，就表明这个字节流是 Little-Endian 的。因此字符”ZERO WIDTH NO- BREAK SPACE”又被称作 BOM。UTF-8 不需要 BOM 来表明字节顺序，但可以用 BOM 来表明编码方式。字符”ZERO WIDTH NO-BREAK SPACE”的 UTF-8 编码是 EF BB BF（读者可以用我们前面介绍的编码方法验证一下）。所以如果接收者收到以 EF BB BF 开头的字节流，就知道这是 UTF-8 编码了。BOM 是为了解决 UTF-16 的双字节编码问题而设计的方案。UTF-8 根本不需要 BOM，但是也可以支持。坑爹的是，Windows 和 linux 对 UTF-8 BOM 的处理不一样。Windows 打开文本文件，会识别是否有 BOM 字符；编辑过的 utf8 文件，都会加上 BOM（现在终于可以指定要不要 BOM）。但是 linux 上的工具一般不对 BOM 做处理 ，结果是被当作非法字符，出现诡异的问题，常见的是 shell、python 执行出错、java 读取 BOM 文件异常。 以前还会用 apache common 的 BOMInputStream 来读取文件专门处理 BOM。12345//Example 1 - Detect and exclude a UTF-8 BOMBOMInputStream bomIn = new BOMInputStream(in);if (bomIn.hasBOM()) &#123; // has a UTF-8 BOM&#125;具体可以参照 BOMInputStream API 文档。 兼容性 从 ASCII、GB2312、GBK（对应 cp936）到 GB18030 的编码方法是向下兼容的。而 Unicode 只与 ASCII 兼容。一个字符编码的例子 Windows 的默认字符编码是 GBK，对于跨平台编程不友好，可以参见这个例子 修复 intellij 打开 properties 文件乱码 参考资料 GB2312、GBK、GB18030 这几种字符集的主要区别是什么？ 字符集编码 cp936、ANSI、UNICODE、UTF-8、GB2312、GBK、GB18030、DBCS、UCS「带 BOM 的 UTF-8」和「无 BOM 的 UTF-8」有什么区别？网页代码一般使用哪个？]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 上切换多个 java 版本：java8 和 java11]]></title>
    <url>%2Fp%2Fchange-multiple-java-version-on-windows%2F</url>
    <content type="text"><![CDATA[Windows 上安装了 java8 和 java11，时不时要切换，于是思考写行命令解决。思路是修改 java_home 变量。我的 java_home 变量是设置在系统级别的。修改环境变量有 2 个命令，set 和 setx：set：临时修改普通的环境变量，只对当前窗口有效。setx：可以永久修改环境变量，包括系统变量。不会影响已经打开的 cmd 窗口。一开始饶了点弯路，用 set 不生效，后来才发现该用 setx。切换 java81setx /m JAVA_HOME "C:\Program Files\Java\jdk1.8.0_212"切换 java111setx /m JAVA_HOME "C:\Program Files\Java\jdk-11.0.3"其中 /m 参数表示修改系统变量。分别保存为 java8.bat 和java11.bat。以管理员权限执行即可。唯一不足是打开时候 cmd 窗口闪屏，先凑合着使用。切换后查看 java 版本1234C:\Users\ycwu&gt;java -versionjava version &quot;11.0.3&quot; 2019-04-16 LTSJava(TM) SE Runtime Environment 18.9 (build 11.0.3+12-LTS)Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.3+12-LTS, mixed mode)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orchestration vs choreography，编排 vs 编制（协同）]]></title>
    <url>%2Fp%2Forchestration-vs-choreography%2F</url>
    <content type="text"><![CDATA[不管是微服务还是 SOA，都会涉及到 2 个容易混淆的概念：orchestration vs choreography。orchestration（编排）和 choreography（协同，这是我喜欢的翻译）。概念 这 2 个概念容易混淆的原因，在于中文翻译太坑爹了，什么编排、编制，让人摸不着头脑。直接看英文解释就很好理解了。orchestration管弦乐编曲 an arrangement of events that attempts to achieve a maximum effect;choreography 舞蹈编排 the representation of dancing by symbols as music is represented by notes 图片来源 管弦乐有一个总指挥，引导各个乐器手怎么演奏，这是 orchestration（编排）。芭蕾舞表演者根据音乐节奏就可以翩翩起舞，这是 choreography（协同）。stackoverflow 上有个关于二者区别很好的回答 Orchestration vs. ChoreographyService orchestration represents a single centralized executable business process (the orchestrator) that coordinates the interaction among different services. The orchestrator is responsible for invoking and combining the services.The relationship between all the participating services are described by a single endpoint (i.e., the composite service). The orchestration includes the management of transactions between individual services.Orchestration employs a centralized approach for service composition.orchestration 是中心化的控制流程。关联服务的关系由中心化节点描述。Service choreography is a global description of the participating services, which is defined by exchange of messages, rules of interaction and agreements between two or more endpoints. Choreography employs a decentralized approach for service composition.choreography 是去中心化的方式，服务之间的交互通过消息交换实现。二者的区别：The choreography describes the interactions between multiple services, where as orchestration represents control from one party’s perspective.图片来源 例子 新注册一个用户，客户服务（customer service）要通知积分服务（loyalty points service）初始化记录、通知邮政服务（post service）发送新客礼包、发送欢迎邮件（email service）。Orchestration 和 choreography 风格的实现分别如下：图片来源 Orchestration: 由 customer service 作为中心服务，分别调用其他 3 个服务。如果发生异常，customer service 会感知并且做出回滚或者补偿操作。Choreography：customer service 向 topic 发送“customer create event”，其他 3 个服务订阅这个 topic，收到消息后执行各自的操作。这些服务是否执行失败，customer service 并不知道；失败后如何操作，也不知道。需要更多的监控和异常协调。 通过上面的例子，可以看到 Orchestration 中心化地控制业务，会产生服务中心大脑。Choreography 去中心地协调业务，降低业务耦合，但是会导致业务流程分散，需要强大的监控和异常协调机制补偿，技术复杂度更高。总结 orchestration vs choreography 要怎么选择呢？No silver bullet。 我的经验是，关键的、少量的、核心的、复杂的、一致性要求高的业务流程，更适合 Orchestration 方式。非关键的、简单的、一致性要求不高的业务流程，用 Choreography 即可。]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊单体、SOA 和微服务]]></title>
    <url>%2Fp%2Fmicroservice-vs-soa%2F</url>
    <content type="text"><![CDATA[从单体应用说起 最初只有一个系统，对外提供简单的功能。随着业务发展，越来越多功能加入到系统。原来简单的系统，滚雪球一样变成巨大的单体应用。功能迭代变慢、部署复杂，开始拖累业务发展。对于单体架构，常见的架构设计方式是分层。比如常见的 MVC 分层方式。分层的好处有：分离关注点（separation of concern），降低复杂度。层与层之间的交互，只要定义好接口，实现高内聚松耦合。可以方便替换层里面的实现。部分分层部署。比如 nodejs 做 view 层渲染，不用在 jsp 里面折腾。但还是 水平切分 。部署的时候，还是一个整体应用部署，牵一发而动全身： 某个模块升级，影响整个应用 某个业务升级，影响整个应用 某个升级带来的故障，影响整个应用 正因为单体应用部署的风险大，会有复杂、严格的流程限制部署频率。结果是迭代缓慢、单次升级内容巨大。解决单体应用的问题，关键是 垂直切分 。把一个大的单体应用，根据业务切分为多个应用。这就产生了 SOA 和微服务。SOA 和微服务的共同点SOA（Service Oriented Architecture）和微服务架构（Micro Service Architecture，MSA）的共同点： 都包含了服务化的理念，微服务是 SOA 的升华 支持跨语言平台 但除此之外，二者的区别比较大。本文的关注点是比较二者的不同。诞生背景 SOA 源于解决遗留异构系统的集成和组件重用。 企业信息化过程中，产生了各种各样的业务系统（CMS、CRM、ERP、TMS 等）。这些系统往往是技术隔离的，采用不同的技术实现、不同的通信方式。可能是不同时期采购的系统，可能是企业并购带来的其他子系统。需要有一种方案把各种异构系统整合起来，实现更大的价值。SOA 的关键字：集成，组件重用，ESB。微服务在互联网产品时代兴起。因为业务要快速试错，变化很快，因此技术上要支持快速迭代、快速上线。大型单体应用由于自身的复杂性，容易牵一发而动全身，做功能的新增修改都更加谨慎；同时为了避免风险，会有一系列复杂的部署流程，需要各个干系人同意，导致发布频率变低、单次变更内容更多。显然在一个大型单体应用上很难达到快速迭代、快速上线的要求。唯有在小型、独立的系统才更容易实现。服务划分是实现小型高内聚、松耦合系统的基础。快速迭代的另一个重要条件是，对具体的业务采用合适的解决方案和技术栈。比如，脚本语言更适合做爬虫系统，java 的技术栈更适合企业信息系统。因此，微服务同样是支持异构系统。每个微服务能够独立迭代、部署的前提是，拥有服务的自治权。一旦大规模采用微服务的架构思想，就要管理大量的小型服务，构建、测试、部署都必须自动化。微服务的关键字：服务划分，自治性，自动化，独立部署，去中心化。服务化 虽然微服务和 SOA 都提及服务化，但是二者有很大的区别。因为 SOA 源于解决遗留系统的集成问题，因此更多是 把子系统服务化 ，CRM 是一个服务、ERP 也是一个服务，然后 invoke 这些服务提供的接口。因此 SOA 的服务是粗粒的，服务尺寸比较大。 微服务吸收了领域驱动的思想，使用限界上下文（bounded countext）划分服务边界和职责 。划分服务边界是实施微服务的关键，因为不好的服务划分，会导致服务臃肿、服务间紧耦合，不能达到微服务的好处。“micro”表明了服务的粒度要小。那么多小的服务粒度才是合适？我的经验是，适配团队规模即可。如果只有 3 个研发，却拆分出 20 个微服务，扯淡去吧。 在 SOA 里，组件重用是服务化的原动力。既然都有现成的 IT 设施，那么就想办法继续使用。在微服务，服务化不仅仅是为了功能重用，更重要是控制系统的复杂度。一个高内聚、专注某个业务的小型系统，要比一个涵盖多个不同领域的巨型单体系统容易理解，更加适合业务的快速迭代。因为微服务是职责分明的小粒度服务，微服务之间通过 bounded context 交互即可，隐藏了服务内部的细节。但是 SOAP 是粗粒度的服务，往往意味着这个组件可能默默承担了更多的业务职责，对于想重用该组件的调用方来说，可能会带来意想不到的副作用。尽管 SOA 也提倡屏蔽服务细节，但是在实际中往往需要对其他组件有更多的了解。小结：微服务强调按照业务领域拆分，具有一定业务能力的轻量服务。SOA 只是要求接口规范化、接口独立，是粗粒度的服务化，每一个功能都是通过一个独立服务来提供，服务定义了明确可以调用的接口，服务之间通过编排、协同交互，完成一个完整的业务功能。异构系统 SOA 和微服务都支持异构系统，但是原因很不一样。SOA 的异构，源于各个遗留系统采用了不同的 IT 技术，采用 RMI、CORBA、HTTP 乃至私有服务协议。 微服务的异构，源于为不同的业务场景，选择最合适的技术方案。但是异构的技术栈，会带来招聘、服务协同、内部人员流动的成本增加，在实践中要限制异构技术栈的数量。异构系统需要解决发现、通信和互操作问题。SOA 的方案是一套基于 XML 的通用协作语言，包括 UDDI 服务发现、SOAP 互操作、WSDL 服务描述语言、BPEL 服务编排语言等。在 SOA 流行的时代，XML 是业界推荐的跨平台数据交互方式。在落地上，主要是各个不同 vendor 厂商的 ESB（Enterprise Service Bus，企业服务总线）方案。为了实现某些特性，例如分布式事务，又要捆绑 vendor 的全家桶。因此，SOA 实际是被 vendor 捆绑的。图片来源于 wikipedia微服务没有官方规定的方案，技术选型灵活。业界推荐使用语言无关的 REST 风格交互。或者是支持各个异构系统的 RPC 框架（thrift，protobuf 等）。在服务发现上选择更多了，zookeeper、consul、eureka 等是常见。去中心化 各个系统通过接入 ESB，依靠 ESB 完成服务发现、服务调用以及数据交换。这样做的好处是，现有系统的改造少，但是 ESB 的职责很重。ESB 是各个服务的通信中心，是通信瓶颈。在业务上，依靠 BPEL 做服务编排（orchestration），实际上形成了服务中心大脑。整个 SOA 是中心化的。微服务架构，是去中心化的。服务发现之后，服务之间通过更加高效的点对点（P2P）通信调用，没用通信总线的概念。在服务调用上，微服务鼓励使用协同（choreography）方式，避免形成服务中心大脑和服务之间的紧耦合。这样带来的问题是，业务流程和状态的跟踪和观察变得复杂。需要对业务流程更加深入的理解，以及更加强大的监控系统。驱动力量 SOA 更多的驱动力量是软件 vendor 厂商，推销自家 SOA 全家桶方案又大赚一笔，是自上而下的驱动。 微服务的驱动力量来自于底层的研发人员以及开源社区。互联网业务快速多变，原有单体架构上搬砖不能满足业务速度，挖坑踩坑血泪史无数，这些经验逐步形成了微服务的方法论，在业界形成自底向上的驱动力量。独立部署 微服务架构强调独立部署，在落地上使用 docker 容器化、kubernete 容器编排协同等技术。部署是限制微服务落地的一大门槛，因为容器化部署对运维团队要求很高。为什么 SOA 并没有很关注部署？SOA 面对的系统，通常已经稳定了，或者不好用、但是改不动（技术上或者组织结构上的原因），或者想改动、但是部署成本高；因此 SOA 架构的迭代周期缓慢。同时也体现 SOA 对重用组件的渴望，因为变更一个服务实在太艰难了。小结：微服务强调独立部署，尤其是存储层的独立。共享数据库被认为是微服务的反模式！SOA 只要求接口独立，不要求内部实现独立部署。其他 服务注册，服务发现，服务路由，安全访问和授权，分布式事务，监控等双方都是有的，只是实现不一样。总结 SOA 和微服务都是服务化的架构，这是二者的共同之处。但是二者对于服务的理解却不太相同，实际应用场景和实施也区别很大。因此有的观点说微服务是去掉 ESB 的 SOA 实现，并不是十分恰当。 在服务化架构中，SOA 是 1.0，微服务是 2.0，service mesh 是 3.0。]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试用例中 stub 和 mock 的区别]]></title>
    <url>%2Fp%2Ftesting-stub-vs-mock%2F</url>
    <content type="text"><![CDATA[今晚看《微服务设计》关于测试的章节，发现平时对 stub 和 mock 理解比较模糊，于是整理下资料。test doublestub 和 mock 都属于 test double（替身）。《xUnit Test Patterns》这本书提到了 5 种 test double：dummyfakestubspymockdummyDummy objects are passed around but never actually used. Usually they are just used to fill parameter lists.dummy 的唯一用途是填充参数，使得编译可以通过。fakeFakes are objects that have working implementations, but not same as production one. Usually they take some shortcut and have simplified version of production code.fake 是一种完整功能的实现，但是通常不会用于生产环境。使用 fake 的原因是更加便捷，例如更简单的配置、更快的启动速度、更快的响应速度、更简单的数据清理等。常见的例子是使用 in memory database（例如 h2）代替生产环境的数据库实现（例如 mysql）。stubStub is an object that holds predefined data and uses it to answer calls during tests. It is used when we cannot or don’t want to involve objects that would answer with real data or have undesirable side effects.stub 是测试用例会使用到的、预先定义 (predefined) 的数据。通常在只读场景使用 stub，无法获得真实数据的返回，或者根本不关心返回数据。例如，测试一个服务 A，它从推荐服务获取数据，再组装。那么服务 A 不需要关心推荐服务返回的真实数据，可以返回几个 hardcode 数据即可。spySpies are stubs that also record some information based on how they were called. One form of this might be an email service that records how many messages it was sent.spy 对象记录方法调用情况。Martin Folwer 这个解释比较模糊，跟下面提到的 mock 容易混淆。mockMocks are objects that register calls they receive.In test assertion we can verify on Mocks that all expected actions were performed.mock 会记录所有调用情况。通过测试断言，检查 mock 提供了期望的行为。典型的场景是，不想调用真实的代码，或者真实代码不方便检查其执行结果（比如返回类型是 void、调用外部服务提供的接口）。比如下单场景，要调用支付网关接口。stub vs mock上面对 stub 和 mock 的解释已经比较清楚了。stub 不关心返回结果、或者难以获取真实的返回结果，通常用于只读场景的返回。mock 关注每次调用的情况，提供预定行为的返回，并且可以被断言检查。mock 关注对行为的检查。1234567891011121314//stubbing using built-in anyInt() argument matcherwhen(mockedList.get(anyInt())).thenReturn("element");//stubbing using custom matcher (let's say isValid() returns your own matcher implementation):when(mockedList.contains(argThat(isValid()))).thenReturn("element");//following prints"element"System.out.println(mockedList.get(999));//you can also verify using an argument matcherverify(mockedList).get(anyInt());//argument matchers can also be written as Java 8 Lambdasverify(mockedList).add(argThat(someString -&gt; someString.length() &gt; 5));spy vs mockspy 和 mock 都会记录方法的调用情况，容易混淆。如果结合 Mockito 的使用方式，就比较清晰了。mockito spy 文档 You can create spies of real objects. When you use the spy then the real methods are called (unless a method was stubbed).Real spies should be used carefully and occasionally, for example when dealing with legacy code.Spying on real objects can be associated with “partial mocking” concept.spy 指向真实对象，可以对部分方法进行 mock（partial mocking）。相反，mock 需要对所有使用到的方法进行 stubbing。 比如一个外部依赖服务包含 a、b、c 三个方法。如果要采用 a、b 方法的真实实现，并且使用 c 的 mock 实现，用 spy 就可以了。如果使用 mock，则需要对 a、b、c 三个方法做处理。可信依赖和隔离 面试新人的时候，问“为什么会在单元测试用例里面做 mock”，经常得到的回答是环境配置方便之类，其实没有回答到点子上。test double 的思想是，外部服务（比如未经良好测试的代码）可能不可靠的，会影响当前对象的测试产生影响，因此要隔离。为什么不对 java.lang.String 类做 mock？因为通常情况下，自己写的代码，不会因为 String 类的缺陷导致失败，或者说，信任 String 类。参考资料Test Doubles — Fakes, Mocks and Stubs.Martin Fowler 的 TestDouble]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>测试</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次运营平台引发的故障]]></title>
    <url>%2Fp%2Fsd-project-operation-platform-cauased-an-accident%2F</url>
    <content type="text"><![CDATA[4 月份发生过一次运营平台故障，产生问题的原因比较典型，记录下来。背景 使用 Node 构建运营平台，直接操作存储在 ElasticSearch 核心数据（和业务服务共享）。故障描述 某日早上，推荐服务响应超时，几乎每隔一两分钟就发生，告警频繁。导致结果是 app 打开首页的个人模式没有收到歌曲推荐。故障排查 最近推荐服务没有升级，基本排除服务升级导致的异常。在对应的推荐接口中，会查询 ElasticSearch 获取数据。从打点日志看，有时候超时 rt &gt; 3000ms，但是通常很快返回。怀疑是 ElasticSearch 的问题。去 ElasticSearch 控制台看，发现慢查询，具体语句就不贴了，是根据某个字段的更新时间扫描并排序，然后一次返回前面 9999 条记录 。基本每分钟都有一条这样的慢查询记录。初步怀疑是某个服务的定时任务触发的。 印象中没有见过哪个后端服务写过有这查询语句。在后端服务用 9999 去全文搜索，没有找到对应的语句。于是问了运营后台同学，得知新增加了一个运营需求，展现最新修改 xxx 字段的记录。实现上是增加了一个每分钟执行的定时任务读取并且缓存数据。至此，故障原因水落石出。故障解决 紧急停掉该运营后台的定时任务。推荐服务的告警就消失了。增加给查询字段索引。跟运营沟通，降低数据刷新间隔。先改为 3 小时刷新一次。后续再做优化。反思 这个故障案例，虽然从发现、排查、临时修复，只用了 20 多分钟，但确是反映了日常研发活动中比较常见的几个问题。测试用例覆盖 只测试了定时任务的功能，看结果是通过的。但是缺少了回归测试，没有对其他服务做回归和常态化压测。本质原因是自动化测试程度不高、服务测试和回归测试意识薄弱。在微服务中、尤其是共享存储的服务，很有必要提高测试质量和效率。优化不足 这个典型会扫描全表的查询，并且没有走索引，再加上当时 ES 分片数量少，一旦数据量稍多，就会严重消耗 ElasticSearch 服务器资源，导致其他查询执行缓慢。对于运营定时任务来说，几秒的慢查询没什么。但是共享存储、对高并发业务来说就是致命的影响了。code review如果有高质量的 code review，完全可以扫描出来这个慢查询。沟通 对于共享存储的服务，知会其他团队相应的修改还是有必要的，相当于有更多的人来 review 修改。可能多留个心眼，就有可能在上线之前觉察到问题。存储隔离 事实上，共享存储通常不是一个很好的做法。但是目前来说，运营平台和业务服务直接共享同一份存储是合理的：目前数据量不大 多份存储，涉及数据同步、更新等问题。目前系统处于快速迭代、不稳定阶段，把有限精力优先处理业务问题。监控和告警 ElasticSearch 控制台有慢记录功能，但是没有对接告警通道。如果接上了，排查这个问题就更快了。 需求的合理性 这个字段的数据需要每分钟刷新吗？在出故障前，肯定说相当重要、无论如何也要做到、做不到就不如不做了、blablabla。发生故障之后，就变成其实也没有那么重要啦、还是可以慢一点更新也每问题的。有的运营需求，当时人总是觉得很合理很有必要，但是实际上用处不大，有时为了实现不合理的需求，引入不好的方案，最后挖坑背锅。开发人员也需要有产品意识，思考一个需求能为用户带来的价值，否则就会沦为搬砖工和背锅侠了。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>故障案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目架构图]]></title>
    <url>%2Fp%2Fsd-project-architecture%2F</url>
    <content type="text"><![CDATA[组件架构大图。点击查看大图。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语音直播房间项目文章汇总]]></title>
    <url>%2Fp%2Fsd-project-collection%2F</url>
    <content type="text"><![CDATA[语音直播房间项目文章汇总。当时对标的竞品是音遇、撕歌。2 个主力后端研发，3 个月，1500+ commit，996、007，纪念那段充满福报的经历😂。系统设计 SD 项目架构图SD 项目：基于状态机和 MQ 的语音游戏方案 设计模式系列：状态模式 java 状态机框架选型简单比较：stateless4j, spring statemachine, squirrelstateless4j 踩坑经历RocketMQ 延迟消息RocketMQ 延迟消息补偿策略SD 项目信令设计总结 基于 ASR 的语音审核方案 性能压测 性能压测怎样做？高并发和性能优化 SD 项目：高并发的性能优化，part 1SD 项目：高并发的性能优化，part 2SD 项目：高并发的性能优化，part 3 一次 young gc 频繁的优化过程 重构 sd 项目重构实践 聊聊“重构”故障案例 SD 项目：使用 arthas 排查问题的经历 一次 MQ 机房迁移引发的血案 记一次运营平台引发的故障]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次 MQ 机房迁移引发的血案]]></title>
    <url>%2Fp%2Faccident-caused-by-mq-migration%2F</url>
    <content type="text"><![CDATA[背景 公共的通道服务 U 向各个业务方提供长连接接入层服务，完成客户端鉴权、连接建立、心跳检测、重连等功能，并且向业务方服务器端开放 API 接口，提供向客户端分发数据（send）、查询客户端状态等能力。send 方法的本质是向指定 RocketMQ topic 写入消息，再由通道服务 U 消费。SD 项目使用通道服务，向客户端下发游戏房间的信令，指导客户端状态切换。故障描述 某日晚上 9 点多，正是 app 业务高峰期，收到反馈越来越多，客户端语音房间卡顿严重，表现为不能正常轮转，等待几秒到几十秒才有反应（当然更多的用户直接就退出了）。故障排查 客户端游戏状态切换等待久，可能的原因有：服务器端问题，没有及时轮转 服务器端正常轮转，并且向通道服务发送指令，但是客户端没有及时收到 于是兵分两路排查。服务器端 首先，抽取几个有问题的语音房间 room_id，作为跟踪。应用日志 grep &lt;room_id&gt;，发现 基本正常，但是少数延迟消息接收有问题，注册 30s 后的延迟消息，真正接收时间在 1min 以上，导致切换异常。服务器端调用通道服务 API，发送信令，立即返回。同时把 msgId 发给客户端跟踪。对于 1），明显是 MQ 出现问题了，联系中间件的运维，得知正在进行机房迁移，带宽有限，产生大量消息堆积……尼玛，这么大的事情为什么没有通知到业务方！客户端 客户端拿着 msgId 去查询通道服务，发现通道服务真正下发消息延后了几十秒到几分钟不等。从通道服务得到的反馈是，由于 MQ 机房迁移，加上某个大业务方 A 的业务高峰，通道服务处理堆积消息，网卡流量都打爆，其他业务方是受到了不同程度的影响……故障解决 挂出升级维护公告，虽然不知道效果如何~等待堆积消息消费完毕。大约 9 点半过后，房间卡顿减少，10 点左右恢复正常。反思 首先梳理这次故障的发生流程。个别延迟消息超时，导致服务器端房间状态切换延迟 服务器端下发的信令，由于通道服务故障，不能及时下发到客户端 客户端也没有主动补偿机制，导致一旦通道服务失效，将无法恢复 此外，迁移升级的事情没有及时通知给业务方。方案优化 增加容灾方案。服务器端 由于严重依赖延迟消息进行异常情况的房间状态切换，一旦 MQ 故障，房间服务不可用。要增加备用检查机制，SLA 略低，但可以保证房间在不依赖 MQ 的情况下也可以轮转。方案：引入分布式定时任务框架，对于已经开始的语音游戏房间，注册定时任务检查 开发成本：中 收益：低。因为 MQ 故障属于低概率事件，产出投入比低。最终实施：待定 增加备用通道 只有通道服务作为长连接服务，并且和大业务方共享底层资源。要增加被用通信通道。方案：自建 websocket 通道，用于应急使用。服务端根据下发策略向通道服务、websocket 发送信令。客户端适配双通道。开发成本：中。收益：高。最终实施：是 客户端主动补偿机制 因为客户端强依赖信令切换状态，一旦出现信令超时或者丢失，就会切换失败，导致卡顿。此时服务器端是正常运作的。最初设计的时候，就要求客户端必须要实现主动补偿机制，达到信令下发的推拉结合。遗憾的是，并没有。方案：客户端必须要实现主动补偿机制，使用服务器端提供的接口，预期时间内收不到信令，主动查询；并且恢复。开发成本：高。对原有客户端代码改动大。收益：高。最终实施：部分实现。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>故障案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[性能压测怎样做？]]></title>
    <url>%2Fp%2Fperformance-testing%2F</url>
    <content type="text"><![CDATA[系统正式上线之前，通常要做性能压测，好处就多了：了解系统能正常处理多少用户请求 尽早发现性能瓶颈并做优化 调整部署资源分配 设置合理的监控指标 开始压测之前 压测目标 首先是设定目标。这次压测的目标，是找到系统能够支持的并发用户数、核心链路的 tps、资源的峰值消耗（cpu、内存、缓存等）、还是某个接口的响应瓶颈？压测方案 为了实现目标，哪些服务需要被压测，它们之间的依赖关系是怎样的，涉及哪些中间件，服务的交互流程，压测流程的设计，执行时间，执行次数，需要重点关注的指标，有哪些数据要反映在压测报表，等等。一般对于一个新的 app，至少要覆盖用户使用的核心路径，即用户打开 app-&gt; 使用核心功能 -&gt; 用户离开。环境准备 除非有特殊原因，不能随便使用线上环境进行压测，否则后患无穷。一般要有配套的环境专门性能压测。硬件规格、软件配置要和线上一致。现在是容器化时代，镜像解决了很多软件配置的问题了。实例部署数量可以适当缩小。指标 “我的系统可以支撑 100W 用户并发”，这是没有多大工程意义的。系统所能够支撑的用户数，还要看并发请求的成功率、TP99 或者 TP90 的响应时间，才有意义。 提高系统并发的方式很多：增大 tcp 的 backlog、增大 tomcat 的 acceptor 线程、增大业务的线程池数量……用户请求是建立了，但是由于系统不能及时处理，导致请求响应时间 10s+、大量请求等待超时、请求失败等。显然，这样并发数对系统是没有意义的。相对于并发数，吞吐量（throughput）指标考虑单位时间的处理能力。1吞吐量 = 处理请求数 / 时间 10W 个请求，总共处理 10s，那么吞吐量是 1W/s。但是，吞吐量不要求所有请求都是同一时间触发。因此，吞吐量 1W/s 不等于支撑 1W 并发请求。另外，吞吐量只反映单位时间处理的请求数，并没有反映单个请求的响应时间。 响应时间（response time，RT）值得多说。10W 个并发请求过来，由于一开始系统资源充裕，前面的请求处理很快，100ms 返回了；随着请求数量变多，平均 RT 变成 1s，有的一直等待资源的请求 RT 要 5s。一个常见的问题是，使用平均响应时间作为性能指标。在统计学上，平均数是很不靠谱的，容易受极端值影响，同时也容易掩盖极端值。而在生活中，被平均的坑爹感觉没少经历过。更为合理的是，使用区间响应时间来描述性能，即 TP99 100ms、TP90 20ms、TP50 5ms 等。TP(top percentile)，即区间的百分之 X 的请求都落在这个值以内。最后，成功率是一个容易被忽略的指标，总是默认总是成功。但是在微服务时代，服务可以降级，因此要根据业务的意义，确定服务链路发生降级是否也算成功。综合起来，合理描述性能的指标，可以是：TP99 小于 10ms、最大 RT 100ms，系统最大支持并发请求 1W。工具 jmeter，ab，python，monkey，blabla。 实施压测 压测预热 一个常见的问题是，系统还没预热，就做性能压测。为什么要预热？tcp 连接建立、缓存不命中回源加载、jvm 热代码编译（分层编译）等都会对性能产生明显的影响，导致系统性能变低。对于没有预热的系统施加高并发压测，得到的冷启动 + 突发流量下系统的性能，而不是平稳状态下的性能。怎么做压测预热呢？先以低的并发度逐渐增加，比如 100、200、500，压测 1min、2min、5min，等 RT 稳定后再逐渐增加到期望值并发值，每次增加 500 到 1000，压测 1min。单压 vs 混压 另一个常见的问题是，只对单个接口、单个服务调用的场景做压测（单压），没有根据用户使用场景同时对多个接口、服务做压测（混压）。单压可以方便发现单个接口、服务的问题。但是单压不能很好的反映真实场景下多个接口、服务的资源竞争导致的性能问题。比如一个服务有 a、b、c 三个接口，都是使用 redis，单压每个接口都能轻松 TP99 5ms + 3W 并发。但是如果同时对这个 3 个接口做压测，可能就只剩下 TP99 5ms + 1W 并发了，因为底层竞争使用 redis 资源。多接口、多服务的混压，可以帮助发现此类性能问题，对衡量系统的真实并发有积极意义。观察指标 cpu load，内存，网络接口流量，磁盘读写次数，存储空间使用，jvm gc 次数等，使用 top，vmstat, tsar，jstat 等工具。 中间件控制台。缓存命中率、缓存空间使用、metaq 消息堆积情况、数据库并发事务数等。RT 的曲线图，TP99 时间。服务调用的耗时分布。压测问题排查 记得首先记录问题复现路径，并且保留现场。报告 单压、混压 TP99、TP90 响应时间 并发数 关键资源的消耗 压测方式 瓶颈分析 结论 使用压测报告 除了修复性能问题，压测报告在调整部署资源上也很有用。资源申请在项目启动阶段就开始做，否则来不及准备软硬件设施。那时候的资源规划大概率是拍脑袋出来的。压测报告给了修正机会，更好的预估系统的资源需求，在上线之前做好资源调整。另外再举个例子，一个 cpu 密集型的服务，60% cpu 使用率，支持 1W 并发；90% cpu 使用率，支持 1.5W 并发。在做资源规划的时候，应该使用 60%-80% 使用率作为参考，参考价值比较高。90% 的使用率，系统已经高负荷的边缘，不能很好应对突发情况，不适合长期稳定运行。用 90% 使用率下的并发数去做资源规划就危险了。此外就是设置告警指标。比如一个接口期望 TP90 小于 100ms，就可以做个监控项。]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>性能压测</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 延迟消息补偿策略]]></title>
    <url>%2Fp%2Fdelay-message-stratedgy%2F</url>
    <content type="text"><![CDATA[上次分析了 RocketMQ 的延迟消息机制。要注意的是，RocketMQ 的是延迟消息，并非任意精度的定时消息。如果业务上需要任秒级别意精度的定时消息，就要做 workaround 的办法了。往期文章：RocketMQ 延迟消息 我的设计思路是：延迟消息 body 增加 expectExecuteAt 字段，表明期望的执行时间 计算最近的、未开始的 delayTimeLevel。例如，期望延迟 6s，最近的延迟时间是 5s在消费者端，增加延迟消息处理策略，根据当前时间和消息的 expectExecuteAt 字段进行处理。延迟消息处理策略：立即执行。消息已经到达执行时间，或者接近处理时间（比如小于 1000s）、可以提前执行。再投递。还没到达执行时间，重新计算 delayTimeLevel，再次发送延迟消息。本地队列。还没到达执行时间，缓冲到本地 Scheduled 队列。如果此时 jvm 重启，会有丢失消息处理的风险，但性能要比再投递好。丢弃。异常情况下，消息已经过时很久，处理消息已经不产生业务意义。不过实际业务中，并没有采用延迟消息处理策略，而是在业务层上做折中设计。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 延迟消息]]></title>
    <url>%2Fp%2Frocketmq-delay-message%2F</url>
    <content type="text"><![CDATA[基于 RocketMQ v4.5.1 源码。延迟级别配置 位于 org.apache.rocketmq.store.config.MessageStoreConfig1private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h"; 定义了 18 种延迟级别。消息延迟时间支持 1 秒、5 秒直到 2 小时。也可以自定义，但是不太建议。不支持任意精度的延迟 。 消费者发送延迟消息 1msg.setDelayTimeLevel(1); 设置 delayTimeLevel 属性。编号从 1 开始（见下文的 delayLevel2QueueId() 函数）延迟消息的存储 org.apache.rocketmq.store.CommitLog 处理消息的存储。如果发现消息 property 包含 PROPERTY_DELAY_TIME_LEVEL 字段，则更新 tagsCode。12345678910111213141516171819public DispatchRequest checkMessageAndReturnSize(java.nio.ByteBuffer byteBuffer, final boolean checkCRC, final boolean readBody) &#123;// more code // Timing message processing&#123; String t = propertiesMap.get(MessageConst.PROPERTY_DELAY_TIME_LEVEL); if (ScheduleMessageService.SCHEDULE_TOPIC.equals(topic) &amp;&amp; t != null) &#123; int delayLevel = Integer.parseInt(t); if (delayLevel &gt; this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel()) &#123; delayLevel = this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel(); &#125; if (delayLevel &gt; 0) &#123; tagsCode = this.defaultMessageStore.getScheduleMessageService().computeDeliverTimestamp(delayLevel, storeTimestamp); &#125; &#125;&#125;// more code &#125;对于延迟消息，保留原来消息的 property，真正保存在 topic 是 SCHEDULE_TOPIC_XXXX，queueId 是设定的延迟级别映射。映射方法也在 ScheduleMessageService1234567891011121314151617181920212223242526272829public PutMessageResult putMessage(final MessageExtBrokerInner msg) &#123; // more code final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag()); if (tranType == MessageSysFlag.TRANSACTION_NOT_TYPE || tranType == MessageSysFlag.TRANSACTION_COMMIT_TYPE) &#123; // Delay Delivery if (msg.getDelayTimeLevel() &gt; 0) &#123; if (msg.getDelayTimeLevel() &gt; this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel()) &#123; msg.setDelayTimeLevel(this.defaultMessageStore.getScheduleMessageService().getMaxDelayLevel()); &#125; topic = ScheduleMessageService.SCHEDULE_TOPIC; queueId = ScheduleMessageService.delayLevel2QueueId(msg.getDelayTimeLevel()); // Backup real topic, queueId MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic()); MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId())); msg.setPropertiesString(MessageDecoder.messageProperties2String(msg.getProperties())); msg.setTopic(topic); msg.setQueueId(queueId); &#125; &#125; // more code &#125; public static int queueId2DelayLevel(final int queueId) &#123; return queueId + 1;&#125;public static int delayLevel2QueueId(final int delayLevel) &#123; return delayLevel - 1;&#125; 延迟消息任务 ScheduleMessageService 初始化一个 Timer 线程。然后对于每个 delayLevelTable 的元素，新建一个 DeliverDelayedMessageTimerTask 任务处理。delayLevelTable的生成参见 ScheduleMessageService.parseDelayLevel()。1234567891011121314151617181920212223242526272829public void start() &#123; if (started.compareAndSet(false, true)) &#123; this.timer = new Timer("ScheduleMessageTimerThread", true); for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) &#123; Integer level = entry.getKey(); Long timeDelay = entry.getValue(); Long offset = this.offsetTable.get(level); if (null == offset) &#123; offset = 0L; &#125; if (timeDelay != null) &#123; this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME); &#125; &#125; this.timer.scheduleAtFixedRate(new TimerTask() &#123; @Override public void run() &#123; try &#123; if (started.get()) ScheduleMessageService.this.persist(); &#125; catch (Throwable e) &#123; log.error("scheduleAtFixedRate flush exception", e); &#125; &#125; &#125;, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval()); &#125;&#125;DeliverDelayedMessageTimerTask 初始化的时候向 timer 注册任务。123456789101112131415161718class DeliverDelayedMessageTimerTask extends TimerTask &#123; private final int delayLevel; private final long offset; @Override public void run() &#123; try &#123; if (isStarted()) &#123; this.executeOnTimeup(); &#125; &#125; catch (Exception e) &#123; // XXX: warn and notify me log.error("ScheduleMessageService, executeOnTimeup exception", e); ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask( this.delayLevel, this.offset), DELAY_FOR_A_PERIOD); &#125; &#125;&#125;核心逻辑在 executeOnTimeup，具体步骤是： 获取 SCHEDULE_TOPIC 指定的 delayLevel 队列 123ConsumeQueue cq = ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC, delayLevel2QueueId(delayLevel)); 从 offest 开始，找到 cq 中已经到期的消息 把到期消息写入原来的 topic 和 queue123456789MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset(offsetPy, sizePy);if (msgExt != null) &#123; try &#123; MessageExtBrokerInner msgInner = this.messageTimeup(msgExt); PutMessageResult putMessageResult = ScheduleMessageService.this.writeMessageStore.putMessage(msgInner); // more code &#125;&#125;其中 messageTimeup() 方法把 SCHEDULE_TOPIC 中的消息转换为原来要投递的消息。主要是恢复 topic 和 queue 字段。更新 offset，再次注册定时任务 1234nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE);ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask( this.delayLevel, nextOffset), DELAY_FOR_A_WHILE);ScheduleMessageService.this.updateOffset(this.delayLevel, nextOffset); 总结ScheduleMessageService 设计采用 Timer 类，而不是 ScheduledExecutorService，有点坑。Timer 类是单线程设计，一旦堆积的延迟消息多，可能发送滞后。RocketMQ 的延迟消息≠定时消息，不支持任意精度的延迟（但是阿里云上的 RocketMQ 商业版支持）。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
      <tags>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目：基于状态机和 MQ 的语音游戏方案]]></title>
    <url>%2Fp%2Fsd-project-solution-by-state-machine-pattern-and-mq%2F</url>
    <content type="text"><![CDATA[业务分析 SD 项目的一个核心功能是语音直播游戏房间。用户在游戏房间内唱歌 PK、语音聊天、评论、点赞等。简单的业务分析如下： 游戏模式：轮唱。每个人都有机会唱歌。抢唱。抢到麦克风才有机会唱歌。房间状态：新建 播放音乐中 等待答题 等待抢麦 等待抢麦答题 展示结果 结束 人在房间的操作, 受限于房间状态和用户角色：加入 退出 解散 开始游戏 唱歌 抢麦克风 放弃作答 点赞 举报 评论 … 性能要满足 10w+ 房间同时游戏 技术角度的思考 状态的改变影响行为 房间状态的改变，影响房间（服务器端）的行为。例如，“播放音乐中”状态下发歌曲，“展示结果”状态下发用户的答题情况。房间状态同样影响用户行为。例如，只有房间处于“等待抢麦”状态，用户才可以发起“抢麦”请求，其他状态下的抢麦请求直接就是非法的。分离关注点 ：拆分房间状态，每个状态只处理自己的职责，状态内的行为是高内聚。 状态的自动转换 异常情况下，用户可能不作答，或者离开游戏，或者由于网络原因，不能正常上报答题结果。这时房间状态也必须要做异常处理，保证继续轮转到下一个状态。高可用 上面的分析表明，房间业务本身是有状态的。并且用户可以是全时段使用 app。服务器端设计必须要 无状态 ，部署过程才不影响已经开始的房间。 客户端和服务器端的交互 服务器端对所有房间内的客户端进行广播或者单播通知，可以抽象为 信令 ，作为客户端行为的触发事件（事件驱动）。 房间内交互频繁，适合使用长连接通道。解决方案 使用状态模式 房间状态多，并且 状态的改变影响对象行为 ，适合使用状态模式。 业务有限状态机 FSM 的抽象 状态 state状态转换的触发器 trigger抽象上下文 context进入状态的前置检查，permitIf两个状态的轮转操作，封装在 transition事件驱动触发，生成 context、fromState、toState、trigger，再由 state machine 进行处理即可 房间状态的切换行为封装在 transition，实现高内聚。对外暴露统一的入口，调用方只需要知道当前状态 state 和触发的路径 trigger 即可。路径合法性检查、切换行为等细节，由状态机引擎处理。采用状态模式的好处：避免了大量 if else 操作和状态检查 不同玩法抽象为不同状态机，互不干扰，扩展性好，回归测试简单 代码可读性高 关于状态模式，可以参考下列文章：设计模式系列：状态模式 java 状态机框架选型简单比较：stateless4j, spring statemachine, squirrelstateless4j 踩坑经历 简化的房间状态机图：状态的持久化 保存到 mysql，外加 redis 作为缓存。状态的自动转换 前面提到一个问题：没人抢麦，或者上报超时，没有客户端的事件触发，服务器的状态机怎么流转？候选方案：注册本地 scheduler 线程，比如 15s 后检查某个房间的游戏状态是否已经超时。缺点：很显然重启 jvm 就丢失任务信息。定时任务 job。每隔 1s 检查所有已经开始游戏的房间状态。缺点：效率太低。分布式定时任务。为每个房间手动注册检查 job，以及触发时间。一旦房间销毁，则撤销该任务。分布式定时任务看上去是可行的。但是考虑下来并没有采用，原因是：每个房间切换状态要手动注册新任务、注销旧定时任务，略显啰嗦。分布式定时任务框架底层通常使用 mysql + zookeeper，10W+ 同时运行的房间，能否提供足够的实时性响应？极端情况下会增加产生 10W+ tps 的读写。以前经历过几次定时任务多了，分布式定时任务启动执行时间滞后的问题。房间服务对时间很敏感。最终使用的是一套新方案：RocketMQ 的延迟消息 。把超时检查打包为一个 RocketMQ 延迟消息。到点再由 RocketMQ 向消费者投递。 用 MQ 延迟消息解决异常情况下状态轮转 服务器端是事件驱动编程模型。如果没有客户端发送的交互事件，房间状态轮转就会有问题。以唱歌为例。正常情况下，用户唱完歌，或者用户放弃作答，客户端都会主动上报，使得状态机可以切换。但是，如果这条上报结果请求丢失，由于没有事件触发，状态机就无法轮转了。解决的方法是，切换状态的时候，注册当前状态超时检查的延迟消息 。 正常的用户行为，会使得房间的状态机顺利切换到下一个状态，因此在下一个状态收到上一个状态注册的延迟消息，就可以直接丢弃。 相反，异常情况下，由于没有收到用户行为，那么到达超时时间后，MQ 服务器发送之前注册的超时消息，就可以触发房间业务对于该状态的处理。还是以唱歌为例。进入唱歌状态，假设最大允许一个人唱歌 60s，那么就注册一个 60s 以后的延迟消息。异常情况下，60s 后消费者收到 RocketMQ 发来的延迟消息，触发状态机引擎轮转。解决方案：在状态的 transition 阶段，注册延迟消息，比如 1s，5s，30s收到延迟消息之后，根据 context 判断是丢弃，还是启动状态机引擎处理 延迟消息的消费速度，可以增加消费者实例、消费者线程来提高 限制：RocketMQ 不支持任意精度的延迟，业务层自己做策略模块解决 RocketMQ 延迟消息相关文章：RocketMQ 延迟消息RocketMQ 延迟消息补偿策略 关于超时 因为 mq 消息可能堆积，导致延迟消息发送滞后。也有可能因为时钟问题，提前发送。因此在消息体增加 expectExecuteAt 字段，表明期望执行时间，服务器端的处理策略更加灵活。事件驱动和 pull 补偿 在游戏房间内，客户端和服务器端的交互主要通过长连接通道上的交互事件实现。由于网络的原因，可能会出现信令延迟甚至丢失。客户端如果纯粹依赖服务器端下发的信令，就有因为信令延迟或者丢失导致不能正常切换。对于这种异常情况，要使用 pull 补偿 方案：服务器端提供聚合接口，返回当前房间的若干信息，客户端根据返回可以进行本地状态切换 客户端本地注册超时事件，一旦正常时间内没有收到服务器端的信令，主动拉取聚合接口，并进行补偿操作 高可用 服务器端无状态 房间状态持久化到 mysql。加上事件驱动设计，房间状态轮转由外部事件（客户端请求、RocketMQ 发送的延迟消息）触发，本身不会在服务器内存维护房间状态。因此服务器端是无状态的设计。更新部署房间服务不会对已经进行的游戏产生影响。强依赖 MQ每个游戏中的房间都会注册一条 MQ 延迟消息，用于异常情况下触发状态轮转。因此该方案强依赖于 MQ 的稳定性。发生 MQ 故障会导致房间服务不可用。为此考虑服务器端的补偿机制，计划在未来实施。引入分布式任务组件，对已经开始游戏的房间进行扫描，扫描频次比正常轮转要低（只是最坏情况下房间状态可以轮转，因此 SLA 可以更长），减少 db 资源消耗。推拉结合的补偿 应对网络问题，客户端有补偿机制，主动请求服务器数据，实现推拉结合。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>架构设计</tag>
        <tag>RocketMQ</tag>
        <tag>状态机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 windows 10 下 java 应用找不到 pid]]></title>
    <url>%2Fp%2Fcant-find-java-pid-on-windows-10%2F</url>
    <content type="text"><![CDATA[问题描述 在 Windows 10 系统，之前写了 使用 arthas 直接操作 redis 遇到一个问题：1234λ java -jar arthas-boot.jar[INFO] arthas-boot version: 3.1.1[INFO] Can not find java process. Try to pass &lt;pid&gt; in command line.Please select an available pid.arthas 找不到 java 应用的 pid。换 jps 同样也没有找到。1λ jps关于 hsperfdatahsperfdata 是 jvm 应用每次运行时记录的性能监控数据。默认在 java 的临时目录创建。arthas、jps、jconsole 等应用，会从 hsperfdata 目录获取 java pid。arthas 找不到 java pid，可能的原因有：hsperfdata 被禁止。hsperfdata 由 jvm 参数 -XX:+UsePerfData 控制，默认情况是是打开的。没有权限创建 hsperfdata 目录。hsperfdata 目录默认在 java 的临时目录创建，名字为 hsperfdata_&lt;username&gt; 当前用户在 hsperfdata 目录没有访问权限，不能读写数据。很有可能是没有 hsperfdata 目录或者没有访问权限。解决 先找到 java 的临时目录。java 的临时目录由 jvm 参数 java.io.tmpdir 控制。1System.out.println(System.getProperty("java.io.tmpdir"));在我的电脑上显示结果是 1C:\Users\ycwu\AppData\Local\Temp\ 账号是 ycwu，对应的 hsperfdata 目录是 hsperfdata_ycwu。打开目录发现有这个文件夹。右键查看属性 没有用户 ycwu 的访问权限。。。点击 编辑 -&gt; 添加...，输入用户名，然后 检查名称 确定之后，选择 完全控制 保存后再次运行 java 应用，hsperfdata 目录下面创建了 pid 文件。arthas 也能找到 java pid 了。ps. linux 系统的 hsperfdata 默认是在 /tmp/hsperfdata_&lt;user&gt; 小结 Windows 10 的java.io.tmpdir 里的 hsperfdata 目录默认访问权限有问题，导致 jps 等工具找不到 Java pid。手动设置一下就好了。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 arthas 直接操作 redis]]></title>
    <url>%2Fp%2Fuse-arthas-to-operate-redis-direclty%2F</url>
    <content type="text"><![CDATA[现在是容器化部署，镜像都是干净的，只有基本的 Linux 和需要的 runtime 库，没有安装日常使用的各种中间件客户端，例如 redis-cli。有时候排查问题，需要读取从中间件数据，但是所在容器没有安装中间件的客户端，或者由于网络隔离不能方法问，该怎么办呢？如果应用有访问中间件的组件，那么可以使用 arthas 来找到对应组件，然后为所欲为了~ 下面以访问 redis 讲解。启动 arthas12wget https://alibaba.github.io/arthas/arthas-boot.jarjava -jar arthas-boot.jar然后选择 java 应用的 pid：123456[INFO] arthas-boot version: 3.1.1[INFO] Found existing java process, please choose one and hit RETURN.* [1]: 10904 com.godzilla.MedicalApplication [2]: 9628 org.jetbrains.jps.cmdline.Launcher1[INFO] Start download arthas from remote server: https://maven.aliyun.com/repository/public/com/taobao/arthas/arthas-packaging/3.1.1/arthas-packaging-3.1.1-bin.ziptt这里要使用 tt 命令，详见 官网介绍 方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测 12345$ tt -n 3 -t com.godzilla.inquiryassisstant.util.RedisService getPress Q or Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 64 ms. INDEX TIMESTAMP COST(ms) IS-RET IS-EXP OBJECT CLASS METHOD--------------------------------------------------------------------------------------------------------------------------------------- 1001 2019-07-04 18:33:09 13.938899 true false 0x14496aa8 RedisService get 命令参数说明：-n: 指定记录的次数，防止 jvm 被撑爆。-t: 记录每次的执行情况 后面是类的全路径，以及观察的方法。在 tt 所有输出参数中，最重要的是 index 列，时间片段记录编号，每一个编号代表着一次调用，后续 tt 还有很多命令都是基于此编号指定记录操作。选择其中一个 index 编号，然后使用 -w 参数执行具体的方法。123$ tt -i 1001 -w &apos;target.get(&quot;kkk&quot;)&apos;@String[12345]Affect(row-cnt:1) cost in 19 ms.其中 target 代表被观察的对象，即要使用的 RedisService 类。]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>java</tag>
        <tag>arthas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stateless4j 踩坑经历]]></title>
    <url>%2Fp%2Fpitfall-on-stateless4j%2F</url>
    <content type="text"><![CDATA[上次聊了 java 状态机框架选型，最后采用 stateless4j。这次继续聊下 stateless4j 使用的坑。stateless4j 的 demo 参照往期文章：java 状态机框架选型简单比较：stateless4j, spring statemachine, squirrel问题 调研的时候使用的代码是从 https://github.com/oxo42/stateless4j 下载。然后写了 test case，简单验证业务逻辑，没毛病。这时候在正式项目中引用 stateless4j12345&lt;dependency&gt; &lt;groupId&gt;com.github.oxo42&lt;/groupId&gt; &lt;artifactId&gt;stateless4j&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;结果 idea 报错方法找不到：permitIf(T trigger, S destinationState, FuncBoolean guard, Action action)master 代码 1234567891011121314151617/** * Accept the specified trigger and transition to the destination state if guard is true * &lt;p&gt; * Additionally a given action is performed when transitioning. This action will be called after * the onExit action of the current state and before the onEntry action of * the destination state. * * @param trigger The accepted trigger * @param destinationState The state that the trigger will cause a transition to * @param guard Function that must return true in order for the trigger to be accepted * @param action The action to be performed "during" transition * @return The receiver */public StateConfiguration&lt;S, T&gt; permitIf(T trigger, S destinationState, FuncBoolean guard, Action action) &#123; enforceNotIdentityTransition(destinationState); return publicPermitIf(trigger, destinationState, guard, action);&#125; 对比 maven repo v2.5.0 的代码，只有这个方法 123456789101112/** * Accept the specified trigger and transition to the destination state * * @param trigger The accepted trigger * @param destinationState The state that the trigger will cause a transition to * @param guard Function that must return true in order for the trigger to be accepted * @return The reciever */public StateConfiguration&lt;S, T&gt; permitIf(T trigger, S destinationState, FuncBoolean guard) &#123; enforceNotIdentityTransition(destinationState); return publicPermitIf(trigger, destinationState, guard);&#125; 解决 在 stateless4j repo 拉下 master 分支，重新打包 2.5.1，放到内部仓库。]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
        <tag>设计模式</tag>
        <tag>状态模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 状态机框架选型简单比较：stateless4j, spring statemachine, squirrel]]></title>
    <url>%2Fp%2Fcompare-state-machine-framework%2F</url>
    <content type="text"><![CDATA[上一篇文章简单聊了状态模式。在实际应用，可以使用框架来实现状态机。往期文章：设计模式系列：状态模式 技术选型考虑 针对 sd 项目的选型要求：简单，容易上手。足够的扩展点，能够进行定制。有真实项目应用。被线上项目验证，前人踩过坑，可靠性比较高。可移植性（加分项）。目前状态机只是应用在服务器端，但实际上客户端的业务逻辑也可以使用状态模式。状态机框架能够移植到客户端，减轻客户端的编码负担。spring statemachine基于 java 的框架，一般优先考虑 spirng 全家桶有无支持，恰好有 spring statemachine。 定义状态和事件 1234567static enum States &#123; STATE1, STATE2&#125;static enum Events &#123; EVENT1, EVENT2&#125; 定义 StateMachine使用 StateMachineBuilder 构建 StateMachine，定义事件转换（transition）和 listener（可以使用注解方式）。12345678910111213141516171819public StateMachine&lt;States, Events&gt; buildMachine() throws Exception &#123; Builder&lt;States, Events&gt; builder = StateMachineBuilder.builder(); builder.configureStates() .withStates() .initial(States.STATE1) .states(EnumSet.allOf(States.class)); builder.configureTransitions() .withExternal() .source(States.STATE1).target(States.STATE2) .event(Events.EVENT1) .and() .withExternal() .source(States.STATE2).target(States.STATE1) .event(Events.EVENT2); return builder.build();&#125; 定义 transition当状态发生改变，触发相应转换行为。StateMachineListener提供多个事件监听器：1234567891011121314151617181920212223242526272829303132333435/** * &#123;@code StateMachineListener&#125; for various state machine events. * * @author Janne Valkealahti * * @param &lt;S&gt; the type of state * @param &lt;E&gt; the type of event */public interface StateMachineListener&lt;S,E&gt; &#123; void stateChanged(State&lt;S,E&gt; from, State&lt;S,E&gt; to); void stateEntered(State&lt;S,E&gt; state); void stateExited(State&lt;S,E&gt; state); void eventNotAccepted(Message&lt;E&gt; event); void transition(Transition&lt;S, E&gt; transition); void transitionStarted(Transition&lt;S, E&gt; transition); void transitionEnded(Transition&lt;S, E&gt; transition); void stateMachineStarted(StateMachine&lt;S, E&gt; stateMachine); void stateMachineStopped(StateMachine&lt;S, E&gt; stateMachine); void stateMachineError(StateMachine&lt;S, E&gt; stateMachine, Exception exception); void extendedStateChanged(Object key, Object value); void stateContext(StateContext&lt;S, E&gt; stateContext);&#125;也可以使用注解方式进行配置 1234567891011@WithStateMachinestatic class MyBean &#123; @OnTransition(target = "STATE1") void toState1() &#123; &#125; @OnTransition(target = "STATE2") void toState2() &#123; &#125;&#125; 使用状态机 1234StateMachine&lt;States, Events&gt; stateMachine = buildMachine();stateMachine.start();stateMachine.sendEvent(Events.EVENT1);stateMachine.sendEvent(Events.EVENT2); 优缺点 优点：上手比较简单 丰富的扩展点，例如 transition、interceptor。缺点：作为 spring 全家桶系列，spring statemachine 并不是很活跃。调研的时候一直停留在 2.x 版本（半年前，现在已经有 3.x snapshot）。某团队反映 spring statemachine 1.x 版本有 bug，导致业务出故障，至于 2.x 版本情况如何，没有找到详细信息。因为支持扩展点多，设计比较重型。stateless4jstateless4j是另一款使用比较多的状态机框架。官网上的例子很简单：12345678910111213141516171819202122232425StateMachineConfig&lt;State, Trigger&gt; phoneCallConfig = new StateMachineConfig&lt;&gt;();phoneCallConfig.configure(State.OffHook) .permit(Trigger.CallDialed, State.Ringing);phoneCallConfig.configure(State.Ringing) .permit(Trigger.HungUp, State.OffHook) .permit(Trigger.CallConnected, State.Connected);// this example uses Java 8 method references// a Java 7 example is provided in /examplesphoneCallConfig.configure(State.Connected) .onEntry(this::startCallTimer) .onExit(this::stopCallTimer) .permit(Trigger.LeftMessage, State.OffHook) .permit(Trigger.HungUp, State.OffHook) .permit(Trigger.PlacedOnHold, State.OnHold);// ...StateMachine&lt;State, Trigger&gt; phoneCall = new StateMachine&lt;&gt;(State.OffHook, phoneCallConfig);phoneCall.fire(Trigger.CallDialed);assertEquals(State.Ringing, phoneCall.getState());优缺点 优点：上手很简单 轻量级设计，即使移植到客户端使用也没问题 代码量少，即使二次开发也容易 网上使用例子多，其他团队也有使用，反馈好 缺点：不活跃。上一个 maven repo release 已经是 2014 年了 扩展点少，只有 onEntry，onExit，permitIf，guard，action 等，没有 transition 的概念 squirrelsquirrel 是一款热门的状态机框架。123456789101112131415161718192021222324252627282930313233public class QuickStartSample &#123; // 1. Define State Machine Event enum FSMEvent &#123; ToA, ToB, ToC, ToD &#125; // 2. Define State Machine Class @StateMachineParameters(stateType=String.class, eventType=FSMEvent.class, contextType=Integer.class) static class StateMachineSample extends AbstractUntypedStateMachine &#123; protected void fromAToB(String from, String to, FSMEvent event, Integer context) &#123; System.out.println("Transition from'"+from+"'to'"+to+"'on event'"+event+ "'with context'"+context+"'."); &#125; protected void ontoB(String from, String to, FSMEvent event, Integer context) &#123; System.out.println("Entry State \'"+to+"\'."); &#125; &#125; public static void main(String[] args) &#123; // 3. Build State Transitions UntypedStateMachineBuilder builder = StateMachineBuilderFactory.create(StateMachineSample.class); builder.externalTransition().from("A").to("B").on(FSMEvent.ToB).callMethod("fromAToB"); builder.onEntry("B").callMethod("ontoB"); // 4. Use State Machine UntypedStateMachine fsm = builder.newStateMachine("A"); fsm.fire(FSMEvent.ToB, 10); System.out.println("Current state is "+fsm.getCurrentState()); &#125;&#125;优缺点 优点：文档写得丰富 支持的扩展点也多 相比 spring statemachine，设计比较轻量，二次开发难度比 spriing statemachine 容易 github 活跃 缺点：要找缺点的话，内部使用的例子少 结论 最后决定使用 stateless4j。主要考虑是轻量级，以及日后打包方案推广到客户端。squirrel 也是不错的，如果纯粹考虑服务器端使用会选择的。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>系统设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：状态模式]]></title>
    <url>%2Fp%2Fdesign-pattern-state%2F</url>
    <content type="text"><![CDATA[引子 在 OA 系统发起请求，可能会经历草稿、等待 1 级审批、等待 2 级审批、审批不通过、转交关联方处理、关联方处理完毕、正常关闭、取消等状态。随着状态变化，系统的行为也不一样，可能是通知审核人，可能是通知外部系统处理，等等。再来一个例子：tcp 连接，经历 3 次握手、数据传输、4 次挥手阶段。在不同阶段，os 进行初始化 socket 资源、设置 tcp 头部字段、校验远程连接的响应内容、注册超时器、释放资源等操作。(图片来源 http://tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm)这个 2 个例子都有共同的特点：内部状态的改变，影响了系统的行为 有大量的状态和状态判断 设计模式中有“状态模式”优化这种场景的代码设计。状态模式 允许一个对象在其内部状改变时改变它的行为。主要解决的是当控制一个对象状态迁移的条件表达式过于复杂时的情况。把状态的判断逻辑转移到表示不同的一系列类当中，可以把复杂的逻辑判断简单化。适用场景 一个对象在其内部状改变时改变它的行为。有大量判断状态的代码。角色 状态管理器（StateManager）：有状态的对象，持有具体状态类，定义感兴趣的行为 抽象状态类（State）：定义与 StateManager 行为相关的接口 具体状态类（ConcreteState）：实现具体与 StateManager 行为相关的接口 demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class TestStatePattern &#123; public static void main(String[] args) &#123; LightManager lightManager = new LightManager(); IState onState = new LightOnState(); lightManager.setState(onState); System.out.println("current state is" + lightManager.getState()); lightManager.tap(); lightManager.tap(); lightManager.tap(); &#125;&#125;class LightManager &#123; private IState state; public IState getState() &#123; return state; &#125; public void setState(IState state) &#123; this.state = state; &#125; public void tap() &#123; if (state == null) &#123; throw new IllegalArgumentException("state is null"); &#125; state.doProcess(this); &#125;&#125;interface IState &#123; void doProcess(LightManager context);&#125;class LightOnState implements IState &#123; @Override public void doProcess(LightManager context) &#123; System.out.println("light is off..."); context.setState(new LightOffState()); &#125; @Override public String toString() &#123; return "on"; &#125;&#125;class LightOffState implements IState &#123; @Override public void doProcess(LightManager context) &#123; System.out.println("light is on..."); context.setState(new LightOnState()); &#125; @Override public String toString() &#123; return "off"; &#125;&#125; 优点 要列出所有状态，强制加深对业务流程的理解。状态判断逻辑分散到各个具体的状态类，消灭了庞大的 if…else、switch…case 判断状态。状态迁移规则封装到各个具体的状态类，修改某个状态类的行为，不影响其他状态类，每个类的行为是高内聚的。缺点 每个状态对应一个类，状态的增加会导致类数量增加。增加新的状态类，涉及到从旧的状态迁移到新的状态，则要修改对应的旧状态类。因此不满足 OCP 原则。对于 1)，算不上什么问题啦。新增一个类，远比修改一个 1000 行的类要安全多了。另外，状态的数量也不是无限膨胀的。对于 2)，才是问题。新的状态引入，肯定会有状态迁移到新的状态，必须要对迁移路径做回归测试。和其他模式的对比 State 模式和 Strategy 模式在实现上比较相似，但意图却大相径庭。State 模式强调对象内部状态的变化改变对象的行为，而客户端不需要关心状态迁移。Strategy 模式重点是外部条件决定对策略的选择，客户端可以指定使用的策略。 实践 状态模式处理的核心问题是状态迁移路径。在实践中，先列出系统的各个状态，再画出转换路径。转换路径要考虑是否可重入（reentrant）。对于状态可重入，要考虑正常业务下，是否会永远在此状态永远循环不能退出的场景。我在实际工作中遇到的问题是，业务定下来，状态机也设计了，砖也般，这时产品想法又改变，引入几个新的状态，导致状态迁移很复杂。技术 leader 对状态模式和状态机理解又不够透彻，为什么加几个状态影响这么大（估计也没搬过类似的砖头）。在实践中，推荐使用状态机框架来实现。尽管自己写状态机也不复杂，采用框架优势的是状态迁移路径变得清晰，代码可读性高。以下代码来自stateless4j。123456789101112131415161718192021StateMachineConfig&lt;State, Trigger&gt; phoneCallConfig = new StateMachineConfig&lt;&gt;();phoneCallConfig.configure(State.OffHook) .permit(Trigger.CallDialed, State.Ringing);phoneCallConfig.configure(State.Ringing) .permit(Trigger.HungUp, State.OffHook) .permit(Trigger.CallConnected, State.Connected);phoneCallConfig.configure(State.Connected) .onEntry(this::startCallTimer) .onExit(this::stopCallTimer) .permit(Trigger.LeftMessage, State.OffHook) .permit(Trigger.HungUp, State.OffHook) .permit(Trigger.PlacedOnHold, State.OnHold);StateMachine&lt;State, Trigger&gt; phoneCall = new StateMachine&lt;&gt;(State.OffHook, phoneCallConfig);phoneCall.fire(Trigger.CallDialed);assertEquals(State.Ringing, phoneCall.getState());phoneCallConfig 把状态迁移路径和行为都集中配置在一起，整体脉络比较清晰。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目信令设计总结]]></title>
    <url>%2Fp%2Fsd-project-signal-design%2F</url>
    <content type="text"><![CDATA[在语音游戏房间，服务器端下发信令，通知客户端房间内状态变化、用户间互动行为。由于当时的接入层对全双工通信支持不友好，并且交互次数和频率相对少，因此客户端向服务器通知的行为采用 http 接口实现。下面是信令协议设计的总结。版本号 协议的升级、修改，可能导致旧客户端发生不兼容行为。因此使用版本号区分，客户端只处理自己接受的版本号。信令 id标识一个信令。序号 序号 sequenceNo 用来表示信令的先后顺序。根据信令的不同，序号生成算法也不一样。和状态轮转相关的、房间行为强相关的，采用 redis 自增，key 为房间 id，保证序号是单调递增。其他类型的信令，允许丢失的，例如点赞，则简单使用时间戳。下发时间 服务器下发信令的时间戳。信令分组 type 字段用来对信令分组。分组字段可以增加可读性，更重要的是，同一类型的信令，客户端可以指定基础的处理策略。 基础信息 每个游戏房间都包含房间号、展示部分用户列表等基础信息。考虑客户端实现的简单和容错性，大部分信令都会携带 baseInfo。扩展字段 每个特定信令的扩展数据。比如一个 xxx 加入房间的信令：12345"extra":&#123; "userId": "123", "name": "","avatar":""&#125;信令策略 信令下发，有可能乱序，有可能超时，有可能瞬间接收大量信令被轰炸。客户端可以根据信令的分组类型配置通用的信令处理策略，如果有必要，还可以对具体某个信令指定处理策略。例如，点赞信令属于可以丢弃的操作。如果接收到新的信令，序号比最近处理的要小，则直接丢弃。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊“重构”]]></title>
    <url>%2Fp%2Fthought-on-refactor%2F</url>
    <content type="text"><![CDATA[上次记录了之前 sd 重构的操作。这次聊聊对重构的一些想法。往期文章 sd 项目重构实践 灵魂的拷问 项目从 demo 发展到有一定数量用户，老板问 给 2 个星期时间重构，能够支撑以后千万 DAU 吗 对于这种问题，一开始我是拒绝的。。。这类问题在过去工作中也遇到不少，值得吐槽一下。量级思维 给位数级别的 DAU，能跑起来就是了。10W 级别，加缓存、优化 sql、多部署几个容器。100W 级别，双机房走起。1000W 级别，双数据中心在路上了。10000W 级别，……越往上的量级，问题难度、解决方案的复杂度、消耗的人力时间物力更是指数级别增长。幻想 2 个星期就能解决几个量级之后的问题，就缺乏工程师的量级思维了。重构是持续进展的行为 每次新增代码都受到需求复杂度、工期、设计水平、编码质量等因素的限制，可能或多或少地带来潜在的问题。随着业务发展，系统不停迭代，问题不断累积，量变到质量，导致系统越来越难理解、性能问题突出、不能快速响应新需求。重构是持续进展的行为，是一项长期投资，在每次迭代中对已有代码进行优化，绝对不是集中折腾 2 个星期，以后就完事。何时开始，何时结束 曾经有过几次这样的经历：在一个专门的重构分支，几个人大搞特搞，折腾两三个星期甚至更久，把能跑的系统改的面目全非，然后和并行的业务一合并就各种问题，最后经历痛苦的合并修复、甚至回滚。这种惨痛的重构经历，始于系统积累了过多问题，又想一步到位都解决，一旦开干又停不下来，同时业务又在原有未重构代码的分支上持续迭代，业务分支和重构分支差异巨大。好的重构实践，随时都能开始，随时都能结束。重构是持续进展的行为，在每一次业务迭代都可以进行：消除代码的 bad smell，修正之前仓促设计的不足，修复某处潜在的性能隐患……每次迭代花点时间小步子快速重构，随时都能开始，随时都能结束，才能够最大限度使得重构和业务迭代保持一致的步伐，让重构为快速响应业务发展赋能。抓大放小，量化结果 作为技术 leader、业务 leader，看到这样大方向的重构效果，内心是爽爽的：页面载入时间从 xxx 缩短到 yyy接口响应时间从 xxx 下降到 yyy存储成本每个月节省 xxxapp 用电量减少 xxx以前做一个这样的功能要 xxx 天，现在只需要 yyy 天就搞定 相反，花了 1 个星期埋头重构，然后告诉老板“这段代码写的啰嗦，我精简了”、“这些命名不规范，全部 refactor 了”、“这个模块设计耦合度高，我改成低耦合了”……通常会被老板和业务喷死。这些小的重构优化，是日常做的事情，是《搬砖工的自我修养》。换位思考，集中式的重构，会占用原本做业务开发的时间。技术 leader、业务 leader 都有压力，需要数据表明投入大量时间做这个重构是值得的。因此，这个阶段的重构要抓大放小，解决核心的问题，并且量化结果。业务驱动 在个位数 DAU 的时候做 1000W DAU 系统的改造。在用户行为稀少，连模型都跑不起来的时候做实时推荐改造。……诚然，这些技术问题有难度，做出来成就感很强。但是业务并不买单，做的事情在对方看来对现在业务发展没有帮助。到头来只是感动了自己，别人不买账。更尴尬的是，业务量真正上来了，之前假想情况下折腾的架构重构却不管用。业务驱动是这种复杂重构最好的动力。why not workingMartin Fowled 在《重构》这本书给出了“重构”的定义：重构（名词）：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本。重构（动词）：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构。重点是 在不改变软件可观察行为的前提下 进行重构。但现实经常是改着改着系统就跑不动了，尤其是重构经验不足的同学。直接修改旧的代码 这是最常见到的危险操作。直接在旧的可以运行的代码上做修改重构，这时候的代码没有经过充分验证，属于不稳定、甚至不可用状态。如果流程不规范，这样的代码被部署到线上，就有可能出故障。更好的实践是，旧的代码不要改变，新增一个方法、类、模块、服务等来承载重构后的代码。然后逐步小批量切换验证，直到最后全部都切换到新代码，线上稳定后再清除旧代码。关于测试 怎么知道重构后的代码是否 改变软件可观察行为 ？ 当然是测试验证，尤其是单元测试。现实是，业务驱动的技术团队，单元测试从来是可遇而不可求。根据我之前的经验，设计和实现单元测试的时间，是单纯开发功能的 1 到 3 倍。告诉业务要写单元测试，而不能开发新功能，那是不可能的事情。 没有单元测试的重构是危险的，更加危险的是回归测试也没有的重构。通知测试，改过代码，需要回归验证，不要过于自信给自己挖坑。]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sd 项目重构实践]]></title>
    <url>%2Fp%2Fsd-project-refactor-case%2F</url>
    <content type="text"><![CDATA[项目从快速原型发展到堆积完基础功能、有一定用户基础。开始还技术的债。重构的价值在这个阶段开始体现。服务拆分重构 这个项目会是微服务架构模式，但是一开始就拆分出很多一堆服务，并不是很好的实践：项目启动初期，业务方向、业务形态变化大，产品需要的快速原型、快速试错。3 个人拆 20 个微服务，没有实际的价值，只会增加协议变更成本、部署成本，不利于业务的快速试错。因此，项目初期，只拆分出 2 个大粒度的核心服务：房间服务和推荐服务。其中房间服务包含个人练歌、唱歌 pk 游戏房间、Channel 鉴权服务。随着几个月的业务迭代，原来个人练歌功能业务地位提升，代码也越来越复杂。唱歌 pk 房间的玩法也从单一的小房间轮唱模式，发展为抢唱、轮唱、小房间、大房间。这时候做服务拆分重构：个人练歌服务 小房间服务 大房间服务 Channel 鉴权服务 这里有个技巧。个人服务和鉴权服务，一开始就被识别为有可能晋升为单独服务部署和管理，因此作为单独一个 domain 处理，相关的代码在同一个 package。到了要做服务拆分就直接整个 package 拷贝出来再修改。类似：12345com.xxx.sd.solo.servicecom.xxx.sd.solo.controllercom.xxx.sd.solo.repositorycom.xxx.sd.solo.domaincom.xxx.sd.solo.util领域模型优化 领域模型是随需求迭代而不断进化的。最大变化是歌曲片段。最初模型参照竞品修改得来，考虑到 schema 可能频繁变更，底层采用 ElasticSearch 存储。业务迭代几个月过后，一堆透传字段、无效字段、非必要字段，导致结构体迅速膨胀，配置和解析踩过几次坑导致服务不可用。于是进行梳理：过时无效字段，删除 命名不规范、有歧义的字段，先增加新的规范字段，原有服务切换到新字段并且通过验证后，再删除旧字段。调整节点结构，重新抽象公共属性。代码重构 剩下就是体力活时间，举几个例子。工厂模式 最初只有小房间模式、轮唱玩法（对应 game_type 字段），房间状态机服务直接根据上下文闭包构造引擎。后来分别增加抢唱玩法、大房间模式，原来的调用方自己判断要 game_type 字端选择状态机引擎，显然不合适。12345678910111213141516171819202122@Servicepublic class RoomStateEngineFactory implements IStateMachine&#123; @Autowired private InTurnRoomStateEngine inTurnRoomStateEngine; @Autowired private RaceRoomStateEngine raceRoomStateEngine; public IStateMachine newInstance(GameTypeEnum gameType, RoomContext roomContext)&#123; if(gameType == GameType.IN_TURN)&#123; return inTurnRoomStateEngine.newInstance(roomContext); &#125; if(gameType == GameType.RACE)&#123; return raceRoomStateEngine.newInstance(roomContext); &#125; throw new IllegalArgumentException("unknown gameType="+gameType); &#125; // 其他方法 &#125; 不同 game_type 的状态机实现逻辑，由不同的 IStateMachine 接口实例提供。RoomStateEngineFactory作为工厂类，封装选择 IStateMachine 逻辑，调用方无需理解细节。后续增加新的玩法（game_type），已有的调用方不需要改动。策略模式和 if…else…延长播放时间有多种策略可以选择，随机增加、等值增加、区间增加等，有配置项控制。123456789if("random".equals(type))&#123; // &#125;else if("interval".equals(type))&#123; //&#125;else if("range".equals(type))&#123; //&#125;else &#123; // unknown config, throws error&#125;未来还想增加，于是重构为 12345678910111213141516171819202122232425262728293031323334353637383940public interface IExtraTimeStrategy&#123; int calculate(int like);&#125;public class ExtraTimeStrategyProcessor implements IExtraTimeStrategy, ApplicationContextAware &#123; @Value("extraTime.strategy") private String strategy; // Spring 应用上下文环境 private ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; public int calculate(int like)&#123; IExtraTimeStrategy s = applicationContext.getBean("strategy", IExtraTimeStrategy.class); if(s==null)&#123; throw new IllegalArgumentException("unkown strategy="+strategy); &#125; return s.calculate(like); &#125;&#125;// 具体的延长时间策略实现类@Service("byRandom")public class RandomExtraTimeStrategy implements IExtraTimeStrategy&#123; // more code&#125;@Service("byInterval")public class IntervalExtraTimeStrategy implements IExtraTimeStrategy&#123; // more code&#125;@Service("byRange")public class RangeExtraTimeStrategy implements IExtraTimeStrategy&#123; // more code&#125; 为了新增策略不修改代码，直接向 ExtraTimeStrategyProcessor 注入 ApplicationContext，根据配置项获取对应策略的 bean。 方法太长 善用 idea 的 Extract Method 功能。修改缓存 key有的缓存已经不再使用，或者需要重命名。应用使用的缓存 key 名，都被封装到 RedisKeyHelper，并且对外提供getXXXKey([param])。缓存 key 重构就只需要更改单一入口。 以前踩过的坑是，同一个缓存 key 没有集中管理的地方，调用方自己拼接，修改的时候出故障。因此项目一开始就强制规定 RedisKeyHelper 管理所有缓存 key。api 接口字段变更 api 中旧的字段在新版本废弃。对应字段加上@Deprecated 注解，等待 3 到 4 星期客户端流量下降到足够低之后，再从代码中删除。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>java</tag>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次 young gc 频繁的优化过程]]></title>
    <url>%2Fp%2Fsd-project-frequently-young-gc%2F</url>
    <content type="text"><![CDATA[这次分享 SD 项目压测过程中遇到的频繁 young gc 问题。“高并发的性能优化”的往期文章 SD 项目：高并发的性能优化，part 1SD 项目：高并发的性能优化，part 2SD 项目：高并发的性能优化，part 3 现状 压测的时候，经常收到 gc 次数告警。jvm young gc 频繁，一分钟大概 20 次。单次 young gc 耗时约 40ms。Full gc 很少发生。使用 java8，使用 CMS 收集器。分析 登入容器，每 1s 打印应用的 jvm 使用情况（百分比），原来的截图没有保留，以下是示意图 123# jstat -gcutil &lt;pid&gt; 1s S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 0.00 71.73 29.58 95.52 93.03 11 0.114 2 0.273 0.387 观察一段时间，发现 E（eden）区消耗很快。O（old）区基本不变（记得是 20 到 21）。表明当前新生代的对象绝大部分生存周期都很短，经历 young gc 就被回收掉。跟内存分配的 jvm 参数，只配置了 1-Xms4096m -Xmx4096m 没有指定新生代（-Xmn 或者 -XX:NewRatio 或者 -XX:NewSize）。按照，那么默认的新生代大小大约是 4G*(1/(1+2))=1.37G。Eden 区的大小是1.37G*0.8=1.096G。 真的是这样吗？12345678910111213# java -server -Xmx4096m -Xms4096m -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -versionjava version &quot;1.8.0_112&quot;Java(TM) SE Runtime Environment (build 1.8.0_112-b15)Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)Heap par new generation total 613440K, used 32721K [0x00000006c0000000, 0x00000006e9990000, 0x00000006e9990000) eden space 545344K, 6% used [0x00000006c0000000, 0x00000006c1ff4420, 0x00000006e1490000) from space 68096K, 0% used [0x00000006e1490000, 0x00000006e1490000, 0x00000006e5710000) to space 68096K, 0% used [0x00000006e5710000, 0x00000006e5710000, 0x00000006e9990000) concurrent mark-sweep generation total 3512768K, used 0K [0x00000006e9990000, 0x00000007c0000000, 0x00000007c0000000) Metaspace used 2388K, capacity 4480K, committed 4480K, reserved 1056768K class space used 261K, capacity 384K, committed 384K, reserved 1048576K实际的 eden 只有 545344K，跟原先估算的 1.096G，相差将近 1 半！在这篇文章找到答案：CMS GC 默认新生代是多大?。解决 手动调整新生代内存大小 1-Xmn1500m-Xmn=1500m 效果等价于 -XX:NewSize=1500m -XX:MaxNewSize=1500m。 再次压测，1 分钟 young gc 次数变为 5-6 次，单次 young gc 耗时约 42ms。more on young gc为什么新生代扩大为原来的 3 倍，但是单次 gc 时间却增加很少呢？新生代通常使用 copy 算法，消耗时间 =t(扫描 eden)+t(扫描 old)+t(复制到 survivor)。 对于 jvm 来说，扫描速度很快，耗时大的操作是复制对象。扩大 eden，触发 young gc 的间隔变长，对于短生命周期的对象，在更长的时间维度上，可能已经不存活，省去了复制步骤，从而节省时间。所以 young gc 耗时和新生代大小不是线性关系。思考 纸上得来终觉浅，绝知此事要躬行。搬砖要多动手才容易发现坑。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>故障案例</tag>
        <tag>jvm</tag>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目：高并发的性能优化，part 3]]></title>
    <url>%2Fp%2Fperformance-tuning-sd-project-part-three%2F</url>
    <content type="text"><![CDATA[总结 SD 项目的性能优化过程和思考，part 3。往期文章 SD 项目：高并发的性能优化，part 1SD 项目：高并发的性能优化，part 2 启动负载优化 每次更新重启应用后压测，cpu load 较高，RT 较大，随后下降并稳定下来。分析 &amp; 解决 这是一个典型的问题，java 应用重启后一段时间，如果请求 tps 很高，那么 load 会很高。一般的 JVM 实现，会采用解释器 + 编译器编译方式来提高性能。解释器负责翻译 java 字节码，运行性能一般。编译器则会把 热点 代码翻译成 native code，运行性能高，但是会产生编译时间开销。JVM 根据应用 runtime 的采样统计，识别热点代码，并且进行编译优化。热点代码发现后，由编译器线程负责编译，和应用线程竞争使用 cpu 资源。编译器线程数由 -XX:CICompilerCount。查看 jvm 启动参数，发现被写死了-XX:CICompilerCount=2，和容器 cpu 配置相比，明显少了。 数据对比 更新 CICompilerCount，重启后压测。短时间 cpu load 更高，但是比之前更快的下降。 扩展 jvm 编译器分为 C1 和 C2。C1 的编译速度快，C2 的编译速度慢，但是机器码性能比 C1 好。java 还有分层编译技术TieredCompilation，在编译速度和机器码性能之间权衡：Level 0 – interpreted codeLevel 1 – simple C1 compiled code (with no profiling)Level 2 – limited C1 compiled code (with light profiling)Level 3 – full C1 compiled code (with full profiling)Level 4 – C2 compiled code (uses profile data from the previous steps)java8 以后默认开启-XX:+TieredCompilation。 房间服务拆分 大小房间共用 mq topic、数据库、缓存等资源。测试典型场景下大房间被小房间影响的程度。优化前 1 个大房间 + 2000 个小房间同时运行，观察大房间轮转时间，偶尔有 1s 到 2s 的卡顿。 分析 &amp; 解决 正常场景，高峰期是一个大房间，加上很多个小房间同时运行。大房间人数多，发生卡顿延迟会影响大量用户的体验，后果比小房间严重多。一个房间只要在轮转，那么就会产生 1 条延迟 mq 消息，延迟队列本身会有轻微堆积。另外还要考虑每条消息的消费耗时，大概在 10-100ms。增加消费者实例、消费者线程是提高消费速度的一个方式。但是更直接的方式是拆分 topic，避免堆积。至于数据库、缓存资源，目前 tps 足够，暂时不考虑拆分。数据对比 拆分 topic 后，1 个大房间 + 2000 个小房间同时运行，大房间没有卡顿。思考 关键业务单独拆分资源，保证可用性。后续对房间服务进行重构，拆分成大房间、小房间服务，单独演化。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目：高并发的性能优化，part 2]]></title>
    <url>%2Fp%2Fperformance-tuning-sd-project-part-two%2F</url>
    <content type="text"><![CDATA[总结 SD 项目的性能优化过程和思考，part 2。往期文章 SD 项目：高并发的性能优化，part 1 加入游戏房间 用户加入某个游戏房间，成功后会向房间内所有人发送信令通知，“xxx 加入房间”。目前有 2 种模式的房间，一是小房间（10 人以下），二是大房间（up to 1000 人）。优化前 单压：10000+ 并发加入 不同的小房间 ，RT &lt; 400ms。1000+ 并发加入 同一个大房间 ，RT &gt; 10000+ms。 客户端显示卡顿、甚至崩溃 。测量出客户端能够正常处理的、最大并发加入信令大概在 50 左右。redis tps 告警。 分析 &amp; 解决 房间允许人数，是一个房间的最大并发度。客户端收到加入房间成功的信令后，要更新、渲染多个组件。现有服务器端逻辑是，每个成功加入都下发一个单独的信令。1000+ 并发情况下，客户端瞬间被信令轰炸，进行组件渲染，消耗大量的 cpu、内存资源，引发了卡顿、崩溃。解决的思路是，减少下发信令的个数。交互协议升级，支持一次返回多个成功加入房间的用户信息。延迟下发。相同的房间 id，合并一秒内的成功加入请求，再统一下发。加入房间的请求会分散到不同的服务器端实例。合并请求的时候要考虑高可用问题，其中一个解法是：以 room_id 作为 key，获取 redis lock，锁定一秒 如果获取成功，则发送一条延迟 mq，时间为一秒 另外，考虑保护 db 资源，给 join 接口限流，分为单个房间（其实就是处理大房间）和整体接口的级别 。 通过减少信令下发，RT 也降下来，redis 告警消失。至于 redis 告警的深层次原因，会在后面讨论。数据对比 优化后：1000+ 并发加入大房间，客户端每 1s 只接受一条加入信令，不会崩溃。TP99 &lt; 200ms。redis tps 告警消失。思考 量变到质变。原来只有小房间模式，因此单个房间不会产生高并发。客户端可以顺利处理每条信令。但是随着大房间模式的出现，情况就不同了。架构设计也随之升级。好的测试方案不但测试单个接口，还要验证整个链路。高并发下要考虑对敏感资源的保护，避免单个接口搞垮系统。房间内点赞功能 对正在演唱的人点赞:一轮有 1 次给他人点赞机会 点赞数每到达一定阈值，延长当前的表演时间，最多延长 X 秒 房间内收到的点赞数会同步到个人页的点赞计数（通过 mq 异步消费）向客户端发送 xx 收到点赞信令 优化前 单压：2000+ tps，点赞接口 RT &gt; 10000ms。客户端卡顿、崩溃。状态机轮转卡顿。redis tps 告警 。 解决：客户端崩溃 同最初的加入房间一样，服务器端处理一个点赞后立即发送信令，导致客户端被轰炸。但是不能直接使用延迟 mq 消息简单解决，因为“点赞数到达一定阈值，延长当前的表演时间”。为了减少下发的信令数量，并且尽可能兼容中间丢失信令的情况，这个延长时间标记是挂在点赞信令一起下发的，对应的结构体示意：1234&#123;"like": 23, // 当前点赞数 "extra": 10 // 总的延长表演时间&#125; 如果一刀切所有点赞信令延迟 1s 合并发送，在边界条件下，会导致延长时间标记不能及时发送，表演时间少了。解决方法 每次触发延长时间，就立即发送该点赞信令。否则延迟 1s 发送。效果 消灭了信令轰炸。客户端不崩溃了。redis tps 告警会在后面讨论。解决：状态机问题 状态机轮转依靠 mq 消息触发。如果产生消息堆积，那么状态机就会卡顿。在 RocketMQ 控制台确认有大量消息堆积，发现是点赞消息。点赞的伪代码 123456789// 前置检查// 同步点赞数到个人页。发送 mq，消费者异步消费，存储到 ESIncrLikeMsg msg = new IncrLikeMsg();msg.setUserId("xxx");msg.setCount(1);mqService.sendIncrLikeMsg(msg);// 其他发送信令相关 问题在于发送点赞消息的 topic，跟状态机轮转是同一个 topic！为什么两个不同类型的消息，使用同一个 topic？因为最初不想申请太多的 topic，直接用同一个 topic，业务在消息体定义 event 字段区分。高并发下对同一个人点赞，消费者落盘到 ES 慢，造成大量点赞 msg 堆积，影响了状态机轮转 msg 的消费。解决：为点赞消息申请一个新的 topic。数据对比 优化后：2000+ tps，TP99 &lt; 10ms。redis tps 告警消失 。 思考 topic 使用要规范，切记偷懒。 发送信令接口 上面 2 个接口都有一个共同情况，大房间 + 高并发请求，如果瞬间发送大量信令，RT 很大，并且 redis 告警。使用 arthas trace 命令，很容易就定位了发送信令接口存在性能问题。发送信令接口，首先拼装接收人列表，伪代码如下 123456789101112List&lt;String&gt; userIdList = roomService.getAllUserIdByRoomId(roomId); // redisList&lt;ParticipantVo&gt; participantList = new ArrayList();for (String userId : userIdList)&#123; UserInfo userInfo = userService.getUserInfo(userId) // redis Long like = redisService.get(getUserLikeKey(userId)); // redis String benifitTag = redisService.get(getUserBenifitKey(userId)); // redis String visitorTag = redisService.get(getVisitorKey(roomId, userId)); // redis // more code on assembly participantList &#125;// sort participantList by like count desc// more other codes 大量依赖 redis。对于 1000 人的大房间，1000 tps 请求，对 redis 的 tps 压力大概是 1000 * 1000 * 3 = 3000000。redis 不跪才怪。 点赞数 like只能从缓存获取。但是用 sorted set 保存点赞数，memeber 是 userId，score 是点赞数。直接 zrevrangebyscorewithscores 一次性读取。1000 个 member 的 key 算是大 key，后续有必要再 hash 拆分为多个，但是 sorted set 的特性就消失了。从 sorted set 获取点赞数后，还可以省略最后对 participantList 排序。用户权益 benifitTag用户获得成就之后，得到一个权益标记，有效期一星期，做特殊展示效果。因为是一星期有效，并且人数有限，使用 vm 缓存 + 过期时间 可以解决，不需要每次从 redis 读取。游客标记 vistorTag只有小房间才有游客标记概念。大房间不需要读取这个标记。直接省略。用户信息 UserInfoUserInfo 包含昵称、头像、设备信息。由用户服务维护，底层用 redis 缓存。用户信息更新不频繁，可以使用 jvm 缓存，使用 guava cache 保存，LRU，目前保留 X 个，有效期 1 小时。本地 jvm 缓存预热的思考 因为请求分散到 N 个容器，缓存预热发生在 N 个容器。一个用户要经历 N 个请求之后，才有可在能所有容器预热用户信息。即使不是所有容器预热，正常用户的操作，也能够预热一部分容器，从而减少 redis 压力。如果容器重启了，jvm 缓存就会丢失，目前避免在业务高峰尤其是大房间开始前重启就可以了。数据对比 优化后，单压发送信令接口，1000 人大房间，1000+ tps，TP99 &lt; 10ms。redis tps 告警消失。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目：高并发的性能优化，part 1]]></title>
    <url>%2Fp%2Fperformance-tuning-sd-project-part-one%2F</url>
    <content type="text"><![CDATA[总结 SD 项目的性能优化过程和思考，part 1。背景说明 使用预发布环境进行压测，只部署一个容器，容器配置和生产环境一致。压测包含单压（同一时间只压测一个接口）和混压（同一时间压测多个接口，模拟用户真实使用操作）。压测对象是房间服务。首页在线人数 打开 app，首页右上角显示当前在线人数“xxx 人在等你”。优化前 在线人数由一个 redis key 存储。后端读取 redis 直接返回。在高并发的情况下，就是典型的热点 redis key 问题，直接拖慢这个 key 所在的 redis 实例。分析 &amp; 解决 redis 集群出现热点 key，同一个 key 被路由到单个分片，导致该分片的 cpu、网络被消耗，而且 redis 是单线程设计，会导致其他请求处理变慢。 对于数据一致性不高的场景，热点 key 容易解决，最简单的是增加本地 jvm 缓存，再由定时任务更新数据。也可以使用 guava cache 来实现。数据对比 优化前：5w 并发，RT 180+ms优化后：TP99 &lt; 1ms。成功率 100%。思考 为什么要显示实时的在线人数呢？这是一开始我就 argue 的地方，这是赤果果地向竞品暴露我方产品情况。。。ps. 后续产品展示终于改为区间显示，99+，999+，9999+ 等。日志堆栈丢失 优化前 后端日志通常是这样输出，包含异常堆栈 1logger.error("error:", e); 但是，混合压测（后续简称“混压”）的时候，日志文件开始出现大量异常，却没有调用堆栈，不知道报错的地方。123java.lang.NullPointerExceptionjava.lang.IllegalArgumentException分析 &amp; 解决 dump stack trace 是一个耗时操作。jvm 默认做了性能优化，对于反复抛出的异常，生成一个不包含 stack trace 的异常返回。 对应的开关参数是 OmitStackTraceInFastThrow，默认开启。在测试环境把这个特性关闭：1-XX:-OmitStackTraceInFastThrow 参见 Release NoteThe compiler in the server VM now provides correct stack backtraces for all “cold” built-in exceptions. For performance purposes, when such an exception is thrown a few times, the method may be recompiled. After recompilation, the compiler may choose a faster tactic using preallocated exceptions that do not provide a stack trace. To disable completely the use of preallocated exceptions, use this new flag: -XX:-OmitStackTraceInFastThrow.之后堆栈都正常打印。fix 了几个边界情况的 NPE、类型判断的问题。思考 真是活久见。平时遇到 OmitStackTraceInFastThrow 的情况比较少，想了一会才想起这个参数。平常的知识储备还是很重要的。首页推荐歌曲片段列表 展示歌曲列表，用户可以试唱。优化前 1W 并发，直接跪了，RT &gt; 10000ms。 分析 &amp; 解决 流程：从推荐服务捞取歌曲片段 id 列表 从 ES 获取歌曲片段元数据，领唱人数据等 从用户服务获取领唱人头像、昵称 从交互服务获取点赞、收藏信息 组装下发 一个接口包含多个服务调用，要看瓶颈在哪里，最直接的方式是每个调用前、后加上打点代码，打印调用时间。不过有了 arthas，一切就简单多了。1# trace -n 3 com.xxx.SoloService recommendAlbum &apos;#cost &gt; 1000&apos;参数的含义:-n: 跟踪次数 com.xxx.SoloService: 包名 + 类名recommendAlbum: 方法名‘#cost &gt; 1000’ : 过滤表达式，只跟踪 cost 大于 1000ms 的请求 完整的调用栈很长，当时也没有留下截图，这里粘贴 arthas 官网的例子输出 12345`---ts=2018-12-04 01:12:02;thread_name=main;id=1;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@3d4eac69 `---[12.033735ms] demo.MathGame:run() +---[0.006783ms] java.util.Random:nextInt() +---[11.852594ms] demo.MathGame:primeFactors() `---[0.05447ms] demo.MathGame:print() 具体使用参见 arthas 的 trace 文档 tips:trace 每次只能跟踪一级方法的调用链路。 记得加上 -n 参数，限制跟踪次数，否则压测场景下很容易 OOM。有了 arthas 这个好使的工具，剩下的就是体力活了。推荐服务 首先跪的，RT &gt; 10000ms。增加 redis，避免直接从 ES 拉取计算数据。优化效果：推荐 RT 50-200ms。接着跪的是查询歌曲元数据。耗时集中在等待 ES 返回。1234// segment 是片段 for(String segmentId : segmentIdList)&#123; // read from ES&#125;step1，片段数据变更不频繁，适合放入缓存，以片段 id 作为 key，expire 1800s。 片段 id 比较充分打散，读写压力均摊到 redis cluster 多个分片，暂时不考虑 jvm 缓存。step2，for 循环串行查询片段数据，改为 java8 的 parallelStream 并发查询。优化效果：全部命中缓存 RT &lt; 10ms，全部不命中缓存 RT &lt; 500ms。用户服务 原先每个片段单独查询用户信息，改为一次批量查询。优化效果：优化前 RT 30-100ms，优化后 RT 5-10ms。交互服务 优化前每个片段单独查询一次点赞状态，改为新增一个批量查询接口。优化效果：优化前总交互服务查询耗时 1000+ms，优化后 50-100ms。数据对比 优化后：1W 并发查询，TP90 &lt; 200ms, TP99 &lt; 1000ms。思考 从领域模型角度，个人推荐歌曲片段功能，不应该划分在房间服务，后续要单独拆分。复杂接口的性能优化套路：使用 arthas trace，找到核心调用链的耗时分布，确定优化的对象。增加缓存、多个单次查询变为一次批量查询、多个单次查询改为并发查询等手段。推荐在线用户 从在线用户列表，选出一批推荐用户。优化前 3W 并发，RT 5000+ms 分析 &amp; 解决 优化思路同“首页推荐歌曲片段列表”，在此不再重复。数据对比 优化后：1W 并发，TP99 &lt; 100ms。房间列表接口 显示当前有效的 N 个游戏房间，返回房间状态、房主信息、人数、歌单等。这个接口的逻辑流程：从 room_info 表过滤房间状态、创建时间，得到房间 id 列表 从 participant 表得到房间人数 调用其他接口获取房主的个人信息 直接落盘查询数据库，在高并发下是肯定会直接跪了，因此 v1.0 的实现是带有缓存，更新时间 15-30s，由后台任务刷新。但是产品认为体验不好，创建房间怎么没有立即显示，blablabla，被迫改先为直接查询数据库（v1.5）。由于对查询条件建立相关索引，因此也不会慢到哪里去，可以撑一段时间。v2.0 设计思路是房间列表直接从缓存数据构建，从新建房间、销毁房间、加入房间、退出房间、房间状态变化等等，全部直接操作缓存，牵涉改造点多（10+ 接口），暂时没有排上日程。优化前 单压：1W 并发查询房间列表，RT 3000+ms。混压：1W 并发查询房间列表 + 1000 并发创建房间，RT 15000+ms。触发数据库中间件慢查询警告。分析 &amp; 解决 从数据库控制台发现慢查询语句，伪代码如下 12select count(1) from participantwhere room_id = xxxparticipant 表根据 room_id 字段分库，并且建立索引。 但是单压查询性能去到 3000+ms，说不过去。第一感觉是没有索引，或者索引没有生效：在数据库检查，真的没有索引。。。接着排查代码 git 历史，有增加索引的 commit。最后定位是几个月前部署单的问题，导致线上没有索引，因此查询是 full table scan，并且当前分表数据 10W+ 行，慢是理所当然。sql 慢查询不仅影响单个接口性能，高并发情况下对系统整体危害很大：从 connection pool 获取一个 connection，执 sql，因为执行的慢，该 connection 没有立即返回，新的请求只能从连接池获取 / 创建 connection 来执行。高并发场景，重复 1)本地连接池打爆 因为是多个 app 实例，接着打爆数据库连接数 其他接口涉及访问 db 的接口也跪了 加上索引后，1W 并发查询，RT 从 3000+ms 降到 100ms。这时候再向产品 argue，就允许了 10s 的缓存。数据对比 优化后：（增加索引、定时任务和 10s jvm 缓存）1w 并发，TP99 &lt; 30ms。消灭了数据库的慢查询。思考 数据库部署单漏掉建立索引，这个失误有点低级。值得考虑的是，人工操作容易失误，也没有相应的部署后检查步骤，流程有提升空间 。trade off。 用户对一个列表数据的实时性有多敏感？产品的角度，理所当然觉得一切都是实时性就是最好。但是工程角度，要考时间限制（一个迭代有好多需求呢），方案复杂度，产出投入比（做了很多，但用户可能根本不 care）。 做架构设计和实现，要明白约束条件，权衡的利弊，产出投入比，将来升级要改动的地方 。 大量日志文件导致磁盘告警 压测半天过去，收到磁盘空间使用率 80% 的告警。优化前 容器分配了 50GB 磁盘，logs 已经使用了 40+GB。以当前并发量的磁盘消耗速率来计算，线上环境也就只撑不过 2 天（因为有多个容器分摊）。分析 &amp; 解决 首先，分析后端都写了哪些日志文件：access log。spring boot 内嵌的 tomcat 写的 access log。一个请求写入 &lt;1KB，方便排查问题，不需要找运维查询前置的 Nginx 查询访问日志，保留。ElasticSearch 操作日志。占用了将近 30GB，每次查询都把返回 body 打印，相当大。application 日志。包括接收 mq 日志、房间轮转日志、信令发送日志、用户操作日志等，占用了 6+ GB。error 日志。主要是异常堆栈，占用了 5+GB。分析完毕，先手动清理了日志文件，防止写爆磁盘。接下来逐个处理。ElasticSearch 操作日志 显然大头在 ElasticSearch 操作日志。最直接是修改日志级别，只在 debug 级别才输出 ES 日志。但是有另一个权衡点，ES 的底层数据主要由爬虫系统、运营管理系统写入和修改，房间服务只是做消费。目前系统还在快速迭代，发生过写入个别脏数据导致房间消费失败。为了避免扯皮，还是尽量保留每个请求的响应数据。思考：各个服务直接对接存储层，并且是 schema free，没少踩坑。如果抽取歌单服务，修改限制在一个服务，那么会有改善。但是目前人力不足，暂时不考虑。于是采取折中方案，依然保留 ES 的响应输出，但是修改 loggback 配置 12345678910111213&lt;appender name="ROLLING" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;es.txt&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!-- rollover daily --&gt; &lt;fileNamePattern&gt;es-%d&#123;yyyy-MM-dd&#125;.%i.txt.gz&lt;/fileNamePattern&gt; &lt;!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB --&gt; &lt;maxFileSize&gt;200MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;4&lt;/maxHistory&gt; &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;!-- 其余配置省略 --&gt;&lt;/appender&gt;logback 是支持文件压缩的，fileNamePattern 增加 gz 后缀就可以自动压缩为 gz 文件。对于纯文本压缩，通常可以达到 10:1 或者更高的压缩比例。只保留最近 4 天的日志。当然，这样的缺点是排查问题可能要先解压缩日志文件，不过问题不大。application 日志 application 日志比较杂乱，多种类型日志穿插，输出混乱，正好做梳理和拆分。 发送信令日志 signal，记录了发送什么信令、向哪些客户端发送、是否成功，是排查”服务器端有无正常发出信令”、”客户端有无收到信令”这 2 个核心问题的基础。不能省去。直接用另一个 logger 配置写到其他文件，加上压缩和滚动配置。mq 接收日志。接收后打印消息体。由于房间的核心是轮转由 mq 消息触发，mq 日志能够排查房间不轮转问题。拆分出来。房间服务日志 room。这是核心部分。存在问题：之前迭代输出调试日志，已经没用，可以删掉 啰嗦的、不能帮助排查问题的无效日志信息，删掉 规范化 INFO 日志内容。统一房间服务的上下文 RoomContext 输出的字段，方便 grep 查询 error 日志 首先，我们封装了业务相关的 BizException。很多 BizException 不需要堆栈，只需要当前上下文信息就可以。举个例子，用户加入房间失败。只需要输出一行日志就足够排查问题 1userId=&#123;&#125;,action=JOIN,roomId=&#123;&#125;,currentRound=&#123;&#125;,currentTurn=&#123;&#125;,reason=ROOM_FULL 如果打印完整堆栈，通常有 20 到 50 行。数据对比 优化之后，在相同压测压力之下，每小时消耗磁盘空间节省 80-90% 左右。application 日志输出更加清晰。error 日志减少无用堆栈，空间节省 90-95%。思考 后续日志优化 目前 mq 日志、房间操作日志、房间状态机日志、房间信令日志分散，可以考虑根据 roomId 进行日志归集优化。结合 Diamond 做动态日志级别调整。另外，对线上环境配置做了优化：默认磁盘空间告警水位线 80%，比较高了，调整到 60%。提前扩展线上磁盘容量。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SD 项目：使用 arthas 排查问题的经历]]></title>
    <url>%2Fp%2Farthas-debug-case%2F</url>
    <content type="text"><![CDATA[业务背景简介 核心的房间语音游戏业务，使用事件驱动 + 状态机轮转设计。一个 event 包含房间号、当前轮次、当前游戏状态、期望转入状态等 metadata。使用 RocketMQ 作为事件源，业务收到 mq 消息后，根据 metadata 去初始化状态机对象，再由状态机去执行业务逻辑。问题描述 测试人员反映，test 环境的房间服务偶发性卡顿，但是等待一会，又自动恢复，状态机继续轮转。半天内发生多次，但是不能稳定复现。故障排查思路 状态机不能轮转，最有可能出问题的地方:没有收到 mq 消息 收到 mq 消息，但是业务 action 处理很久 排查 RocketMQtest 环境的 RocketMQ 资源是共享的，偶尔会抽风，先排查 RMQ 的问题。对于第 1 点，很好排查。在 mq 控制台，确认这段时间没有消息堆积。消息是正常下发的。收到 mq 消息，都会打印一条日志 1logger.info("receive msg=&#123;&#125;", msg); 根据 mq 的接收日志分析，消息正常收到，并且进行了处理。因此排除是 mq 中间件的问题。排查业务 action根据日志，有异常的环节是新一轮开始，包括初始化该轮游戏数据、下发歌曲、切换状态、注册超时事件等操作。都在 PlaySongAction 类处理，先缩小排查范围。另外，每次卡顿，大概 10 分钟就自动恢复 。PlaySongAction 的日志就只有一行1INFO [ConsumeMessageThread_x] PlaySongAction enter play_song[roomId=&#123;&#125;, currentRound=&#123;&#125;, currentTurn=&#123;&#125;, roomState=&#123;&#125;, singerId=&#123;&#125;] 不足以排查问题。PlaySongAction 类是 fali-safe 设计 123456try&#123; // business logic&#125;catch(Exception e)&#123; logger.error("error, RoomContext=[" + roomContext + "]," e) // more code for generic error handling&#125; 如果发生一般异常（除了是抛出 Throwable，概率很小），一定会捕获到，并且打印日志。但是 error 日志并没有。因此猜测是长时间执行，方法没有返回。方法执行时间长，常见的原因：有大量耗时运算 等待 IO 资源 等待锁 等待其他服务、中间件的返回 遇上 gc stw其他待补充 代码 review，没有复杂的计算，没有显式使用锁，没有大量写磁盘的操作。在监控看这段时间的 load、cpu、io 和平常基本一样，没有特别可疑。gc 问题可以直接不考虑。再把范围缩小，猜测是对于其他服务的访问慢造成的。PlaySongAction 访问了 mysql、redis、ElasticSearch 等中间件，还有其他服务提供的 http 接口，看来只能逐个排查。这时候测试同事说又出现卡顿情况。赶紧 jstack -l &lt;pid&gt; | grep ConsumeMessageThread_，可是没有看到 block 之类的提示。 因为不能稳定复现，机会难得，来不及增加 debug 日志，赶紧用 arthas 看看。使用 arthas 排查问题 根据 roomId 和 currentTurn，定位到发生卡顿的日志 1INFO [ConsumeMessageThread_3] PlaySongAction enter play_song[roomId=&#123;&#125;, currentRound=&#123;&#125;, currentTurn=&#123;&#125;, roomState=&#123;&#125;, singerId=&#123;&#125;]mq worker 线程 ConsumeMessageThread_3 接收了消息，并且进行处理。 使用 thread 命令找到线程 ConsumeMessageThread_3 对应的 thread ID123456789$ thread Threads Total: 32, NEW: 0, RUNNABLE: 13, BLOCKED: 0, WAITING: 15, TIMED_WAITING: 4, TERMINATED: 0 ID NAME GROUP PRIORITY STATE %CPU TIME INTERRUPTED DAEMON 44 as-command-execute-daemon system 10 RUNNABLE 87 0:0 false true 41 nioEventLoopGroup-2-3 system 10 RUNNABLE 12 0:0 false false 32 AsyncAppender-Worker-arthas-cache.result.Async system 9 WAITING 0 0:0 false true 30 Attach Listener system 9 RUNNABLE 0 0:0 false true 11 Catalina-utility-1 main 1 TIMED_WAITING 0 0:3 false false // 以下省略 thread &lt;thread ID&gt; 看当前线程的调用栈 1234567891011121314151617181920212223242526272829$ thread 310"ConsumeMessageThread_3" Id=310 RUNNABLE (in native) at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:153) at org.apache.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:282) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:138) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56) at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259) at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163) at org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:165) at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108)// 此处打码 at ***************************.service.zsearch.ZSHttpClient.post(ZSHttpClient.java:131) // 以下省略 Number of locked synchronizers = 1 - java.util.concurrent.ThreadPoolExecutor$Worker@607eb62ZSHttpClient 是我们封装的服务，底层使用 httpclient，访问 ElasticSearch。这里要获取歌曲、专辑、以及人工运营的元数据，组装下发。 从调用栈看，线程等待 java.net.SocketInputStream.socketRead0(Native Method) 的返回。httpclient 底层使用 BIO 访问网络。如果没有返回，会一直等待。但是，祖传的 ZSHttpClient，不可能没有访问超时配置，test 环境 10 秒 read timeout 超时失败。 看起来是超时配置没有生效。google 一下，说是 socketRead0 的实现有可能导致超时设置无效 How to prevent hangs on SocketInputStream.socketRead0 in Java?。既然是访问 ElasticSearch 的问题，顺便问了负责运维的同事，这 2 天刚好在做迁移和扩容 ElasticSearch，确实有间歇性服务不稳定。梳理一下 ES 迁移和扩容，导致服务不稳定httpclient 使用 BIO 访问网络，socketRead0 的实现可能触发超时无效一直等待的 bug 好像说得过去哦。。。但是，socketRead0 的 case 应该 very rare，一个早上出现多次，说不过去 出问题的总是 ZSHttpClient.post()方法 ，ZSHttpClient.get() 方法没有报告异常，从概率来说，有点说不通 于是打开 ZSHttpClient.post()的代码，没有设置 SocketTimeout，但是 get()方法是有的。再次梳理 ES 迁移和扩容，导致服务不稳定ZSHttpClient.post() 没有设置 SocketTimeout，一直等待 ConsumeMessageThread_3 线程等待 ZSHttpClient.post() 返回 于是房间卡顿 这就合理多了。还有一个疑问，为什么固定 10 分钟后又会自动轮转呢 ？ 猜测是中间网络设备的空闲超时 reset，导致 socketRead0() 返回。于是去查找 ZSHttpClient.post()的 error 日志，果然在发生卡顿之后的 10min，有网络异常日志。ZSHttpClient 直接捕获了异常并且返回 null，上层调用方 PlaySongAction 继续执行，由于有 null 判断，导致 PlaySongAction 不会出错，但是下发的歌曲数据是有问题的（有日志验证）。故障过程还原 ES 迁移和扩容，导致服务不稳定ZSHttpClient.post() 没有设置 SocketTimeout，一直等待 ConsumeMessageThread_3 线程等待 ZSHttpClient.post() 返回，效果是线程 hang 住 于是房间卡顿 中间网络设备的空闲超时 reset，导致 socketRead0 返回。ZSHttpClient.post()捕捉了异常返回 null，PlaySongAction 继续执行，由于有 null 判断，不会报错，但是下发错误的歌曲信息，并且进入轮转 解决问题 fix ZSHttpClient.post()，增加超时配置12345678RequestConfig requestConfig = RequestConfig.custom() .setConnectionRequestTimeout(CONNECTION_TIMEOUT_MS) .setConnectTimeout(CONNECTION_TIMEOUT_MS) .setSocketTimeout(CONNECTION_TIMEOUT_MS) .build();HttpPost httpPost = new HttpPost(URL);httpPost.setConfig(requestConfig); 访问 ElasticSearch 的代码监控打点不完善，增加新打点。PlaySongAction 对于 null 的歌曲信息，增加一个日志打点。验证通过。思考 祖传代码分分钟有惊喜，要有怀疑精神。根据已有日志，先缩小范围，做出假设，收集证据，验证。对于不能稳定重现的情况，保留现场（thread dump，heap dump 等）很关键。arthas 查看线程 stack 很方便。如果不是使用 arthas，那么要逐个排查 PlaySongAction 访问的服务、中间件，相当费时费力。不要轻易拍结论，多反问几次。如果随便用 socketRead0()和 ES 迁移搪塞过去，就发现不了 ZSHttpClient.post()的问题了。]]></content>
      <categories>
        <category>SD项目</category>
      </categories>
      <tags>
        <tag>SD项目</tag>
        <tag>故障案例</tag>
        <tag>java</tag>
        <tag>性能调优</tag>
        <tag>arthas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码仓库，CI，项目管理的调研]]></title>
    <url>%2Fp%2Fproject-management-code-repo-and-ci%2F</url>
    <content type="text"><![CDATA[背景 几个人业余折腾项目，要考虑代码托管，自动化构建，任务管理问题。需求 免费私有仓库 项目管理，支持需求管理，关联任务，进度统计等 自动化构建和部署，从源码到云平台 尽量在一个平台搞定 约束 使用阿里云平台 ecs 等中间件 于是调研了国内常见的码云、coding、阿里云 code 等。码云 oschinagood支持阿里云 codepipeline，自动构建和部署 bad 项目管理功能看上去一般 界面一般 codinggood 界面好看 项目管理友好 bad 自动化构建和部署，不支持和阿里云集成（鹅厂旗下的。。。）。要自己搭建 jenkin 做 ci。阿里云 code + 阿里云云效 阿里云 code 是代码仓库。阿里云云效 是项目管理，持续集成，持续交付平台。good 支持阿里云构建（废话）项目管理就是 aone 的商业版，用习惯了（ui 毫无惊喜）企业成员（研发、测试）小于 30 可申请扶持计划：审核通过可 0 元享一站式研发套餐。云效收费 其他 GitHub 是不考虑的。速度慢，还有众所周知的风险。GitLab 也是不考虑的。一是协作的人少，也没有到达足够的商业保密需要；二是 GitLab 以前用过挺消耗资源，减少不必要的机器成本和运维成本。 结论 阿里云 code + 阿里云云效 符合我们的需求。（ps. @阿里云 可以加个鸡腿吗）]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu 软件包版本问题]]></title>
    <url>%2Fp%2Fubuntu-package-version%2F</url>
    <content type="text"><![CDATA[和朋友业余捣鼓项目，继续全 (zhe) 栈(teng)的感觉。在前东家申请了企业免费试用后，买买买之后就是安装软件。Ubuntu 的包管理机制安装软件通常比较方便。但是软件版本可能不是想要的。例如 Ubuntu 18.04 默认的 Nginx 包版本是 1.14，但是我想安装 1.16。查看软件包的版本 123456# apt-cache madison nginx nginx | 1.14.0-0ubuntu1.2 | http://mirrors.cloud.aliyuncs.com/ubuntu bionic-updates/main amd64 Packages nginx | 1.14.0-0ubuntu1.2 | http://mirrors.cloud.aliyuncs.com/ubuntu bionic-updates/main i386 Packages nginx | 1.14.0-0ubuntu1.2 | http://mirrors.cloud.aliyuncs.com/ubuntu bionic-security/main amd64 Packages nginx | 1.14.0-0ubuntu1.2 | http://mirrors.cloud.aliyuncs.com/ubuntu bionic-security/main i386 Packages// 后面省略 或者 1# apt-cache policy nginx 要安装其他版本的 Nginx，除了从官网下载源码构建，还可以切换软件包来源实现。完整步骤参考 Nginx 官网 nginx: Linux packages：设置 stable 的源（此时 1.16 是 stable version）：12echo "deb http://nginx.org/packages/ubuntu `lsb_release -cs` nginx" \ | sudo tee /etc/apt/sources.list.d/nginx.list把 nginx 的签名安装到 apt-key：1curl -fsSL https://nginx.org/keys/nginx_signing.key | sudo apt-key add -接着更新 apt 和安装 12sudo apt updatesudo apt install nginx 然而安装报错 1Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'xxx' doesn't support architecture'i386' 找到一篇资料 Skipping acquire of configured file ‘main/binary-i386/Packages’ as repository ‘xxx’ doesn’t support architecture ‘i386’，是因为安装包的 arch 问题，i386 对应 32bit，我的 ecs 是 64bit，强制设置为 amd64。1234567891011# cd /etc/apt/sources.list.d# ll total 24drwxr-xr-x 2 root root 4096 May 31 09:25 ./drwxr-xr-x 7 root root 4096 May 31 09:23 ../-rw-r--r-- 1 root root 64 May 31 09:25 nginx.list-rw-r--r-- 1 root root 110 May 31 09:03 nodesource.list# cat nginx.list deb http://nginx.org/packages/ubuntu bionic nginxnginx.list修改为 1deb [arch=amd64] http://nginx.org/packages/ubuntu bionic nginx 再次安装即可。这时再看看软件包版本1234567891011# apt-cache policy nginxnginx: Installed: 1.16.0-1~bionic Candidate: 1.16.0-1~bionic Version table: *** 1.16.0-1~bionic 500 500 http://nginx.org/packages/ubuntu bionic/nginx amd64 Packages 100 /var/lib/dpkg/status 1.14.2-1~bionic 500 500 http://nginx.org/packages/ubuntu bionic/nginx amd64 Packages// 后面省略]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
